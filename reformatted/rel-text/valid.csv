prompt,label
"LEFT id: NA
RIGHT id: 2211

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: including group-by in query optimization

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1849

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases on the web : technologies for federation architectures and case studies

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer
",n
"LEFT id: NA
RIGHT id: 1815

LEFT text: Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk. While compression can be used to decrease the size of the dataset, compressed data is notoriously difficult to index or access. In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the sequence is a numerical value. We show how to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Experiments on large, real world datasets (AT&T customer calling patterns) show that the proposed method achieves an average of less than 5% error in any data value after compressing to a mere 2.5% of the original space (i.e., a 40:1 compression ratio), with these numbers not very sensitive to dataset size. Experiments on aggregate queries achieved a 0.5% reconstruction error with a space requirement under 2%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficiently supporting ad hoc queries in large datasets of time sequences

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: flip korn , h. v. jagadish , christos faloutsos
",y
"LEFT id: NA
RIGHT id: 1153

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: heuristic and randomized optimization for the join ordering problem

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael steinbrunn , guido moerkotte , alfons kemper
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: The majority of data reduction techniques for approximate query processing (such as wavelets, histograms, kernels, and so on) are not usually applicable to categorical data. There has been something of a disconnect between research in this area and the reality of data-base data; much recent research has focused on approximate query processing over ordered or numerical attributes, but arguably the majority of database attributes are categorical: country, state, job_title, color, sex, department, and so on.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 583

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on prototypes of deductive database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: k. ramamohanarao
",n
"LEFT id: NA
RIGHT id: 485

LEFT text: Several DBMS vendors have implemented the ANSI standard SQL isolation levels for transaction processing. This has created a gap between database practice and textbook accounts of transaction processing which simply equate isolation with serializability. We extend the notion of conflict to cover lower isolation levels and we present improved characterisations of classes of schedules achieving these levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: diluting acid

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tim kempster , colin stirling , peter thanisch
",y
"LEFT id: NA
RIGHT id: 1035

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 705

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: gea : a toolkit for gene expression analysis

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jessica m. phan , raymond ng
",y
"LEFT id: NA
RIGHT id: 1557

LEFT text: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a structured approach for the definition of the semantics of active databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: piero fraternali , letizia tanca
",y
"LEFT id: NA
RIGHT id: 991

LEFT text: Various types of computer systems are used behind the scenes in many parts of the telecommunications network to ensure its efficient and trouble-free operation. These systems are large, complex, and expensive real-time computer systems that are mission critical, and contains a database engine as a critical component. These systems share some of common database issues with conventional applications, but they also exhibit rather unique characteristics that present challenging database issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database system for real-time event aggregation in telecommunication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , stephen blott , henry f. korth , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 743

LEFT text: Specifically, an initial glossary of temporal database concepts and a. test suite of temporal queries were distributed before the workshop. Both of these document*s were amended based on the analysis and critique of the workshop. A language design committee was constituted after the workshop to develop a consensus temporal query la,nguage extension to SQL-92; this design also benefited from the discussion at the workshop. This report documents the discussions and consensus reached at the workshop. The report. reflects the conclusions rea.ched at the workshop in June, 1993 and further discussions amongst the group participants through electronic mail.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on building an infrastructure for mobile and wireless systems : report on the nsf workshop on an infrastructure for mobile and wireless systems , oct. 15 , 2001

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: birgitta k &#246; nig-ries , kia makki , sam makki , charles perkins , niki pissinou , peter reiher , peter scheuermann , jari veijalainen , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 445

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and cost-effective techniques for browsing and indexing large video databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghwan oh , kien a. hua
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: What is English Query? Microsoft English Query (EQ) lets users pose database queries in plain English. To do this, a developer need only define the database semantics, in effect building a conceptual model of the database. EQ provides an Authoring Tool that allows the developer to define the set of the entities and relationships in the database along with the database objects that the entities are associated with. Once this model is defined, the English Query Engine converts any English question posed in terms of the defined entities and relationships into a SQL statement. The application developer may use this engine, specifically it’s a COM automation server, inside her Web, C++, Java, or Visual Basic application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 120

LEFT text: In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic service matchmaking among agents in open information environments

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: katia sycara , matthias klusch , seth widoff , jianguo lu
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Building such a database system requires fundamental changes in the architecture of the query processing engine; we present the system-level interfaces of PREDATOR that support E-ADTs, and describe the internal design details.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1336

LEFT text: Most modern DBMS optimizers rely upon a cost model to choose the best query execution plan (QEP) for any given query. Cost estimates are heavily dependent upon the optimizer’s estimates for the number of rows that will result at each step of the QEP for complex queries involving many predicates and/or operations. These estimates rely upon statistics on the database and modeling assumptions that may or may not be true for a given database. In this paper we introduce LEO, DB2's LEarning Optimizer, as a comprehensive way to repair incorrect statistics and cardinality estimates of a query execution plan. By monitoring previously executed queries, LEO compares the optimizer’s estimates with actuals at each step in a QEP, and computes adjustments to cost estimates and statistics that may be used during future query optimizations. This analysis can be done either on-line or off-line on a separate system, and either incrementally or in batches. In this way, LEO introduces a feedback loop to query optimization that enhances the available information on the database where the most queries have occurred, allowing the optimizer to actually learn from its past mistakes. Our technique is general and can be applied to any operation in a QEP, including joins, derived results after several predicates have been applied, and even to DISTINCT and GROUP-BY operators. As shown by performance measurements on a 10 GB TPCH data set, the runtime overhead of LEO’s monitoring is insignificant, whereas the potential benefit to response time from more accurate cardinality and cost estimates can be orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: leo - db2 's learning optimizer

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: michael stillger , guy m. lohman , volker markl , mokhtar kandil
",y
"LEFT id: NA
RIGHT id: 1675

LEFT text: We describe the TIGUKAT objectbase management system, which is under development at the Laboratory for Database Systems Research at the University of Alberta. TIGUKAT has a novel object model, whose identifying characteristics include a purely behavioral semantics and a uniform approach to objects. Everything in the system, including types, classes, collections, behaviors, and functions, as well as meta-information, is a first-class object with well-defined behavior. In this way, the model abstracts everything, including traditional structural notions such as instance variables, method implementation, and schema definition, into a uniform semantics of behaviors on objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 785

LEFT text: Many database applications make extensive use of bitmap indexing schemes. In this paper, we study how to improve the efficiencies of these indexing schemes by proposing new compression schemes for the bitmaps. Most compression schemes are designed primarily to achieve good compression. During query processing they can be orders of magnitude slower than their uncompressed counterparts. The new schemes are designed to bridge this performance gap by reducing compression effectiveness and improving operation speed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance measurements of compressed bitmap indices

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: theodore johnson
",y
"LEFT id: NA
RIGHT id: 48

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 865

LEFT text: Spatial data mining, i.e., discovery of interesting characteristics and patterns that may implicitly exist in spatial databases, is a challenging task due to the huge amounts of spatial data and to the new conceptual nature of the problems which must account for spatial distance. Clustering and region oriented queries are common problems in this domain. Several approaches have been presented in recent years, all of which require at least one scan of all individual objects (points). Consequently, the computational complexity is at least linearly proportional to the number of objects to answer each query. In this paper, we propose a hierarchical statistical information grid based approach for spatial data mining to reduce the cost further. The idea is to capture statistical information associated with spatial cells in such a manner that whole classes of queries and clustering problems can be answered without recourse to the individual objects. In theory, and confirmed by empirical studies, this approach outperforms the best previous method by at least an order of magnitude, especially when the data set is very large.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a case-based approach to information integration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: maurizio panti , luca spalazzi , alberto giretti
",n
"LEFT id: NA
RIGHT id: 739

LEFT text: VERSANT is the industry’s leading object database management system (ODBMS) for developing applications in multi-user, dstribtrted environments. VERSANT ODBMS has an objectbased client-server architecture which is particularly suitable for such complex applications as telecommunications, trartsportations, and utilities network management systems. Because these applications are usually mission-critical and do not allow my dtia. base down time, one major requirement of our customers is 24x7 (24 hours a day, 7 days a week) high-availability of object databases, even in the presence of software, hardware, or network failures. While many VERSANT features, such as on-line backup and dynamic schema evohrtio~ have already supported part of this requiremen~ they do not address the failure cases. Traditional asynchronous replication (e.g., Sybase Replication Server, DEC Data Distributor, and Oracle Symmetric Replication) is not suitable for the following two reasons, Fhs~ data integrity maybe lost as a result of either delay during propagation or conflicting update requests from different replicates. Second, any failure in the local replica database or central primary database is not transparent to the applications. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: phoenix project : fault-tolerant applications

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: roger barga , david lomet
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: The 5th East European Conference ADBIS'2001 was organized by the Vilnius Gediminas Technical University, Institute of Mathematics and Informatics (Lithuania), Lithuanian Computer Society in cooperation with Moscow ACM SIGMOD Chapter and Law University of Lithuania in Vilnius, Lithuania, September 25-28, 2001. The call for papers attracted 82 submissions from 30 countries. The international program committee, consisting of 47 researchers from 21 countries, selected 25 papers for long presentations and 19 research communications for regular sessions. Additionally, 9 professional communications and reports have been selected for industrial sessions. The authors of accepted papers come from 29 countries, indicating the truly international recognition of the ADBIS conference series. The conference had 127 registered participants from 23 countries and included invited lectures, tutorials, regular sessions, and industrial sessions. This report describes the goals of the conference and summarizes the issues discussed during the sessions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 108

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: indexing large metric spaces for similarity search queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1155

LEFT text: We propose a new multi-attribute index. Our approach combines the hB-tree, a multi-attribute index, and the $\Pi$-tree, an abstract index which offers efficient concurrency and recovery methods. We call the resulting method the hB $^\Pi$-tree. We describe several versions of the hB $^\Pi$-tree, each using a different node-splitting and index-term-posting algorithm. We also describe a new node deletion algorithm. We have implemented all the versions of the hB $^\Pi$-tree. Our performance results show that even the version that offers no performance guarantees, actually performs very well in terms of storage utilization, index size (fan-out), exact-match and range searching, under various data types and distributions. We have also shown that our index is fairly insensitive to increases in dimension. Thus, it is suitable for indexing high-dimensional applications. This property and the fact that all our versions of the hB $^\Pi$-tree can use the $\Pi$-tree concurrency and recovery algorithms make the hB $^\Pi$-tree a promising candidate for inclusion in a general-purpose DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency and recovery for index trees

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david lomet , betty salzberg
",n
"LEFT id: NA
RIGHT id: 215

LEFT text: Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules. This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides SQL-like operator, named MINE RULE, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 986

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 1643

LEFT text: We generalize the method of constructing windows in subsequence matching. By this generalization, we can explain earlier subsequence matching methods as special cases of a common framework. Based on the generalization, we propose a new subsequence matching method, General Match. The earlier work by Faloutsos et al. (called FRM for convenience) causes a lot of false alarms due to lack of point-filtering effect. Dual Match, recently proposed as a dual approach of FRM, improves performance significantly over FRM by exploiting point filtering effect. However, it has the problem of having a smaller allowable window size---half that of FRM---given the minimum query length. A smaller window increases false alarms due to window size effect. General Match offers advantages of both methods: it can reduce window size effect by using large windows like FRM and, at the same time, can exploit point-filtering effect like Dual Match. General Match divides data sequences into generalized sliding windows (J-sliding windows) and the query sequence into generalized disjoint windows (J-disjoint windows). We formally prove that General Match is correct, i.e., it incurs no false dismissal. We then propose a method of estimating the optimal value of the sliding factor J that minimizes the number of page accesses. Experimental results for real stock data show that, for low selectivities (10-6∼10-4), General Match improves average performance by 117% over Dual Match and by 998% over FRM; for high selectivities (10-3∼10-1), by 45% over Dual Match and by 64% over FRM. The proposed generalization provides an excellent theoretical basis for understanding the underlying mechanisms of subsequence matching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast subsequence matching in time-series databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christos faloutsos , m. ranganathan , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 1444

LEFT text: 1. Document Acquisition There is a broad spectrum of techniques how to acquire documents in such a way, that they are in computerreadable form and can be stored in a document base. This spectrum ranges from fully automatic at low cost via semiautomatic using tools like scanners and optical character recognition (OCR) to manual acquisition according to elaborate rules and regulations. The purpose of high quality document acquisition is to capture the structure and the semantic content of a document not as far as possible but as far as affordable. Presently the state of the art of semiautomatic acquisition of paper documents is scanning followed by OCR. This yields a facsimile image and the text content, but no structure and no real semantics. In many cases the text produced by OCR is low quality and must be corrected to be useful for effective retrieval. Various projects are under way to capture structure and semantics automatically [1,12], but they have not reached sufficient maturity to be used in production environments like libraries or businesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: document management as a database problem

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rudolf bayer
",y
"LEFT id: NA
RIGHT id: 62

LEFT text: . In more deteil, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We develop new evaluation strategies essential to obtaining good performance, including a stack-based TermJoin algorithm for efficiently scoring composite elements. We report results from an extensive experimental evaluation, which show, among other things, that the new TermJoin access method outperforms a direct implementation of the same functionality using standard operators by a large factor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 636

LEFT text: Fibonacci is an object-oriented database programming language characterized by static and strong typing, and by new mechanisms for modeling data-bases in terms of objects with roles, classes, and associations. A brief introduction to the language is provided to present those features, which are particularly suited to modeling complex databases. Examples of the use of Fibonacci are given with reference to the prototype implementation of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: fibonacci : a programming language for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: antonio albano , giorgio ghelli , renzo orsini
",y
"LEFT id: NA
RIGHT id: 1463

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1396

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: agflow : agent-based cross-enterprise workflow management system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: liangzhao zeng , boualem benatallah , phuong nguyen , anne h. h. ngu
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: Privacy – the control over one’s personal data – and security – the attempted access to data by unauthorized others – are two critical problems for both e-commerce consumers and sites alike. Without either, consumers will not visit or shop at a site, nor can sites function effectively without considering both. This chapter reviews the current state of the art and the relevance for privacy and security respectively. We examine privacy from social psychological, organizational, technical, regulatory, and economic perspectives. We then examine security from technical, social and organizational, and economic perspectives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 696

LEFT text: Authors and publishers who wish their publications to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4. All relevant books received will be listed, but not all can be reviewed. Technical reports (other than dissertations) will not be listed or reviewed. Authors should be aware that some publishers will not send books for review (even when instructed to do so); authors wishing to inquire as to whether their book has been received for review may contact the book review editor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: proxy-based acceleration of dynamically generated content on the world wide web : an approach and implementation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: anindya datta , kaushik dutta , helen thomas , debra vandermeer , suresha , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 121

LEFT text: There is currently considerable interest in developing multimedia digital libraries. However, it has become clear that existing architectures for management systems do not support the particular requirements of continuous media types. This is particularly the case in the important area of quality of service support. In this correspondence, we discuss quality of service issues within digital libraries and present a reference architecture able to support some quality aspects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: contextualizing the information space in federated digital libraries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. p. papazoglou , j. hoppenbrouwers
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1121

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation and analysis of a parallel collection query language

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2047

LEFT text: A semistructured information space consists of multiple collections of textual documents containing fielded or tagged sections. The space can be highly heterogeneous, because each collection has its own schema, and there are no enforced keys or formats for data items across collections. Thus, structured methods like SQL cannot be easily employed, and users often must make do with only full-text search. In this paper, we describe an approach that provides structured querying for particular types of entities, such as companies and people. Entity-based retrieval is enabled by normalizing entity references in a heuristic, type-dependent manner. The approach can be used to retrieve documents and can also be used to construct entity profiles — summaries of commonly sought information about an entity based on the documents' content. The approach requires only a modest amount of meta-information about the source collections, much of which is derived automatically. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting structured data from web pages

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arvind arasu , hector garcia-molina , stanford university
",n
"LEFT id: NA
RIGHT id: 2279

LEFT text: To ease schema evolution, we propose to support exceptions to the behavioral consistency rules without sacrificing type safety. The basic idea is to detect unsafe statements in a method code at compile-time and check them at run-time. The run-time check is performed by a specific clause that is automatically inserted around unsafe statements. This check clause warns the programmer of the safety problem and lets him provide exception-handling code. Schema updates can therefore be performed with only minor changes to the code of methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting exceptions to schema consistency to ease schema evolution in oodbms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eric amiel , marie-jo bellosta , eric dujardin , eric simon
",n
"LEFT id: NA
RIGHT id: 2165

LEFT text: French government has launch in 2000 a public debate about conservation of data and electronic documents. Due to the widespread use of Internet and extranet technologies, especially with electronic document exchange, we had have to adapt the politic of document conservation to this new challenge: “avoid the lack of memory in public administration”. A guide-book was produced. This guide-book is an important step to introduce a reflection about the conservation and to give some guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: french government activity in the conservation of data and electronic documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: serge novaretti
",y
"LEFT id: NA
RIGHT id: 810

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1100

LEFT text: Parallel database systems have to support the effective parallelization of complex queries in multi-user mode, i.e. in combination with inter-query~mter-transaction parallelism. For this purpose, dynamic scheduling and load balancing strategies’ are necessary that umsider the current system state for dekrminhg the degree of intra-query parallelism and for selecting the processors for executing subqueries. We study these issues for parallel hash joinprocessing and show that the two subproblems should be addressed in au integrated way. Even more importantly, however, is the use of a multimannce load balancing approach that considers all potential bottleneck resources. in particular memory, disk and CPU. We discuss basic performance tradeoffs to consider and evalGate the performauce of several oad balancing strategies by means of a detailed simulation model. Simulation results will be analyzed for multiuser configurations with both homogeneous andheterogeneous (query/OLTP) workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing in hierarchical parallel database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: luc bouganim , daniela florescu , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1954

LEFT text: Relational database systems do not effectively support complex queries containing quantifiers (quantified queries) that are increasingly becoming important in decision support applications. Generalized quantifiers provide an effective way of expressing such queries naturally. In this paper, we consider the problem of processing quantified queries within the generalized quantifier framework. We demonstrate that current relational systems are ill-equipped, both at the language and at the query processing level, to deal with such queries. We also provide insights into the intrinsic difficulties associated with processing such queries. We then describe the implementation of a quantified query processor, Q2P, that is based on multidimensional and boolean matrix structures. We provide results of performance experiments run on Q2P that demonstrate superior performance on quantified queries. Our results indicate that it is feasible to augment relational systems with query subsystems like Q2P for significant performance benefits for quantified queries in decision support applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql query optimization : reordering for a general class of queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 2069

LEFT text: Temporal data is pervasive, and challenging to manage in SQL. The June through October issues of Database Programming and Design (volume 11, issues 6–10) included a special series on temporal databases; the five articles in that series are reproduced here. Three separate case studies: a neonatal intensive care unit, a commercial cattle feed yard, and astronomical star catalogs, were used to illustrate how temporal applications can be implemented in SQL. The concepts of valid time versus transaction time and of current, sequenced and nonsequenced integrity constraints, queries, and modifications were emphasized. 1 Of Duplicates and Septuplets This special series explores the many issues that arise when attempting to define and manage time-varying data. Such data is pervasive. It has been estimated that one of every 50 lines of database application code involves a date or time value. Data warehouses are by definition time-varying: Ralph Kimball states that every data warehouse has a time dimension. Often the time-oriented nature of the data is what lends it value. DBAs and application programmers constantly wrestle with the vagaries of such data. They find that overlaying simple concepts, such as duplicate prevention, on time-varying data can be surprisingly subtle and complex. And they are perplexed that trade publications and books do not provide guidance and techniques for handling such data. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: standards

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: andrew eisenberg
",n
"LEFT id: NA
RIGHT id: 274

LEFT text: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dissemination of dynamic data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pavan deolasee , amol katkar , ankur panchbudhe , krithi ramamritham , prashant shenoy
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner, that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 849

LEFT text: Nearest neighbor search in high dimensional spaces is an interesting and important problem which is relevant for a wide variety of novel database applications. As recent results show, however, the problem is a very di cult one, not only with regards to the performance issue but also to the quality issue. In this paper, we discuss the quality issue and identify a new generalized notion of nearest neighbor search as the relevant problem in high dimensional space. In contrast to previous approaches, our new notion of nearest neighbor search does not treat all dimensions equally but uses a quality criterion to select relevant dimensions (projections) with respect to the given query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what is the nearest neighbor in high dimensional spaces ?

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alexander hinneburg , charu c. aggarwal , daniel a. keim
",y
"LEFT id: NA
RIGHT id: 649

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1896

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 1120

LEFT text: There is mounting evidence [Man77, SchSI] that real datasets are statistically self-similar, and thus, ‘fractal’. This is an important insight since it permits a compact statistical description of spatial datasets; subsequently, as we show, it also forms the basis for the theoretical analysis of spatial access methods, without using the typical, but unrealistic, uniformity assumption. In this paper, we focus on the estimation of the number of quadtree blocks that a real, spatial dataset will require. Using the the wellknown Hausdorff fractal dimension, we derive some closed formulas which allow us to predict the number of quadtree blocks, given some few parameters. Using our formulas, it is possible to predict the space overhead and the response time of linear quadtrees/z-ordering [OM88], which are widely used in practice. In order to verify our analytical model, we performed an extensive experimental investigation using several real datasets coming from different domains. In these experiments, we found that our analytical model agrees well with our experiments as well as with older empirical observations on 2-d [Gae95b] and 3-d [ACF+94] data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: analysis of n-dimensional quadtrees using the hausdorff fractal dimension

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: christos faloutsos , volker gaede
",y
"LEFT id: NA
RIGHT id: 1795

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integrating association rule mining with relational database systems : alternatives and implications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sunita sarawagi , shiby thomas , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 7

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the jungle database search engine

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael b &#246; hlen , linas bukauskas , curtis dyreson
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1402

LEFT text: Our paper addresses the problem by proposing the MV3R-tree, a structure that utilizes the concepts of multi-version B-trees and 3D-Rtrees. Extensive experimentation proves that MV3R-trees compare favorably with specialized structures aimed at timestamp and interval window queries, both in terms of time and space requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the bt-tree : a branched and temporal access method

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: linan jiang , betty salzberg , david b. lomet , manuel barrena garc &#237; a
",n
"LEFT id: NA
RIGHT id: 1542

LEFT text: A multimedia database is a controlled collection of multimedia data items such as text, images, graphic objects, video and audio. A multimedia database management system (DBMS) provides support for the creation, storage, access, querying and control of a multimedia database. The requirements of a multimedia DBMS are: multimedia data modeling; multimedia object storage; multimedia indexing, retrieval and browsing; and multimedia query support. This paper discusses a general framework for multimedia database systems and describes the requirements and architecture for these systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: More than two decades ago, DB researchers faced up to the question of how to design a data-independent database management system (DBMS), that is, a DBMS which offers an appropriate application programming interface (API) to the user and whose architecture is open for permanent evolution. For this purpose, an architectural model based on successive data abstraction steps of record-oriented data was proposed as kind of a standard and later refined to a five-layer hierarchical DBMS model. We review the basic concepts and implementation techniques of this model and survey the major improvements achieved in the system layers to date. Furthermore, we consider the interplay of the layered model with the transactional ACID properties and again outline the progress obtained. In the course of the last 20 years, this DBMS architecture was challenged by a variety of new requirements and changes as far as processing environments, data types, functional extensions, heterogeneity, autonomy, scalability, etc. are concerned. We identify the cases which can be adjusted by our standard system model and which need major extensions or other types of system models.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 78

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: closest pair queries in spatial databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: antonio corral , yannis manolopoulos , yannis theodoridis , michael vassilakopoulos
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: Research work in programming languages and in database systems is combating the same problems of scale, change and complexity. This paper looks at the present difficulties of relating persistent data with changing programs. It illustrates the influence that the present interfaces have on programming methodology and algorithm design. It recognises the need for new language primitives to encapsulate database concepts and a few putative primitives are examined. It is suggested that such primitives could simplify the use of databases by programmers. These ideas are illustrated with examples from geometric modelling using Algol 68.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 980

LEFT text: Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: datablitz : a high performance main-memory storage manager

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , philip bohannon , s. gogate , s. joshi , c. gupta , a. khivesera , henry f. korth , peter mcilroy , j. miller , p. p. s. narayan , m. nemeth , rajeev rastogi , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 46

LEFT text: Performance needs of many database applications dictate that the entire database be stored in main memory. The Dali system is a main memory storage manager designed to provide the persistence, availability and safety guarantees one typically expects from a diskresident database, while at the same time providing very high performance by virtue of being tuned to support in-memory data. Dali follows the philosophy of treating all data, including system data, uniformly as database files that can be memory mapped and directly accessed/updated by user processes. Direct access provides high performance; slower, but more secure, access is also provided through the use of a server process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: datablitz storage manager : main-memory database performance for critical applications

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: j. baulier , p. bohannon , s. gogate , c. gupta , s. haldar
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 106

LEFT text: Much of the work on conceptual modeling involves the use of an entity-relationship model in which binary relationships appear as associations between two entities. Relationships involving more than two entities are considered rare and, therefore, have not received adequate attention. This research provides a general framework for the analysis of relationships in which binary relationships simply become a special case. The framework helps a designer to identify ternary and other higher-degree relationships that are commonly represented, often inappropriately, as either entities or binary relationships. Generalized rules are also provided for representing higher-degree relationships in the relational model. This uniform treatment of relationships should significantly ease the burden on a designer by enabling him or her to extract more information from a real-world situation and represent it properly in a conceptual design.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: database design for incomplete relations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mark levene , george loizou
",n
"LEFT id: NA
RIGHT id: 2092

LEFT text: Model management aims at reducing the amount of programming needed for the development of metadata-intensive applications. We present a first complete prototype of a generic model management system, in which high-level operators are used to manipulate models and mappings between models. We define the key conceptual structures: models, morphisms, and selectors, and describe their use and implementation. We specify the semantics of the known model-management operators applied to these structures, suggest new ones, and develop new algorithms for implementing the individual operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rondo : a programming platform for generic model management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sergey melnik , erhard rahm , philip a. bernstein
",y
"LEFT id: NA
RIGHT id: 688

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 441

LEFT text: Answering aggregate queries like SUM, COUNT, MIN, MAX, AVG in an approximate manner is often desirable when the exact answer is not needed or too costly to compute. We present an algorithm for answering such queries in multi-dimensional databases, using selective traversal of a Multi-Resolution Aggregate (MRA) tree structure storing point data. Our approach provides 100% intervals of confidence on the value of the aggregate and works iteratively, coming up with improving quality answers, until some error requirement is satisfied or time constraint as reached. Using the same technique we can also answer aggregate queries exactly and our experiments indicate that even for exact answering the proposed data structure and algorithm are very fast.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximating multi-dimensional aggregate range queries over real attributes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi
",n
"LEFT id: NA
RIGHT id: 627

LEFT text: Spatial data types or algebras for database systems should (1) be fully general, that is, closed under set operations, (2) have formally defined semantics, (3) be defined in terms of finite representations available in computers, (4) offer facilities to enforce geometric consistency of related spatial objects, and (5) be independent of a particular DBMS data model, but cooperate with any. We present an algebra that usesrealms as geometric domains underlying spatial data types. A realm, as a general database concept, is a finite, dynamic, user-defined structure underlying one or more system data types. Problems of numerical robustness and topological correctness are solved within and below the realm layer so that spatial algebras defined above a realm have very nice algebraic properties. Realms also interact with a DMBS to enforce geometric consistency on object creation or update. The ROSE algebra is defined on top of realms and offers general types to represent point, line, and region features, together with a comprehensive set of operations. It is described within a polymorphic type system and interacts with a DMBS data model and query language through an abstractobject model interface. An example integration of ROSE into the object-oriented data model O2 and its query language is presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: realm-based spatial data types : the rose algebra

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting , markus schneider
",y
"LEFT id: NA
RIGHT id: 1645

LEFT text: Performance needs of many database applications dictate that the entire database be stored in main memory. The Dali system is a main memory storage manager designed to provide the persistence, availability and safety guarantees one typically expects from a diskresident database, while at the same time providing very high performance by virtue of being tuned to support in-memory data. Dali follows the philosophy of treating all data, including system data, uniformly as database files that can be memory mapped and directly accessed/updated by user processes. Direct access provides high performance; slower, but more secure, access is also provided through the use of a server process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 17

LEFT text: We illustrate basic features of the Lixto wrapper generator such as the user and system interaction, the capacious visual interface, the marking and selecting procedures, and the extraction tasks by describing the construction of a simple example program in the current Lixto prototype.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an xjml-based wrapper generator for web information extraction

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ling liu , wei han , david buttler , calton pu , wei tang
",n
"LEFT id: NA
RIGHT id: 670

LEFT text: There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: supply chain infrastructures : system integration and information sharing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: michael o. ball , meng ma , louiqa raschid , zhengying zhao
",n
"LEFT id: NA
RIGHT id: 103

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: nsf workshop on industrial/academic cooperation in database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mike carey , len seligman
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. Sara Rynes Incoming Editor I am thankful to Sara for inviting me to write this editorial column encouraging scholars to submit their qualitative research to the Academy of Man-I wish to thank Torn Lee and Sara Rynes for their helpful comments and encouragement in preparing this editorial. 454 agement Journal. Qualitative research is important to AMI Qualitative research is actively sought and supported by the Journal, its editors, and its editorial review board. Alv1Jhas published many qualitative papers. The coveted A/'v1jBest Article Award has been won by three qualitative papers-Gersick (1989), Isabella (1990), and Dutton and Duckerich (1991)-and by one paper that combined qualitative and quantitative methods: Sutton and Rafuclli, (1988). Despite these successes, most …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 644

LEFT text: We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of queries in a mediator for websources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vladimir zadorozhny , louiqa raschid , maria esther vidal , tolga urhan , laura bright
",n
"LEFT id: NA
RIGHT id: 1273

LEFT text: Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices.Our first contribution is a practical scheme that models magic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM's DB2 C/S V2 database system. Our performance measurements demonstrate that the cost-based magic optimization technique performs well, and that without it, several poor decisions could be made.Our second contribution is a formal algebraic model of magic sets rewriting, based on an extension of the multiset relational algebra, which cleanly defines the search space and can be used in a rule-based optimizer. We introduce the multiset &theta;-semijoin operator, and derive equivalence rules involving this operator. We demonstrate that magic sets rewriting for non-recursive SQL queries can be modeled as a sequential composition of these equivalence rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: context-based prefetch - an optimization for implementing objects on relations

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: philip a. bernstein , shankar pal , david shutt
",n
"LEFT id: NA
RIGHT id: 463

LEFT text: This article gives methods for statically analyzing sets of active database rules to determine if the rules are (1) guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an algebraic approach to static analysis of active database rules

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: elena baralis , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1535

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the naos system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: c. collet , t. coupaye
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a thery and practice of intellectual property cost management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 680

LEFT text: Tree pattern is at the core of XML queries. The tree patterns in XML queries typically contain redundancies, especially when broad integrity constraints (ICs) are present and considered. Apparently, tree pattern minimization has great significance for efficient XML query processing. Although various minimization schemes/algorithms have been proposed, none of them can exploit broad ICs for thoroughly minimizing the tree patterns in XML queries. The purpose of this research is to develop an innovative minimization scheme and provide a novel implementation algorithm.Design/methodology/approach – Query augmentation/expansion was taken as a necessary first‐step by most prior approaches to acquire XML query pattern minimization under the presence of certain ICs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for minimizing tree pattern queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: prakash ramanan
",y
"LEFT id: NA
RIGHT id: 1459

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 2039

LEFT text: A Bloom Filter is a space-efficient randomized data structure allowing membership queries over sets with certain allowable errors. It is widely used in many applications which take advantage of its ability to compactly represent a set, and filter out effectively any element that does not belong to the set, with small error probability. This paper introduces the Spectral Bloom Filter (SBF), an extension of the original Bloom Filter to multi-sets, allowing the filtering of elements whose multiplicities are below a threshold given at query time. Using memory only slightly larger than that of the original Bloom Filter, the SBF supports queries on the multiplicities of individual keys with a guaranteed, small error probability. The SBF also supports insertions and deletions over the data set. We present novel methods for reducing the probability and magnitude of errors. We also present an efficient data structure and algorithms to build it incrementally and maintain it over streaming data, as well as over materialized data with arbitrary insertions and deletions. The SBF does not assume any a priori filtering threshold and effectively and efficiently maintains information over the entire data-set, allowing for ad-hoc queries with arbitrary parameters and enabling a range of new applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spectral bloom filters

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: saar cohen , yossi matias
",y
"LEFT id: NA
RIGHT id: 1752

LEFT text: In late 2000, work was completed on yet another part of the SQL standard, to which we introduced our readers in an earlier edition of this column.Although SQL database systems manage an enormous amount of data, it certainly has no monopoly on that task. Tremendous amounts of data remain in ordinary operating system files, in network and hierarchical databases, and in other repositories. The need to query and manipulate that data alongside SQL data continues to grow. Database system vendors have developed many approaches to providing such integrated access.In this (partly guested) article, SQL's new part, Management of External Data (SQL/MED), is explored to give readers a better notion of just how applications can use standard SQL to concurrently access their SQL data and their non-SQL data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: management of semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 715

LEFT text: Query answers are ranked using extended information-retrieval techniques and are generated in an order similar to the ranking. Advanced indexing techniques were developed to facilitate efficient implementation of XSEarch. The performance of the different techniques as well as the recall and the precision were measured experimentally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xcache : a semantic caching system for xml queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: li chen , elke a. rundensteiner , song wang
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Active database systems have been a hot research topic for quite some years now. However, while “active functionality” has been claimed for many systems, and notions such as “active objects” or “events” are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of “active database management system” as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 999

LEFT text: 1 Introduction Histograms are commonly used to capture attribute value distribution statistics for query optimizers. More recently, histograms have also been considered as a way to produce quick approximate answers to decision support queries. This widespread interest in histograms motivates the problem of computing his-tograms that are good under a given error metric. In particular, we are interested in an efficient algorithm for choosing the bucket boundaries in a way that either minimizes the estimation error for a given amount of space (number of buckets) or, conversely, minimizes the space needed for a given upper bound on the error. Under the assumption that finding optimal bucket boundaries is computationally inefficient, previous research has focused on heuristics with no provable bounds on the quality of the solutions. In this paper, we present algorithms for computing optimal bucket boundaries in time proportional to the square of the number of distinct data values, for a broad class of optimality metrics. This class includes the V-Optimality constraint, which has been shown to result in the most accurate histograms for several selectivity estimation problems. Through experiments , we show that optimal histograms can achieve substantially lower estimation errors than histograms produced by popular heuristics. We also present new heuristics with provably good space-accuracy trade-offs that are significantly faster than the optimal algorithm. Finally, we present an enhancement to traditional histograms that allows us to provide quality guarantees on individual selectivity estimates. In our experiments, these quality guarantees were highly effective in isolating outliers in selectivity estimates. It is often the case that a data set cannot be stored or processed in its entirety; only a summarized form is stored. A typical way in which data is summarized is by means of a histogram. The summarized data can be used to answer various kinds of queries, in the same way the original data would have been used. The answer obtained is not exact but approximate, and contains an error due to the information lost when the data was summarized. This error can be measured according to some appropriate metric such as the maximum , average, or mean squared error of the estimate. This basic idea has long been used in a database context to estimate the result sizes of relational operators for the purpose of cost-based query optimization. The objective is to approximate the data distribution of the values in a column, and to use that …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimal histograms with quality guarantees

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , s. muthukrishnan , viswanath poosala , kenneth c. sevcik , torsten suel
",y
"LEFT id: NA
RIGHT id: 1914

LEFT text: The volume of medical imaging data produced per year is rapidly increasing, overtaxing the capabilities of Picture Archival and Communication (PACS) systems. Image compression methods can lessen the problem by encoding digital images into more space-efficient forms. Image compression is achieved by reducing redundancy in the imaging data. Existing methods reduce redundancy in individual images. However, these methods ignore an additional source of redundancy, which is based on the common information stored in more than one image in a set of similar images. We use the term ""set redundancy"" to describe this type of redundancy. Medical image databases contain large sets of similar images, therefore they also contain significant amounts of set redundancy.This paper presents two methods that extract set redundancy from medical imaging data: the Min-Max Differential (MMD), and the Min-Max Predictive (MMP) methods. These methods can improve compression of standard image compression techniques for sets of medical images. Our tests compressing CT brain scans have shown an average of as much as 129% improvement for Huffman encoding, 93% for Arithmetic Coding, and 37% for Lempel-Ziv compression when they are combined with Min-Max methods. Both MMD and MMP are based on reversible operations, hence they provide lossless compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: min-max compression methods for medical image databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kosmas karadimitriou , john m. tyler
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2059

LEFT text: As WWW becomes more and more popular and powerful, how to search information on the web in database way becomes an important research topic. COMMIX, which is developed in the DB group in Peking University (China), is a system towards building very large database using data from the Web for information extraction, integration and query answering. COMMIX has some innovative features, such as ontology-based wrapper generation, XML-based information integration, view-based query answering, and QBE-style XML query interface.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using sets of feature vectors for similarity search on voxelized cad objects

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans-peter kriegel , stefan brecheisen , peer kr &#246; ger , martin pfeifle , matthias schubert
",y
"LEFT id: NA
RIGHT id: 1676

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as a deductive database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 109

LEFT text: In this paper we describe a Patricia tree-based B-tree variant suitable for OLTP. In this variant, each page of the B-tree contains a local Patricia tree instead of the usual sorted array of keys. It has been implemented in iAnywhere ASA Version 8.0. Preliminary experience has shown that these indexes can provide significant space and performance benefits over existing ASA indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: specification and implementation of exceptions in workflow management systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: Integrity constraint checking for stratifiable deductive databases has been studied by many authors. However, most of these methods may perform unnecessary checking if the update is irrelevant to the constraints. [Lee94] proposed a set called relevant set which can be incorporated in these works to reduce unnecessary checking. [Lee94] adopts a top-down approach and makes use of constants and evaluable functions in the constraints and deductive rules to reduce the search space. In this paper, we further extend this idea to make use of relational predicates, instead of only constants and evaluable functions in [Lee94]. We first show that this extension is not a trivial one as extra database retrieval cost is incurred. We then present a new method to construct a pre-test which can be incorporated in most existing methods to reduce the average checking costs in terms of database accesses by a significant factor. Our method also differs from other partial checking methods as we can handle multiple updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: Concurrency control is essential to the correct functioning of a database due to the need for correct, reproducible results. For this reason, and because concurrency control is a well-formulated problem, there has developed an enormous body of literature studying the performance of concurrency control algorithms. Most of this literature uses either analytic modeling or random number-driven simulation, and explicitly or implicitly makes certain assumptions about the behavior of transactions and the patterns by which they set and unset locks. Because of the difficulty of collecting suitable measurements, there have been only a few studies which use trace-driven simulation, and still less study directed toward the characterization of concurrency control behavior of real workloads. In this paper, we present a study of three database workloads, all taken from IBM DB2 relational database systems running commercial applications in a production environment. This study considers topics such as frequency of locking and unlocking, deadlock and blocking, duration of locks, types of locks, correlations between applications of lock types, two-phase versus non-two-phase locking, when locks are held and released, etc. In each case, we evaluate the behavior of the workload relative to the assumptions commonly made in the research literature and discuss the extent to which those assumptions may or may not lead to erroneous conclusions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 67

LEFT text: Nearest neighbor search in high dimensional spaces is an interesting and important problem which is relevant for a wide variety of novel database applications. As recent results show, however, the problem is a very di cult one, not only with regards to the performance issue but also to the quality issue. In this paper, we discuss the quality issue and identify a new generalized notion of nearest neighbor search as the relevant problem in high dimensional space. In contrast to previous approaches, our new notion of nearest neighbor search does not treat all dimensions equally but uses a quality criterion to select relevant dimensions (projections) with respect to the given query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: finding generalized projected clusters in high dimensional spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1618

LEFT text: Currently, most data accessed on large servers is structured data stored in traditional databases. Networks are LAN based and clients range from simple terminals to powerful workstations. The user is corporate and the application developer is an MIS professional. With the introduction of broadband communications to the home and better than 100-to-1 compression techniques, a new form of network-based computing is emerging. Structured data is still important, but the bulk of data becomes unstructured: audio, video, news feeds, etc. The predominant user becomes the consumer. The predominant client device becomes the television set. The application developer becomes the storyboard developer, director, or the video production engineer. The Oracle Media Server supports access to all types of conventional data stored in Oracle relational and text databases. In addition, we have developed a real-time stream server that supports storage and playback of real-time audio and video data. The Media Server also provides access to data stored in file systems or as binary large objects (images, executables, etc.) The Oracle Media Server provides a platform for distributed client-server computing and access to data over asymmetric real-time networks. A service mechanism allows applications to be split such that client devices (set-top boxes, personal digital assistants, etc.) can focus on presentation, while backend services running in a distributed server complex, provide access to data via messaging or lightweight RPC (Remote Procedure Call).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: oracle media server : providing consumer based interactive access to multimedia data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: andrew laursen , jeffrey olkin , mark porter
",y
"LEFT id: NA
RIGHT id: 220

LEFT text: We propose a new dynamic method for multidimensional selectivity estimation for range queries that works accurately independent of data distribution. Good estimation of selectivity is important for query optimization and physical database design. Our method employs the multilevel grid file (MLGF) for accurate estimation of multidimensional data distribution. The MLGF is a dynamic, hierarchical, balanced, multidimensional file structure that gracefully adapts to nonuniform and correlated distributions. We show that the MLGF directory naturally represents a multidimensional data distribution. We then extend it for further refinement and present the selectivity estimation method based on the MLGF. Extensive experiments have been performed to test the accuracy of selectivity estimation. The results show that estimation errors are very small independent of distributions, even with correlated and/or highly skewed ones. Finally, we analyze the cause of errors in estimation and investigate the effects of various parameters on the accuracy of estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1794

LEFT text: There has been much research on various aspects of Approximate Query Processing (AQP), such as different sampling strategies, error estimation mechanisms, and various types of data synopses. However, many subtle challenges arise when building an actual AQP engine that can be deployed and used by real world applications. These subtleties are often ignored (or at least not elaborated) by the theoretical literature and academic prototypes alike. For the first time to the best of our knowledge, in this article, we focus on these subtle challenges that one must address when designing an AQP system. Our intention for this article is to serve as a handbook listing critical design choices that database practitioners must be aware of when building or using an AQP system, not to prescribe a specific solution to each challenge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: new sampling-based summary statistics for improving approximate query answers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias
",n
"LEFT id: NA
RIGHT id: 272

LEFT text: Real-time Virtual Walkthrough Data representing virtual environments (VEs) are getting increasingly large in order to better simulate real scenes. This poses interesting challenges to organize, store, and render the data for interactive navigation in VEs, or walkthrough. A large VE usually consists of thousands of 3D objects, each of which can be represented by hundreds of polygons, and may take thousands of megabytes of storage space. The amount of data is so large that it is impossible to store all of them in the main memory. Even for memory resident models, the graphics pipeline can become a bottleneck quickly with a large amount of data and slow down the rendering to an unacceptable frame rate for the walkthrough.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: review : a real-time virtual walkthrough system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: l. shou , c. h. chionh , z. huang , y. ruan , kian-lee tan
",y
"LEFT id: NA
RIGHT id: 1586

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive parallel aggregation algorithms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ambuj shatdal , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: The MOMIS project (Mediator envirOnment for Multiple Information Sources) developed in the past years allows the integration of data from structured and semi-structured data sources. SI-Designer (Source Integrator Designer) is a designer support tool implemented within the MOMIS project for semi-automatic integration of heterogeneous sources schemata. It is a java application where all modules involved are available as CORBA Object and interact using established IDL interfaces. The goal of this demonstration is to present a new tool: SI-Web (Source Integrator on Web), it offers the same features of SI-Designer but it has got the great advantage of being usable on Internet through a web browser.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1863

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quasi-cubes : exploiting approximations in multidimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , mark sullivan
",n
"LEFT id: NA
RIGHT id: 1033

LEFT text: In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer ""in step"". The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient cost-driven index selection tool for microsoft sql server

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , vivek r. narasayya
",y
"LEFT id: NA
RIGHT id: 305

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spartan : a model-based semantic compression system for massive data tables

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shivnath babu , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 1583

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at university of hagen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes
",n
"LEFT id: NA
RIGHT id: 1902

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applying database visualization to the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: masum z. hasan , alberto o. mendelzon , dimitra vista
",n
"LEFT id: NA
RIGHT id: 425

LEFT text: We present WSQ/DSQ (pronounced “wisk-disk”), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wsq/dsq : a practical approach for combined querying of databases and the web

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",y
"LEFT id: NA
RIGHT id: 2255

LEFT text: Abstract. We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By “categorical data,” we mean tables with fields that cannot be naturally ordered by a metric – e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the impact of global clustering on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1431

LEFT text:  The database systems have nowadays an increasingly important role in the knowledge-based society, in which computers have penetrated all fields of activity and the Internet tends to develop worldwide. In the current informatics context, the development of the applications with databases is the work of the specialists. Using databases, reach a database from various applications, and also some of related concepts, have become accessible to all categories of IT users. This paper aims to summarize the curricular area regarding the fundamental database systems issues, which are necessary in order to train specialists in economic informatics higher education. The database systems integrate and interfere with several informatics technologies and therefore are more difficult to understand and use. Thus, students should know already a set of minimum, mandatory concepts and their practical implementation: computer systems, programming techniques, programming languages, data structures. The article also presents the actual trends in the evolution of the database systems, in the context of economic informatics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pc database systems - present and future

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: philip a. bernstein
",y
"LEFT id: NA
RIGHT id: 1317

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 619

LEFT text: A book title cannot be more timely or accurate. Information rules society and it always has. The key difference is, that in our generation, the manner in which information is managed is more apparent to the everyday person and as more information becomes readily available the curse is that information can overload and intimidate us with little or no effort. Prior to the personal computer the everyday person could more easily manage the flow—such is not the case today. Throw into this fray the fact that information is a force in economics and the everyday person may become bewildered and perplexed. Many of these concerns are addressed in this excellent new book that focuses on the information economy and its effect on society and culture. In ten engaging chapters, key concepts such as pricing, versioning, rights management, recognizing and managing lock-in, networks, cooperation and compatibility, standards, and information policy are dissected, discussed, and explained. Most chapters end with lessons that reflect key points made in the chapter. The first chapter presents the foundation of the thesis of the book—the material is relatively general in nature—and sets the stage for the following nine interesting chapters. In discussing pricing, the authors cite the case of Encyclopedia Britannica and its inability to compete with the more popular and less expensive Microsoft product, Encarta. An associated concept, “versioning” is discussed and the authors show how a business can offer information products in different versions for differing markets to the benefit of the bottom line. The heady and confusion issue of copyright management, especially as related to internet economy is examined in chapter four of the book. Another issue of concern, lock-in, which results from switching from one technology to another, is discussed in chapters five and six. In chapter seven the authors discuss how the old industrial economy was driven by economies of scale whereas the information economy is driven by economics of networks. The last three chapters push the envelope and advise the reader how to affect real changes in their relationship with the information economy. The last chapter is key in that it discusses current government information policies in light of advice provided earlier in the book. This book may be one of the best to examine the theory and implications of the information economy. Although written by heavyweights in the field of economics and information management, the authors present a well written and thoughtful treatment of a subject that non-academics and academics alike should enjoy and refer to often. More importantly, this book offers direct advice that could well affect the bottom line of many entrepreneurs and existing companies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 2278

LEFT text: The land use categories of the Global Land Cover 2000 (GLC2000; Global Land Cover 2000 database, 2003, European Commission, Joint Research Centre; resolution 1 km) are then calibrated with the Swiss dataset in order to derive a Europe-wide birch distribution dataset and aggregated onto the 7 km COSMO-ART grid. This procedure thus assumes that a certain GLC2000 land use category has the same birch density wherever it may occur in Europe. In order to reduce the strict application of this crucial assumption, the birch density distribution as obtained from the previous steps is weighted using the mean Seasonal Pollen Index (SPI; yearly sums of daily pollen concentrations). For future improvement, region-specific birch densities for the GLC2000 categories could be integrated into the mapping procedure.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: standards for databases on the grid

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: susan malaika , andrew eisenberg , jim melton
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1560

LEFT text: DataJoiner (DJ) is a heterogeneous database system that provides a single database image of multiple databases. It provides transparent access to tables at remote databases through user defined aliases (nicknames) that can be accessed as if they were local tables. DJ is also a fully functional relational database system. A couple of salient features of the DataJoiner query optimizer are: (1) A query submitted to DataJoiner is optimized using a cost model that takes into account the remote optimizer’s capabilities in addition to the remote query processing capabilities and (2) If a remote database system lacks some functionality (eg: sorting), DataJoiner compensates for it. In this paper, we present the design of the Datajoiner query optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",n
"LEFT id: NA
RIGHT id: 459

LEFT text: We describe the design and implementation of the Glue-Nail deductive database system. Nail is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code are both compiled into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm and supports well-founded models. The Glue compiler's static optimizer uses peephole techniques and data flow analysis to improve code.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the implementation and performance of compressed databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: till westmann , donald kossmann , sven helmer , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 1763

LEFT text: Integrated access to heterogeneous information is an increasingly important topic as more and more sources that developed independently from each other become accessible over networks. The structure of the information and the abilities of the sources to answer queries may vary widely. Therefore, systems are needed that are able to use knowledge about the contents and the capabilities of sources to break down a global query into portions that can be processed locally and to reassemble the answers. Problems of this kind arise when one …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: intelligent access to heterogeneous information sources : report on the 4th workshop on knowledge representation meets databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: franz baader , manfred a. jeusfeld , werner nutt
",y
"LEFT id: NA
RIGHT id: 2259

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 88

LEFT text: Anomaly detection is an important challenge for tasks such as fault diagnosis and intrusion detection in energy constrained wireless sensor networks. A key problem is how to minimise the communication overhead in the network while performing in-network computation when detecting anomalies. Our approach to this problem is based on a formulation that uses distributed, one-class quarter-sphere support vector machines to identify anomalous measurements in the data. We demonstrate using sensor data from the Great Duck Island Project that our distributed approach is energy efficient in terms of communication overhead while achieving comparable accuracy to a centralised scheme.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: distributed transactions in practice

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: prabhu ram , lyman do , pamela drew
",n
"LEFT id: NA
RIGHT id: 1145

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance of externally materialized views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: martin staudt , matthias jarke
",n
"LEFT id: NA
RIGHT id: 599

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 176

LEFT text: This diploma thesis covers information system SAP R3 which dates in year 1972. During decades it has been improved and spread almost in all areas of data handling in companies. By help of different modules the system covers human resources, production planning, material management, plant maintenance, project system, etc. The basic navigations in SAP R3 run by help of shortcuts of programs, called transaction. The business applications of system are divided in standard and customer applications. Standard applications are complex, extensive and general thus for easier application usually customer programs are developed covering only smaller area of company’s needs. Thus, customer applications are easier to use for end-users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: java and relational databases ( tutorial ) : sqlj

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gray clossman , phil shaw , mark hapner , johannes klein , richard pledereder , brian becker
",n
"LEFT id: NA
RIGHT id: 942

LEFT text: We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) >= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast computation of sparse datacubes

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 2109

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 594

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: describing semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luca cardelli
",n
"LEFT id: NA
RIGHT id: 1016

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bulk-loading techniques for object databases and an application to relational data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sophie cluet , claude delobel
",n
"LEFT id: NA
RIGHT id: 368

LEFT text: Introduction and Main Contributions Providing mechanisms that allow the user to retrieve desired multimedia information by their semantic content is now an important issue in multimedia databases. However, current prototypes (e.g. Oracle 8i interMedia and Informix Datablade Modules) index mostly only low-level features of multimedia objects. Therefore special techniques are needed for semantic indexing and retrieval of multimedia objects. In this context we present the SMOOTH system, a prototype of a distributed multimedia database system. It implements an integrated querying, annotating, and navigating framework relying on a generic video indexing model. The framework allows the structuring of videos into logical and physical units, and the annotation of these units by typed semantic objects. An index-database stores these structural and semantic information. We provide further a clear concept for capturing and querying the semantic content of multimedia objects, their correlation with low-level objects, as well as their spatio-temporal relationships.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: standard for multimedia databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john r. smith
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: Research work in programming languages and in database systems is combating the same problems of scale, change and complexity. This paper looks at the present difficulties of relating persistent data with changing programs. It illustrates the influence that the present interfaces have on programming methodology and algorithm design. It recognises the need for new language primitives to encapsulate database concepts and a few putative primitives are examined. It is suggested that such primitives could simplify the use of databases by programmers. These ideas are illustrated with examples from geometric modelling using Algol 68.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 940

LEFT text: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries across diverse data sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laura m. haas , donald kossmann , edward l. wimmers , jun yang
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: We describe the open, extensible architecture of SQL for accessing data stored in external data sources not managed by the SQL engine. In this scenario, SQL engines act as middleware servers providing access to external data using SQL DML statements and joining external data with SQL tables in heterogeneous queries. We describe the state-of-the art in object-relational systems and their companion products, and provide an outlook on future directions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 368

LEFT text: Relational databases supported applications in a centralized environment in the 1960's and 1970's. They progressed to a client/server environment in the 1980's. The 1990's saw application servers with a multi-tiered architecture, in most cases supported by an RDBMS. Most recently we have seen the emergence of XML, XML storage in DBMS's, navigation within an XML document via XPath, and the XQuery query language for XML. In this article, Susan provides an introduction to the Grid and describes how databases will be used in this new environment. The Global Grid Forum (GGF) is producing technical specification to enable both Relational and XML databases to be located, accessed, and replicated in this environment. They make use of a variety of existing an emerging database, file, networking, and web services standards. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: standard for multimedia databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john r. smith
",n
"LEFT id: NA
RIGHT id: 1369

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xperanto : middleware for publishing object-relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael j. carey , jerry kiernan , jayavel shanmugasundaram , eugene j. shekita , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 817

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: probabilistic optimization of top n queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: donko donjerkovic , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 789

LEFT text: Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregation algorithms for very large compressed data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jianzhong li , doron rotem , jaideep srivastava
",n
"LEFT id: NA
RIGHT id: 214

LEFT text: In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: querying network directories

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , laks v. s. lakshmanan , tova milo , divesh srivastava , dimitra vista
",y
"LEFT id: NA
RIGHT id: 1253

LEFT text: Since relational database management systems typically support only diadic join operators as primitive operations, a query optimizer must choose the “best” scquence of two-way joins to achieve the N-way join of tables requested by a query. The computational complexity of this optimization process is dominated by the number of such possible sequences that must bc evaluated by the optimizer. This paper describes and measures the performance of the Starburst join enumerator, which can parameterically adjust for each query the space of join sequences that arc evaluated by the optimizer to allow or disallow (I) composite tables (i.e., tables that are themselves the result of a join) as the inner operand of a join and (2) joins between two tables having no join predicate linking them (i.e., Cartesian products). To limit the size of their optimizer’s search space, most earlier systems excludcd both of these types of plans, which can exccutc significantly faster for some queries. Dy experimentally varying the parameters of the Starburst join enumerator, we have validated analytic formulas for the number of join sequcnccs under a variety of conditions, and have proven their dependence upon the “shape” of the query. Specifically, ‘linear” queries, in which tables arc connectcd by binary predicates in a straight lint, can hc optimized in polynomial time. llence the dynamic programming techniques of System R and R* can still be used to optimize linear queries of as many as 100 tables in a reasonable amount of time! A query optimizer in a relational DRMS translates non-procedural queries into a pr0cedura.l plan for execution, typically hy generating many alternative plans, estimating the execution cost of each, and choosing the plan having the lowest estimated cost. Increasing this set offeasilile plans that it evaluates improves the chances but dots not guarantee! that it will find a bcttct plan, while increasing the (compile-time) cost for it to optimize the query. A major challenge in the design of a query optimizer is to ensure that the set of feasible plans contains cflicient plans without making the :set too big to he gcncratcd practically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing relational queries in connection hypergraphs : nested queries , views , and binding propagations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jia liang han
",y
"LEFT id: NA
RIGHT id: 577

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently instantiating view-objects from remote relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: byung suk lee , gio wiederhold
",y
"LEFT id: NA
RIGHT id: 1522

LEFT text: Publisher Summary The support provided by enTrans can be employed by users to realize long running activities with transaction properties—where the activities access one or more Lightweight Directory Access Protocol (LDAP) servers. The philosophy underlying enTrans allows any standard LDAP server. It also provides for a pluggable and hence customizable integrity constraint manager. Directories were designed for data-intensive applications where reads are more frequent than writes, and they were primarily meant for the standard white and yellow page applications. LDAP is an open industry standard for accessing directory-based information. Due to its natural way of representing data in hierarchical form, efficient read access, and support for heterogeneous data, LDAP is being used for more demanding applications, such as policy enabled networks and identity and access management. While the early LDAP applications mostly involved reads, as directory applications mature, the need for updates is increasing and updates are typically spread across data repositories which store user profiles in a decentralized fashion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 247

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sonar : system for optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1586

LEFT text: This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sence that it changes the replication scheme of the object i.e., the set of processors at which the object inreplicated) as changes occur in the read-write patern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of ta distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorith.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive parallel aggregation algorithms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ambuj shatdal , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1125

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 490

LEFT text: Web servers are increasingly being used to deliver dynamic content rather than static HTML pages. In order to generate web pages dynamically, servers need to execute a script, which typically connects to a DBMS. Although CGI was the first approach at server side scripting, it has significant performance shortcomings. Currently, there are many alternative server side scripting architectures which offer better performance than CGI. In this paper, we report our experiences using mod_perl, an Apache Server module, which can improve the performance of CGI scripts by at least an order of magnitude. Except for presenting results from our experiments, we also briefly describe the implementation of an industrial strength database-backed web site that we recently built and give a quick overview of the various server-side scripting mechanisms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: generating dynamic content at database-backed web servers : cgi-bin vs. mod_perl

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alexandros labrinidis , nick roussopoulos
",y
"LEFT id: NA
RIGHT id: 1262

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: indexing very high-dimensional sparse and quasi-sparse vectors for similarity searches

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: changzhou wang , x. sean wang
",n
"LEFT id: NA
RIGHT id: 1074

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 39

LEFT text: There are also problems in using values obtained from the study populations to those in economic models and the difficulty of predicting health state values in those who avoid a fracture. The review recommends a set of health state values as part of a “reference case” for use in economic models. Due to the paucity of good quality of estimates in this area, further recommendations are made regarding the design of future studies to collect HSVs relevant to economic models.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a database perspective on lotus domino/notes

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: c. mohan
",y
"LEFT id: NA
RIGHT id: 622

LEFT text: The Semantic Web is a vision the idea of having data on the Web defined and linked in such a way that it can be used by machines not just for display purposes but for automation, integration and reuse of data across various applications. Technically, however, there is a widespread misconception that the Semantic Web is primarily a rehash of existing AI and database work focused on encoding knowledge representation formalisms in markup languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler, and Moran seek to dispel this notion by presenting the broad dimensions of this emerging Semantic Web and the multi-disciplinary technological underpinnings like machine learning, information retrieval, service-oriented architectures, and grid computing, thus combining the informational and computational aspects needed to realize the full potential of the Semantic Web vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the &#961; operator : discovering and ranking associations on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kemafor anyanwu , amit sheth
",n
"LEFT id: NA
RIGHT id: 1348

LEFT text: Motivated by this, we propose a new index structure called the TPR*- tree, which takes into account the unique features of dynamic objects through a set of improved construction algorithms. In addition, we provide cost models that determine the optimal performance achievable by any data-partition spatio-temporal access method. Using experimental comparison, we illustrate that the TPR*-tree is nearly-optimal and significantly outperforms the TPR-tree under all conditions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mv3r-tree : a spatio-temporal access method for timestamp and interval queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 428

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards self-tuning data placement in parallel database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 1577

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: acm multimedia '94 conference workshop on multimedia database management systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bruce berra , kingsley nwosu , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 1397

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: feedbackbypass : a new approach to interactive similarity query processing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ilaria bartolini , paolo ciaccia , florian waas
",n
"LEFT id: NA
RIGHT id: 639

LEFT text: The paper describes the ARANEUS Wel-Base Management System [l, 5, 4, 61, a system developed at Universitb di Roma Tre, which represents a proposal towards the definition of a new kind of data-repository, designed to manage Web data in the database style. We call a WebBase a collection of data of heterogeneous nature, and more specifically: (i) highly structured data, such as the ones typically stored in relational or objectoriented database systems; (G) semistructured data, in the Web style.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tigukat : a uniform behavioral objectbase management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu , randal peters , duane szafron , boman irani , anna lipka , adriana mu &#241; oz
",n
"LEFT id: NA
RIGHT id: 1123

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the structured information manager : a database system for sgml documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ron sacks-davis
",y
"LEFT id: NA
RIGHT id: 268

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 2175

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",y
"LEFT id: NA
RIGHT id: 883

LEFT text: The naive solution would be to do an exhaustive search across all possible subsets of items and count how many satisfy the predicate conditions we are looking for. This approach, although it would be efficient space-wise (only store the combinations we need) would waste a lot of time (creating all possible combinations). This paper presents a few algorithms that start with a seed itemset (one that already satisfies the boolean predicates we wish to evaluate) and grow them into itemsets of maximal size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents for selective dissemination of information

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mehmet altinel , michael j. franklin
",n
"LEFT id: NA
RIGHT id: 100

LEFT text: For several years now, you've been hearing and reading about an emerging standard that everybody has been calling SQL3. Intended as a major enhancement of the current second generation SQL standard, commonly called SQL-92 because of the year it was published, SQL3 was originally planned to be issued in about 1996…but things didn't go as planned.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql : 1999 , formerly known as sql3

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: andrew eisenberg , jim melton
",y
"LEFT id: NA
RIGHT id: 1084

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general ""sketch""-based methods for capturing various linear projections and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 2163

LEFT text: The European Commission opens in its 7th IST call for proposals an action line for Semantic Web Technologies. It builds on ideas that have been looming for many years but have received their greatest push when the World Wide Web Consortium set up an interest group on that theme. The Semantic Web aims to make content machine understandable in order to automate a wide range of new tasks within the context of heterogeneous and distributed systems. The action line centres on four aspects: formalisation of the semantics, derivation of attributes, intelligent ltering and information visualisation. The Web is currently a mighty collection of ashy data but diAEcult to exploit. Adding semantics to content and ensuring their interoperability will turn it into an eAEcient knowledge source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the semantic web paving the way to the knowledge society

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pierre-paul sondag
",y
"LEFT id: NA
RIGHT id: 1827

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 473

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: re-designing distance functions and distance-based applications for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 1666

LEFT text: The Teradata Multimedia Object Manager is a general-purpose content analysis multimedia server designed for symmetric multiprocessing and massively parallel processing environments. The Multimedia Object Manager defines and manipulates user-defined functions (UDFs), which are invoked in parallel to analyze or manipulate the contents of multimedia objects. Several computationally intensive applications of this technology, which use large persistent datasets, include fingerprint matching, signature verification, face recognition, and speech recognition/translation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ptool : a scalable persistent object manager

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. l. grossman , x. qin
",n
"LEFT id: NA
RIGHT id: 2117

LEFT text: In many application fields, such as production lines or stock analysis, it is substantial to create and process high amounts of data at high rates. Such continuous data flows with unknown size and end are also called data streams. The processing and analysis of data streams are a challenge for common data management systems as they have to operate and deliver results in real time. Data Stream Management Systems (DSMS), as an advancement of database management systems, have been implemented to deal with these issues. DSMS have to adapt to the notion of data streams on various levels, such as query languages, processing or optimization. In this chapter we give an overview of the basics of data streams, architecture principles of DSMS and the used query languages. Furthermore, we specifically detail data quality aspects in DSMS as these play an important role for various applications based on data streams. Finally, the chapter also includes a list of research and commercial DSMS and their key properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: issues in data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lukasz golab , m. tamer &#214; zsu
",y
"LEFT id: NA
RIGHT id: 14

LEFT text: There has been a lot of talk about how the Internet is going to change the world economy. Companies will come together in a “plug and play” fashion to form trading partner networks. Virtual companies will be established and new business models can be created based on access to information and agents that can carry it around the world using computer networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xml and electronic commerce : enabling the network economy

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bart meltzer , robert glushko
",y
"LEFT id: NA
RIGHT id: 1521

LEFT text: We are pleased to announce an excellent technical program for the 6th International Conference on Pervasive Computing and Communications. The program covers a broad cross section of topics in pervasive computing and communications. This year, 160 papers were submitted for consideration to the program committee. As a result, the selection process was highly competitive, and the result is a program of high-quality papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 321

LEFT text: As object technology is adopted by software systems for analysis and design, language, GUI, and frameworks, the database community also is working to support objects, and to develop standards for that support. A key benefit of object technology is the ability for different objects and object tools to interoperate, so it's critical that such DBMS object standards interoperate with those of the rest of the object world. Starting with a discussion of the new issues objects bring to query standards, we present the efforts of various groups relevant to this, including ODMG, OMG, ANSI X3H2 (SQL3), and recent merger efforts feeding into SQL3. What's different with Objects? 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: object query standards

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: andrew e. wade
",y
"LEFT id: NA
RIGHT id: 1630

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: A load, such as a logic network of the TTL type, is connected in parallel across a plurality of direct-current sources designed to maintain a substantially constant operating voltage. Each source includes a control unit which compares the load voltage with a reference level in order to stabilize the output voltage of an associated current generator at that level. If the generator current drops below a certain minimum value, however, a threshold sensor in the control unit raises the reference level up to an amount equaling about twice the maximum divergence possible between the reference levels of different control units, thereby ensuring that all sources contribute simultaneously to the load current.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 2179

LEFT text: Clustering of large databases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understand the results, which is especially important if the data under consideration is high dimensional and has not been collected for the purpose of being analyzed. Visualization technology may help to solve this problem since it allows an effective support of different clustering paradigms and provides means for a visual inspection of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: actiview : adaptive data presentation using supersql

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yoko maeda , motomichi toyama
",n
"LEFT id: NA
RIGHT id: 1614

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 2093

LEFT text: Most previous solutions to the schema matching problem rely in some fashion upon identifying ""similar"" column names in the schemas to be matched, or by recognizing common domains in the data stored in the schemas. While each of these approaches is valuable in many cases, they are not infallible, and there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are ""opaque"" or very difficult to interpret. In this paper we propose a two-step technique that works even in the presence of opaque column names and data values. In the first step, we measure the pair-wise attribute correlations in the tables to be matched and construct a dependency graph using mutual information as a measure of the dependency between attributes. In the second stage, we find matching node pairs in the dependency graphs by running a graph matching algorithm. We validate our approach with an experimental study, the results of which suggest that such an approach can be a useful addition to a set of (semi) automatic schema matching techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on schema matching with opaque column names and data values

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jaewoo kang , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 1127

LEFT text: A data warehouse is a redundant collection of data replicated from several possibly distributed and loosely coupled source databases, organized to answer OLAP queries. Relational views are used both as a specification technique and as an execution plan for the derivation of the warehouse data. In this position paper, we summarize the versatility of relational views and their potential.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what is the data warehousing problem ? ( are materialized views the answer ? )

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ashish gupta , inderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 339

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a case for dynamic view management

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 811

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: relational databases for querying xml documents : limitations and opportunities

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 340

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distributed query evaluation on semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1458

LEFT text: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic multi-resource load balancing in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: erhard rahm , robert marek
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 268

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 903

LEFT text: This paper describes the ACM Multimedia '94 Conference Workshop on Multimedia Database Management Systems held on 21 October 1994 in San Francisco, California. The workshop consisted of four sessions: designing multimedia database management systems, video and continuous media service, multimedia storage and retrieval management, and miscellaneous topics in multimedia data management. The workshop concluded with a discussion session on directions for multimedia database management. Twenty-eight participants from U.S.A., U.K., Germany, Norway, and Egypt attended the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a multi-paradigm querying approach for a generic multimedia database management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ji-rong wen , qing li , wei-ying ma , hong-jiang zhang
",n
"LEFT id: NA
RIGHT id: 1306

LEFT text: Complex similarity queries, i.e., multi-feature multi-object queries, are needed to express the information need of a user against a large multimedia repository. Even if a user initially issues a single-object query over one feature, a system with relevance feedback will automatically generate a complex similarity query. Relevance feedback is only useful if response times are interactive. Therefore, this article contributes to the important problem how to evaluate such complex queries efficiently. We describe a new evaluation technique called Generalized VA-File-based Search (GeVAS). It builds on the VA-File [27], supports queries over several feature types, and borrows the idea to search an index structure with several query objects in parallel from Ciaccia et al. [8]. Our main contributions are twofold: 1) we show that GeVAS does not degenerate for queries with many objects or many feature types. 2) We develop a number of variants of GeVAS, tailored to the different distance measures and distancecombining functions, and we show that they yield a significant performance improvement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast evaluation techniques for complex similarity queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , michael mlivoncic , hans-j &#246; rg schek , roger weber
",y
"LEFT id: NA
RIGHT id: 917

LEFT text: Research work in programming languages and in database systems is combating the same problems of scale, change and complexity. This paper looks at the present difficulties of relating persistent data with changing programs. It illustrates the influence that the present interfaces have on programming methodology and algorithm design. It recognises the need for new language primitives to encapsulate database concepts and a few putative primitives are examined. It is suggested that such primitives could simplify the use of databases by programmers. These ideas are illustrated with examples from geometric modelling using Algol 68.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: The wonderfully clean and beautiful scheme put ""on its head"" the world of query optimization I had assumed was the only one possible. In fact, this paper is all about questioning implicit assumptions behind classic query optimization. Is it always true that query-evaluation performance does not fluctuate during query execution?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 527

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross , theodore johnson , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 37

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: clustering methods for large databases : from the past to the future

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 1823

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infosleuth : agent-based semantic integration of information in open and dynamic environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. bayardo , jr. , w. bohrer , r. brice , a. cichocki , j. fowler , a. helal , v. kashyap , t. ksiezyk , g. martin , m. nodine , m. rashid , m. rusinkiewicz , r. shea , c. unnikrishnan , a. unruh , d. woelk
",n
"LEFT id: NA
RIGHT id: 135

LEFT text: The World Wide Web (WWW) is a fast growing global information resource. It contains an enormous amount of information and provides access to a variety of services. Since there is no central control and very few standards of information organization or service offering, searching for information and services is a widely recognized problem. To some degree this problem is solved by “search services,” also known as “indexers,” such as Lycos, AltaVista, Yahoo, and others. These sites employ search engines known as “robots” or “knowbots” that scan the network periodically and form text-based indices. These services are limited in certain important aspects. First, the structural information, namely, the organization of the document into parts pointing to each other, is usually lost. Second, one is limited by the kind of textual analysis provided by the “search service.” Third, search services are incapable of navigating “through” forms. Finally, one cannot prescribe a complex database-like search. We view the WWW as a huge database. We have designed a high-level SQL-like language called W3QL to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. We have implemented a system called W3QS to execute W3QL queries. In W3QS, query results are declaratively specified and continuously maintained as views when desired. The current architecture of W3QS provides a server that enables users to pose queries as well as integrate their own data analysis tools. The system and its query language set a framework for the development of database-like tools over the WWW. A significant contribution of this article is in formalizing the WWW and query processing over it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: information gathering in the world-wide web : the w3ql query language and the w3qs system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david konopnicki , oded shmueli
",y
"LEFT id: NA
RIGHT id: 104

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 535

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 623

LEFT text: In the discussion of research issues in spatial databases (SIGMOD Record vol. 19, no. 4, Dec 1990) we stated the need for a robust framework for analytical comparison of a broad range of spatial access methods. The utility of such a comparison, even of very closely related access methods, was shown in [FALO87]. A necessary precondition for a meaningful analytical comparison is the existence of strong analytical results for individual access methods. In the following paper, Salzberg and Lomet take the worst case analytical results on fan-out and average storage utilization they obtained for their hB-tree [LOME89,LOME90] and extend the analysis to another robust method, Z-order encoding [OREN84]. We think this paper is a start on the comparative assessment of access methods based on analytical results. We hope to see future work extend the framework beyond worst case analysis, and to other access methods as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a framework for semantic gossiping

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer , philippe cudr &#233; - mauroux , manfred hauswirth
",n
"LEFT id: NA
RIGHT id: 494

LEFT text: We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an extensible compressor for xml data

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hartmut liefke , dan suciu
",n
"LEFT id: NA
RIGHT id: 1132

LEFT text: Classification is an important data mining problem. Although classification is a well-studied problem, most of the current classi-fication algorithms require that all or a por-tion of the the entire dataset remain perma-nently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algo-rithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data min-ing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sprint : a scalable parallel classifier for data mining

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john c. shafer , rakesh agrawal , manish mehta
",y
"LEFT id: NA
RIGHT id: 1915

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1028

LEFT text: We formulate this task as an inverse problem, specifying a well-defined cost function that has to be optimized under constraints. We show that our formulation includes the uniformity and independence assumptions as a special case, and that it can achieve better reconstruction results if we maximize the smoothness as opposed to the uniformity. In our experiments on real and synthetic datasets, the proposed method almost consistently outperforms its competitor, improving the rootmean-square error by up to 20 per cent for stock price data, and up to 90 per cent for smoother data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: recovering information from summary data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: christos faloutsos , h. v. jagadish , nikolaos sidiropoulos
",y
"LEFT id: NA
RIGHT id: 2185

LEFT text: Cache FusionTM is a fundamental component of Oracle’s Real Application Cluster configuration, a shared-cache clustered-database architecture that transparently extends database applications from single systems to multi-node shared-disk clusters. In classic shared-disk implementations, the disk is the medium for data sharing and data blocks are shipped between nodes through disk writes and reads under the arbitration of a distributed lock manager. Cache Fusion extends this capability of a shared-disk architecture by allowing nodes to share the contents of their volatile buffer caches through the cluster interconnect. Using Cache Fusion, data blocks are shipped directly from one node to another using interconnect messaging, eliminating the need for extra disk I/Os to facilitate data sharing. Cache Fusion thus greatly improves the performance and scalability characteristics of shared-disk clusters while continuing to preserve the availability benefits of shared-disk architectures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache fusion : extending shared-disk clusters with shared caches

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tirthankar lahiri , vinay srihari , wilson chan , n. macnaughton , sashikanth chandrasekaran
",y
"LEFT id: NA
RIGHT id: 512

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 268

LEFT text: Traditional protocols for distributed database management have high message overhead, lock or restrain access to resources during protocol execution, and may become impractical for some scenarios like real-time systems and very large distributed databases. In this paper we present the demarcation protocol; it overcomes these problems through the use of explicit linear arithmetic consistency constraints as the correctness criteria. The method establishes safe limits as “lines drawn in the sand” for updates and gives a way of changing these limits dynamically, enforcing the constraints at all times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 215

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 388

LEFT text: Web services have become a mainstream developmemt in the industry. Their emergence is based on the mechanisms that made Web Browsing popular: the existence of a pervasive networking infrastructure, the widely deployed availability of communication protocols such as IP, TCP, UDP and HTTP, the standardization of XML documents and their display by browsers, and the emergence of higher level transports such as SOAP. In additiom standard services such as UDDI and description languages such as WSDL established a base on which services could be specified, described, and published. Thus, the begning of the inter Web Service communication was founded.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data access ( tutorial session )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , anand deshpande
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 701

LEFT text: As parallel computing has become increasingly common, the need for scalable and efficient ways of storing and locating data has become increasingly acute. For years, both grid and cloud computing have distributed data across machines and even clusters at different geographic locations (sites). However not all sites need all of the data in a particular data set, or have the (perhaps specialized) processing capabilities required. These facts challenge the conventional wisdom that we should always move the computation to the data rather than the data to the computation. Sometimes the data actually required is small. In other cases, the site with specialized processing capabilities (such as a GPU equipped cluster) cannot handle the demands placed on it unless a way is found to let that cluster select the data that is actually needed, even if it is not stored locally. Our system finds partial spatial replicas that intersect with a region of interest.The work is intended to be a key component of a distributed spatial data system.Address the unique problems of accessing subsets of very large spatial datasets.Store R-tree in a relational database and investigate its design and performance.R-tree prefetching, query aggregation and use of a Morton curve in the R-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fractal prefetching b + - trees : optimizing both cache and disk performance

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: shimin chen , phillip b. gibbons , todd c. mowry , gary valentin
",y
"LEFT id: NA
RIGHT id: 1527

LEFT text: MineSetTM is a highly integrated suite of client-server tools for the high-end mining and visualization of very large enterprise databases. MineSet represents the confluence of several important software and hardware technologies: data mining algorithms, fast multiprocessing database servers, novel techniques for interactive 3-D data visualization, and powerful graphics workstations. MineSet provides integrated facilities for the extraction of data from varied sources, algorithms for mining the extracted data, and tools for the 3-D visualization of results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1534

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 594

LEFT text: Database systems offer efficient and reliable technology to query structured data. However, because of the explosion of the World Wide Web [11], an increasing amount of information is stored in repositories organized according to less rigid structures, usually as hypertextual documents, and data access is based on browsing and information retrieval techniques. Since browsing and search engines present important limitations [8], several query languages [19, 20, 23] for the Web have been recently proposed. These approaches are mainly based on a loose notion of structure, and tend to see the Web as a huge collection of unstructured objects, organized as a graph. Clearly, traditional database techniques are of little use in this field, and new techniques need to be developed. In this paper, we present the approach to the management of Web data as attacked in the ArtANEUS project carried out by the database group at Universith di l=toma Tre. Our approach is based on a generalization of the notion of view to the Web framework. In fact, in traditional databases, views represent an essential tool for restructuring and integrating da ta to be presented to the user. Since the Web is becoming a major computing platform and a uniform interface for sharing data, we believe that also in this field a sophisticate view mechanism is needed, with novel features due to the semi-structured nature of the Web. First, in this context, restructuring and presenting da ta under different perspectives requires the generation of derived Web hypertexts, in order to re-organize and re-use portions of the Web. To do this, da ta from existing Web sites must be extracted, and then queried and integrated in order to build new hypertexts, i.e., hypertextual views over the original sites; these manipulations can be better attained in a more structured framework, in which traditional database technology can be leveraged to analyze and correlate information. Therefore, there seem to be different view levels in this framework: (i) at the first level, da ta are extracted from the sites of interest and given a database structure, which represents a first structured view over the original semi-structured data; (ii) then, further database views can be built by means of reorganizations and integrations based on traditional database techniques; (iii) finally, a derived hypertext can be generated offering an alternative or integrated hypertextual view over the original sites. In the process, data go from a loosely structured organizat ion-the Web pages-to a very structured onethe database--and then again to Web structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: describing semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luca cardelli
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic. Probably because many problems remained unsolved, most research works were only able to address separate topics, without a clear context of an overall application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",y
"LEFT id: NA
RIGHT id: 644

LEFT text: As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, τXQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a τXQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach: selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of queries in a mediator for websources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vladimir zadorozhny , louiqa raschid , maria esther vidal , tolga urhan , laura bright
",n
"LEFT id: NA
RIGHT id: 1478

LEFT text: Data on the Internet is increasingly presented in XML format. This enables novel applications that pose queries over “all the XML data on the Internet.” Queries over XML data use path expressions to navigate through the structure of the data, and optimizing these queries requires estimating the selectivity of these path expressions. In this paper, we propose two techniques for estimating the selectivity of simple XML path expressions over complex large-scale XML data as would be handled by Internet-scale applications: path trees and Markov tables. Both techniques work by summarizing the structure of the XML data in a small amount of memory and using this summary for selectivity estimation. We experimentally demonstrate the accuracy of our proposed techniques, and explore the different situations that would favor one technique over the other. We also demonstrate that our proposed techniques are more accurate than the best previously known alternative.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimating the selectivity of spatial queries using the ` correlation ' fractal dimension

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: alberto belussi , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 768

LEFT text: Existing studies on outliers focus only on the identiication aspect; none provides any inten-sional knowledge of the outliers|by which we mean a description or an explanation of why an identiied outlier is exceptional. For many applications, a description or explanation is at least as vital to the user as the identii-cation aspect. Speciically, intensional knowledge helps the user to: (i) evaluate the validity of the identiied outliers, and (ii) improve one's understanding of the data. The two main issues addressed in this paper are: what kinds of intensional knowledge to provide, and how to optimize the computation of such knowledge. With respect to the rst issue, we propose nding strongest and weak outliers and their corresponding structural intensional knowledge. With respect to the second issue, we rst present a naive and a semi-naive algorithm. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining distance-based outliers in large datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1571

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast algorithms for universal quantification in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: goetz graefe , richard l. cole
",n
"LEFT id: NA
RIGHT id: 489

LEFT text: Many database applications require the storage and manipulation of different versions of data objects. To satisfy the diverse needs of these applications, current database systems support versioning at a very low level. This article demonstrates that application-independent versioning can be supported at a significantly higher level. In particular, we extend the EXTRA data model and EXCESS query language so that configurations can be specified conceptually and non-procedurally. We also show how version sets can be viewed multidimensionally, thereby allowing configurations to be expressed at a higher level of abstraction. The resulting model integrates and generalizes ideas in CAD systems, CASE systems, and temporal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: evolution and change in data management - issues and directions

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: john f. roddick , lina al-jadir , leopoldo bertossi , marlon dumas , florida estrella , heidi gregersen , kathleen hornsby , jens lufter , federica mandreoli , tomi m &#228; nnist &#246; , enric mayol , lex wedemeijer
",n
"LEFT id: NA
RIGHT id: 330

LEFT text: Whereas physical database tuning has received a lot of attention over the last decade, logical database tuning seems to be under-studied. We have developed a project called DBA Companion devoted to the understanding of logical database constraints from which logical database tuning can be achieved.In this setting, two main data mining issues need to be addressed: the first one is the design of efficient algorithms for functional dependencies and inclusion dependencies inference and the second one is about the interestingness of the discovered knowledge. In this paper, we point out some relationships between database analysis and data mining. In this setting, we sketch the underlying themes of our approach. Some database applications that could benefit from our project are also described, including logical database tuning.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: i/o reference behavior of production database workloads and the tpc benchmarks-an analysis at the logical level

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: windsor w. hsu , alan jay smith , honesty c. young
",n
"LEFT id: NA
RIGHT id: 1691

LEFT text: We identify an emergent class of database systems that has not been dealt with extensively in the literature that we call ARCS (Active, Rapidly Changing data Systems) databases. These systems impose certain unique requirements on databases that monitor and control them. These requirements are such that traditional data and transaction management models appear inadequate. We present an analysis of data and transaction characteristics in ARCS systems and identify relevant research issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in active database systems : report from the closing panel at ride-ads '94

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jennifer widom
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: Abstract Spatial-Query-by-Sketch is the design of a query language for geographic information systems. It allows a user to formulate a spatial query by drawing the desired configuration with a pen on a touch-sensitive computer screen and translates this sketch into a symbolic representation that can be processed against a geographic database. Since the configurations queried usually do not match exactly the sketch, it is necessary to relax the spatial constraints drawn. This paper describes the representation of a sketch and outlines the design of the constraint relaxation methods used during query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",y
"LEFT id: NA
RIGHT id: 134

LEFT text: This second editionsystematically introduces the notion of ontologies to the non-expert reader and demonstrates in detail how to apply this conceptual framework for improved intranet retrieval of corporate information and knowledge and for enhanced Internet-based electronic commerce. He also describes ontology languages (XML, RDF, and OWL) and ontology tools, and the application of ontologies. In addition to structural improvements, the second edition covers recent developments relating to the Semantic Web, and emerging web-based standard languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: towards a theory of cost management for digital libraries and electronic commerce

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson , yelena yesha , robert sloan
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",y
"LEFT id: NA
RIGHT id: 777

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1150

LEFT text: The InfoSleuth T M Project at MCC has developed a distributed agent architecture that addresses the need for semantic interoperability among information sources and analytical tools within diverse application domains. InfoSleuth is being used as a significant component of the Environmental Data Exchange Network (EDEN) 2. The current EDEN pilot demonstration enables integrated access via web browser to environmental information resources provided by offices of these agencies located in several states. At the application level, InfoSleuth provides for semantic interchange among users by allowing an application developer to express the concepts and relationships of the application domain in high-level terms that are then translated into the low-level types of database schemas or semantic analyses of text and image resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 1817

LEFT text: Traditionally, database systems have been evaluated in isolation on the basis of standardized benchmarks (e.g., Wisconsin, TPC-C, TPC-D). We argue that very often such a performance analysis does not reflect the actual use of the DBMSs in the “real world.” End users typically don't access a stand-alone database system; rather they use a comprehensive application system, in which the database system constitutes an integrated component. In order to derive performance evaluations of practical relevance to the end users, the application system including the database system has to be benchmarked. In this paper, we present TPC-D benchmark results carried out using the SAP R/3 system, an integrated business administration system. Like many other application systems SAP R/3 is based on a commercial relational database system. We compare the SAP R/3 benchmark results with TPC-D results of an isolated database system, the database product that served as SAP R/3's back-end.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database performance in the real world : tpc-d and sap r/3

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joachen doppelhammer , thomas h &#246; ppler , alfons kemper , donald kossmann
",y
"LEFT id: NA
RIGHT id: 1715

LEFT text: We describe the design and implementation of the Glue-Nail deductive database system. Nail is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code are both compiled into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm and supports well-founded models. The Glue compiler's static optimizer uses peephole techniques and data flow analysis to improve code.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 943

LEFT text: In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: merging ranks from heterogeneous internet sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: luis gravano , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1676

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as a deductive database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 942

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast computation of sparse datacubes

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1291

LEFT text: Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to the self-similarity of network traffic. We present a hypothesized explanation for the possible self-similarity of traffic by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we examine the dependence structure of WWW traffic. While our measurements are not conclusive, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: self-similarity in the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: stephen dill , ravi kumar , kevin s. mccurley , sridhar rajagopalan , d. sivakumar , andrew tomkins
",y
"LEFT id: NA
RIGHT id: 1614

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 838

LEFT text: In this paper, I describe first the background behind the development of the original ARIES recovery method, and its significant impact on the commercial world and the research community. Next, I provide a brief introduction to the various concurrency control and recovery methods in the ARIES family of algorithms. Subsequently, I discuss some of the recent developments affecting the transaction management area and what these mean for the future. In ARIES, the concept of repeating history turned out to be an important paradigm. As I examine where transaction management is headed in the world of the internet, I observe history repeating itself in the sense of requirements that used to be considered significant in the mainframe world (e.g., performance, availability and reliability) now becoming important requirements of the broader information technology community as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: repeating history beyond aries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: c. mohan
",y
"LEFT id: NA
RIGHT id: 356

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic content acceleration : a caching solution to enable scalable dynamic web page generation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anindya datta , kaushik dutta , krithi ramamritham , helen thomas , debra vandermeer
",n
"LEFT id: NA
RIGHT id: 2023

LEFT text: While the XML Stylesheet Language for Transformations (XSLT) was not designed as a query language, it is well-suited for many query-like operations on XML documents including selecting and restructuring data. Further, it actively fulfills the role of an XML query language in modern applications and is widely supported by application platform software. However, the use of database techniques to optimize and execute XSLT has only recently received attention in the research community. In this paper, we focus on the case where XSL transformations are to be run on XML documents defined as views of relational databases. For a subset of XSLT, we present an algorithm to compose a transformation with an XML view, eliminating the need for the XSLT execution. We then describe how to extend this algorithm to handle several additional features of XSLT, including a proposed approach for handling recursion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: trex : dtd-conforming xml to xml transformations

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: aoying zhou , qing wang , zhimao guo , xueqing gong , shihui zheng , hongwei wu , jianchang xiao , kun yue , wenfei fan
",n
"LEFT id: NA
RIGHT id: 72

LEFT text: Incremental refresh of a materialized join view is often less expensive than a full, non-incremental refresh. However, it is still a potentially costly atomic operation. This paper presents an algorithm that performs incremental view maintenance as a series of small, asynchronous steps. The size of each step can be controlled to limit contention between the refresh process and concurrent operations that access the materialized view or the underlying relations. The algorithm supports point-in-time refresh, which allows a materialized view to be refreshed to any time between the last refresh and the present.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: how to roll a join : asynchronous incremental view maintenance

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth salem , kevin beyer , bruce lindsay , roberta cochrane
",y
"LEFT id: NA
RIGHT id: 1622

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 828

LEFT text: In a lazy master replicated database, a transaction can commit after updating one replica copy (primary copy) at some master node. After the transaction commits, the updates are propagated towards the other replicas (secondary copies), which are updated in separate refresh transactions. A central problem is the design of algorithms that maintain replica's consistency while at the same time minimizing the performance degradation due to the synchronization of refresh transactions. In this paper, we propose a simple and general refreshment algorithm that solves this problem and we prove its correctness. The principle of the algorithm is to let refresh transactions wait for a certain “deliver time” before being executed at a node having secondary copies. We then present two main optimizations to this algorithm. One is based on specific properties of the topology of replica distribution across nodes. In particular, we characterize the nodes for which the deliver time can be null. The other improves the refreshment algorithm by using an immediate update propagation strategy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for maintaining replica consistency in lazy master replicated databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: esther pacitti , pascale minet , eric simon
",y
"LEFT id: NA
RIGHT id: 1638

LEFT text: Hash-based scalable distributed data structures (SDDSs), like LH* and DDH, for networks of interconnected computers (multicomputers) were shown to open new perspectives for file management. We propose a family of ordered SDDSs, called RP*, providing for ordered and dynamic files on multicomputers, and thus for more efficient processing of range queries and of ordered traversals of files. The basic algorithm termed RP*N, builds the file with the same key space partitioning as a B-tree, but avoids indexes through the use of multicast. The algorithms, RP*C and RP*S enhance throughput for faster networks, adding the indexes on clients, or on clients and servers, while either decreasing or avoiding multicast. RP* files are shown highly efficient with access performance exceeding traditional files by an order of magnitude or two, and, for non-range queries, very close to LH*.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distributing a search tree among a growing number of processors

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: brigitte kr &#246; ll , peter widmayer
",y
"LEFT id: NA
RIGHT id: 1526

LEFT text: From beginning of to the end of the IRO DB ESPRIT project has developed tools for accessing relational and object oriented databases in an integrated way The sys tem is based on the ODMG standard as pivot model and language It consists of three lay ers The local layer provides for an ODMG in terface to heterogeneous DBMSs the commu nication layer implements object oriented re mote data access and the interoperable layer supports design and querying of integrated views This paper describes the architecture and main design choices of IRO DB and re views them against the experiences gained with implementation and application It con cludes with analyzing the revisions and ex tensions needed for applying the developed technology to inter and intranet federations which are tackled in the follow up ESPRIT project MIRO Web

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efendi : federated database system of cadlab

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: e. radeke , r. b &#246; ttger , b. burkert , y. engel , g. kachel , s. kolmschlag , d. nolte
",n
"LEFT id: NA
RIGHT id: 2155

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: large databases for remote sensing and gis

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: a. r. dasgupta
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: This paper reports on experience obtained during the design, implementation and use of a multi-paradigm query interface to an object-oriented database. The specific system which has been developed allows equivalent data retrieval tasks to be expressed using textual, form-based and graph-based notations, and supports automatic translation of queries between these three paradigms. The motivation behind the development of such an interface is presented, as is the software architecture which supports the multi-paradigm functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1166

LEFT text: Businesses today are searching for information solutions that enable them to compete in the global marketplace. To minimize risk, these solutions must build on existing investments, permit the best technology to be applied to the problem, and be manageable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the impact of object technology on commercial transaction processing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: edward e. cobb
",y
"LEFT id: NA
RIGHT id: 1596

LEFT text: We propose a novel data model and its language for querying object-oriented databases where objects may hold spatial, temporal or constraint data, conceptually represented by linear equality and inequality constraints. The proposed LyriC language is designed to provide a uniform and flexible framework for diverse application realms such as (1) constraint-based design in two-, three-, or higher-dimensional space, (2) large-scale optimization and analysis, based mostly on linear programming techniques, and (3) spatial and geographic databases. LyriC extends flat constraint query languages, especially those for linear constraint databases, to structurally complex objects. The extension is based on the object-oriented paradigm, where constraints are treated as first-class objects that are organized in classes. The query language is an extension of the language XSQL, and is built around the idea of extended path expressions. Path expressions in a query traverse nested structures in one sweep. Constraints are used in a query to filter stored constraints and to create new constraint objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the lyric language : querying constraint objects

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: alexander brodsky , yoram kornatzky
",y
"LEFT id: NA
RIGHT id: 979

LEFT text: The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the cubetree storage organization

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nick roussopoulos , yannis kotidis
",y
"LEFT id: NA
RIGHT id: 753

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a pictorial query language for querying geographic databases using positional and olap operators

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: elaheh pourabbas , maurizio rafanelli
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 316

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: generating efficient plans for queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: foto n. afrati , chen li , jeffrey d. ullman
",n
"LEFT id: NA
RIGHT id: 947

LEFT text: On-Line Analytical Processing (OLAP) and Data Warehousing are decision support technologies. Their goal is to enable enterprises to gain competitive advantage by exploiting the ever-growing amount of data that is collected and stored in corporate databases and files for better and faster decision making. Over the past few years, these technologies have experienced explosive growth, both in the number of products and services offered, and in the extent of coverage in the trade press. Vendors, including all database companies, are paying increasing attention to all aspects of decision support.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental organization for data recording and warehousing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: h. v. jagadish , p. p. s. narayan , s. seshadri , s. sudarshan , rama kanneganti
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules. This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides SQL-like operator, named MINE RULE, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1121

LEFT text: XML is becoming the most relevant new standard for data representation and exchange on the WWW. Novel languages for extracting and restructuring the XML content have been proposed, some in the tradition of database query languages (i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query language has yet been decided, but the discussion is ongoing within the World Wide Web Consortium and within many academic institutions and Internet-related major companies. We present a comparison of five, representative query languages for XML, highlighting their common features and differences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation and analysis of a parallel collection query language

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: Over the last few years, the information technology industry has witnessed revolutions in multiple dimensions. Increasing ubiquitous sources of data have posed two connected challenges to data management solutions -- processing unprecedented volumes of data, and providing ad-hoc real-time analysis in mainstream production data stores without compromising regular transactional workload performance. In parallel, computer hardware systems are scaling out elastically, scaling up in the number of processors and cores, and increasing main memory capacity extensively. The data processing challenges combined with the rapid advancement of hardware systems has necessitated the evolution of a new breed of main-memory databases optimized for mixed OLTAP environments and designed to scale.    The Oracle RDBMS In-memory Option (DBIM) is an industry-first distributed dual format architecture that allows a database object to be stored in columnar format in main memory highly optimized to break performance barriers in analytic query workloads, simultaneously maintaining transactional consistency with the corresponding OLTP optimized row-major format persisted in storage and accessed through database buffer cache. In this paper, we present the distributed, highly-available, and fault-tolerant architecture of the Oracle DBIM that enables the RDBMS to transparently scale out in a database cluster, both in terms of memory capacity and query processing throughput. We believe that the architecture is unique among all mainstream in-memory databases. It allows complete application-transparent, extremely scalable and automated distribution of Oracle RDBMS objects in-memory across a cluster, as well as across multiple NUMA nodes within a single server. It seamlessly provides distribution awareness to the Oracle SQL execution framework through affinitized fault-tolerant parallel execution within and across servers without explicit optimizer plan changes or query rewrites.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1208

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the gmap : a versatile tool for physical data independence

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: odysseas g. tsatalos , marvin h. solomon , yannis e. ioannidis
",y
"LEFT id: NA
RIGHT id: 1834

LEFT text: This paper presents a number of new techniques for parallelizing geo-spatial database systems and discusses their implementation in the Paradise object-relational database system. The effectiveness of these techniques is demonstrated using a variety of complex geo-spatial queries over a 120 GB global geo-spatial data set.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: building a scaleable geo-spatial dbms : technology , implementation , and evaluation

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jignesh patel , jiebing yu , navin kabra , kristin tufte , biswadeep nag , josef burger , nancy hall , karthikeyan ramasamy , roger lueder , curt ellmann , jim kupsch , shelly guo , johan larson , david de witt , jeffrey naughton
",y
"LEFT id: NA
RIGHT id: 546

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management. Even though the idea of randomized linear projections (a.k.a., sketches) was known for some time in the domain of functional analysis (dating back to the famous Johnson-Lindenstrauss Lemma), Alon, Matias, and Szegedy were the first to exploit sketches for small-space data-stream computation, through the use of limited-independence random variates that can be constructed in small space and time. Of course, in addition to small-space sketching, the AMS paper also makes a number of other fundamental contributions in data streaming, including practical approximation algorithms for other frequency moments (e.g., the number of distinct values in a stream), as well as several inapproximability results (i.e., lower bounds) based on beautiful communication-complexity arguments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1857

LEFT text: Publisher Summary  Traditional databases allow for the storage and retrieval of large amounts of data, but do not make any concessions for uncertainty in the data. In many domains, it is difficult, if not impossible, to state all information with 100% certainty. Scientific research, for example, is subject to a great deal of uncertainty and error that cannot be modeled by traditional database systems. Error-prone experimental machinery, polluted samples, and simple human error are a few of the many possible sources of this uncertainty. With the recent importance of the web, and the many textual (and HTML encoded) sources of information that it makes available, information extraction has become a hot area. The idea is to use natural language analysis tools to create structured representations of free-form text documents. This information extraction is an error-prone endeavor: even the best systems can only hope to be right part of the time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probview : a flexible probabilistic database system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , nicola leone , robert ross , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: Though the query is posted in key words, the returned results contain exactly the information that the user is querying for, which may not be explicitly specified in the input query. The required information is often not contained in the Web pages whose URLs are returned by a search engine. FACT is capable of navigating in the neighborhood of these pages to find those that really contain the queried segments. The system does not require a prior knowledge about users such as user profiles or preprocessing of Web pages such as wrapper generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1218

LEFT text: The rapid growth of the Internet and support for interoperability protocols has increased the number of Web accessible sources, WebSources. Current wrapper mediator architectures need to be extended with a wrapper cost model (WCM) for WebSources that can estimate the response time (delays) to access sources as well as other relevant statistics. In this paper, we present a Web prediction tool (WebPT), a tool that is based on learning using query feedback from WebSources. The WebPT uses dimensions time of day, day, and quantity of data, to learn response times from a particular WebSource, and to predict the expected response time (delay) for some query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: learning response time for websources using query feedback and application in query optimization

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jean-robert gruser , louiqa raschid , vladimir zadorozhny , tao zhan
",y
"LEFT id: NA
RIGHT id: 1315

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate string joins in a database ( almost ) for free

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luis gravano , panagiotis g. ipeirotis , h. v. jagadish , nick koudas , s. muthukrishnan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 295

LEFT text: This paper describes the Dwarf structure and the Dwarf cube construction algorithm. Further optimizations are then introduced for improving clustering and query performance. Experiments with the current implementation include comparisons on detailed measurements with real and synthetic datasets against previously published techniques. The comparisons show that Dwarfs by far out-perform these techniques on all counts: storage space, creation time, query response time, and updates of cubes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: space-efficient online computation of quantile summaries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: michael greenwald , sanjeev khanna
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1228

LEFT text: We consider the problem of evaluating a large number of XPath expressions on an XML stream. Our main contribution consists in showing that Deterministic Finite Automata (DFA) can be used effectively for this problem: in our experiments we achieve a throughput of about 5.4MB/s, independent of the number of XPath expressions (up to 1,000,000 in our tests). The major problem we face is that of the size of the DFA. Since the number of states grows exponentially with the number of XPath expressions, it was previously believed that DFAs cannot be used to process large sets of expressions. We make a theoretical analysis of the number of states in the DFA resulting from XPath expressions, and consider both the case when it is constructed eagerly, and when it is constructed lazily. Our analysis indicates that, when the automaton is constructed lazily, and under certain assumptions about the structure of the input XML data, the number of states in the lazy DFA is manageable. We also validate experimentally our findings, on both synthetic and real XML data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an xml query engine for network-bound data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: zachary g. ives , a. y. halevy , d. s. weld
",y
"LEFT id: NA
RIGHT id: 534

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial : charter and scope

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1796

LEFT text: herefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enhanced hypertext categorization using hyperlinks

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: soumen chakrabarti , byron dom , piotr indyk
",y
"LEFT id: NA
RIGHT id: 848

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 205

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional selectivity estimation using compressed histogram information

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ju-hong lee , deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 142

LEFT text: With the spreading of the World Wide Web as a uniform and ubiquitous interface to computer applications and information, novel opportunities are offered for introducing significant changes in all organizations and their processes. This demo presents the IDEA Web Laboratory (Web Lab), a Web-based software design environment available on the Internet, which demonstrates a novel approach to the software production process on the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the idea web lab

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",y
"LEFT id: NA
RIGHT id: 542

LEFT text: We are living in an exciting time in the field of clinical research. There are changes occurring that will alter how medical care is provided, and it is the work of clinical research professionals that will lead these changes. In this column, the Chair of ACRP’s Association Board of Trustees considers the impact of genetics in medicine and their application through precision medicine. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: This tutorial presents the primary constructs of the consensus temporal query language TSQL2 via a media planning scenario. Media planning is a series of decisions involved in the delivery of a promotional message via mass media. We will follow the planning of a particular advertising campaign. We introduce the scenario by identifying the marketing objective. The media plan involves placing commercials, and is recorded in a temporal database. The media plan must then be evaluated; we show how TSQL2 can be used to derive information from the stored data. We then give examples that illustrate storing and querying indeterminate information, comparing multiple versions of the media plan, accommodating changes to the schema, and vacuuming a temporal database of old data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is 𝒩𝒫-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 2273

LEFT text: In this paper, we present an image retrieval system that employs both the color and spatial information of images to facilitate the retrieval process. The basic unit used in our technique is a single-colored cluster, which bounds a homogeneous region of that color in an image. Two clusters from two images are similar if they are of the same color and overlap in the image space. The number of clusters that can be extracted from an image can be very large, and it affects the accuracy of retrieval. We study the effect of the number of clusters on retrieval effectiveness to determine an appropriate value for “optimal'' performance. To facilitate efficient retrieval, we also propose a multi-tier indexing mechanism called the Sequenced Multi-Attribute Tree (SMAT).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental indexing for full-text information retrieval

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eric w. brown , james p. callan , w. bruce croft
",n
"LEFT id: NA
RIGHT id: 616

LEFT text: We show that adaptive agents on the Internet can learn to exploit bidding agents who use a (limited) number of fixed strategies. These learning agents can be generated by adapting a special kind of finite automata with evolutionary algorithms (EAs). Our approach is especially powerful if the adaptive agent participates in frequently occurring micro-transactions, where there is sufficient opportunity for the agent to learn online from past negotiations. More in general, results presented in this paper provide a solid basis for the further development of adaptive agents for Internet applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: emergent semantics and the multimedia semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: w. i. grosky , d. v. sreenath , f. fotouhi
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: Those of us who were able to come to our Annual Meeting in Cancun had a most enjoyable time. Thanks to Laurie Levinson, Janet Szydlo, and their Program Committee, the scientific sessions were stimulating and productive ones. Our three English guests — Mark Solms, Peter Fonagy, and Mary Target — presented informative and challenging papers and both Peter Blos’ discussion of Solms’ paper and the ensuing discussion groups raised many important issues about the mind-body relationship in psychosomatic conditions in children.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 729

LEFT text: Spatial indexing has been one of the active focus areas in recent database research. Several variants of Quadtree and R-tree indexes have been proposed in database literature. In this paper, we first describe briefly our implementation of Quadtree and R-tree index structures and related optimizations in Oracle Spatial. We then examine the relative merits of two structures as implemented in Oracle Spatial and compare their performance for different types of queries and other operations. Finally, we summarize experiences with these different structures in indexing large GIS datasets in Oracle Spatial.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: quadtree and r-tree indexes in oracle spatial : a comparison using gis data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ravi kanth v kothuri , siva ravada , daniel abugov
",y
"LEFT id: NA
RIGHT id: 1751

LEFT text: Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 395

LEFT text: Indexes and access methods have been a staple of database research – and indeed of computer science in general – for decades. A glance at the contents of this year’s SIGMOD and PODS proceedings shows another bumper crop of indexing papers. Given the hundreds of indexing papers published in the database literature, a pause for reflection seems in order. From a scientific perspective, it is natural to ask why definitive indexing solutions have eluded us for so many years. What is the grand challenge in indexing? What basic complexities or intricacies underlie this large body of work? What would constitute a successful completion of this research agenda, and what steps will best move us in that direction? Or is it the case that the problem space branches in so many ways that we should expect to continuously need to solve variants of the indexing problem? From the practitioner’s perspective, the proliferation of indexing solutions in the literature may be more confusing than helpful. Comprehensively evaluating the research to date is a near-impossible task.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: index research ( panel session ) : forest or trees ?

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , hans-peter kriegel , david comet , christos falsutsos , raghu ramakrishnan , paul brown
",y
"LEFT id: NA
RIGHT id: 528

LEFT text: This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: top-k selection queries over relational databases : mapping strategies and performance evaluation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 385

LEFT text: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an approximate search engine for structural databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jason t. l. wang , xiong wang , dennis shasha , bruce a. shapiro , kaizhong zhang , qicheng ma , zasha weinberg
",n
"LEFT id: NA
RIGHT id: 1998

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 2058

LEFT text: The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a few one-pass clustering algorithms have been developed for the data stream problem. Although such methods address the scalability issues of the clustering problem, they are generally blind to the evolution of the data and do not address the following issues: (1) The quality of the clusters is poor when the data evolves considerably over time. (2) A data stream clustering algorithm requires much greater functionality in discovering and exploring clusters over different portions of the stream.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for diagnosing changes in evolving data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 2152

LEFT text: Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1379

LEFT text: EAEcient support for set-valued attributes is likely to grow in importance as object-relational database systems, which either support set-valued attributes or propose to do so soon, begin to replace their purely relational predecessors. One of the most interesting and challenging operations on set-valued attributes is the set containment join, because it provides a concise and elegant way to express otherwise complex queries. Unfortunately, evaluating these joins is diAEcult, and naive approaches lead to algorithms that are very expensive. In this paper, we develop a new partition based algorithm for set containment joins: the Partitioning Set Join Algorithm (PSJ), which uses a replicating multilevel partitioning scheme based on a combination of set elements and signatures. We present a detailed performance study with a complete implementation in the Paradise object-relational database system. Our results show that PSJ outperforms previously proposed set join algorithms over a wide range of data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: set containment joins : the good , the bad and the ugly

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: karthikeyan ramasamy , jignesh m. patel , jeffrey f. naughton , raghav kaushik
",y
"LEFT id: NA
RIGHT id: 1983

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1858

LEFT text: Over the last decade, a dramatic increase has been observed in the ability of individual experimental scientists to generate and store data, which has not been matched by an equivalent development of adequate data management tools. In this paper, we present the results of our efforts to develop a Desktop Experiment Management Environment that many experimental scientists would like to have on their desk. The environment is called ZOO and is developed in collaboration with domain scientists from Soil Sciences and Biochemistry. We first describe the overall architecture of ZOO, and then focus on key features of its various components. We specifically emphasize aspects of the object-oriented database server that is at the core of the system, the experimentation manager that initiates the execution of experiments as a result of scientists’ requests, and the mechanisms that the modules of the system use to communicate between them. Finally, we briefly discuss our experiences with the use of the current ZOO prototype in the context of plant-growth simulation experiments and NMR spectroscopy experiments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: zoo : a desktop experiment management environment

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , miron livny , anastassia ailamaki , anand narayanan , andrew therber
",y
"LEFT id: NA
RIGHT id: 780

LEFT text: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 921

LEFT text: We propose a product specification database which is suited to product evolution, modeling the product specification as an object. In this database, we propose a behavioral constraint to maintain consistency. Furthermore, this database can manage visual specification, such as operational specification, which is hard to handle in an ordinary database. We have been developing Visual CASE: an object-oriented software development system for home appliances. Visual CASE is a visual prototyping system based on the object model we propose. In this paper, we show that the product specification is easy to examine, using visual prototyping. We also discuss implementation issues of the database applied to the home appliance software development process. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a database approach to quality of service specification in video databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: elisa bertino , ahmed k. elmagarmid , mohand-sa &#239; d hacid
",n
"LEFT id: NA
RIGHT id: 1306

LEFT text: Bulk loading refers to the process of creating an index from scratch for a given data set. This problem is well understood for B-trees, but so far, non-traditional index structures received modest attention. We are particularly interested in fast generic bulk loading techniques whose implementations only employ a small interface that is satisfied by a broad class of index structures. Generic techniques are very attractive to extensible database systems since different user-implemented index structures implementing that small interface can be bulk-loaded without any modification of the generic code. The main contribution of the paper is the proposal of two new generic and conceptually simple bulk loading algorithms. These algorithms recursively partition the input by using a main-memory index of the same type as the target index to be build. In contrast to previous generic bulk loading algorithms, the implementation of our new algorithms turns out to be much easier. Another advantage is that our new algorithms possess fewer parameters whose settings have to be taken into consideration. An experimental performance comparison is presented where different bulk loading algorithms are investigated in a system-like scenario. Our experiments are unique in the sense that we examine the same code for different index structures (R-tree and Slim-tree). The results consistently indicate that our new algorithms outperform asymptotically worst-case optimal competitors. Moreover, the search quality of the target index will be better when our new bulk loading algorithms are used.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast evaluation techniques for complex similarity queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , michael mlivoncic , hans-j &#246; rg schek , roger weber
",n
"LEFT id: NA
RIGHT id: 2249

LEFT text: The property management database system under development at the Northern Ireland Housing Executive (NIHE) is a large relational database system. The application system has a high expected transaction processing rate approximately 37000 transactions per day (most of them accessing mutliple tables) from about 250 on-line users. Performance is of critical importance in its success. In this paper we consider the effect of the Ingres Search Accelerator on the transaction processing efficiency of the system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an empirical performance study of the ingres search accelerator for a large property management database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: sarabjot s. anand , david a. bell , john g. hughes
",y
"LEFT id: NA
RIGHT id: 413

LEFT text: Though the query is posted in key words, the returned results contain exactly the information that the user is querying for, which may not be explicitly specified in the input query. The required information is often not contained in the Web pages whose URLs are returned by a search engine. FACT is capable of navigating in the neighborhood of these pages to find those that really contain the queried segments. The system does not require a prior knowledge about users such as user profiles or preprocessing of Web pages such as wrapper generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fact : a learning based web query processing system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: songting chen , yanlei diao , hongjun lu , zengping tian
",y
"LEFT id: NA
RIGHT id: 139

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1848

LEFT text: They can reduce the need to scan large tables in joins. Results from the customer benchmark demonstrate their usefulness. We also describe hashed groupings, which eliminate sorts to form groups for subsequent aggregation, Hashed groupings allow execution of queries in the benchmark that were previously impossible. Maintenance of the decision support database (updates, addition of indexes, changes in its physical layout) must be performed regularly, and it is increasingly desirable that the database be available during these operations. To this end, Tandem is introducing a host of new on-line data management operations, including data partition adds, drops, splits and moves, as well as on-line index creation. We describe the implementation of partition moves as an example. The basic idea is to move a “dirty” copy of the data, and then to bring the new copy up to date by applying log records describing the effects of transactions that took place during the move. Other on-line data management operations are similar

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data warehousing and olap for decision support

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 113

LEFT text: The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda — broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the asilomar report on database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phil bernstein , michael brodie , stefano ceri , david dewitt , mike franklin , hector garcia-molina , jim gray , jerry held , joe hellerstein , h. v. jagadish , michael lesk , dave maier , jeff naughton , hamid pirahesh , mike stonebraker , jeff ullman
",y
"LEFT id: NA
RIGHT id: 503

LEFT text: This paper discusses issues related to the integration of spatial operators into the new generation of SQL-like query languages. Starting from spatial data models, current spatial extensions of query languages are briefly reviewed and research directions are highlighted. A taxonomy of requirements to be satisfied by spatial operators is proposed with emphasis on users' needs and on the introduction of data uncertainty support. Further, spatial operators are classified into the three important categories of topological, projective, and metric operators and for each of them the state of the art is outlined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: spatial operators

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: eliseo clementini , paolino di felice
",y
"LEFT id: NA
RIGHT id: 1114

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 78

LEFT text: In this paper we propose an approach that enables mobile clients to determine the validity of previous queries based on their current locations. In order to make this possible, the server returns in addition to the query result, a validity region around the client's location within which the result remains the same. We focus on two of the most common spatial query types, namely nearest neighbor and window queries, define the validity region in each case and propose the corresponding query processing algorithms. In addition, we provide analytical models for estimating the expected size of the validity region. Our techniques can significantly reduce the number of queries issued to the server, while introducing minimal computational and network overhead compared to traditional spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: closest pair queries in spatial databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: antonio corral , yannis manolopoulos , yannis theodoridis , michael vassilakopoulos
",n
"LEFT id: NA
RIGHT id: 1484

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a transaction replication scheme for a replicated database with node autonomy

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ada wai-chee fu , david wai-lok cheung
",n
"LEFT id: NA
RIGHT id: 1207

LEFT text: Abstract Many modern programming languages support basic generics, sufficient to implement type-safe polymorphic containers. Some languages have moved beyond this basic support, and in doing so have enabled a broader, more powerful form of generic programming. This paper reports on a comprehensive comparison of facilities for generic programming in eight programming languages: C++, Standard ML, Objective Caml, Haskell, Eiffel, Java, C# (with its proposed generics extension), and Cecil. By implementing a substantial example in each of these languages, we illustrate how the basic roles of generic programming can be represented in each language. We also identify eight language properties that support this broader view of generic programming: support for multi-type concepts, multiple constraints on type parameters, convenient associated type access, constraints on associated types, retroactive modeling, type aliases, separate compilation of algorithms and data structures, and implicit argument type deduction for generic algorithms. We find that these features are necessary to avoid awkward designs, poor maintainability, and painfully verbose code. As languages increasingly support generics, it is important that language designers understand the features necessary to enable the effective use of generics and that their absence can cause difficulties for programmers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the design and implementation of k : a high-level knowledge-base programming language of osam * . kbms

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yuh-ming shyy , javier arroyo , stanley y.w. su , herman lam
",y
"LEFT id: NA
RIGHT id: 1588

LEFT text: Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the merge/purge problem for large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: mauricio a. hern &#225; ndez , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 1828

LEFT text: In this paper, we first focus our attention on the question of how much space remains for performance improvement over current association rule mining algorithms. Our strategy is to compare their performance against an “Oracle algorithm” that knows in advance the identities of all frequent itemsets in the database and only needs to gather their actual supports to complete the mining process. Our experimental results show that current mining algorithms do not perform uniformly well with respect to the Oracle for all database characteristics and support thresholds. In many cases there is a substantial gap between the Oracle’s performance and that of the current mining algorithms. Second, we present a new mining algorithm, called ARMOR, that is constructed by making minimal changes to the Oracle algorithm. ARMOR consistently performs within a factor of two of the Oracle on both real and synthetic datasets over practical ranges of support specifications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: association rules over interval data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. miller , y. yang
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 1130

LEFT text: Specifically, we use state-of-the-art concepts from morphology, n;,mely the ‘pattern spectrum’ of a shape, to map each shape to a point in n-dimensional space. FollowingThis text is a guide to the foundations of method engineering, a developing field concerned with the definition of techniques for designing software systems. The approach is based on metamodeling, the construction of a model about a collection of other models. The book applies the metamodeling approach in five case studies, each describing a solution to a problem in a specific domain. Suitable for classroom use, the book is also useful as a reference for practitioners. The book first presents the theoretical basis of metamodeling for method engineering, discussing information modeling, the potential of metamodeling for software systems development, and the introduction of the metamodeling tool ConceptBase. , we organize the n-d points in an R-tree. We show that the L, (= max) norm in the n-d space lower-bounds the actual distance. This guarantees no false dismissals for range queries. In addition, we present a nearest neighbor algorithm that also guarantees no false dismissals.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",y
"LEFT id: NA
RIGHT id: 1799

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1505

LEFT text: We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation.We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we suggest heuristics that can be used to efficiently determine a useful set of additional views to materialize.Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient maintenance of materialized mediated views

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: The advent of new database applications such as engineering design stresses the need for new functie nalities in database systems. It includes the management of multiple representations for database objects, long transactions as well as dynamic data structures. This paper presents the approach used in CADB, a prototype expert database system dedicated to CAD, for the management and control of the consistency of design objects. It concerns both the operations on the object property values and the interactive manipulation of their structure. They involve concepts of the object models as well as the application semantics. They rely therefore on concepts used for representing the design objects and on semantic notions related to expert knowledge in the application domain. Among these are the notions of consistency and of completeness of the objects. These two complementary aspects are detailed and their relationships described. The emphasis is on the heuristic rules that provide a unified knowledge-based approach for their management independent of the particular application being considered. Dynamic inheritance mechanisms are also presented that sup port the manipulations performed on the object structures. It is shown how they help providing expert database facilities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 1864

LEFT text: Data warehouses store materialized views over base data from external sources. Clients typically perform complex read-only queries on the views. The views are refreshed periodically by maintenance transactions, which propagate large batch updates from the base tables. In current warehousing systems, maintenance transactions usually are isolated from client read activity, limiting availability and/or size of the warehouse. We describe an algorithm called 2VNL that allows warehouse maintenance transactions to run concurrently with readers. By logically maintaining two versions of the database, no locking is required and serializability is guaranteed. We present our algorithm, explain its relationship to other multi-version concurrency control algorithms, and describe how it can be implemented on top of a conventional relational DBMS using a query rewrite approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the whips prototype for data warehouse creation and maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wilburt j. labio , yue zhuge , janet l. wiener , himanshu gupta , h &#233; ctor garc &#237; a-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 205

LEFT text: The extensible mark-up language (XML) is gaining widespread use as a format for data exchange and storage on the World Wide Web. Queries over XML data require accurate selectivity estimation of path expressions to optimize query execution plans. Selectivity estimation of XML path expression is usually done based on summary statistics about the structure of the underlying XML repository. All previous methods require an off-line scan of the XML repository to collect the statistics. In this paper, we propose XPathLearner, a method for estimating selectivity of the most commonly used types of path expressions without looking at the XML data. XPathLearner gathers and refines the statistics using query feedback in an on-line manner and is especially suited to queries in Internet scale applications since the underlying XML repository is either inaccessible or too large to be scanned in its entirety. Besides the on-line property, our method also has two other novel features: (a) XPathLearner is workload-aware in collecting the statistics and thus can be more accurate than the more costly off-line method under tight memory constraints, and (b) XPathLearner automatically adjusts the statistics using query feedback when the underlying XML data change. We show empirically the estimation accuracy of our method using several real data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional selectivity estimation using compressed histogram information

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ju-hong lee , deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1979

LEFT text: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: improved histograms for selectivity estimation of range predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: viswanath poosala , peter j. haas , yannis e. ioannidis , eugene j. shekita
",n
"LEFT id: NA
RIGHT id: 1254

LEFT text: This paper describes the design and implementation of an OODBMS, namely the METU Object-Oriented DBMS (MOOD). MOOD [Dog 94b] is developed on the Exodus Storage Manager (ESM) [ESM 92] and therefore some of the kernel functions like storage management, concurrency control, backup and recovery of data were readily available through ESM. In addition ESM has a client-server architecture and each MOOD process is a client application in ESM. The kernel functions provided by MOOD are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management. SQL statements are interpreted whereas functions (which have been previously compiled with C++) within SQL statements are dynamically linked and executed. A query optimizer is implemented by using the Volcano Query Optimizer Generator. A graphical user interface, namely Mood-View [Arp 93a, Arp 93b], is developed using Motif. MoodView displays both the schema information and the query results graphically. Additionally it is possible to update the database schema and to traverse the references in query results graphically.The system is coded in GNU C++ on Sun Sparc 2 workstations. MOOD has a SQL-like object-oriented query language, namely MOODSQL [Ozk 93b, Dog 94c]. MOOD type system is derived from C++, thus eliminating the impedance mismatch between MOOD and C++. The users can also access the MOOD Kernel from their application programs written in C++. For this purpose MOOD Kernel defines a class named UserRequest that contains a method for the execution of MOODSQL statements. The MOOD source code is available both for anonymous ftp users from ftp.cs.wisc.edu and for the WWW users from the site http://www.srdc.metu.edu.tr along with its related documents.In MOOD, each object is given a unique Object Identifier (OID) at object creation time by the ESM which is the disk start address of the object returned by the ESM. Object encapsulation is considered in two parts, method encapsulation and attribute encapsulation. These encapsulation properties are similar to the public and private declarations of C++.Methods can be defined in C++ by users to manipulate user defined classes and after compilation, they are dynamically linked and executed during the interpretation of SQL statements. This late binding facility is essential since database environments enforce run-time modification of schema and objects. With our approach, the interpretation of functions are avoided thus increasing the efficiency of the system. Dynamic linking primitives are implemented by the use of the shared object facility of SunOS [Sun 90]. Overloading is realized by making use of the signature concept of C++.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 2201

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization by predicate move-around

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , inderpal singh mumick , yehoshua sagiv
",n
"LEFT id: NA
RIGHT id: 1413

LEFT text: Publisher Summary eXtensible Markup Language (XML) is becoming the predominant data exchange format in a variety of application domains (supply-chain, scientific data processing, telecommunication infrastructure, etc.). Not only is an increasing amount of XML data now being processed, but XML is also increasingly being used in business-critical applications. Efficient and reliable storage is an important requirement for these applications. By relying on relational engines for this purpose, XML developers can benefit from a complete set of data management services (including concurrency control, crash recovery, and scalability) and from the highly optimized relational query processors. Strategies that automate the process of generating XML to relational mappings have been proposed in the literature. Due to the flexibility of the XML infrastructure, different XML applications exhibit widely different characteristics (for example, permissive vs. strict schemas, different access patterns).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 720

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : database caching for web application servers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mehmet altinel , qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , bruce g. lindsay , honguk woo , larry brown
",n
"LEFT id: NA
RIGHT id: 2190

LEFT text: Abstract.The requirements for effective search and management of the WWW are stronger than ever. Currently Web documents are classified based on their content not taking into account the fact that these documents are connected to each other by links. We claim that a page’s classification is enriched by the detection of its incoming links’ semantics. This would enable effective browsing and enhance the validity of search results in the WWW context. Another aspect that is underaddressed and strictly related to the tasks of browsing and searching is the similarity of documents at the semantic level. The above observations lead us to the adoption of a hierarchy of concepts (ontology) and a thesaurus to exploit links and provide a better characterization of Web documents. The enhancement of document characterization makes operations such as clustering and labeling very interesting. To this end, we devised a system called THESUS. The system deals with an initial sets of Web documents, extracts keywords from all pages’ incoming links, and converts them to semantics by mapping them to a domain’s ontology. Then a clustering algorithm is applied to discover groups of Web documents. The effectiveness of the clustering process is based on the use of a novel similarity measure between documents characterized by sets of terms. Web documents are organized into thematic subsets based on their semantics. The subsets are then labeled, thereby enabling easier management (browsing, searching, querying) of the Web. In this article, we detail the process of this system and give an experimental analysis of its results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: thesus : organizing web document collections based on link semantics

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: maria halkidi , benjamin nguyen , iraklis varlamis , michalis vazirgiannis
",y
"LEFT id: NA
RIGHT id: 2044

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: estimating compilation time of a query optimizer

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ihab f. ilyas , jun rao , guy lohman , dengfeng gao , eileen lin
",n
"LEFT id: NA
RIGHT id: 1877

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on the cost of monitoring and reorganization of object bases for clustering

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: carsten a. gerlhof , alfons kemper , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 947

LEFT text: We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications. The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental organization for data recording and warehousing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: h. v. jagadish , p. p. s. narayan , s. seshadri , s. sudarshan , rama kanneganti
",n
"LEFT id: NA
RIGHT id: 1531

LEFT text: XML - the eXtensible Markup Language has recently emerged as a new standard for data representation and exchange on the Interact. It is believed that it will become a universal format for data exchange on the Web and that in the near future we will find vast amounts of documents in XML format on the Web. As a result, it has become crucial to address the question of how large collections of XML documents can be sorted and retrieved efficiently and effectively.To date, most work on storing, indexing, querying, and searching documents in XML has stemmed from the database community's work on semi-structured data. An alternative approach, that has received less attention to date, is to view XML documents as a collection of text documents with additional tags and relations between these tags. IR techniques have traditionally been applied to search large sets of textual data and should thus be extended to encode the structure and semantics inherent in XML documents. Integrating IR and XML search techniques will enable more sophisticated search on the structure as well as the content of these documents, while leveraging the success of IR techniques in document similarity ranking and keyword search.The SIGIR workshop on XML and information retrieval was held July 28th, in Athens Greece. The goal of the workshop was to bring together researchers and practitioners interested in XML and IR to discuss and define the most relevant topics in the relation between these two technologies, present recent results, and propose future directions for research. The topics for discussion included:&bull; How to extend IR technologies to search XML documents&bull; How to integrate XML structure in IR indexing structures&bull; How to query XML documents both on content and structure&bull; How to introduce the semantics inherent in XML into the search process&bull; How to adopt database indexing techniques in an IR frameworkThe opening session of the workshop consisted of a survey of search engines for XML documents. This was followed by three technical sessions: query languages, retrieval algorithms, and IR systems for XML documents. The final talk of the day, ""Searching Annotated Language Resources in XML"", by Nancy Ide was given from the perspective of potential users of XML search systems and opened many topics for discussion. The workshop was concluded with a panel discussion where the panelists outlined their vision of the future of XML search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infoharness : a system for search and retrieval of heterogeneous information

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leon shklar , amit sheth , vipul kashyap , satish thatte
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 1150

LEFT text: Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 1549

LEFT text: In addition to research on the chemical properties of natural aroma compounds (NACs) that cause the perception of flavour and aroma, several studies have reported their potential applications for human health due to their antioxidant, anti-inflammatory, anti-cancer and anti-obesity properties. Furthermore, consumer demand shows a tendency towards natural products; most research in the industry and academic fields has focused on the bio-generation of commercially relevant NACs, particularly microbial production via de novo synthesis or biotransformation using enzymes or whole cells in conventional aqueous media.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: research and products-are they relevant to each other ? ( panel session )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: herb edelstein
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 391

LEFT text: Relational Database Management Systems (RDBMS) have been very successful at managing structured data with well-defined schemas. Despite this, relational systems are generally not the first choice for management of data where schemas are not pre-defined or must be flexible in the face of variations and changes. Instead, No-SQL database systems supporting JSON are often selected to provide persistence to such applications. JSON is a light-weight and flexible semi-structured data format supporting constructs common in most programming languages. In this paper, we analyze the way in which requirements differ between management of relational data and management of JSON data. We present three architectural principles that facilitate a schema-less development style within an RDBMS so that RDBMS users can store, query, and index JSON data without requiring schemas. We show how these three principles can be applied to industry-leading RDBMS platforms, such as the Oracle RDBMS Server, with relatively little effort. Consequently, an RDBMS can unify the management of both relational data and JSON data in one platform and use SQL with an embedded JSON path language as a single declarative language to query both relational data and JSON data. This SQL/JSON approach offers significant benefits to application developers as they can use one product to manage both relational data and semi-structured flexible schema data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management in ecommerce ( tutorial session ) : the good , the bad , and the ugly

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: avigdor gal
",y
"LEFT id: NA
RIGHT id: 590

LEFT text: This paper attempts a comprehensive study of deadlock detection in distributed database systems. First, the two predominant deadlock models in these systems and the four different distributed deadlock detection approaches are discussed. Afterwards, a new deadlock detection algorithm is presented. The algorithm is based on dynamically creating deadlock detection agents (DDAs), each being responsible for detecting deadlocks in one connected component of the global wait-for-graph (WFG). The DDA scheme is a “self-tuning” system: after an initial warm-up phase, dedicated DDAs will be formed for “centers of locality”, i.e., parts of the system where many conflicts occur. A dynamic shift in locality of the distributed system will be responded to by automatically creating new DDAs while the obsolete ones terminate. In this paper, we also compare the most competitive representative of each class of algorithms suitable for distributed database systems based on a simulation model, and point out their relative strengths and weaknesses. The extensive experiments we carried out indicate that our newly proposed deadlock detection algorithm outperforms the other algorithms in the vast majority of configurations and workloads and, in contrast to all other algorithms, is very robust with respect to differing load and access profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: As computational power and storage capacity increase, processing and analyzing large volumes of data play an increasingly important part in many domains of scientific research. Typical examples of large scientific datasets include long running simulations of time-dependent phenomena that periodically generate snapshots of their state (e.g. hydrodynamics and chemical transport simulation for estimating pollution impact on water bodies [4, 6, 20], magnetohydrodynamics simulation of planetary magnetospheres [32], simulation of a flame sweeping through a volume [28], airplane wake simulations [21]), archives of raw and processed remote sensing data (e.g. AVHRR [25], Thematic Mapper [17], MODIS [22]), and archives of medical images (e.g. confocal light microscopy, CT imaging, MRI, sonography).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 162

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: filenet integrated document management database usage and issues

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniel s. whelan
",n
"LEFT id: NA
RIGHT id: 1276

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: unql : a query language and algebra for semistructured data based on structural recursion

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter buneman , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 2169

LEFT text: Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). Each object is assigned an overall grade, that is obtained by combining the attribute grades using a fixed monotone aggregation function, or combining rule, such as min or average. In this overview, we discuss and compare algorithms for determining the top k objects, that is, k objects with the highest overall grades.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovering web services : an overview

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: vadim draluk
",n
"LEFT id: NA
RIGHT id: 1947

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: solving satisfiability and implication problems in database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sha guo , wei sun , mark a. weiss
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 578

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the coral deductive system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raghu ramakrishnan , divesh srivastava , s. sudarshan , praveen seshadri
",n
"LEFT id: NA
RIGHT id: 789

LEFT text: Detecting and extracting modifications from information sources is an integral part of data warehousing. For unsophisticated sources, in practice it is often necessary to infer modifications by periodically comparing snapshots of data from the source. Although this sapshot di/rem tial problem is closely related to traditional joins and outerjoins, there are significant differences, which lead to simple new algorithms. In particular, we present algorithms that perform (possibly lossy) compression of records. We also present a window algorithm that works very well if the snapshots are not ""very different."" The algorithms are studied via analysis and an implementation of two of them; the results illustrate the potential gains achievable with the new algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregation algorithms for very large compressed data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jianzhong li , doron rotem , jaideep srivastava
",n
"LEFT id: NA
RIGHT id: 614

LEFT text: This paper presents, QuickStore, a memory-mapped storage system for persistent C++ built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. The paper also presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. These systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. Both systems use the same underlying storage manager and compiler allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 680

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for minimizing tree pattern queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: prakash ramanan
",n
"LEFT id: NA
RIGHT id: 412

LEFT text: Publisher Summary  This chapter presents the first XPath query evaluation algorithm that runs in polynomial time with respect to the size of both the data and of the query. XPath has been proposed by the W3C as a practical language for selecting nodes from XML document trees. XPath is important because of its potential application as an XML query language per se, it being at the core of several other XML-related technologies, such as XSLT, XPointer, and XQuery, and the great and well-deserved interest such technologies receive. Since XPath and related technologies will be tested in ever-growing deployment scenarios, its implementations need to scale well both with respect to the size of the XML data and the growing size and intricacy of the queries (usually referred to as combined complexity).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: javax.xxl : a prototype for a library of query processing algorithms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jochen van den bercken , jens-peter dittrich , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1063

LEFT text: Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious algorithms for relational query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ambuj shatdal , chander kant , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1947

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: solving satisfiability and implication problems in database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sha guo , wei sun , mark a. weiss
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",n
"LEFT id: NA
RIGHT id: 195

LEFT text: Despite decades of research on AQP (approximate query processing), our understanding of sample-based joins has remained limited and, to some extent, even superficial. The common belief in the community is that joining random samples is futile. This belief is largely based on an early result showing that the join of two uniform samples is not an independent sample of the original join, and that it leads to quadratically fewer output tuples. Unfortunately, this early result has little applicability to the key questions practitioners face. For example, the success metric is often the final approximation's accuracy, rather than output cardinality. Moreover, there are many non-uniform sampling strategies that one can employ. Is sampling for joins still futile in all of these settings? If not, what is the best sampling strategy in each case? To the best of our knowledge, there is no formal study answering these questions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: join synopses for approximate query answering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 159

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: free parallel data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bin li , dennis shasha
",n
"LEFT id: NA
RIGHT id: 246

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac , ugur halici , ebru kilic , gokhan ozhan , fatma ozcan , sena nural , cevdet dengi , sema mancuhan , budak arpinar , pinar koksal , cem evrendilek
",y
"LEFT id: NA
RIGHT id: 1141

LEFT text: We propose a definition of a spatial database system as a database system that offers spatial data types in its data model and query language, and supports spatial data types in its implementation, providing at least spatial indexing and spatial join methods. Spatial database systems offer the underlying database technology for geographic information systems and other applications. We survey data modeling, querying, data structures and algorithms, and system architecture for such systems. The emphasis is on describing known technology in a coherent manner, rather than listing open problems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: filter trees for managing spatial data over a range of size granularities

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth c. sevcik , nick koudas
",y
"LEFT id: NA
RIGHT id: 1665

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a language based multidatabase system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eva k &#252; hn , thomas tschernko , konrad schwarz
",n
"LEFT id: NA
RIGHT id: 821

LEFT text: An infrared generator wherein an ellipsoidal reflector has a source rich in infra red at one focus thereof. The end of the reflector at the other focus merges with a paraboloidal reflector positioned so that the focus of the latter reflector coincides with the said other focus of the former. A second ellipsoidal reflector may be inserted between the former said reflectors with one of its foci coinciding with the said other focus of the former said ellipsoidal reflector, and its other focus coinciding with the focus of the paraboloidal reflector. The axes of the reflectors are coincident, and a lens and plug or equivalent may be provided to recover radiation that might otherwise be lost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: spirit : sequential pattern mining with regular expression constraints

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: minos n. garofalakis , rajeev rastogi , kyuseok shim
",y
"LEFT id: NA
RIGHT id: 1448

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a non-uniform data fragmentation strategy for parallel main-menory database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: n. bassiliades , ioannis p. vlahavas
",n
"LEFT id: NA
RIGHT id: 312

LEFT text: There is a new emerging world of web services. In this world, services will be combined in innovative ways to form elaborate services out of building blocks of other services. This is predicated on having a common ground of vocabulary and communication protocols operating in a secured environment. Currently, massive standardization efforts [UDDI, WSDL, ebXML, RosettaNet] are aiming at achieving this common ground. We explore possible architectures for deploying computerized traders internal services. This encompasses both the structure and the functionalities of “traders” and “services” and the form in which these functionalities could be realized in actual implementations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: proxy-server architectures for olap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: panos kalnis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1393

LEFT text: We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative formalism for specifying these kinds of probabilistic information, and we propose algorithms for ordering the information sources. Finally, we discuss a preliminary experimental evaluation of these algorithms on the domain of bibliographic sources available on the WWW.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information integration : the momis project demonstration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , silvana castano , alberto corni , r. guidetti , g. malvezzi , michele melchiori , maurizio vincini
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: Because of the Internet we believe that in the long run there will be alternative providers for all of these three resources for any given application. Data providers will bring more and more data and more and more different kinds of data to the net. Likewise, function providers will develop new methods to process and work with the data; e.g., function providers might develop new algorithms to compress data or to produce thumbnails out of large images and try to sell these on the Internet. It is also conceivable, that some people allow other people to use spare cycles of their idle machines in the Internet (as in the Condor system of the University of Wisconsin) or that some companies (cycle providers) even specialize on selling computing time to businesses that occasionally need to carry out very complex operations for which regular hardware is not sufficient.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 385

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an approximate search engine for structural databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jason t. l. wang , xiong wang , dennis shasha , bruce a. shapiro , kaizhong zhang , qicheng ma , zasha weinberg
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: The naive solution would be to do an exhaustive search across all possible subsets of items and count how many satisfy the predicate conditions we are looking for. This approach, although it would be efficient space-wise (only store the combinations we need) would waste a lot of time (creating all possible combinations). This paper presents a few algorithms that start with a seed itemset (one that already satisfies the boolean predicates we wish to evaluate) and grow them into itemsets of maximal size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1508

LEFT text: Complex queries containing outer joins are, for the most part, executed by commercial DBMS products in an ""as written"" manner. Only a very few reorderings of the operations are considered and the benefits of considering comprehensive reordering schemes are not exploited. This is largely due to the fact there are no readily usable results for reordering such operations for relations with duplicates and/or outer join predicates that are other than ""simple."" Most previous approaches have ignored duplicates and complex predicates; the very few that have considered these aspects have suggested approaches that lead to a possibly exponential number of, and redundant intermediate joins. Since traditional query graph models are inadequate for modeling outer join queries with complex predicates, we present the needed hypergraph abstraction and algorithms for reordering such queries with joins and outer joins. As a result, the query optimizer can explore a significantly larger space of execution plans, and choose one with a low cost. Further, these algorithms are easily incorporated into well known and widely used enumeration methods such as dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hypergraph based reorderings of outer join queries with complex predicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gautam bhargava , piyush goel , bala iyer
",y
"LEFT id: NA
RIGHT id: 1909

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 881

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional database allocation for parallel data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: thomas st &#246; hr , holger m &#228; rtens , erhard rahm
",n
"LEFT id: NA
RIGHT id: 152

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the araneus web-based management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. mecca , p. atzeni , a. masci , g. sindoni , p. merialdo
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",y
"LEFT id: NA
RIGHT id: 1472

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",y
"LEFT id: NA
RIGHT id: 796

LEFT text: Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious indexing for decision-support in main memory

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jun rao , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 813

LEFT text: Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimal grid-clustering : towards breaking the curse of dimensionality in high-dimensional clustering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 1866

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: With t,ha exponential proliferation of databases and advances in wide area networking, interest, in worldwide dat,abase interoperability has gained momentum. Scalability and language support for this new environment remain open questSions. We propose a scheme where database nodes are dynamically clustered around current, areas of ibresl. Data sharing is then pursued, with any relationship informat8ion discovered being fed bac,k for reclustering. In order to achieve scalability, t,he proposed architect(ure sub-divides both the rrlntionship and illformntiorl spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1797

LEFT text: Relational database systems do not effectively support complex queries containing quantifiers (quantified queries) that are increasingly becoming important in decision support applications. Generalized quantifiers provide an effective way of expressing such queries naturally. In this paper, we consider the problem of processing quantified queries within the generalized quantifier framework. We demonstrate that current relational systems are ill-equipped, both at the language and at the query processing level, to deal with such queries. We also provide insights into the intrinsic difficulties associated with processing such queries. We then describe the implementation of a quantified query processor, Q2P, that is based on multidimensional and boolean matrix structures. We provide results of performance experiments run on Q2P that demonstrate superior performance on quantified queries. Our results indicate that it is feasible to augment relational systems with query subsystems like Q2P for significant performance benefits for quantified queries in decision support applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based optimization of decision support queries using transient-views

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: subbu n. subramanian , shivakumar venkataraman
",n
"LEFT id: NA
RIGHT id: 447

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: privacy-preserving data mining

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 956

LEFT text: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating sql databases with content-specific search engines

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan de &#223; loch , nelson mendon &#231; a mattos
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: This paper provides an introduction to the major research directions in biodiversity informatics. The biodiversity enterprise is a vast and complex information domain. I describe the need to build infrastructure for this domain, major research thrusts needed to improve its work practices, and areas of research that could contribute to the advancement of the field. I emphasize that the science of biodiversity is fundamentally an information science, worthy of special attention from the computer and information science communities because of its distinctive attributes of scale and socio-technical complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 2105

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: minicon : a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rachel pottinger , alon halevy
",n
"LEFT id: NA
RIGHT id: 1989

LEFT text: Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view maintenance and integrity constraint checking : trading space for time

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1068

LEFT text: One of the central tasks in managing, monitoring and mining data streams is that of identifying outliers. There is a long history of study of various outliers in statistics and databases, and a recent focus on mining outliers in data streams. Here, we adopt the notion of ""deviants"" from Jagadish et al. (1999) as outliers. Deviants are based on one of the most fundamental statistical concept of standard deviation (or variance). Formally, deviants are defined based on a representation sparsity metric, i.e., deviants are values whose removal from the dataset leads to an improved compressed representation of the remaining items. Thus, deviants are not global maxima/minima, but rather these are appropriate local aberrations. Deviants are known to be of great mining value in time series databases. We present first-known algorithms for identifying deviants on massive data streams. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining deviants in a time series database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , s. muthukrishnan
",y
"LEFT id: NA
RIGHT id: 968

LEFT text: Association Rule Mining algorithms operate on a data matrix (e.g., customers products) to derive association rules [2, 23]. We propose a new paradigm, namely, Ratio Rules, which are quanti able in that we can measure the \goodness"" of a set of discovered rules. We propose to use the \guessing error"" as a measure of the \goodness"", that is, the rootmean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can \guess"" the amount spent on, say, butter. Thus, we can perform a variety of important tasks such as forecasting, answering \what-if"" scenarios, detecting outliers, and visualizing the data. Moreover, we show how to compute Ratio Rules in a single pass over the dataset with small memory requirements (a few small matrices), in contrast to traditional association rule mining methods that require multiple passes and/or large memory. ExperWork performed while at the University of Maryland. This research was partially funded by the Institute for Systems Research (ISR), and by the National Science Foundation under Grants No. EEC-94-02384, IRI-9205273 and IRI-9625428. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 24th VLDB Conference New York, USA, 1998 iments on several real datasets (e.g., basketball and baseball statistics, biological data) demonstrate that the proposed method consistently achieves a \guessing error"" of up to 5 times less than the straightforward competitor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: ratio rules : a new paradigm for fast , quantifiable data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: flip korn , alexandros labrinidis , yannis kotidis , christos faloutsos
",y
"LEFT id: NA
RIGHT id: 1988

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintaining database consistency in presence of value dependencies in multidatabase systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: claire morpain , mich &#233; le cart , jean ferri &#233; , jean-fran &#231; ois pons
",n
"LEFT id: NA
RIGHT id: 26

LEFT text: In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the aqua approximate query answering system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 804

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of round-trips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose the use of the context in which an object is loaded as a predictor of future accesses, where a context can be a stored collection of relationships, a query result, or a complex object. When an object O's state is loaded, similar state for other objects in O's context is prefetched. We present a design for maintaining context and for using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe several variations of the optimization: selectively applying the technique based on application and database characteristics, using application-supplied performance hints, using concurrent database queries to support asynchronous prefetch, prefetching across relationship paths, and delayed prefetch to save database round-trips.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: context-based prefetch for implementing objects on relations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: philip a. bernstein , shankar pal , david shutt
",n
"LEFT id: NA
RIGHT id: 825

LEFT text: Users often can not easily express their queries. For example, in a multimedia/image by content setting, the user might want photographs with sunsets; in current systems, like QBIC, the user has to give a sample query, and to specify the relative importance of color, shape and texture. Even worse, the user might want correlations between attributes, like, for example, in a traditional, medical record database, a medical researcher might want to find “mildly overweight patients”, where the implied query would be “weight/height M 4 lb/inch”. Our goal is to provide a user-friendly, but theoretically solid method, to handle such queries. We allow the user to give several examples, and, optionally, their ‘goodness’ scores, and we propose a novel method to “guess” which attributes are important, which correlations are important, and with what weight. Our contributions are twofold: (a) we formalize the problem as a minimization problem and show how to solve for the optimal solution, completely avoiding the ad-hoc heurist Part of this work was done while this author was vising University of Maryland and Carnegie Mellon University. Experiments on synthetic and real datasets show that our method estimates quickly and accurately the ‘hidden’ distance function in the user’s mind.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: miro web : integrating multiple data sources through semistructured data types

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: luc bouganim , tatiana chan-sine-ying , tuyet-tram dang-ngoc , jean-luc darroux , georges gardarin , fei sha
",n
"LEFT id: NA
RIGHT id: 1033

LEFT text: Enterprise-class databases require database administrators who are responsible for performance tuning. With large-scale deployment of databases, minimizing database administration function becomes important. One important task of a database administrator is selecting indexes that are appropriate for the workload on the system. In data intensive applications such as decision support and data warehousing picking the right set of indexes becomes crucial for performance. Moreover, the indexes chosen should track changes in the workload. While automating the process of index selection can greatly reduce administration cost, enterprise databases are simply too complex for the administrator to hit the “accept” button on the recommendations of an index selection tool without doing a quantitative impact analysis of the recommendations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient cost-driven index selection tool for microsoft sql server

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 1409

LEFT text: Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing ” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1365

LEFT text: Internet based eCommerce is expected to grow at a phenomenal rate. As businesses rapidly move to deploy business-to-business eCommerce solutions, systems designers are likely to face new challenges while integrating and managing data. In this presentation, I propose to discuss some of the new business models and the impact on data management in the context of these evolving eCommerce scenarios.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: linking business to deliver value : a data management challenge

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: anand deshpande
",y
"LEFT id: NA
RIGHT id: 545

LEFT text: Data warehouses form the essential infrastructure for many data analysis tasks. A core operation in a data warehouse is the construction of a data cube, which can be viewed as a multi-level, multi-dimensional database with aggregate data at multiple granularity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data analysis and mining in the life sciences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nam huyn
",y
"LEFT id: NA
RIGHT id: 1870

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1267

LEFT text: We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:<ul><li>Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: combining multi-visual features for efficient indexing in a large image database

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anne h. h. ngu , quan z. sheng , du q. huynh , ron lei
",n
"LEFT id: NA
RIGHT id: 566

LEFT text: This paper provides a formal, high-level operational semantics for a complex-value OQL-like query language that can create fresh database objects, and invoke external methods. We define a type system for our query language and prove an important soundness property.We define a simple effect typing discipline to delimit the computational effects within our queries. We prove that this effect system is correct and show how it can be used to detect cases of non-determinism and to define correct query optimizations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xquery formal semantics state and challenges

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter fankhauser
",n
"LEFT id: NA
RIGHT id: 1156

LEFT text: Protecting rights over relational data is of ever increasing interest, especially considering areas where sensitive, valuable content is to be outsourced. A good example is a data mining application, where data is sold in pieces to parties specialized in mining it.Different avenues for rights protection are available, each with its own advantages and drawbacks. Enforcement by legal means is usually ineffective in preventing theft of copyrighted works, unless augmented by a digital counter-part, for example watermarking.Recent research of the authors introduces the issue of digital watermarking for generic number sets. In the present paper we expand on this foundation and introduce a solution for relational database content rights protection through watermarking.Our solution addresses important attacks, such as data re-sorting, subset selection, linear data changes (applying a linear transformation on arbitrary subsets of the data). Our watermark also survives up to 50% and above data loss.Finally we present wmdb.*, a proof-of-concept implementation of our algorithm and its application to real life data, namely in watermarking the outsourced Wal-Mart sales data that we have available at our institute.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",n
"LEFT id: NA
RIGHT id: 204

LEFT text: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that traditional relational DBMSs can process eciently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to a relational DBMS, and the impact of the quality of these statistics on the retrieval eciency of the resulting scheme.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an efficient bitmap encoding scheme for selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chee-yong chan , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1076

LEFT text: In this paper we examine the problem of duplicates, in transformation-based enumeration. In general, different sequences of transformation rules may end up deriving the same element, and the optimizer must detect and discard these duplicate elements generated by multiple paths. We show that the usual commutativity/associativity rules for joins generate O(4^n) duplicate operators. We then propose a scheme ---within the generic transformation-based framework --- to avoid the generation of duplicates, which does achieve the O(3^n) lower bound on join enumeration. Our experiments show an improvement of up to a factor of 5 in the optimization of a query with 8 tables, when duplicates are avoided rather than detected.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the complexity of transformation-based join enumeration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: arjan pellenkoft , c &#233; sar a. galindo-legaria , martin l. kersten
",y
"LEFT id: NA
RIGHT id: 388

LEFT text: The tutorial surveys state-of-the-art methods for storing and retrieving multimedia data from large databases. Records (= documents) may consist of formatted fields, text, images, voice, animation etc. .4 sample query that we would like to support is ‘in a collection of 2-d color images, find images that are similar to a sunset photograph’. Indexing for images and other media is a new, active area of research; the tutorial will present recent approaches and prototype systems, for 2-d and 3-d medical image databases, 2-d color image databases, and l-d time series databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data access ( tutorial session )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , anand deshpande
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: This paper gives an overview of the IRO-DB architecture and describes in detail the cost evaluator currently under elaboration for the next version of the distributed query optimizer. The cost model is composed of a set of mathematical formulas with coefficients to estimate the cost of the search operators. The coefficients are deduced from a calibrating objectoriented database composed of linked collections of objects. A tuning application is run on each local site to adjust the cost formulas and fix the coefficients. We report on the tuning of O2 and ObjectStore. We show that the estimation is quite accurate for path traversals with the OO7 benchmark on top of ObjectStore.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 1272

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: malcolm p. atkinson
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 246

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac , ugur halici , ebru kilic , gokhan ozhan , fatma ozcan , sena nural , cevdet dengi , sema mancuhan , budak arpinar , pinar koksal , cem evrendilek
",n
"LEFT id: NA
RIGHT id: 1886

LEFT text: Once again, scientists were called upon to take greater role in the political process. And this time, they did! We report on the continuous debate on the nation's R&D policy. We also cover funding opportunities from DoD and NSF.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: scientists called upon to take actions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: xiaolei qian
",y
"LEFT id: NA
RIGHT id: 912

LEFT text: The overall theme of the FQAS conferences is innovative query systems that are aimed at providing easy, flexible and intuitive access to information. Such systems are intended to facilitate retrieval from information repositories such as databases, libraries, and the World Wide Web. These repositories are typically equipped with standard query systems, which are often inadequate, and the focus of FQAS is the development of query systems that are more expressive, informative, cooperative and productive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international conference on ontologies , databases and applications of semantics ( odbase ) : part of the federated conference on the move to meaningful internet systems 2002

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1057

LEFT text: In recent years, new developments in genetics have generated a lot of interest in genomic and proteomic data, investing international significance (and competition) in the fledgling discipline of bioinformatics. Researchers in pharmaceutical and biotech companies have found that database products can bring a wide range of relevant technologies to bear on their problems. Benefiting from a number of new technology enhancements, Oracle has emerged as a popular platform for pharmaceutical knowledge management and bioinformatics. We look at four powerful technologies that show promise for solving hitherto intractable problems in bioinformatics: the extensibility architecture to store gene sequence data natively and perform high-dimensional structure-searches in the database; warehousing technologies and data mining on genetic patterns; data integration technologies to enable heterogeneous queries across distributed biological sources, and internet portal technologies that allow life sciences information to be published and managed across intranets and the internet.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information management for genome level bioinformatics

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: norman w. paton , carole a. goble
",n
"LEFT id: NA
RIGHT id: 1632

LEFT text: This article concentrates on query unnesting (also known as query decorrelation), an optimization that, even though it improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature, and is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of our method is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform method of unnesting queries, regardless of their type of nesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing disjunctive queries with expensive predicates

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: a. kemper , g. moerkotte , k. peithner , m. steinbrunn
",n
"LEFT id: NA
RIGHT id: 2024

LEFT text: Users often can not easily express their queries. For example, in a multimedia/image by content setting, the user might want photographs with sunsets; in current systems, like QBIC, the user has to give a sample query, and to specify the relative importance of color, shape and texture. Even worse, the user might want correlations between attributes, like, for example, in a traditional, medical record database, a medical researcher might want to find “mildly overweight patients”, where the implied query would be “weight/height M 4 lb/inch”. Our goal is to provide a user-friendly, but theoretically solid method, to handle such queries. We allow the user to give several examples, and, optionally, their ‘goodness’ scores, and we propose a novel method to “guess” which attributes are important, which correlations are important, and with what weight. Our contributions are twofold: (a) we formalize the problem as a minimization problem and show how to solve for the optimal solution, completely avoiding the ad-hoc heurist Part of this work was done while this author was vising University of Maryland and Carnegie Mellon University. Experiments on synthetic and real datasets show that our method estimates quickly and accurately the ‘hidden’ distance function in the user’s mind.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: panel : querying networked databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 2172

LEFT text: In this article, we study how we can maintain local copies of remote data sources ""fresh,"" when the source data is updated autonomously and independently. In particular, we study the problem of Web crawlers that maintain local copies of remote Web pages for Web search engines. In this context, remote data sources (Websites) do not notify the copies (Web crawlers) of new changes, so we need to periodically poll the sources to maintain the copies up-to-date. Since polling the sources takes significant time and resources, it is very difficult to keep the copies completely up-to-date.This article proposes various refresh policies and studies their effectiveness. We first formalize the notion of ""freshness"" of copied data by defining two freshness metrics, and we propose a Poisson process as the change model of data sources. Based on this framework, we examine the effectiveness of the proposed refresh policies analytically and experimentally. We show that a Poisson process is a good model to describe the changes of Web pages and we also show that our proposed refresh policies improve the ""freshness"" of data very significantly. In certain cases, we got orders of magnitude improvement from existing policies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: effective page refresh policies for web crawlers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: junghoo cho , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 927

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1290

LEFT text: In this paper we investigate the problem of incremental maintenance of materialized views in data warehouses. We consider views defined by relational algebraic operators and aggregate functions. We show that a materialized view can be maintained without accessing the view itself by materializing and maintaining additional relations. These relations are derived from the intermediate results of the view computation. We first give an algorithm for determining what additional relations need to be materialized in order to maintain a materialized view incrementally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate maintenance for data warehousing in informix red brick vista

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: craig j. bunker , latha s. colby , richard l. cole , william j. mckenna , gopal mulagund , david wilhite
",y
"LEFT id: NA
RIGHT id: 1459

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Almost all practicing scientists see the peer review process from two perspectives: that of the reviewer and that of the reviewee. My own views on these two aspects of scienti® c publishing have evolved signi® cantly in the eighteen years that I have been writing scienti® c papers. My introduction to the review process from the author’s point of view was memorable and unpleasant. My research advisor received the reviews for a paper that I had written and passed them directly to me with instructions to prepare responses to the reviewers’ criticisms. The longer of the two reviews was biting and sarcastic. I responded in kind and returned the responses to my advisor, thinking that he would ® lter them before returning them to the editor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 2044

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: estimating compilation time of a query optimizer

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ihab f. ilyas , jun rao , guy lohman , dengfeng gao , eileen lin
",n
"LEFT id: NA
RIGHT id: 1746

LEFT text: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information systems research at george mason university

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sushil jajodia , daniel barbar &#225; , alex brodsky , larry kerschberg , ami motro , edgar sibley , x. sean wang
",y
"LEFT id: NA
RIGHT id: 13

LEFT text: This paper describes issues and solutions related to the creation of a product information database in the enterprise, and using this database as a foundation for deploying an electronic catalog. Today, product information is typically managed in document composition systems and communicated on paper. In the new wired world, these processes are undertaking fundamental changes to cope with the time to market pressure and the need for accurate, complete, and structured presentation of product information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: building database-driven electronic catalogs

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sherif danish
",y
"LEFT id: NA
RIGHT id: 1064

LEFT text: Introduction Modern Database Management Systems have numerous mechanisms for allowing administrators to man usage of system resources such as disk space and memory, yet they have typically not included mechanism trolling the usage of the all-important system resource of CPU-time. Traditionally, this functionality has been the host Operating System scheduler. Because of this, the only recourse for Oracle to control users’ CPU usag impose hard limits on CPU consumption. If a user session exceeded its CPU limit, it was terminated. In Orac i we introduced the Oracle Database Resource Manager, a novel DBMS CPU management mechanism that allow base administrator to delineate logically distinct units of a workload and to partition CPU resources between units. Performance investigations show that running with the Database Resource Manager imposes no meas overheads on a workload, and the use of the Resource Manager actually improves performance with large u lations. Several commercial Operating Systems now support CPU resource management via a fair-share schedu mechanism. We believe that for large, complex applications such as Oracle, CPU resource management is be dled within the application itself. Since the scheduling is done within the Oracle application and the applicatio portable across numerous platforms, the Oracle Resource Manager is able to consistently enforce its schedu cies, independent of OS platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: resource scheduling in enhanced pay-per-view continuous media databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: minos n. garofalakis , banu &#214; zden , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 1384

LEFT text: Analysts and decision-makers use what-if analysis to assess the e®ects of hypotheti- cal scenarios. What-if analysis is currently supported by spreadsheets and ad-hoc O L AP tools. Unfortunately, the former lack seam- less integration with the data and the lat- ter lack °exibility and performance appropri- ate for O L AP applications. To tackle these problems we developed the Sesamesystem, which models an hypothetical scenario as a list of hypothetical modications on the ware- house views and fact data. We provide formal scenario syntax and semantics, which extend view update semantics for accomodating the special requirements of O L AP. We focus on query algebra operators suitable for perform- ing spreadsheet-style computations. Then we present Sesame's optimizer and its corner- stone substitution and rewriting mechanisms. Substitution enables lazy evaluation of the hy- pothetical updates. The substitution module delivers orders-of-magnitude optimizations in cooperation withtherewriterthatusesknowl- edge of arithmetic, relational, ¯nancial and other operators. Finally we discuss the chal- lenges that the size of the scenario specica- tionsandthe arbitrarynatureof theoperators pose to the rewriter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: temporal queries in olap

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alberto o. mendelzon , alejandro a. vaisman
",n
"LEFT id: NA
RIGHT id: 1559

LEFT text: The national and international standards committees responsible for Database Language SQL have proposed a candidate extension for SQL Persistent Stored Modules (SQL/PSM). The purpose of this extension is to provide a computationally complete language for the declaration and invocation of SQL stored modules and routines. Typically, such routines are stored in a database Server and executed from an application Client in a Client/Server environment.The proposed SQL/PSM consists of syntax and semantics for variable and cursor declarations, function and procedure (routines) invocations, condition handling, and control statements for looping and branching. An SQL routine is block structured, with each block consisting of local variable and condition handler declarations, a list of SQL statements, and local condition handler execution.Condition handling is a major new feature of SQL/PSM (henceforth referred to as PSM), although the style and comprehensiveness of the specification is still an issue in further progression of the standard. The specification currently under ballot includes conditions for exceptions, warnings, and other completions such as success of no data, and handlers for Continue, Exit, Redo, and Undo.Condition handling allows the user to separate condition handling code from the main flow of a routine, thereby eliminating the need to write numerous short and redundant code fragments to handle each unique condition. In some database products, one cannot even resolve the condition in the Server and must instead resort to the Client application program for resolution. Such approaches are often tedious, error-prone, and inflexible. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: condition handling in sql persistent stored modules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jeff richey
",y
"LEFT id: NA
RIGHT id: 2182

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: incremental computation and maintenance of temporal aggregates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun yang , jennifer widom
",n
"LEFT id: NA
RIGHT id: 507

LEFT text: The Java programming language [1,3] from its inception has been publicized as a web programming language. Many programmers have developed simple applications such as games, clocks, news tickers and stock tickers in order to create informative, innovative web sites. However, it is important to note that the Java programming language possesses much more capability. The language components and constructs originally designed to enhance the functionality of Java as a web-based programming language can be utilized in a broader extent. Java provides a developer with the tools allowing for the creation of innovative network, database, and Graphical User Interface (GUI) applications. In fact, Java and its associated technologies such as JDBC API [11,5], JDBC drivers [2,12], threading [10], and AWT provide the programmer with the much-needed assistance for the development of platform-independent database-independent interfaces. Thus, it is possible to build a graphical database interface capable of connecting and querying distributed databases [13,14]. Here are components that are important for building the database interface we have in mind.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sqlj part 1 : sql routines using the java programming language

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: andrew eisenberg , jim melton
",n
"LEFT id: NA
RIGHT id: 2087

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: d ( k ) - index : an adaptive structural summary for graph-structured data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: qun chen , andrew lim , kian win ong
",n
"LEFT id: NA
RIGHT id: 1367

LEFT text: From recent conferences, position papers, special journal issues, and informal hallway discussions, we have witnessed a quickly rising tide of interest among the database research community in querying and processing data streams. Numerous modem applications operate over data that arrives in the form of rapid, continuous streams, and in many of these applications simply diverting the streams in their entirety into a conventional DBMS and querying them in a conventional fashion is infeasible in terms of performance and functionality. Example applications include sensor networks, Web tracking, financial monitoring, telecommunications, network monitoring and traffic engineering, and manufacturing processes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: model-based information integration in a neuroscience mediator system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bertram lud &#228; scher , amarnath gupta , maryann e. martone
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 308

LEFT text: We describe a system that supports arbitrarily complex SQL queries with ”uncertain” predicates. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is query evaluation. We describe an optimization algorithm that can compute eciently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any ecient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: global optimization of histograms

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: h. v. jagadish , hui jin , beng chin ooi , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 2024

LEFT text: In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: panel : querying networked databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1888

LEFT text: In the discussion of research issues in spatial databases (SIGMOD Record vol. 19, no. 4, Dec 1990) we stated the need for a robust framework for analytical comparison of a broad range of spatial access methods. The utility of such a comparison, even of very closely related access methods, was shown in [FALO87]. A necessary precondition for a meaningful analytical comparison is the existence of strong analytical results for individual access methods. In the following paper, Salzberg and Lomet take the worst case analytical results on fan-out and average storage utilization they obtained for their hB-tree [LOME89,LOME90] and extend the analysis to another robust method, Z-order encoding [OREN84]. We think this paper is a start on the comparative assessment of access methods based on analytical results. We hope to see future work extend the framework beyond worst case analysis, and to other access methods as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",y
"LEFT id: NA
RIGHT id: 1522

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 1122

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: calibrating the query optimizer cost model of iro-db , an object-oriented federated database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , fei sha , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1393

LEFT text: A long-term water quality study has been initiated by the Korean Ministry of Environment(MOE) - The G-7 Project--in cooperation with two national research institutes, an University research tn and a consulting firm. This study includes the development of computer software for total water quality management system, so called ISWQM (Integrated System of Water Quality Management). ISWQM includes four major components: a GIS database; two artificial intelligence (AI) based expert systems to estimate pollutant loadings and to provide cost-effective wastewater treatment system for small and medium size urban areas; and computer programs to integrate the database and expert systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information integration : the momis project demonstration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , silvana castano , alberto corni , r. guidetti , g. malvezzi , michele melchiori , maurizio vincini
",n
"LEFT id: NA
RIGHT id: 354

LEFT text: The design of external index structures for one- and multidimensional extended objects is a long and well studied subject in basic database research. Today, more and more commercial applications rely on spatial datatypes and require a robust and seamless integration of appropriate access methods into reliable database servers. This paper proposes an efficient, dynamic and scalable approach to manage one-dimensional interval sequences within off-the-shelf object-relational database systems. The presented technique perfectly fits to the concept of space-filling curves and, thus, generalizes to spatially extended objects in multidimensional data spaces. Based on the Relational Interval Tree, the method is easily embedded in modern extensible indexing frameworks and significantly outmatches Linear Quadtrees and Relational R-trees with respect to usability, concurrency, and performance. As demonstrated by our experimental evaluation on an Oracle server with real GIS and CAD data, the competing methods are outperformed by factors of up to 4.6 (Linear Quadtree) and 58.3 (Relational R-tree) for query response time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial data management for computer aided design

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hans-peter kriegel , andreas m &#252; ller , marco p &#246; tke , thomas seidl
",y
"LEFT id: NA
RIGHT id: 448

LEFT text: The naive solution would be to do an exhaustive search across all possible subsets of items and count how many satisfy the predicate conditions we are looking for. This approach, although it would be efficient space-wise (only store the combinations we need) would waste a lot of time (creating all possible combinations). This paper presents a few algorithms that start with a seed itemset (one that already satisfies the boolean predicates we wish to evaluate) and grow them into itemsets of maximal size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: using quantitative information for efficient association rule generation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: b. p &#244; ssas , m. carvalho , r. resende , w. meita , jr.
",y
"LEFT id: NA
RIGHT id: 1125

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 279

LEFT text: Electronic commerce systems (retail, auction, etc.) are good examples of data-based systems that operate under correctness and resilience requirements of a transactional nature but go beyond conventional databases, as they are formed by the aggregation of heterogeneous, autonomous components. We introduce a framework to specify, analyze and reason about the behavior of such systems, focusing on how they are designed to make consistent progress in spite of failures. The contributions are: (a) the introduction of the Guarantee abstraction to deal with transactional applications; (b) a framework based on guarantees and protocols to specify the behaviors of systems and their components and reason about the properties of systems and their components; and (c) application of the framework to a common e-commerce scenario. The framework allows the hierarchical composition of transactional systems and their properties, as well as the proofs of these properties: we specify a system's behavior at its most abstract level, and proceed to decompose the specification mirroring the structure of the system's components, considering the role of guarantee-preserving component systems and recovery in each case. In particular we show how the lower-level properties are supported by the component systems, which we also characterize within the same framework.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: orthogonal optimization of subqueries and aggregation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , milind joshi
",n
"LEFT id: NA
RIGHT id: 1319

LEFT text: Statistical databases often use random data perturbation (RDP) methods to protect against disclosure of confidential numerical attributes. One of the key requirements of RDP methods is that they provide the appropriate level of security against snoopers who attempt to obtain information on confidential attributes through statistical inference. In this study, we evaluate the security provided by three methods of perturbation. The results of this study allow the database administrator to select the most effective RDP method that assures adequate protection against disclosure of confidential information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a modified random perturbation method for database security

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: patrick tendick , norman matloff
",n
"LEFT id: NA
RIGHT id: 1900

LEFT text: DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentation of large tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework is being implemented in a tool that has been already successfully applied to a variety of real applications by a number of user groups.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: pixel-oriented database visualizations

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 2108

LEFT text: Zusammenfassung Obwohl der Einsatz von Data-Warehouse-Systemen zur gängigen Praxis moderner IT-Landschaften gehört, haben methodische Untersuchungen zum qualitätsorientierten Schemaentwurf erst in jüngerer Zeit begonnen. Ausgehend von einem Schemaentwurfsprozess für Data-Warehouse-Systeme werden aktuelle Entwicklungen zur Qualitätssicherung und Data-Warehouse-spezifische Qualitätskriterien wie Summierbarkeit sowie deren Verallgemeinerung zu mehrdimensionalen Normalformen und Selbstwartbarkeit vorgestellt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: monotonic complements for independent data warehouses

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: d. laurent , j. lechtenb &#246; rger , n. spyratos , g. vossen
",y
"LEFT id: NA
RIGHT id: 836

LEFT text: Amongst the wide range of parking solutions that can contribute to reduce parking problems or regulate parking activities, e Parking looks at developing and applying an innovative e-business application for parking space optimization. The purpose of this paper is to present the innovative e-business platform that has been deve loped, from a technical point of view, by the University of Zurich. The ideas are coming from a transcross European consortium within the framework of the IST Information Society Technologies of the 5th framework program. E-Parking provides a database-centered Web application solution based on our proposed conceptual model CIA (Channel, Integration, Application) for Web applications. The WAP, WEB and Bluetooth communication channels enable drivers to obtain early information on available parking space, make a reservation, access the reserved place and pay for the service booked. In reaching this goal, the innovative solutions seek to benefit all social segments, to optimize existing parking resources, and to contribute to achieving a more sustainable urban transport, reducing congestion and pollution.t

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data-driven , one-to-one web site generation for data-intensive applications

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1929

LEFT text: A data warehouse is a repository of integrated information from distributed, autonomous, and possibly heterogeneous, sources. In effect, the warehouse stores one or more materialized views of the source data. The data is then readily available to user applications for querying and analysis. Figure 1 shows the basic architecture of a warehouse: data is collected from each source, integrated with data from other sources, and stored at the warehouse. Users then access the data directly from the warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of data cubes and summary tables in a warehouse

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: inderpal singh mumick , dallan quass , barinderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 462

LEFT text: As the size of data warehouses increase to several hundreds of gigabytes or terabytes, the need for methods and tools that will automate the process of knowledge extraction, or guide the user to subsets of the dataset that are of particular interest, is becoming prominent. In this survey paper we explore the problem of identifying and extracting interesting knowledge from large collections of data residing in data warehouses, by using data mining techniques. Such techniques have the ability to identify patterns and build succinct models to describe the data. These models can also be used to achieve summarization and approximation. We review the associated work in the OLAP, data mining, and approximate query answering literature. We discuss the need for the traditional data mining techniques to adapt, and accommodate the specific characteristics of OLAP systems. We also examine the notion of interestingness of data, as a tool to guide the analysis process. We describe methods that have been proposed in the literature for determining what is interesting to the user and what is not, and how these approaches can be incorporated in the data mining algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: knowledge discovery in data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: themistoklis palpanas
",y
"LEFT id: NA
RIGHT id: 225

LEFT text: In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX has to be updated every time an object is updated. This has previously been shown to be a potential bottleneck, and in this paper, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 212

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: belief reasoning in mls deductive databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: hasan m. jamil
",y
"LEFT id: NA
RIGHT id: 451

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 482

LEFT text: The idea of building data warehouses as central data collections made available for decision support applications in a company is widely accepted. The concrete design and management of a data warehouse from a technical as well as from an organizational point of view, however, turns out to be far from trivial but requires sophisticated and time consuming efforts. The DMDW workshop was held at the CAiSE’99 conference in Heidelberg on June 14-15, 1999. It had the intention to bring together practitioners and researchers to discuss the design and management of data warehouses. The various presentations gave a broad view on the data warehouse life cycle covering aspects relevant at design time, at build time and at run time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and management of data warehouses report on the dmdw '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stella gatziu , manfred jeusfeld , martin staudt , yannis vassiliou
",y
"LEFT id: NA
RIGHT id: 1827

LEFT text: Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 2193

LEFT text: Traditional approaches to versioning documents are edit-based, and represent successive versions using edit scripts. This paper proposes a reference-based version-ing scheme that preserves the rich logical structure of the evolving document via object references. This approach produces better support for queries, and reconciles the storage-level and transport-level representations of multiversioned XML documents. In particular , we present eecient algorithms for supporting projection and selection queries, and for querying the document evolution history. Then, we show that our representation is also eecient at the transport level, where XML documents are exchanged between remote parties. In fact, with the reference-based scheme, an XML document's history can also be viewed and processed as yet another XML document. Finally, we demonstrate the eeectiveness of the new scheme at the storage level, for which we deene a usefulness-based page management policy, adapted from transaction-time databases, to ensure eecient temporal clustering between versions. The experimental evaluation of the new scheme against previous representations used in temporal databases and persistent-object managers shows the performance advantages of the new approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient management of multiversion documents by object referencing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shu-yao chien , vassilis j. tsotras , carlo zaniolo
",y
"LEFT id: NA
RIGHT id: 1520

LEFT text: The World Wide Web (WWW) is a fast growing global information resource. It contains an enormous amount of information and provides access to a variety of services. Since there is no central control and very few standards of information organization or service offering, searching for information and services is a widely recognized problem. To some degree this problem is solved by “search services,” also known as “indexers,” such as Lycos, AltaVista, Yahoo, and others. These sites employ search engines known as “robots” or “knowbots” that scan the network periodically and form text-based indices. These services are limited in certain important aspects. First, the structural information, namely, the organization of the document into parts pointing to each other, is usually lost. Second, one is limited by the kind of textual analysis provided by the “search service.” Third, search services are incapable of navigating “through” forms. Finally, one cannot prescribe a complex database-like search. We view the WWW as a huge database. We have designed a high-level SQL-like language called W3QL to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. We have implemented a system called W3QS to execute W3QL queries. In W3QS, query results are declaratively specified and continuously maintained as views when desired. The current architecture of W3QS provides a server that enables users to pose queries as well as integrate their own data analysis tools. The system and its query language set a framework for the development of database-like tools over the WWW. A significant contribution of this article is in formalizing the WWW and query processing over it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: information translation , mediation , and mosaic-based browsing in the tsimmis system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joachim hammer , h &#233; ctor garc &#237; a-molina , kelly ireland , yannis papakonstantinou , jeffrey ullman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 2191

LEFT text: We provide a concise yet complete formal definition of the semantics of XPath 1 and summarize efficient algorithms for processing queries in this language. Our presentation is intended both for the reader who is looking for a short but comprehensive formal account of XPath as well as the software developer in need of material that facilitates the rapid implementation of XPath engines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on processing xml in ldap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pedro jos &#233; marr &#243; n , georg lausen
",n
"LEFT id: NA
RIGHT id: 222

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: snakes and sandwiches : optimal clustering strategies for a data warehouse

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 47

LEFT text: The Oracle RDBMS recently introduced an innovative compression technique for reducing the size of relational tables. By using a compression algorithm specifically designed for relational data, Oracle is able to compress data much more effectively than standard compression techniques. More significantly, unlike other compression techniques, Oracle incurs virtually no performance penalty for SQL queries accessing compressed tables. In fact, Oracle's compression may provide performance gains for queries accessing large amounts of data, as well as for certain data management operations like backup and recovery. Oracle's compression algorithm is particularly well-suited for data warehouses: environments, which contains large volumes of historical data, with heavy query workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing medium-dimensionality data in oracle

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , siva ravada , jayant sharma , jay banerjee
",n
"LEFT id: NA
RIGHT id: 410

LEFT text: Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- ""losing"" customers, ""misplacing"" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ajax : an extensible data cleaning tool

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: helena galhardas , daniela florescu , dennis shasha , eric simon
",n
"LEFT id: NA
RIGHT id: 581

LEFT text: This paper presents a number of new techniques for parallelizing geo-spatial database systems and discusses their implementation in the Paradise object-relational database system. The effectiveness of these techniques is demonstrated using a variety of complex geo-spatial queries over a 120 GB global geo-spatial data set.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the glue-nail deductive database system : design , implementation , and evaluation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marcia a. derr , shinichi morishita , geoffrey phipps
",n
"LEFT id: NA
RIGHT id: 1053

LEFT text: In addition to facilitating querying over the Web, XML query languages may provide high level constructs for useful facilities in traditional DBMSs that do not currently exist. In particular, current DBMS query languages do not allow querying across database object types to yield heterogeneous results. This paper motivates the usefulness of heterogeneous querying in traditional DBMSs and investigates XQuery, an emerging standard for XML query languages, to express such queries. The usefulness of querying and storing heterogeneous types is also applied to XML data within a Web information system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pesto : an integrated query/browser for object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , laura m. haas , vivekananda maganty , john h. williams
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1244

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the persistent cache : improving oid indexing in temporal object-oriented database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kjetil n &#248; rv &#229; g
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: Encrypted databases, a popular approach to protecting data from compromised database management systems (DBMS's), use abstract threat models that capture neither realistic databases, nor realistic attack scenarios. In particular, the ""snapshot attacker"" model used to support the security claims for many encrypted databases does not reflect the information about past queries available in any snapshot attack on an actual DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 1063

LEFT text: Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious algorithms for relational query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ambuj shatdal , chander kant , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 677

LEFT text: Our goal is to enhance multidimensional database systems with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. In this paper, we present a key component of our system that characterizes the information content of a cell based on a user's prior familiarity with the cube and provides a context-sensitive exploration of the cube. There are three main modules of this component. A Tracker, that continuously tracks the parts of the cube that a user has visited. A Modeler, that pieces together the information in the visited parts to model the user's expected values in the unvisited parts. An Informer, that processes user's queries about the most informative unvisited parts of the cube. The mathematical basis for the expected value modeling is provided by the classical maximum entropy principle. Accordingly, the expected values are computed so as to agree with every value that is already visited while reducing assumptions about unvisited values to the minimum by maximizing their entropy. The most informative values are defined as those that bring the new expected values closest to the actual values. We believe and prove through experiments that such a user-in-the-loop exploration will enable much faster assimilation of all significant information in the data compared to existing manual explorations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic multidimensional histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nitin thaper , sudipto guha , piotr indyk , nick koudas
",n
"LEFT id: NA
RIGHT id: 1848

LEFT text: Data replication has recently become a topic of increased interest among customers. Several database vendors provide products that perform data replication, The capabilities of these products and the customer problems they solve vary widely. This talk starts by identifying some of the dimensions of the replication solution space including latency, concurrency, logical and physical units of replication, network link requirements, heterogeneity, replica topology, replica transparency, and data transformation requirements. Digital Equipment Corporation provides three products that allow customers to replicate data. The distributed, two-phase commit products allow customers to program and coordinate replicated updates. DECTM Reliable Transaction Router provides an OLTP environment with transactional data replication. Transactions succeed in the face of site and network failures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data warehousing and olap for decision support

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 550

LEFT text: This article serves three purposes. First of all, to introduce dbjobs, the database of database jobs, and also describe its functionality and architecture. Secondly, to present statistics for the dbgrads system, after 18 months of continuous operation. Finally, to describe exciting future projects for SIGMOD Online.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: career-enhancing services at sigmod online

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alexandros labrinidis , alberto o. mendelzon
",y
"LEFT id: NA
RIGHT id: 2110

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic integrity support in sql :1999 and commercial ( object - ) relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: can t &#252; rker , michael gertz
",n
"LEFT id: NA
RIGHT id: 1619

LEFT text: Sybase is a leading RDBMS vendor that started by providing OLTP systems for the client-server environment and is currently maturing into an enterprise-wide data management solution provider. As a critical part of this enterprise-wide data management strategy, Sybase Replication Server supports data replication in a distributed environment. In such an enviromnent, the same data may be replicated at multiple sites for quick data access and for high data availability. Replicated data can be maintained using strong consistency or weak consistence algorithms. Algorithms for maintaining strong consistency, such as primary copy and quorum consensus, severely limit data availability during network partition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sybase replication server

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alex gorelik , yongdong wang , mark deppe
",y
"LEFT id: NA
RIGHT id: 2215

LEFT text: The PBSM algorithm partitions the inputs into manageable chunks, and joins them using a computational geometry based plane-sweeping technique. This paper also presents a performance study comparing the the traditional indexed nested loops join algorithm, a spatial join algorithm based on joining spatial indices, and the PBSM algorithm. These comparisons are based on complete implementations of these algorithms in Paradise, a database system for handling GIS applications. Using real data sets, the performance study examines the behavior of these spatial join algorithms in a variety of situations, including the cases when both, one, or none of the inputs to the join have an suitable index. The study also examines the effect of clustering the join inputs on the performance of these join algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on spatially partitioned temporal join

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: hongjun lu , beng chin ooi , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1

LEFT text: Introduction and Main Contributions Providing mechanisms that allow the user to retrieve desired multimedia information by their semantic content is now an important issue in multimedia databases. However, current prototypes (e.g. Oracle 8i interMedia and Informix Datablade Modules) index mostly only low-level features of multimedia objects. Therefore special techniques are needed for semantic indexing and retrieval of multimedia objects. In this context we present the SMOOTH system, a prototype of a distributed multimedia database system. It implements an integrated querying, annotating, and navigating framework relying on a generic video indexing model. The framework allows the structuring of videos into logical and physical units, and the annotation of these units by typed semantic objects. An index-database stores these structural and semantic information. We provide further a clear concept for capturing and querying the semantic content of multimedia objects, their correlation with low-level objects, as well as their spatio-temporal relationships.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a user-centered interface for querying distributed multimedia databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: isabel f. cruz , kimberly m. james
",n
"LEFT id: NA
RIGHT id: 2247

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: investigation of algebraic query optimisation techniques for database programming languages

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 1587

LEFT text: We present a two-phase Web Query Optimizer (WQO). In a pre-optimization phase, the WQO selects one or more WSIs for a pre-plan; a pre-plan represents a space of query evaluation plans (plans) based on this choice of WSIs. The WQO uses cost-based heuristics to evaluate the choice of WSI assignment in the pre-plan and to choose a good pre-plan. The WQO uses the pre-plan to drive the extended relational optimizer to obtain the best plan for a pre-plan. A prototype of the WQO has been developed. We compare the effectiveness of the WQO, i.e., its ability to efficiently search a large space of plans and obtain a low cost plan, in comparison to a traditional optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel evaluation of multi-join queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: annita n. wilschut , jan flokstra , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 196

LEFT text: Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in the presence of limited access patterns

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniela florescu , alon levy , ioana manolescu , dan suciu
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: This paper describes the design and implementation of PEST0 (Portable Explorer of Snuctured Objects), a user interface that supports browsing and querying of object databases. PEST0 allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (“query-in-place”) that presents querying as a natural extension of browsing. PEST0 is designed to be portable to any object database system that supports a high-level query language; in addition, PEST0 is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text). uniformly and manipulated using an object-oriented dialect of SQL. One component of this project, which is joint work between IBM Almaden and the University of Wisconsin, is the development of a graphical user interface called PEST0 (Portable Explorer of STructured Objects). We refer to the PEST0 interface as a query/browser, as it marries navigational object browsing’ with declarative querying; it integrates browsing and querying via a “query-in-place” paradigm that provides a powerful yet natural user interface for exploring the contents of object databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 174

LEFT text: Microsoft Universal Data Access defines a platform for developing multi-tier enterprise applications that require efficient access to diverse relational or non-relational data sources across intranets or the Internet. Universal Data Access consists of a collection of software components that interact with each other using system-level interfaces defined by OLE DB and providing an application-level data access model called ActiveX Data Objects (ADO). This talk provides an overview of the platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enterprise java platform data access

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: seth white , rick cattell , shel finkelstein
",n
"LEFT id: NA
RIGHT id: 1989

LEFT text: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view maintenance and integrity constraint checking : trading space for time

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: For a number of reasons, even the best query optimizers can very often produce sub-optimal query execution plans, leading to a significant degradation of performance. This is especially true in databases used for complex decision support queries and/or object-relational databases. In this paper, we describe an algorithm that detects sub-optimality of a query execution plan during query execution and attempts to correct the problem. The basic idea is to collect statistics at key points during the execution of a complex query. These statistics are then used to optimize the execution of the query, either by improving the resource allocation for that query, or by changing the execution plan for the remainder of the query. To ensure that this does not significantly slow down the normal execution of a query, the Query Optimizer carefully chooses what statistics to collect, when to collect them, and the circumstances under which to re-optimize the query. We describe an implementation of this algorithm in the Paradise Database System, and we report on performance studies, which indicate that this can result in significant improvements in the performance of complex queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1750

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop on workflow management in scientific and engineering applications-report

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. mcclatchey , g. vossen
",n
"LEFT id: NA
RIGHT id: 403

LEFT text: The AQR-Toolkit divides the query routing task into two cooperating processes: query refinement and source selection. It is well known that a broadly defined query inevitably produces many false positives. Query refinement provides mechanisms to help the user formulate queries that will return more useful results and that can be processed efficiently. As a complimentary process, source selection reduces false negatives by identifying and locating a set of relevant information providers from a large collection of available sources. By pruning irrelevant information sources, source selection also reduces the overhead of contacting the information servers that do not contribute to the answer of the query. The system architecture of AQR-Toolkit consists of a hierarchical network (a directed acyclic graph) with external information providers at the leaves and query routers as mediating nodes. The end-point information providers support query-based access to their documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aqr-toolkit : an adaptive query routing middleware for distributed data intensive systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ling liu , calton pu , david buttler , wei han , henrique paques , wei tang
",y
"LEFT id: NA
RIGHT id: 1786

LEFT text: Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: changing the rules : transformations for rule-based optimizers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mitch cherniack , stan zdonik
",n
"LEFT id: NA
RIGHT id: 1018

LEFT text: This paper describes an advanced development program to create a medical information system called the National Medical Knowledge Bank (NMKB). This five year program is sponsored in part by a grant from the National Institute of Standards and Technology Advanced Technology Program. The goals of the program, covering computer-assisted diagnosis, medical training, remote consultation, and medical records storage, are defined. The webbased architecture of the medical knowledge bank is presented, including the Teradata Multimedia Services, an object/relational database which serves as the central data repository for medical data stored in multiple data types. Also described are the applications of physician support, including case-based reasoning and image analysis for determining case similarity; virtual medical conferences; and initial/continuing medical education.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the national medical knowledge bank

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: warren sterling
",y
"LEFT id: NA
RIGHT id: 1462

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 1975

LEFT text: Existing methods for spatial joins assume the existence of indices for the participating data sets. This assumption is not realistic for applications involving multiple map layer overlays or for queries involving non-spatial selections. In this paper, we explore a spatial join method that dynamically constructs index trees called seeded trees at join time. This methods uses knowledge of the data sets involved in the join process. Seeded trees are R-tree like structures, and are divided into the seed levels and the grown levels. The nodes in the seed levels are used to guide tree growth during tree construction. The seed levels can also be used to filter out some input data during construction, thereby reducing tree size. We develop a technique that uses intermediate linked lists during tree construction and significantly speeds up the tree construction process. The technique allows a large number of random disk accesses during tree construction to be replaced by smaller numbers of sequential accesses. Our performance studies show that spatial joins using seeded trees outperform those using other methods significantly in terms of disk I/O. The CPU penalties incurred are also lower except when seed-level filtering is used.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial hash-joins

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 1271

LEFT text: As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: one-dimensional and multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 199

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1031

LEFT text: Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: This paper addresses the problem of finding the K closest pairs between two spatial data sets, where each set is stored in a structure belonging in the R-tree family. Five different algorithms (four recursive and one iterative) are presented for solving this problem. The case of 1 closest pair is treated as a special case. An extensive study, based on experiments performed with synthetic as well as with real point data sets, is presented. A wide range of values for the basic parameters affecting the performance of the algorithms, especially the effect of overlap between the two data sets, is explored.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 36

LEFT text: Text models focus on the manipulation of textual data. They describe texts by their structure, operations on the texts, and constraints on both structure and operations. In this article common characteristics of machine readable texts in general are outlined. Subsequently, ten text models are introduced. They are described in terms of the datatypes that they support, and the operations defined by these datatypes. Finally, the models are compared.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hypertext databases and data mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: soumen chakrabarti
",n
"LEFT id: NA
RIGHT id: 1512

LEFT text: Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic assumptions and query evaluation in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: claudio bettini , x. sean wang , elisa bertino , sushil jajodia
",n
"LEFT id: NA
RIGHT id: 659

LEFT text: Data Preprocessing for Data Mining addresses one of the most important issues within the well-known Knowledge Discovery from Data process. Data directly taken from the source will likely have inconsistencies, errors or most importantly, it is not ready to be considered for a data mining process. Furthermore, the increasing amount of data in recent science, industry and business applications, calls to the requirement of more complex tools to analyze it. Thanks to data preprocessing, it is possible to convert the impossible into possible, adapting the data to fulfill the input demands of each data mining algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data mining : practical machine learning tools and techniques with java implementations

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ian h. witten , eibe frank
",n
"LEFT id: NA
RIGHT id: 1001

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized view selection for multidimensional datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 1962

LEFT text: Internet search engines have popularized keyword based search. While relational database systems offer powerfifl structured query languages such as SQL, there is no support for keyword search over databases. The simplicity of keyword search as a querying paradigm offers compelling values for data exploration. Specifically, keyword search does not require a priori knowledge of the schema. The above is significant as much information in a corporation is increasingly being available at its intranet. However, it is unrealistic to expect users who would browse and query such information to have detailed knowledge of the schema of available databases. Therefore, just as keyword search and classification hierarchies complement each other for document search, keyword search over databases can be effective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 2117

LEFT text: E-commerce is not a static field, but is constantly evolving to discover new and more effective ways of supporting businesses. Data management is an integral part of this effort, This special issue aims to report on some of the recent developments and identify some research directions in this area. Initially, e-commeree involved the use of ED] and intranets. Today we see the dominance of XML. Almost all recent elecn'onic commerce standards are based on X1VD... As a consequence, the amount of XML data being stored is large, and it is increasing. This naturally leads to the question of how to store and query the XML documents. The paper by Tian, DeWitt, Chen and Zhang describes the design and performance evaluation of alternative XM]., storage strategies. The results of this performance study provide valuable hints on how to store the XM1., files depending on the application. Personalization in e-commerce is about building customer loyalty by understanding and thus addressing the needs of each individual. E-commerce systems need customers' profiles to provide better services, 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: issues in data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lukasz golab , m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 313

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: epsilon grid order : an algorithm for the similarity join on massive high-dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian b &#246; hm , bernhard braunm &#252; ller , florian krebs , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 817

LEFT text: Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: probabilistic optimization of top n queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: donko donjerkovic , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1657

LEFT text: The IFO data model was proposed by Abiteboul and Hull [Abiteboul 87] as a formalized semantic database model. It has been claimed by the authors that the model subsumes the Relational model [Codd 70], the Entity-Relationship model [Chen 76], the Functional Data Model [Kerschberg 76] and virtually all of the structured aspects of the Semantic Data Model [Hammer 81], the INSYDE Model [King 85], and the Extended Semantic Hierarchy Model [Brodie 84].This paper examines the IFO data model as presented in [Abiteboul 87], compares it to other models, and thus concludes that the IFO data model is actually a subset of the Semantic Data Model proposed by Hammer in [Hammer 81]. The paper also shows that the IFO data model has failed to support concepts that are essential to both the E-R model and the Semantic Data Model which are claimed to be subsumed by the IFO model.Section 2 discusses the three IFO constructs, objects, fragments, and relationships. The mapping of these constructs to constructs in the Semantic Data Model is established as an informal proof of the result that the IFO model is subsumed by the SDM.Section 3 lists constructs supported by the Entity-Relationship model [Chen 76, Teorey 86] as will as constructs supported by SDM [Hammer 81]that the IFO data model fails to support.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a close look at the ifo data model

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: magdy s. hanna
",y
"LEFT id: NA
RIGHT id: 668

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1535

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the naos system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: c. collet , t. coupaye
",n
"LEFT id: NA
RIGHT id: 1619

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sybase replication server

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alex gorelik , yongdong wang , mark deppe
",n
"LEFT id: NA
RIGHT id: 1773

LEFT text: The idea of building data warehouses as central data collections made available for decision support applications in a company is widely accepted. The concrete design and management of a data warehouse from a technical as well as from an organizational point of view, however, turns out to be far from trivial but requires sophisticated and time consuming efforts. The DMDW workshop was held at the CAiSE’99 conference in Heidelberg on June 14-15, 1999. It had the intention to bring together practitioners and researchers to discuss the design and management of data warehouses. The various presentations gave a broad view on the data warehouse life cycle covering aspects relevant at design time, at build time and at run time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 1407

LEFT text: In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: transaction timestamping in ( temporal ) databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian s. jensen , david b. lomet
",n
"LEFT id: NA
RIGHT id: 868

LEFT text: Fast indexing in time sequence databases for similarity searching has attracted a lot of research recently. Most of the proposals, however, typically centered around the Euclidean distance and its derivatives. We examine the problem of multimodal similarity search in which users can choose the best one from multiple similarity models for their needs. In this paper, we present a novel and fast indexing scheme for time sequences, when the distance function is any of arbitrary Lp norms (p = 1; 2; : : : ;1). One feature of the proposed method is that only one index structure is needed for all Lp norms including the popular Euclidean distance (L2 norm). Our scheme achieves significant speedups over the state of the art: extensive experiments on real and synthetic time sequences show that the proposed method is comparable to the best competitor forL2 andL1 norms, but significantly (up to 10 times) faster for L1 norm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast time sequence indexing for arbitrary lp norms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: byoung-kee yi , christos faloutsos
",y
"LEFT id: NA
RIGHT id: 547

LEFT text: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 127

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",y
"LEFT id: NA
RIGHT id: 202

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate computation of multidimensional aggregates of sparse data using wavelets

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jeffrey scott vitter , min wang
",y
"LEFT id: NA
RIGHT id: 176

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: java and relational databases ( tutorial ) : sqlj

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gray clossman , phil shaw , mark hapner , johannes klein , richard pledereder , brian becker
",n
"LEFT id: NA
RIGHT id: 1891

LEFT text: Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental data structures and algorithms for dynamic query interfaces

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: egemen tanin , richard beigel , ben shneiderman
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1259

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic heterogeneity resolution in federated databases by metadata implantation and stepwise evolution

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: goksel aslan , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 185

LEFT text: We are witnessing a profound change in the global information infrastructure that has the potential to fundamentally impact many facets of our life. An important aspect of the evolving infrastructure is the seamless, ubiquitous wireless connectivity which engenders continuous interactions between people and interconnected computers. A challenging area of future ubiquitous wireless computing is the area of providing mobile users with integrated Personal Information Services and Applications (PISA). In this paper, a wireless client/server computing architecture will be discussed for the delivery of PISA. Data management issues such as transactional services and cache consistency will be examined under this architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient transparent application recovery in client-server information systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 888

LEFT text: We use conventional hardware for servers and clients and examine bottlenecks and optimization options systematically, in order to reduce jitter and increase the maximum number of clients that the system can support. We show that the diversity of client performance characteristics can be taken into account, so that all clients are well supported for delay-sensitive retrieval in a heterogeneous environment. We also show that their characteristics can be exploited to maximize server throughput under server memory constrains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design and development of a stream service in a heterogenous client environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: nikos pappas , stavros christodoulakis
",y
"LEFT id: NA
RIGHT id: 2182

LEFT text: Abstract.We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates, considering separatelycases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: incremental computation and maintenance of temporal aggregates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun yang , jennifer widom
",y
"LEFT id: NA
RIGHT id: 300

LEFT text:  In this paper we present a second enhancement: a single operator that lets the analyst get summarized reasons for drops or increases observed at an aggregated level. This eliminates the need to manually drill-down for such reasons. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design a dynamic programming algorithm that requires only one pass of the data improving significantly over our initial greedy algorithm that required multiple passes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing multidimensional index trees for main memory access

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kihong kim , sang k. cha , keunjoo kwon
",n
"LEFT id: NA
RIGHT id: 1963

LEFT text: We present a framework for designing, in a declarative and flexible way, efficient migration programs and an undergoing implementation of a migration tool called RelOO whose targets are any ODBC compliant system on the relational side and the 02 system on the object side. The framework consists of (i) a declarative language to specify database transformations from relations to objects, but also physical properties on the object database (clustering and sorting) and (ii) an algebrabased program rewriting technique which optimizes the migration processing time while taking into account physical properties and transaction decomposition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: two techniques for on-line index modification in shared nothing parallel databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kiran j. achyutuni , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1967

LEFT text: Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traffic gives a thousand fold increase in deadlocks or reconciliations. Master copy replication (primary copy) schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the dangers of replication and a solution

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jim gray , pat helland , patrick o'neil , dennis shasha
",y
"LEFT id: NA
RIGHT id: 1292

LEFT text: We illustrate basic features of the Lixto wrapper generator such as the user and system interaction, the capacious visual interface, the marking and selecting procedures, and the extraction tasks by describing the construction of a simple example program in the current Lixto prototype.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supervised wrapper generation with lixto

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: robert baumgartner , sergio flesca , georg gottlob
",y
"LEFT id: NA
RIGHT id: 1440

LEFT text: The cost functions are parameterized to accommodate various hardware characteristics appropriately. Combining the basic patterns, we can describe the memory access patterns of database operations. The cost functions of database operations can automatically be derived by combining the basic patterns' cost functions accordingly. To validate our approach, we performed experiments using our DBMS prototype Monet. The results presented here confirm the accuracy of our cost models for different operations. Aside from being useful for query optimization, our models provide insight to tune algorithms not only in a main-memory DBMS, but also in a disk-based DBMS with a large main-memory buffer cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: generalized search trees for database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton , avi pfeffer
",n
"LEFT id: NA
RIGHT id: 1074

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 916

LEFT text: The proliferation of mobile and pervasive computing devices has brought energy constraints into the limelight, together with performance considerations. Energy-conscious design is important at all levels of the system architecture, and the software has a key role to play in conserving the battery energy on these devices. With the increasing popularity of spatial database applications, and their anticipated deployment on mobile devices (such as road atlases and GPS based applications), it is critical to examine the energy implications of spatial data storage and access methods for memory resident datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: energy-performance trade-offs for spatial access methods on memory-resident data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ning an , sudhanva gurumurthi , anand sivasubramaniam , narayanan vijaykrishnan , mahmut kandemir , mary jane irwin
",n
"LEFT id: NA
RIGHT id: 1044

LEFT text: In these environments, the best retrieval performance can be achieved only if the data is clustered on the tertiary storage by all searchable attributes of the events. Since the number of these attributes is high, the underlying data-management facility must be able to cope with extremely large volumes and very high dimensionalities of data at the same time. The proposed indexing technique is designed to facilitate both clustering and efficient retrieval of high-dimensional data on tertiary storage. The structure uses an original space-partitioning scheme, which has numerous advantages over other space-partitioning techniques. While the main objective of the design is to support high-energy physics experiments, the proposed solution is appropriate for many other scientific applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the x-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 16

LEFT text: The AQR-Toolkit divides the query routing task into two cooperating processes: query refinement and source selection. It is well known that a broadly defined query inevitably produces many false positives. Query refinement provides mechanisms to help the user formulate queries that will return more useful results and that can be processed efficiently. As a complimentary process, source selection reduces false negatives by identifying and locating a set of relevant information providers from a large collection of available sources. By pruning irrelevant information sources, source selection also reduces the overhead of contacting the information servers that do not contribute to the answer of the query. The system architecture of AQR-Toolkit consists of a hierarchical network (a directed acyclic graph) with external information providers at the leaves and query routers as mediating nodes. The end-point information providers support query-based access to their documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbis-toolkit : adaptable middleware for large scale data delivery

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mehmet altinel , demet aksoy , thomas baby , michael franklin , william shapiro , stan zdonik
",n
"LEFT id: NA
RIGHT id: 1226

LEFT text: This paper describes the architecture of OPERA, a generic platform for building distributed systems over stand alone applications. The main contribution of this research effort. is t,o propose a “kernel” system providing the “essentials” for distributed processing and to show the important role database technology may play in supporting such functionality. These include a powerful process management environment. created as a generalization of workflow ideas and incorporating transactional notions such as spheres of isolation, atomicit.y, and persistence and a transactional engine enforcing correctness based on the nested and multi-level models. It also includes a tool-kit providing externalized database functionality enabling physical database design over heterogeneous data repositories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: distance-based outliers : algorithms and applications

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng , vladimir tucakov
",n
"LEFT id: NA
RIGHT id: 2202

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 202

LEFT text:  In this paper we present a second enhancement: a single operator that lets the analyst get summarized reasons for drops or increases observed at an aggregated level. This eliminates the need to manually drill-down for such reasons. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design a dynamic programming algorithm that requires only one pass of the data improving significantly over our initial greedy algorithm that required multiple passes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate computation of multidimensional aggregates of sparse data using wavelets

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 1900

LEFT text: In this paper, we present visualizations of parts of the network of documents comprising the World Wide Web. We describe how we are using the Hy+ visualization system to visualize the portion of the World Wide Web explored during a browsing session. As the user browses, the web browser communicates the URL and title of each document fetched as well as all the anchors contained in the document. Hy+ displays graphically the history of the navigation and multiple views of the structure of that portion of the web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: pixel-oriented database visualizations

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 2021

LEFT text: At Berkeley, we are developing TelegraphCQ [1, 2], a dataflow system for processing continuous queries over data streams. TelegraphCQ is based on a novel, highly-adaptive architecture supporting dynamic query workloads in volatile data streaming environments. In this demonstration we show our current version of TelegraphCQ, which we implemented by leveraging the code base of the open source PostgreSQL database system. Although TelegraphCQ differs significantly from a traditional database system, we found that a significant portion of the PostgreSQL code was easily reusable. We also found the extensibility features of PostgreSQL very useful, particularly its rich data types and the ability to load user-developed functions. Challenges: As discussed in [1], sharing and adaptivity are our main techniques for implementing a continuous query system. Doing this in the codebase of a conventional database posed a number of challenges:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: telegraphcq : continuous dataflow processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sirish chandrasekaran , owen cooper , amol deshpande , michael j. franklin , joseph m. hellerstein , wei hong , sailesh krishnamurthy , samuel r. madden , fred reiss , mehul a. shah
",y
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 283

LEFT text: Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: clio : a semi-automatic tool for schema mapping

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mauricio a. hern &#225; ndez , ren &#233; e j. miller , laura m. haas
",n
"LEFT id: NA
RIGHT id: 1247

LEFT text: We present HOMER, a CASE tool for building and maintaining complex, data-intensive Web sites. In HOMER the processes of creation and maintenance of a Web site are completely based on the adoption of suitable models, to describe the various aspects of the site (content~ navigation structure, presentation). The development of a site does not require any code writing activity: based on the results of the design process, the system automatically creates programs to implement the site, statically and/or dynamically, as needed; also, the system does not depend on any specific tool or language: it has a modular architecture, which integrates external servers for specific tasks; finally, the system supports site administrators for several maintenance activities, which can involve changes over the site at different levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: building and customizing data-intensive web sites using weave

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: Database systems offer efficient and reliable technology to query structured data. However, because of the explosion of the World Wide Web [11], an increasing amount of information is stored in repositories organized according to less rigid structures, usually as hypertextual documents, and data access is based on browsing and information retrieval techniques. Since browsing and search engines present important limitations [8], several query languages [19, 20, 23] for the Web have been recently proposed. These approaches are mainly based on a loose notion of structure, and tend to see the Web as a huge collection of unstructured objects, organized as a graph. Clearly, traditional database techniques are of little use in this field, and new techniques need to be developed. In this paper, we present the approach to the management of Web data as attacked in the ArtANEUS project carried out by the database group at Universith di l=toma Tre. Our approach is based on a generalization of the notion of view to the Web framework. In fact, in traditional databases, views represent an essential tool for restructuring and integrating da ta to be presented to the user. Since the Web is becoming a major computing platform and a uniform interface for sharing data, we believe that also in this field a sophisticate view mechanism is needed, with novel features due to the semi-structured nature of the Web. First, in this context, restructuring and presenting da ta under different perspectives requires the generation of derived Web hypertexts, in order to re-organize and re-use portions of the Web. To do this, da ta from existing Web sites must be extracted, and then queried and integrated in order to build new hypertexts, i.e., hypertextual views over the original sites; these manipulations can be better attained in a more structured framework, in which traditional database technology can be leveraged to analyze and correlate information. Therefore, there seem to be different view levels in this framework: (i) at the first level, da ta are extracted from the sites of interest and given a database structure, which represents a first structured view over the original semi-structured data; (ii) then, further database views can be built by means of reorganizations and integrations based on traditional database techniques; (iii) finally, a derived hypertext can be generated offering an alternative or integrated hypertextual view over the original sites. In the process, data go from a loosely structured organizat ion-the Web pages-to a very structured onethe database--and then again to Web structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1767

LEFT text: Since the Web was not originally designed to support such applications, Web application development efforts increasingly run into limitations of the basic Web infrastructure.If the Web is to be used as the basis of complex enterprise applications, it must provide generic capabilities similar to those provided by the OMA (although these may need to be adapted to the more open, flexible nature of the Web, and specific requirements of Web applications).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards a richer web object model

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: frank manola
",y
"LEFT id: NA
RIGHT id: 721

LEFT text: LEO is a comprehensive way to repair incorrect statistics and cardinality estimates of a query execution plan. LEO introduces a feedback loop to query optimization that enhances the available information on the database where the most queries have occurred, allowing the optimizer to actually learn from its past mistakes. We demonstrate how LEO learns outdated table access statistics on a TPC-H like database schema and show that LEO improves the estimates for table cardinalities as well as filter factors for single predicates. Thus LEO enables the query optimizer to choose a better query execution plan, resulting in more efficient query processing. We not only demonstrate learning by repetitive execution of a single query, but also illustrate how similar, but not identical queries benefit from learned knowledge. In addition, we show the effect of both learning cardinalities and adjusting related statistics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: learning table access cardinalities with leo

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: volker markl , guy lohman
",y
"LEFT id: NA
RIGHT id: 1667

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1844

LEFT text: Authors and publishers who wish their publications to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4. All relevant books received will be listed, but not all can be reviewed. Technical reports (other than dissertations) will not be listed or reviewed. Authors should be aware that some publishers will not send books for review (even when instructed to do so); authors wishing to inquire as to whether their book has been received for review may contact the book review editor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the distributed information search component ( disco ) and the world wide web

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: anthony tomasic , r &#233; my amouroux , philippe bonnet , olga kapitskaia , hubert naacke , louiqa raschid
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 1632

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing disjunctive queries with expensive predicates

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: a. kemper , g. moerkotte , k. peithner , m. steinbrunn
",y
"LEFT id: NA
RIGHT id: 1636

LEFT text: With the proliferation of the world's “information highways” a renewed interest in efficient document indexing techniques has come about. In this paper, the problem of incremental updates of inverted lists is addressed using a new dual-structure index. The index dynamically separates long and short inverted lists and optimizes retrieval, update, and storage of each type of list. To study the behavior of the index, a space of engineering trade-offs which range from optimizing update time to optimizing query performance is described. We quantitatively explore this space by using actual data and hardware in combination with a simulation of an information retrieval system. We then describe the best algorithm for a variety of criteria.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental updates of inverted lists for text document retrieval

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: anthony tomasic , h &#233; ctor garc &#237; a-molina , kurt shoens
",y
"LEFT id: NA
RIGHT id: 1393

LEFT text: We present an architecture and a set of challenges for peer database management systems. These systems team up to build a network of nodes (peers) that coordinate at run time most of the typical DBMS tasks such as the querying, updating, and sharing of data. Such a network works in a way similar to conventional multidatabases. Conventional multidatabase systems are founded on key concepts such as those of a global schema, central administrative authority, data integration, global access to multiple databases, permanent participation of databases, etc. Instead, our proposal assumes total absence of any central authority or control, no global schema, transient participation of peer databases, and constantly evolving coordination rules among databases. In this work, we describe the status of the Hyperion project, present our current solutions, and outline remaining research issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information integration : the momis project demonstration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , silvana castano , alberto corni , r. guidetti , g. malvezzi , michele melchiori , maurizio vincini
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1079

LEFT text: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multidimensional access methods : trees have grown everywhere

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: timos k. sellis , nick roussopoulos , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 862

LEFT text: Conduct of scientific and engineering research is becoming critically dependent on effective management of scientific and engineering data and technical information. The rapid advances in scientific instrumentation, computer and communication technologies enable the scientists to collect, generate, process, and share unprecedented volumes of data. For example, the Earth Observing System Data and Information System (EOSDIS) has the task to manage the data from NASA’s Earth science research satellites and field measurement programs, and other data essential for the interpretation of these measurements in support of global change research. Apart from being able to handle a stream of 1 terabyte of data daily by the year 2000, EOSDIS will also need to provide transparent access to heterogeneous data held in the archives of several US government agencies, organizations and countries. A single graphical user interface employing the Global Change Master Directory needs to help users locate data sets of interest among massive and diverse data sets, or find the appropriate data analysis tools, regardless of their location. Another major international effort in the area of human genome research faces some similar, as well as unique issues due to the complexity of the genome data, special querying requirements and much more heterogeneous collections of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: panel : future directions of database research - the vldb broadening strategy , part 2

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael l. brodie
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management. Even though the idea of randomized linear projections (a.k.a., sketches) was known for some time in the domain of functional analysis (dating back to the famous Johnson-Lindenstrauss Lemma), Alon, Matias, and Szegedy were the first to exploit sketches for small-space data-stream computation, through the use of limited-independence random variates that can be constructed in small space and time. Of course, in addition to small-space sketching, the AMS paper also makes a number of other fundamental contributions in data streaming, including practical approximation algorithms for other frequency moments (e.g., the number of distinct values in a stream), as well as several inapproximability results (i.e., lower bounds) based on beautiful communication-complexity arguments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 562

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of illinois at urbana-champaign

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: m. winslett , k. chang , a. doan , j. han , c. zhai , y. zhou
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1690

LEFT text: This paper examines the characteristics and challenges presented by medical databases and medical information systems. It begins with a survey of medical databases/information systems. This is followed by a list of challenges for database management systems generated by the needs of these systems. It concludes with a look at some systems which address these challenges. In the context of this background information, the database community is asked to consider whether the results of database research are reaching those who are making day-to-day decisions regarding design and implementation of medical information systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: medical information systems : characterization and challenges

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jorge c. g. ramirez , lon a. smith , lynn l. peterson
",y
"LEFT id: NA
RIGHT id: 2040

LEFT text: Publisher Summary  This chapter presents the first XPath query evaluation algorithm that runs in polynomial time with respect to the size of both the data and of the query. XPath has been proposed by the W3C as a practical language for selecting nodes from XML document trees. XPath is important because of its potential application as an XML query language per se, it being at the core of several other XML-related technologies, such as XSLT, XPointer, and XQuery, and the great and well-deserved interest such technologies receive. Since XPath and related technologies will be tested in ever-growing deployment scenarios, its implementations need to scale well both with respect to the size of the XML data and the growing size and intricacy of the queries (usually referred to as combined complexity).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream processing of xpath queries with predicates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ashish kumar gupta , dan suciu
",n
"LEFT id: NA
RIGHT id: 1709

LEFT text: Although many extended transaction models have been proposed [Elm93], few practical implementations exist and even fewer can support more than one model. We present the Reflective Transaction Framework, as a practical and modular method to implement extended transaction models. We achieve modularity by applying the Open Implementation approach [Kic92] (also known as meta-object protocol [KdRBSl]) to the design of the reflective transaction framework. We achieve practicality by implementing on top of a commercial transaction processing monitor. For our implementation of the reflective transaction framework, we introduce transaction adapters, add-on modules built on top of existing commercial TP components, such as Encina, that extend their functionality to support extended transaction features and semantics. Since our framework design is based on the transaction processing monitor architecture [GR93], it is widely applicable to many modern TP monitors. The reflective transaction framework enables us to implement a wide range of independently proposed extended transaction models, which we demonstrate by implementing the split/join model [PKH88] and cooperative transaction groups [MP92, RC92].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: synthesis of extended transaction models using acta

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: panos k. chrysanthis , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: In the early 1980's, researchers recognized that semantic information stored in databases as integrity constraints could be used for query optimization. A new set of techniques called semantic query optimization (SQO) was developed. Some of the ideas developed for SQO have been used commercially, but to the best of our knowledge, no extensive implementations of SQO exist today. In this paper, we describe an implementation of two SQO techniques, Predicate Introduction and Join Elimination, in DB2 Universal Database. We present the implemented algorithms and performance results using the TPCD and APB-1 OLAP benchmarks. Our experiments show that SQO can lead to dramatic query performance improvements. A crucial aspect of our implementation of SQO is the fact that it does not rely on complex integrity constraints (as many previous SQO techniques did); we use only referential integrity constraints and check constraints.  

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 780

LEFT text: Video is composed of audio-visual information. Providing content based access to video data is essential for the sucessful integration of video into computers. Organizing video for content based access requires the use of video metadata. This paper explores the nature video metadata. A data model for video databases is presented based on a study of the applications of video, the nature of video retrieval requests, and the features of video. The data model is used in the architectural framework of a video database. The current state of technology in video databases is summarized and research issues are highlighted.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: The paper presents a systematic review of the relative efficacy of traditional listing and the USPS address list as sampling frames for national probability samples of households. NORC and ISR collaborated to compare these two national area-probability sampling frames for household surveys. We conducted this comparison in an ongoing survey operation which combines the current wave of the HRS with the first wave of NSHAP. Since 2000, survey samplers have been exploring the potential of the USPS address lists to serve as a sampling frame for probability samples from the general population. We report the relative coverage properties of the two frames, as well as predictors of the coverage and performance of the USPS frame. The research provides insight into the coverage and cost/benefit trade-offs that researchers can expect from traditionally listed frames and USPS address databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 2086

LEFT text: We propose an novel method of computing and storing DataCubes. Our idea is to use Bayesian Networks, which can generate approximate counts for any query combination of attribute values and “don’t cares.” A Bayesian network represents the underlying joint probability distribution of the data that were used to generate it. By means of such a network the proposed method, NetCube, exploits correlations among attributes. Our proposed preprocessing algorithm scales linearly on the size of the database, and is thus scalable; it is also parallelizable with a straightforward parallel implementation. Moreover, we give an algorithm to estimate counts of arbitrary queries that is fast ( constant on the database size). Experimental results show that NetCubes have fast generation and use (a few

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpress : a queriable compression for xml data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun-ki min , myung-jae park , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: Recent work on querying data streams has focused on systems where newly arriving data is processed and continuously streamed to the user in real time. In many emerging applications, however, ad hoc queries and/or intermittent connectivity also require the processing of data that arrives prior to query submission or during a period of disconnection. For such applications, we have developed PSoup, a system that combines the processing of ad hoc and continuous queries by treating data and queries symmetrically, allowing new queries to be applied to old data and new data to be applied to old queries. PSoup also supports intermittent connectivity by separating the computation of query results from the delivery of those results. PSoup builds on adaptive query-processing techniques developed in the Telegraph project at UC Berkeley. In this paper, we describe PSoup and present experiments that demonstrate the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1554

LEFT text: Rapid growth in the volume of documents, their diversity, and terminological variations render federated digital libraries increasingly difficult to manage. Suitable abstraction mechanisms are required to construct meaningful and scalable document clusters, forming a cross-digital library information space for browsing and semantic searching. This paper addresses the above issues, proposes a distributed semantic framework that achieves a logical partitioning of the information space according to topic areas, and provides facilities to contextualize and landscape the available document sets in subject-specific categories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information finding in a digital library : the stanford perspective

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: tak w. yan , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX has to be updated every time an object is updated. This has previously been shown to be a potential bottleneck, and in this paper, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 1924

LEFT text: The Summary Schemas Model (SSM) is proposed as an extension to multidatabase systems to aid in semantic identification. The SSM uses a global data structure to abstract the information available in a multidatabase system. This abstracted form allows users to use their own terms (imprecise queries) when accessing data rather than being forced to use system-specified terms. The system uses the global data structure to match the user's terms to the semantically closest available system terms. A simulation of the SSM is presented to compare imprecise-query processing with corresponding query-processing costs in a standard multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the semantics of now in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james clifford , curtis dyreson , tom &#225; s isakowitz , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1825

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient view maintenance at data warehouses

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. agrawal , a. el abbadi , a. singh , t. yurek
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 676

LEFT text: Temporal, spatial and spatiotemporal queries are inherently multidimensional, combining predicates on explicit attributes with predicates on time dimension(s) and spatial dimension(s). Much confusion has prevailed in the literature on access methods because no consistent notation exists for referring to such queries. As a contribution towards eliminating this problem, we propose a new and simple notation for spatiotemporal queries. The notation aims to address the selection-based spatiotemporal queries commonly studied in the literature of access methods. The notation is extensible and can be applied to more general multidimensional, selection-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation for spatio-temporal queries to moving objects

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong-jin choi , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 668

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: We propose and evaluate two indexing schemes for improving the efficiency of data retrieval in high-dimensional databases that are incomplete. These schemes are novel in that the search keys may contain missing attribute values. The first is a multi-dimensional index structure, called the Bitstring-augmented R-tree (BR-tree), whereas the second comprises a family of multiple one-dimensional one-attribute (MOSAIC) indexes. Our results show that both schemes can be superior over exhaustive search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 959

LEFT text: When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caprera : an activity framework for transaction processing on wide-area networks

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: suresh kumar , eng-kee kwang , divyakant agrawal
",n
"LEFT id: NA
RIGHT id: 846

LEFT text: 1. Electronic Commerce Electronic commerce challenges our notions of distributed transactions in several ways. I discuss issues how distributed transactions can apply to electronic transactions, with special emphasis on the role of atomic@. I discuss the application of these ideas to two systems I have helped design and build: NetBill (a system for highly atomic micro-transactions) and Cryptographic Postage Indicia (a system for generating postage on laser printers attached to PCs or other devices.) I discuss the dijjficulties in integrating atomic, anonymous payment systems and some issues in supporting anonymous auctions. Finally, I conclude with a set of open questions. Electronic commerce is clearly among the most exciting developments in Internet based applications today. Here are some measures: Dell reports selling more than three million dollars worth of computers each day from their web site. Ernst & Young reports for that the online stores now offer the best prices for 90% of all consumer goods. 10% of all flower orders received by I-800-FLOWERS now arrive via the world wide web. Estimates vary on the amount of electronic commerce now occurring. Here is one measure of the excitement over electronic commerce: the 12 June 1995 issue of Business Week includes the following projection of the role of electronic commerce. This projection is probably overly optimistic, but it indicates that electronic commerce is being taken seriously in some quarters. *Effective September 1998, the author will hold a joint appointment in between the Electrical Engineering & Computer Science Department and the School of Information Management & Systems, both at the University of California, Berkeley, 94720. The author’s e-mail addresses will be tygar@cs.berkeley.edu and tygar@sims.berkeley.edu. This work was in part supported by DARPA (Contract F19628-96-C0061). NSF (Cooperative Agreement IRI-9411299), and the US Postal Service. The U.S Government is authorized to reproduce and distribute reprints for government purposes, not withstanding any copyright notations thereon. The views and conclusions contained in this document are those of the author and should not be interpreted as reflecting the official policies, either expressed or implied, of any of the supporting agencies or the U.S. Government. Traditional Commerce (billion $) Electronic

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: active views for electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: serge abiteboul , bernd amann , sophie cluet , adi eyal , laurent mignet , tova milo
",n
"LEFT id: NA
RIGHT id: 2110

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic integrity support in sql :1999 and commercial ( object - ) relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: can t &#252; rker , michael gertz
",n
"LEFT id: NA
RIGHT id: 1068

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining deviants in a time series database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , s. muthukrishnan
",n
"LEFT id: NA
RIGHT id: 1673

LEFT text: Inasmuch for speed to customers desires change and large completion that describe day world. To lead technology and operations technology to form general, to achieve competitive advantage and special form design technology is master key to determine nature and form product, and what tolerable quality levels that work fit product to uses, and all of features and preferences determine through design technology. For importance CAD/CAM subject, we introduce in this research that offer primary components to CAM system, and styles this system to achieve details work steps, and details design steps. From within completely program in (AutoCAD) system with details steps to how design transportation to manufacturing operations to series achieve to desired product. With offer conclusions that fitness between CAD and CAM to introduce direction communication between design and manufacturing lead to mistakes reduction to large ratio.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the myriad federated database prototype

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: s.-y . hwang , e.-p . lim , h.-r . yang , s. musukula , k. mediratta , m. ganesh , d. clements , j. stenoien , j. srivastava
",n
"LEFT id: NA
RIGHT id: 1609

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 2269

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementing lazy database updates for an object database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari
",n
"LEFT id: NA
RIGHT id: 1058

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: physical data independence , constraints , and optimization with universal plans

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , lucian popa , val tannen
",n
"LEFT id: NA
RIGHT id: 1042

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a new sql-like operator for mining association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rosa meo , giuseppe psaila , stefano ceri
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 2171

LEFT text: Interesting patterns often occur at varied levels of support. The classic association mining based on a uniform minimum support, such as Apriori, either misses interesting patterns of low support or suuers from the bottleneck of itemset generation. A better solution is to exploit support constraints, which specify what minimum support is required for what itemsets, so that only necessary itemsets are generated. In this paper, we present a framework of frequent itemset mining in the presence of support constraints. Our approach is to \push"" support constraints into the Apriori itemset generation so that the \best"" minimum support is used for each itemset at run time to preserve the essence of Apriori.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: efficient dynamic mining of constrained frequent sets

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , carson kai-sang leung , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 452

LEFT text: VERSANT is the industry’s leading object database management system (ODBMS) for developing applications in multi-user, dstribtrted environments. VERSANT ODBMS has an objectbased client-server architecture which is particularly suitable for such complex applications as telecommunications, trartsportations, and utilities network management systems. Because these applications are usually mission-critical and do not allow my dtia. base down time, one major requirement of our customers is 24x7 (24 hours a day, 7 days a week) high-availability of object databases, even in the presence of software, hardware, or network failures. While many VERSANT features, such as on-line backup and dynamic schema evohrtio~ have already supported part of this requiremen~ they do not address the failure cases. Traditional asynchronous replication (e.g., Sybase Replication Server, DEC Data Distributor, and Oracle Symmetric Replication) is not suitable for the following two reasons, Fhs~ data integrity maybe lost as a result of either delay during propagation or conflicting update requests from different replicates. Second, any failure in the local replica database or central primary database is not transparent to the applications. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: moving up the food chain : supporting e-commerce applications on databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: anant jhingran
",n
"LEFT id: NA
RIGHT id: 768

LEFT text: Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining distance-based outliers in large datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",y
"LEFT id: NA
RIGHT id: 790

LEFT text: We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: quality-driven integration of heterogenous information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: felix naumann , ulf leser , johann christoph freytag
",n
"LEFT id: NA
RIGHT id: 599

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 601

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of data distribution for selectivity estimation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kyu young whang , sang wook kim , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 2001

LEFT text: We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: gigascope : a stream database for network applications

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chuck cranor , theodore johnson , oliver spataschek , vladislav shkapenyuk
",y
"LEFT id: NA
RIGHT id: 1108

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 10

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and implementation of rmp : a virtual electronic market place

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: susanne boll , wolfgang klas , bernard battaglin
",n
"LEFT id: NA
RIGHT id: 1020

LEFT text: We believe that the support of video on mobile systems will indeed make possible many new interesting applications. However, providing mobile video is a non-trivial task, and much work needs to be done before practical systems are widely available. In this short note we address the issue of mobile multimedia from a practitioner's perspective. We note what software and hardware are currently available in the market in support of mobile multimedia, and point out some of their deficiencies. We also discuss some of the communication and data management research issues that need to be tackled in order to address said deficiencies. Exploring these research issues is the focus of our project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental clustering for mining in a data warehousing environment

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: martin ester , hans-peter kriegel , j &#246; rg sander , michael wimmer , xiaowei xu
",n
"LEFT id: NA
RIGHT id: 1809

LEFT text: In this paper, we study a simple SQL extension that enables query writers to explicitly limit the cardinality of a query result. We examine its impact on the query optimization and run-time execution components of a relational DBMS, presenting two approaches—a Conservative approach and an Aggressive approach—to exploiting cardinality limits in relational query plans. Results obtained from an empirical study conducted using DB2 demonstrate the benefits of the SQL extension and illustrate the tradeoffs between our two approaches to implementing it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on saying enough already ! in sql

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. carey , donald kossmann
",y
"LEFT id: NA
RIGHT id: 2019

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1342

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing : taming the terabytes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: minos n. garofalakis , phillip b. gibbon
",n
"LEFT id: NA
RIGHT id: 2078

LEFT text: Publisher Summary This chapter provides a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages. Stream data are also generated naturally by web services, in which loosely coupled systems interact by exchanging high volumes of business data tagged in XML, forming continuous XML data streams. A central aspect of web services is the ability to efficiently operate on these XML data streams executing queries to continuously match, extract, and transform parts of the XML data stream to drive legacy back-end business applications. Manipulating stream data presents many technical challenges, which are just beginning to be addressed in the database, systems, algorithms, networking, and other computer science communities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 260

LEFT text: This paper introduces bifocal sampling, a new technique for estimating the size of an equi-join of two relations. Bifocal sampling classifies tuples in each relation into two groups, sparse and dense, based on the number of tuples with the same join value. Distinct estimation procedures are employed that focus on various combinations for joining tuples (e.g., for estimating the number of joining tuples that are dense in both relations). This combination of estimation procedures overcomes some well-known problems in previous schemes, enabling good estimates with no a priori knowledge about the data distribution. The estimate obtained by the bifocal sampling algorithm is proven to lie with high probability within a small constant factor of the actual join size, regardless of the skew, as long as the join size is &Omega;(n lg n), for relations consisting of n tuples. The algorithm requires a sample of size at most O(&radic;n lg n). By contrast, previous algorithms using a sample of similar size may require the join size to be &Omega;(n&radic;n) to guarantee an accurate estimate. Experimental results support the theoretical claims and show that bifocal sampling is practical and effective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: applying the golden rule of sampling for query estimation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yi-leh wu , divyakant agrawal , amr el abbadi
",n
"LEFT id: NA
RIGHT id: 1918

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management for earth system science

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james frew , jeff dozier
",y
"LEFT id: NA
RIGHT id: 675

LEFT text: The STREAM project at Stanford is developing a general-purpose system for processing continuous queries over multiple continuous data streams and stored relations. It is designed to handle high-volume and bursty data streams with large numbers of complex continuous queries. We describe the status of the system as of early 2003 and outline our ongoing research directions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining long sequential patterns in a noisy environment

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jiong yang , wei wang , philip s. yu , jiawei han
",y
"LEFT id: NA
RIGHT id: 34

LEFT text: The Web today consists exclusively of HTML documents designed for the human eye. While many of them are generated automatically by applications, it is difficult for other applbcations to read and process them. This may soon change, due to a series of new standards frorn the World Wide Web Consortium centered around XML (Extensible Markup Language). XML is designed to express the document content, while HTML expresses its presentation. In short, XML is a data exchange format, easily understood by applications. It enables data exchange on the Web, both intra-enterprise, across platforms (intranet), and inter-enterprise (internet). The focus of the Web shifts from document management to data management, and topics like queries, views, data warehouses, mediators, which were the domain of databases, become of interest to the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: managing web data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: dan suciu
",y
"LEFT id: NA
RIGHT id: 535

LEFT text: In this paper, we propose a new method for indexing large amounts of point and spatial data in high-dimensional space. An analysis shows that index structures such as the R*-tree are not adequate for indexing high-dimensional data sets. The major problem of R-tree-based index structures is the overlap of the bounding boxes in the directory, which increases with growing dimension. To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap. Our experiments show that for high-dimensional data, the X-tree outperforms the well-known R*-tree and the TV-tree by up to two orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 1443

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of multiple-level association rules from large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu
",n
"LEFT id: NA
RIGHT id: 515

LEFT text: We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time ""twist"": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of ""big data"". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a ""big data"" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle ""big"" as well as ""fast"" data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a study on the management of semantic transaction for efficient data retrieval

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: shi ming huang , irene kwan , chih he li
",n
"LEFT id: NA
RIGHT id: 1214

LEFT text: Object-oriented databases enforce behavioral schema consistency rules to guarantee type safety, i.e., that no run-time type error can occur. When the schema must evolve, some schema updates may violate these rules. In order to maintain complete behavioral schema consistency, traditional solutions require significant changes to the types, the type hierarchy and the code of existing methods. Such operations are very expensive in a database context. To ease schema evolution, we propose to support exceptions to the behavioral consistency rules without sacrificing type safety for all that. The basic idea is to detect unsafe statements at compile-time and check them at run-time. The run-time check is performed by a specific clause that is automatically inserted around unsafe statements. This check clause warns the programmer of the safety problem and lets him provide exception-handling code. Schema updates can therefore be performed with only minor changes to the code of methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: type-safe relaxing of schema consistency rules for flexible modelling in oodbms

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: eric amiel , marie-jo bellosta , eric dujardin , eric simon
",n
"LEFT id: NA
RIGHT id: 1399

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching strategies for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1274

LEFT text: The FileNet Integrated Document Management (IDM) products consists of a family of client applications and Imaging and Electronic Document Management (EDM) services. These services provide robust facilities for document creation, update, and deletion along with document search capabilities. Document properties are stored in an underlying relational database (RDBMS); document content is stored in files or in a specialized optical disk hierarchical storage manager. FileNet Corporation's Visual WorkFlo® and Ensemble® workflow products can be utilized in conjunction with FileNet's IDM technologies to automate production and ad hoc business processes respectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: view management in multimedia databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , eric lemar , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: We describe the external data manager component of the Lore database system for semistructured data. Lore's external data manager enables dynamic retrieval and integration of data from arbitrary, heterogeneous external sources during query processing. The distinction between Lore-resident and external data is invisible to the user. We introduce a flexible notion of arguments that limits the amount of data fetched from an external source, and we have incorporated optimizations to reduce the number of calls to an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: Semantic Binary Object-oriented Data Model (Sem-ODM) provides an expressive data model (similar to Object-oriented Data Models) with a well-known declarative query facility - SQL (similar to relational databases). Advantages of using Sem-ODM include (i.) friendlier and more intelligent generic user interfaces; (ii.) comprehensive enforcement of integrity constraints; (iii.) greater flexibility; (iv.) substantially shorter application programs; and (v.) easier query facility. SemanticAccess is a set of tools developed to provide a semantic interface to Semantic Binary Object-oriented Databases (Sem-ODB) as well as relational databases. This presentation focuses on the system architecture of SemanticAccess including Semantic Binary Object-oriented Data Model, Semantic SQL query language, Semantic Binary Database and a wrapper developed for relational databases. 1. Purpose

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic. Probably because many problems remained unsolved, most research works were only able to address separate topics, without a clear context of an overall application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: Multimedia information systems have emerged as an essential component of many application domains ranging from library information systems to entertainment technology. However, most implementations of these systems cannot support the continuous display of multimedia objects and suffer from frequent disruptions and delays termed hiccups. This is due to the low I/O bandwidth of the current disk technology, the high bandwidth requirement of multimedia objects, and the large size of these objects that almost always requires them to be disk resident. One approach to resolve this limitation is to decluster a multimedia object across multiple disk drives in order to employ the aggregate bandwidth of several disks to support the continuous retrieval (and display) of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 1230

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: functional-join processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: r. braumandl , j. claussen , a. kemper , d. kossmann
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. Sara Rynes Incoming Editor I am thankful to Sara for inviting me to write this editorial column encouraging scholars to submit their qualitative research to the Academy of Man-I wish to thank Torn Lee and Sara Rynes for their helpful comments and encouragement in preparing this editorial. 454 agement Journal. Qualitative research is important to AMI Qualitative research is actively sought and supported by the Journal, its editors, and its editorial review board. Alv1Jhas published many qualitative papers. The coveted A/'v1jBest Article Award has been won by three qualitative papers-Gersick (1989), Isabella (1990), and Dutton and Duckerich (1991)-and by one paper that combined qualitative and quantitative methods: Sutton and Rafuclli, (1988). Despite these successes, most …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 950

LEFT text: System developments and research on parallel query processing have concentrated either on “Shared Everything” or “Shared Nothing” architectures so far. While there are several commercial DBMS based on the “Shared Disk” alternative, this architecture has received very little attention with respect to parallel query processing. A comparison between Shared Disk and Shared Nothing reveals many potential benefits for Shared Disk with respect to parallel query processing. In particular, Shared Disk supports more flexible control over the communication overhead for intra-transaction parallelism, and a higher potential for dynamic load balancing and efficient processing of mixed OLTP/query workloads. We also sketch necessary extensions for transaction management (concurrency/coherency control, logging/recovery) to support intra-transaction parallelism in the Shared Disk environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: parallel algorithms for high-dimensional similarity joins for data mining applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: john c. shafer , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1715

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 2075

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the computation of relational view complements

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jens lechtenb &#246; rger , gottfried vossen
",n
"LEFT id: NA
RIGHT id: 275

LEFT text: Personalization, which will be a key success factor for forthcoming XML-based Web standards like ebXML or MPEG-7, needs a powerful framework for preferences. The research program ”It’s a Preference World” at the University of Augsburg treats preferences as first class citizens in personalized e-services. Based on a rich theory of preferences modeled as partial orders we are developing enabling technologies for personalized client and middleware components. In this positional paper we sketch an architecture for content syndication in such a Preference World.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cosima - your smart , speaking e-salesperson

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: werner kie &#946; ling , stefan holland , stefan fischer , thorsten ehm
",y
"LEFT id: NA
RIGHT id: 980

LEFT text: Many of today’s applications need massive real-time data processing. In-memory database systems have become a good alternative for these requirements. These systems maintain the primary copy of the database in the main memory to achieve high throughput rates and low latency. However, a database in RAM is more vulnerable to failures than in traditional disk-oriented databases because of the memory volatility. DBMSs implement recovery activities (logging, checkpoint, and restart) for recovery proposes. Although the recovery component looks similar in disk- and memory-oriented systems, these systems differ dramatically in the way they implement their architectural components, such as data storage, indexing, concurrency control, query processing, durability, and recovery. This survey aims to provide a thorough review of in-memory database recovery techniques. To achieve this goal, we reviewed the main concepts of database recovery and architectural choices to implement an in-memory database system. Only then, we present the techniques to recover in-memory databases and discuss the recovery strategies of a representative sample of modern in-memory databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: datablitz : a high performance main-memory storage manager

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , philip bohannon , s. gogate , s. joshi , c. gupta , a. khivesera , henry f. korth , peter mcilroy , j. miller , p. p. s. narayan , m. nemeth , rajeev rastogi , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 253

LEFT text: The objective of the project HyperStorM (’Hypermedia Document Storage and Modeling’) is to use objec~oriented database technology to administer structured documents like SGMLand HyTime-documents. It 1s an asset of formats such as SGML to allow for the seam!ess integration of metainformation, HyTime provides a set of archi~ectural forms, i.e., templates with a predefine semantics to be used in hypermedia documents for scheduling or hyperlinking. to give examples. Requirements. We have identified the following requirements in the context of structured document storage: the database application has to administer documents conformant to arbitrary document-type definit~ons ( DTDs) With regard to declarative access, it must be possible to formulate queries in a more precise way, e.g., by referring to documents’ structure, in order tO COPe with th~ increase of documents in number and size. Updates on documents are advantageous as, first,, it may be pointless to only insert entire documents when considering certain document types, such as encyclopedia or codes of law. Second, t“ormats such as SGML are m use to capture structured reformation going beyond the conventional notion of ‘document,’. Document components’ semantics should be available within the dat,abase application to ensure adequate performance, and to allow for querying based on such concepts Finally, we think that. wnth regard to access vm the WWW documents’ conversion to HTML should take place at the server site: transformation of docume~ts might not be straightforward, hut may instead be driven by the database context. Then, the process is more efficient when carried out in the database. System Overview. Due to the dynamic nature of the sys~em. most structures comprising the document conrent have to be generated at mntime. & a first

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hyperstorm-administering structured documents using object-oriented database technology

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , karl aberer
",y
"LEFT id: NA
RIGHT id: 833

LEFT text: Electronic commerce systems (retail, auction, etc.) are good examples of data-based systems that operate under correctness and resilience requirements of a transactional nature but go beyond conventional databases, as they are formed by the aggregation of heterogeneous, autonomous components. We introduce a framework to specify, analyze and reason about the behavior of such systems, focusing on how they are designed to make consistent progress in spite of failures. The contributions are: (a) the introduction of the Guarantee abstraction to deal with transactional applications; (b) a framework based on guarantees and protocols to specify the behaviors of systems and their components and reason about the properties of systems and their components; and (c) application of the framework to a common e-commerce scenario. The framework allows the hierarchical composition of transactional systems and their properties, as well as the proofs of these properties: we specify a system's behavior at its most abstract level, and proceed to decompose the specification mirroring the structure of the system's components, considering the role of guarantee-preserving component systems and recovery in each case. In particular we show how the lower-level properties are supported by the component systems, which we also characterize within the same framework.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: histogram-based approximation of set-valued query-answers

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1353

LEFT text: We describe the open, extensible architecture of SQL for accessing data stored in external data sources not managed by the SQL engine. In this scenario, SQL engines act as middleware servers providing access to external data using SQL DML statements and joining external data with SQL tables in heterogeneous queries. We describe the state-of-the art in object-relational systems and their companion products, and provide an outlook on future directions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: Efficient user-adaptable similarity search more and more increases in its importance for multimedia and spatial database systems. As a general similarity model for multi-dimensional vectors that is adaptable to application requirements and user preferences, we use quadratic form distance functions which have been successfully applied to color histograms in image databases [Fal+ 94]. The components aij of the matrix A denote similarity of the components i and j of the vectors. Beyond the Euclidean distance which produces spherical query ranges, the similarity distance defines a new query type, the ellipsoid query. We present new algorithms to efficiently support ellipsoid query processing for various user-defined similarity matrices on existing precomputed indexes. By adapting techniques for reducing the dimensionality and employing a multi-step query processing architecture, the method is extended to high-dimensional data spaces. In particular, from our algorithm to reduce the similarity matrix, we obtain the greatest lowerbounding similarity function thus guaranteeing no false drops. We implemented our algorithms in C++ and tested them on an image database containing 12,000 color histograms. The experiments demonstrate the flexibility of our method in conjunction with a high selectivity and efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1123

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the structured information manager : a database system for sgml documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ron sacks-davis
",n
"LEFT id: NA
RIGHT id: 141

LEFT text: Many of today’s applications need massive real-time data processing. In-memory database systems have become a good alternative for these requirements. These systems maintain the primary copy of the database in the main memory to achieve high throughput rates and low latency. However, a database in RAM is more vulnerable to failures than in traditional disk-oriented databases because of the memory volatility. DBMSs implement recovery activities (logging, checkpoint, and restart) for recovery proposes. Although the recovery component looks similar in disk- and memory-oriented systems, these systems differ dramatically in the way they implement their architectural components, such as data storage, indexing, concurrency control, query processing, durability, and recovery. This survey aims to provide a thorough review of in-memory database recovery techniques. To achieve this goal, we reviewed the main concepts of database recovery and architectural choices to implement an in-memory database system. Only then, we present the techniques to recover in-memory databases and discuss the recovery strategies of a representative sample of modern in-memory databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xmas : an extensible main-memory storage system for high-performance applications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jang ho park , yong sik kwon , ki hong kim , sang ho lee , byoung dae park , sang kyun cha
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 1881

LEFT text: XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: semantics for update rule programs and implementation in a relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: louiqa raschid , jorge lobo
",n
"LEFT id: NA
RIGHT id: 628

LEFT text: In this article metadata for mulimedia documents are classified in conformity with their nature, and the different kinds of metadata are brought into relation with the different purposes intended. We describe how metadata may be organized in accordance with the ISO standards SGML, which facilitates the handling of structured documents, and DFR, which supports the storage of collections of documents. Finally, we outline the impact of our observations on future developments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: hyperfile : a data and query model for documents

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chris clifton , hector garcia-molina , david bloom
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: With the growing popularity of the internet and the World Wide Web (Web), there is a fast growing demand for access to database management systems (DBMS) from the Web. We describe here techniques that we invented to bridge the gap between HTML, the standard markup language of the Web, and SQL, the standard query language used to access relational DBMS. We propose a flexible general purpose variable substitution mechanism that provides cross-language variable substitution between HTML input and SQL query strings as well as between SQL result rows and HTML output thus enabling the application developer to use the full capabilities of HTML for creation of query forms and reports, and SQL for queries and updates. The cross-language variable substitution mechanism has been used in the design and implementation of a system called DB2 WWW Connection that enables quick and easy construction of applications that access relational DBMS data from the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 2014

LEFT text: While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : middle-tier database caching for highly scalable e-business architectures

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: christof bornh &#246; vd , mehmet altinel , sailesh krishnamurthy , c. mohan , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1614

LEFT text: This annotated bibliography presents a collection of published papers, technical reports, Master's and PhD Theses that have investigated various aspects of object database performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 1790

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining and pruning optimizations of constrained associations rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 690

LEFT text: Rapid advances in networking and Internet technologies have fueled the emergence of the ""software as a service"" model for enterprise computing. Successful examples of commercially viable software services include rent-a-spreadsheet, electronic mail services, general storage services, disaster protection services. ""Database as a Service"" model provides users power to create, store, modify, and retrieve data from anywhere in the world, as long as they have access to the Internet. It introduces several challenges, an important issue being data privacy. It is in this context that we specifically address the issue of data privacy.There are two main privacy issues. First, the owner of the data needs to be assured that the data stored on the service-provider site is protected against data thefts from outsiders. Second, data needs to be protected even from the service providers, if the providers themselves cannot be trusted. In this paper, we focus on the second challenge. Specifically, we explore techniques to execute SQL queries over encrypted data. Our strategy is to process as much of the query as possible at the service providers' site, without having to decrypt the data. Decryption and the remainder of the query processing are performed at the client site. The paper explores an algebraic framework to split the query to minimize the computation at the client site. Results of experiments validating our approach are also presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: executing sql over encrypted data in the database-service-provider model

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: hakan hacig &#252; m &#252; &#351; , bala iyer , chen li , sharad mehrotra
",y
"LEFT id: NA
RIGHT id: 601

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of data distribution for selectivity estimation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kyu young whang , sang wook kim , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 179

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sap r/3 ( tutorial ) : a database application system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann , florian matthes
",n
"LEFT id: NA
RIGHT id: 1664

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mosaico-a system for conceptual modeling and rapid prototyping of object-oriented database application

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. missikoff , m. toiati
",n
"LEFT id: NA
RIGHT id: 122

LEFT text: Providing concept level access to video data requires, video management systems tailored to the domain of the data. Effective indexing and retrieval for high-level access mandates the use of domain knowledge. This paper proposes an approach based on the use of knowledge models to building domain specific video information systems. The key issues in such systems are identified and discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic video indexing : approach and issues

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arun hampapur
",y
"LEFT id: NA
RIGHT id: 518

LEFT text: MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: introduction to constraint databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: bart kuijpers
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: We follow the stack-baaed approach to query languages which is a new formal and intellectual paradigm for integrating querying and programming for object-oriented databases. Queries are considered generalized programing expressions which may be used within macroscopic imperative statements, such as creating, updating, inserting, and deleting data objects. Queries may be also used as procedures’ parameters, as well as determine the output from functional procedures (SQL-like views). The semantics, including generalized query operators (selection, projection, navigation, join, quantifiers, etc.), is defined in terms of operations on two stacks. The environment stack deals with the scope control and binding names.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: Publisher Summary This chapter provides a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages. Stream data are also generated naturally by web services, in which loosely coupled systems interact by exchanging high volumes of business data tagged in XML, forming continuous XML data streams. A central aspect of web services is the ability to efficiently operate on these XML data streams executing queries to continuously match, extract, and transform parts of the XML data stream to drive legacy back-end business applications. Manipulating stream data presents many technical challenges, which are just beginning to be addressed in the database, systems, algorithms, networking, and other computer science communities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 992

LEFT text: The processing of spatial joins can be greatly improved by the use of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of nonintersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. This article presents a polygon approximation for spatial join processing which we call four-colors raster signature (4CRS). The performance of a filter using this approximation was evaluated with real world data sets. The results showed that our approach, when compared to other approaches presented in the related literature, reduced the inconclusive answers by a factor of more than two. As a result, the need for retrieving the representation of polygons and carrying out exact geometry tests is reduced by a factor of more than two, as well. A Raster Approximation for the Processing of Spatial Joins

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a raster approximation for processing of spatial joins

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: geraldo zimbrao , jano moreira de souza
",y
"LEFT id: NA
RIGHT id: 1398

LEFT text: Current-day crawlers retrieve content only from the publicly indexable Web, i.e., the set of Web pages reachable purely by following hypertext links, ignoring search forms and pages that require authorization or prior registration. In particular, they ignore the tremendous amount of high quality content “hidden” behind search forms, in large searchable electronic databases. In this paper, we address the problem of designing a crawler capable of extracting content from this hidden Web. We introduce a generic operational model of a hidden Web crawler and describe how this model is realized in HiWE (Hidden Web Exposer), a prototype crawler built at Stanford. We introduce a new Layout-based Information Extraction Technique (LITE) and demonstrate its use in automatically extracting semantic information from search forms and response pages. We also present results from experiments conducted to test and validate our techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: crawling the hidden web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sriram raghavan , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",y
"LEFT id: NA
RIGHT id: 2115

LEFT text: Schema matching is the task of finding semantic correspondences between elements of two schemas. It is needed in many database applications, such as integration of web data sources, data warehouse loading and XML message mapping. To reduce the amount of user effort as much as possible, automatic approaches combining several match techniques are required. While such match approaches have found considerable interest recently, the problem of how to best combine different match algorithms still requires further work. We have thus developed the COMA schema matching system as a platform to combine multiple matchers in a flexible way. We provide a large spectrum of individual matchers, in particular a novel approach aiming at reusing results from previous match operations, and several mechanisms to combine the results of matcher executions. We use COMA as a framework to comprehensively evaluate the effectiveness of different matchers and their combinations for real-world schemas. The results obtained so far show the superiority of combined match approaches and indicate the high value of reuse-oriented strategies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a survey of approaches to automatic schema matching

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: erhard rahm , philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1002

LEFT text: Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",n
"LEFT id: NA
RIGHT id: 964

LEFT text: DISCOVER operates on relational databases and facilitates information discovery on them by allowing its user to issue keyword queries without any knowledge of the database schema or of SQL. DISCOVER returns qualified joining networks of tuples, that is, sets of tuples that are associated because they join on their primary and foreign keys and collectively contain all the keywords of the query. DISCOVER proceeds in two steps. First the Candidate Network Generator generates all candidate networks of relations, that is, join expressions that generate the joining networks of tuples. Then the Plan Generator builds plans for the efficient evaluation of the set of candidate networks, exploiting the opportunities to reuse common subexpressions of the candidate networks.    We prove that DISCOVER finds without redundancy all relevant candidate networks, whose size can be data bound, by exploiting the structure of the schema. We prove that the selection of the optimal execution plan (way to reuse common subexpressions) is NP-complete. We provide a greedy algorithm and we show that it provides near-optimal plan execution time cost. Our experimentation also provides hints on tuning the greedy algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast high-dimensional data search in incomplete databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: beng chin ooi , cheng hian goh , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1189

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: fabio casati , ming-chien shan , dimitrios georgakopoulos
",n
"LEFT id: NA
RIGHT id: 1134

LEFT text: High speed computer and telephone networks carry large amounts of data and signalling traffic. The engineers who build and maintain these networks use a combination of hardware and software tools to monitor the stream of network traffic. Some of these tools operate directly on the live network; others record data on magnetic tape for later offline analysis by software. Most analysis tasks require tens to hundreds of gigabytes of data. Traffic analysis applications include protocol performance analysis, conformance testing, error monitoring and fraud detection.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tribeca : a stream database manager for network traffic analysis

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark sullivan
",y
"LEFT id: NA
RIGHT id: 777

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 835

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: finding intensional knowledge of distance-based outliers

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text:  In addition it constructs a special node Authors() and connects it to all pages corresponding to ""Author""s. The output graph is called SiteGraph. One way to write this in StruQL is: input DataGraph where Root(x); x ! ! y; y ! l ! z; l in f""Paper"", ""TechReport"", ""Title"", ""Abstract"", ""Author""g create Authors(); Page(y); Page(z) link Page(y) ! l ! Page(z) where x ! ! y1; y1 ! ""Author"" ! z1 link Authors() ! ""Author"" ! Page(z1) output SiteGraph 2 In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph. Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together: input DataGraph where Root

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 933

LEFT text: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: parallel query scheduling and optimization with time - and space-shared resources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 110

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distance browsing in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1376

LEFT text: Abstract. Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating the ub-tree into a database system kernel

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: frank ramsak , volker markl , robert fenk , martin zirkel , klaus elhardt , rudolf bayer
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 823

LEFT text: The integration of knowledge for multiple sources is an important aspect of automated reasoning systems. When different knowledge bases are used to store knowledge provided by multiple sources, we are faced with the problem of integrating multiple knowledge bases: Under these circumstances, we are also confronted with the prospect of inconsistency. In this paper we present a uniform theoretical framework, based on annotated logics, for amalgamating multiple knowledge bases when these knowledge bases (possibly) contain inconsistencies, uncertainties, and nonmonotonic modes of negation. We show that annotated logics may be used, with some modifications, to mediate between different knowledge bases. The multiple knowledge bases are amalgamated by a transformation of the individual knowledge bases into new annotated logic programs, together with the addition of a new axiom scheme. We characterize the declarative semantics of such amalgamated knowledge bases and study how the semantics of the amalgam is related to the semantics of the individual knowledge bases being combined.—Author's Abstract

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: extracting large-scale knowledge bases from the web

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ravi kumar , prabhakar raghavan , sridhar rajagopalan , andrew tomkins
",n
"LEFT id: NA
RIGHT id: 2234

LEFT text: Model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guest editor 's introduction

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 902

LEFT text: HYPERQUERY is a hypertext query language for object-oriented pictorial database systems. First, we discuss object calculus based on term rewriting. Then, example queries are used to illustrate language facilities. This query language has been designed with a flavor similar to QBE as the highly nonprocedural and conversational language for object-oriented pictorial database management system OISDBS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a graphical query language for mobile information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ya-hui chang
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: Much of the work on conceptual modeling involves the use of an entity-relationship model in which binary relationships appear as associations between two entities. Relationships involving more than two entities are considered rare and, therefore, have not received adequate attention. This research provides a general framework for the analysis of relationships in which binary relationships simply become a special case. The framework helps a designer to identify ternary and other higher-degree relationships that are commonly represented, often inappropriately, as either entities or binary relationships. Generalized rules are also provided for representing higher-degree relationships in the relational model. This uniform treatment of relationships should significantly ease the burden on a designer by enabling him or her to extract more information from a real-world situation and represent it properly in a conceptual design.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 1969

LEFT text: An order-dependent query is one whose result (interpreted as a multiset) changes if the order of the input records is changed. In a stock-quotes database, for instance, retrieving all quotes concerning a given stock for a given day does not depend on order, because the collection of quotes does not depend on order. By contrast, finding a stock's five-price moving-average in a trades table gives a result that depends on the order of the table. Query languages based on the relational data model can handle order-dependent queries only through add-ons. SQL:1999, for instance, has a new ""window"" mechanism which can sort data in limited parts of a query. Add-ons make order-dependent queries di_cult to write and to optimize. In this paper we show that order can be a natural property of the underlying data model and algebra. We introduce a new query language and algebra, called AQuery, that supports order from-the-ground-up. New order-related query transformations arise in this setting. We show by experiment that this framework - language plus optimization techniques - brings orders-of-magnitude improvement over SQL:1999 systems on many natural order-dependent queries

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language for multidimensional arrays : design , implementation , and optimization techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: leonid libkin , rona machlin , limsoon wong
",n
"LEFT id: NA
RIGHT id: 1270

LEFT text: Relational database systems do not effectively support complex queries containing quantifiers (quantified queries) that are increasingly becoming important in decision support applications. Generalized quantifiers provide an effective way of expressing such queries naturally. In this paper, we consider the problem of processing quantified queries within the generalized quantifier framework. We demonstrate that current relational systems are ill-equipped, both at the language and at the query processing level, to deal with such queries. We also provide insights into the intrinsic difficulties associated with processing such queries. We then describe the implementation of a quantified query processor, Q2P, that is based on multidimensional and boolean matrix structures. We provide results of performance experiments run on Q2P that demonstrate superior performance on quantified queries. Our results indicate that it is feasible to augment relational systems with query subsystems like Q2P for significant performance benefits for quantified queries in decision support applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exploiting early sorting and early partitioning for decision support query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: j. claussen , a. kemper , d. kossmann , c. wiesner
",n
"LEFT id: NA
RIGHT id: 1318

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: vxmlr : a visual xml-relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: aoying zhou , hongjun lu , shihui zheng , yuqi liang , long zhang , wenyun ji , zengping tian
",n
"LEFT id: NA
RIGHT id: 734

LEFT text: In this paper, we introduce Probabilistic Wavelet Synopses, the first wavelet-based data reduction technique with guarantees on the accuracy of individual approximate answers. Whereas earlier approaches rely on deterministic thresholding for selecting a set of ""good"" wavelet coefficients, our technique is based on a novel, probabilistic thresholding scheme that assigns each coefficient a probability of being retained based on its importance to the reconstruction of individual data values, and then flips coins to select the synopsis. We show how our scheme avoids the above pitfalls of deterministic thresholding, providing highly-accurate answers for individual data values in a data vector.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wavelet synopses with error guarantees

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: minos garofalakis , phillip b. gibbons
",y
"LEFT id: NA
RIGHT id: 1899

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner, that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an orthogonally persistent java

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: m. p. atkinson , l. dayn &#232; s , m. j. jordan , t. printezis , s. spence
",n
"LEFT id: NA
RIGHT id: 244

LEFT text: A Problem Solving Agent (PSA) is either an hardware or software system or a human, with an ability to execute a finite set of tasks in an application domain. An activity consists of one or more tasks which can be executed by one or more PSAS. Activity management consists of decomposition of activities into tasks, coordination and data sharing among multiple PSAS executing the activity, and monitoring, scheduling and controlling the execution of multiple tasks of an activity. The CapBased-AMS [KYH95] (based on cooperative problem solving paradigm [CKNT93]) is composed of an activity specification and decomposition module and an activity execution and monitoring module. Capability-based activity specification and decomposition [Hun95]: Each PSA has its competence defined by a set of capabilities it has to execute tasks, and a task requires a certain competence (i.e. has needs) from the PSAS for its execution. Each activity is decomposed into a set of tasks by using the property that each task must be executed by exactly one PSA. Further, each task is matched to a PSA by selecting a PSA that has the capabilities to meet the needs of the task. Tokens are used to model the capability/need of a PSA/task, respectively. The specification of activities, sub-activities, tasks and PSAS are all user-driven.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: capbased-ams : a capability-based and event-driven activity management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: patrick c. k. hung , helen p. yeung , kamalakar karlapalem
",y
"LEFT id: NA
RIGHT id: 1863

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quasi-cubes : exploiting approximations in multidimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , mark sullivan
",y
"LEFT id: NA
RIGHT id: 1125

LEFT text: An Object-Oriented database can utilize the benefits of both the design and implementation of any application. Due to the increased popularity of database systems many new database systems based on varying data model and implementation have entered in the market. Database systems have complex architecture but they are the key factors behind the business transformations. Choosing the best one in any category is an important task based on performance analysis. This chapter deals with the database estimation methodology which integrates the database analysis task and performance analysis task. There are three major techniques for the performance estimation which are analytical modeling, simulation modeling and benchmarking.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 74

LEFT text: XML is widely regarded as a promising means for data representation integration, and exchange. As companies transact business over the Internet, the sensitive nature of the information mandates that access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to a specific XML data item can hence be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this paper, we propose a space- and time-efficient solution to the access control problem for XML data. Our solution is based on a novel notion of a compressed accessibility map (CAM), which compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xmill : an efficient compressor for xml data

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hartmut liefke , dan suciu
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: The similarity join is an important database primitive which has been successfully applied to speed up applications such as similarity search, data analysis and data mining. The similarity join combines two point sets of a multidimensional vector space such that the result contains all point pairs where the distance does not exceed a parameter ε. In this paper, we propose the Epsilon Grid Order, a new algorithm for determining the similarity join of very large data sets. Our solution is based on a particular sort order of the data points, which is obtained by laying an equi-distant grid with cell length ε over the data space and comparing the grid cells lexicographically. A typical problem of grid-based approaches such as MSJ or the ε-kdB-tree is that large portions of the data sets must be held simultaneously in main memory. Therefore, these approaches do not scale to large data sets. Our technique avoids this problem by an external sorting algorithm and a particular scheduling strategy during the join phase. In the experimental evaluation, a substantial improvement over competitive techniques is shown.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 2133

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2064

LEFT text: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1745

LEFT text: Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in federated database systems : report of efdbs '97 workshop

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. conrad , b. eaglestone , w. hasselbring , m. roantree , m. sch &#246; hoff , m. str &#228; ssler , m. vermeer , f. saltor
",n
"LEFT id: NA
RIGHT id: 811

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: relational databases for querying xml documents : limitations and opportunities

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 1239

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 1268

LEFT text: In the past decade, advances in the speed of commodity CPUs have far out-paced advances in memory latency. Main-memory access is therefore increasingly a performance bottleneck for many computer applications, including database systems. In this article, we use a simple scan test to show the severe impact of this bottleneck. The insights gained are translated into guidelines for database architecture, in terms of both data structures and algorithms. We discuss how vertically fragmented data structures optimize cache performance on sequential data access. We then focus on equi-join, typically a random-access operation, and introduce radix algorithms for partitioned hash-join. The performance of these algorithms is quantified using a detailed analytical model that incorporates memory access cost. Experiments that validate this model were performed on the Monet database system. We obtained exact statistics on events such as TLB misses and L1 and L2 cache misses by using hardware performance counters found in modern CPUs. Using our cost model, we show how the carefully tuned memory access pattern of our radix algorithms makes them perform well, which is confirmed by experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing database architecture for the new bottleneck : memory access

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: stefan manegold , peter a. boncz , martin l. kersten
",y
"LEFT id: NA
RIGHT id: 1292

LEFT text: There has been tremendous interest in information integration systems that automatically gather, manipulate, and integrate data from multiple information sources on a user's behalf. Unfortunately, web sites are primarily designed for human browsing rather than for use by a computer program. Mechanically extracting their content is in general a rather di cult job if not impossible [4]. Software systems using such web information sources typically use hand-coded wrappers to extract information content of interest from web sources and translate query responses to a more structured format (e.g., relational form) before unifying them into an integrated answer to a user's query. The most recent generation of information mediator systems (e.g., Ariadne [3], CQ [5, 7], Internet Softbots [4], TSIMMIS [2]) addresses this problem by enabling a pre-wrapped set of web sources to be accessed via database-like queries. However, hand-coding a wrapper is time consuming and error-prone. We have also observed that, by using a good design methodology, only a relatively small part of the code deals with the source-speci c access details, the rest of the code is either common among wrappers or can be expressed in a high level, more structured fashion. As the Web grows, maintaining a reasonable number of wrappers becomes impractical. First, the number of information sources of interest to a user query can be quite large, even within a particular domain. Second, new information sources are constantly added on the Web. Thirdly, the content and presentation format of the existing information sources may change frequently and autonomously. With these observations in mind, we have developed a wrapper generation system, called XWrap, for semi-automatic construction of wrappers for Web information sources. The system contains a library of commonly used functions, such as receiving queries from applications, handling of lter queries, and packaging results. It also contains some source-speci c facilities that are in charge of mapping a mediator query to a remote connection call to fetch the relevant pages and translating the retrieved page(s) into a more structured format (such as XML documents or relational tables).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supervised wrapper generation with lixto

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: robert baumgartner , sergio flesca , georg gottlob
",n
"LEFT id: NA
RIGHT id: 1128

LEFT text: Recent progress in technologies for data input have made it easier for finance and retail organizations to collect massive amounts of data and to store them on disk at a low cost. Such organizations are interested in extracting from these huge databases previously unnoticed information that inspires new marketing strategies. In this demonstration, we introduce SOAJAR, a system for mining optimized association rules from databases with numeric data as well as Boolean data. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: constructing efficient decision trees by using optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1583

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at university of hagen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes
",n
"LEFT id: NA
RIGHT id: 1495

LEFT text: Extended transaction models have drawn much interest recently in academia and industry [2]. Such models seek to address the limitations of traditional ACID transactions for supporting multisystem applications that operate in heterogeneous environments. Such applications are increasingly proving to be of strategic importance to a number of businesses and governmental agencies. Different transaction models, however, tend to be closed in that they cannot be easily combined with other such models, thus limiting their applicability to situations which exactly match one of them. We do not propose yet another transaction model. Instead, we have developed a general specification facility that enables the formalization of any transaction model that can be stated in terms of dependencies amongst significant events in different subtransactions. Such significant events include start, commit, and abort. We make no assumptions that these are the only kinds of events. Our approach is viable because most extended transaction models can be naturally formalized in terms of dependencies among different subtransactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enterprise transaction processing on windows nt

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: greg hope
",n
"LEFT id: NA
RIGHT id: 60

LEFT text: Text models focus on the manipulation of textual data. They describe texts by their structure, operations on the texts, and constraints on both structure and operations. In this article common characteristics of machine readable texts in general are outlined. Subsequently, ten text models are introduced. They are described in terms of the datatypes that they support, and the operations defined by these datatypes. Finally, the models are compared.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automatic discovery of language models for text databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jamie callan , margaret connell , aiqun du
",n
"LEFT id: NA
RIGHT id: 1652

LEFT text: We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases for networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. v. jagadish
",n
"LEFT id: NA
RIGHT id: 971

LEFT text: In the early 1980's, researchers recognized that semantic information stored in databases as integrity constraints could be used for query optimization. A new set of techniques called semantic query optimization (SQO) was developed. Some of the ideas developed for SQO have been used commercially, but to the best of our knowledge, no extensive implementations of SQO exist today. In this paper, we describe an implementation of two SQO techniques, Predicate Introduction and Join Elimination, in DB2 Universal Database. We present the implemented algorithms and performance results using the TPCD and APB-1 OLAP benchmarks. Our experiments show that SQO can lead to dramatic query performance improvements. A crucial aspect of our implementation of SQO is the fact that it does not rely on complex integrity constraints (as many previous SQO techniques did); we use only referential integrity constraints and check constraints.  

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: heterogeneous database query optimization in db2 universal datajoiner

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shivakumar venkataraman , tian zhang
",n
"LEFT id: NA
RIGHT id: 699

LEFT text: In this paper, we ask if the traditional relational query acceleration techniques of summary tables and covering indexes have analogs for branching path expression queries over tree- or graph-structured XML data. Our answer is yes --- the forward-and-backward index already proposed in the literature can be viewed as a structure analogous to a summary table or covering index. We also show that it is the smallest such index that covers all branching path expression queries. While this index is very general, our experiments show that it can be so large in practice as to offer little performance improvement over evaluating queries directly on the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: covering indexes for branching path queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: raghav kaushik , philip bohannon , jeffrey f naughton , henry f korth
",y
"LEFT id: NA
RIGHT id: 414

LEFT text: Abstract. In meta-searchers accessing distributed Web-based information repositories, performance is a major issue. Efficient query processing requires an appropriate caching mechanism. Unfortunately, standard page-based as well as tuple-based caching mechanisms designed for conventional databases are not efficient on the Web, where keyword-based querying is often the only way to retrieve data. In this work, we study the problem of semantic caching of Web queries and develop a caching mechanism for conjunctive Web queries based on signature files. Our algorithms cope with both relations of semantic containment and intersection between a query and the corresponding cache items. We also develop the cache replacement strategy to treat situations when cached items differ in size and contribution when providing partial query answers. We report results of experiments and show how the caching mechanism is realized in the Knowledge Broker system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: homer : a model-based case tool for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo merialdo , paolo atzeni , marco magnante , giansalvatore mecca , marco pecorone
",n
"LEFT id: NA
RIGHT id: 1987

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semi-automatic , self-adaptive control of garbage collection rates in object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jonathan e. cook , artur w. klauser , alexander l. wolf , benjamin g. zorn
",n
"LEFT id: NA
RIGHT id: 365

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online query processing : a tutorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter j. haas , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 106

LEFT text: We consider the problem of answering queries from databases that may be incomplete. A database is incomplete if some tuples may be missing from some relations, and only a part of each relation is known to be complete. This problem arises in several contexts. For example, systems that provide access to multiple heterogeneous information sources often encounter incomplete sources. The question we address is to determine whether the answer to a specific given query is complete even when the database is incomplete. We present a novel sound and complete algorithm for the answer-completeness problem by relating it to the problem of independence of queries from updates. We also show an important case of the independence problem (and therefore ofthe answer-completeness problem) that can be decided in polynomial time, whereas the best known algorithm for this case is exponential.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: database design for incomplete relations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mark levene , george loizou
",n
"LEFT id: NA
RIGHT id: 1088

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sampling large databases for association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hannu toivonen
",n
"LEFT id: NA
RIGHT id: 1235

LEFT text: Traditional approaches to versioning documents are edit-based, and represent successive versions using edit scripts. This paper proposes a reference-based version-ing scheme that preserves the rich logical structure of the evolving document via object references. This approach produces better support for queries, and reconciles the storage-level and transport-level representations of multiversioned XML documents. In particular , we present eecient algorithms for supporting projection and selection queries, and for querying the document evolution history. Then, we show that our representation is also eecient at the transport level, where XML documents are exchanged between remote parties. In fact, with the reference-based scheme, an XML document's history can also be viewed and processed as yet another XML document. Finally, we demonstrate the eeectiveness of the new scheme at the storage level, for which we deene a usefulness-based page management policy, adapted from transaction-time databases, to ensure eecient temporal clustering between versions. The experimental evaluation of the new scheme against previous representations used in temporal databases and persistent-object managers shows the performance advantages of the new approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient schemes for managing multiversionxml documents

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s.-y . chien , v. j. tsotras , c. zaniolo
",n
"LEFT id: NA
RIGHT id: 1234

LEFT text: With the rapid growth of XML-document traffic on the Internet, scalable content-based dissemination of XML documents to a large, dynamic group of consumers has become an important research challenge. To indicate the type of content that they are interested in, data consumers typically specify their subscriptions using some XML pattern specification language (e.g., XPath). Given the large volume of subscribers, system scalability and efficiency mandate the ability to aggregate the set of consumer subscriptions to a smaller set of content specifications, so as to both reduce their storage-space requirements as well as speed up the document-subscription matching process. In this paper, we provide the first systematic study of subscription aggregation where subscriptions are specified with tree patterns (an important subclass of XPath expressions). The main challenge is to aggregate an input set of tree patterns into a smaller set of generalized tree patterns such that: (1) a given space constraint on the total size of the subscriptions is met, and (2) the loss in precision (due to aggregation) during document filtering is minimized. We propose an efficient tree-pattern aggregation algorithm that makes effective use of document-distribution statistics in order to compute a precise set of aggregate tree patterns within the allotted space budget. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tree pattern query minimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s. amer-yahia , s. cho , l. v. s. lakshmanan , d. srivastava
",n
"LEFT id: NA
RIGHT id: 1750

LEFT text: The OOPSLA '97 Workshop on Experiences Using Object Data Management in the Real-World was held at the Cobb Galleria Centre in Atlanta, Georgia on Monday 6 October 1997. This report summarises some of the commercial case-study presentations made by workshop participants.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop on workflow management in scientific and engineering applications-report

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. mcclatchey , g. vossen
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1001

LEFT text: Multidimensional inter-transactional association rules extend the traditional association rules to describe more general associations among items with multiple properties across transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transactional association rules tends to be extremely large, mining inter-transactional associations poses more challenges on efficient processing than mining traditional intra-transactional associations. In order to make such association rule mining truly practical and computationally tractable, in this study we present a template model to help users declare the interesting multidimensional inter-transactional associations to be mined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized view selection for multidimensional datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 534

LEFT text: This paper examines the effect on product development of project scope: the extent to which a new product is based on unique parts developed in-house. Using data from a larger study of product development in the world auto industry, the paper presents evidence on the impact of scope on lead time and engineering productivity. Studies of the automotive supplier industry suggest that very different structures and relationships exist in Japan, the U.S., and Europe. Yet there has been little study of the impact of these differences on development performance. Further, little is known about the effects of different parts strategies (i.e. unique versus common or carryover parts) on development. The evidence presented here suggests that project scope differs significantly in the industry, even for comparable products. These differences in strategy, in turn, explain an important part of differences in performance. In particular, it appears that a distinctive approach to scope among Japanese firms---high levels of unique parts, intensive supplier involvement in engineering---accounts for a significant fraction of their advantage in lead time and cost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial : charter and scope

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. Sara Rynes Incoming Editor I am thankful to Sara for inviting me to write this editorial column encouraging scholars to submit their qualitative research to the Academy of Man-I wish to thank Torn Lee and Sara Rynes for their helpful comments and encouragement in preparing this editorial. 454 agement Journal. Qualitative research is important to AMI Qualitative research is actively sought and supported by the Journal, its editors, and its editorial review board. Alv1Jhas published many qualitative papers. The coveted A/'v1jBest Article Award has been won by three qualitative papers-Gersick (1989), Isabella (1990), and Dutton and Duckerich (1991)-and by one paper that combined qualitative and quantitative methods: Sutton and Rafuclli, (1988). Despite these successes, most …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: Tree patterns form a natural basis to query tree-structured data such as XML and LDAP. To improve the efficiency of tree pattern matching, it is essential to quickly identify and eliminate redundant nodes in the pattern. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. In the absence of ICs, we develop a polynomial-time query minimization algorithm called CIM, whose efficiency stems from two key properties: (i) a node cannot be redundant unless its children are; and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we develop a technique for query minimization based on three fundamental operations: augmentation (an adaptation of the well-known chase procedure), minimization (based on homomorphism techniques), and reduction. We show the surprising result that the algorithm, referred to as ACIM, obtained by first augmenting the tree pattern using ICs, and then applying CIM, always finds the unique minimal equivalent query. While ACIM is polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating ”information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1183

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 1519

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an effective hash-based algorithm for mining association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jong soo park , ming-syan chen , philip s. yu
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1242

LEFT text: We propose an novel method of computing and storing DataCubes. Our idea is to use Bayesian Networks, which can generate approximate counts for any query combination of attribute values and “don’t cares.” A Bayesian network represents the underlying joint probability distribution of the data that were used to generate it. By means of such a network the proposed method, NetCube, exploits correlations among attributes. Our proposed preprocessing algorithm scales linearly on the size of the database, and is thus scalable; it is also parallelizable with a straightforward parallel implementation. Moreover, we give an algorithm to estimate counts of arbitrary queries that is fast ( constant on the database size). Experimental results show that NetCubes have fast generation and use (a few

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: netcube : a scalable tool for fast data mining and compression

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: dimitris margaritis , christos faloutsos , sebastian thrun
",y
"LEFT id: NA
RIGHT id: 1799

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1459

LEFT text: The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 210

LEFT text: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient concurrency control for broadcast environments

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , arvind nithrakashyap , rajendran sivasankaran , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 2068

LEFT text: The W3C XML Scheme language is becomimg increasingly popular for expressing the data model for XML documents. It is a powerful language that incorporates both strutural and datatype modeling features. There are many benefits to storing XML Schema compliant data in a database system, including better queryability, optimied updates and stronger validation. However, the fidelity of the XML document cannot be sacrificed. Thus, the fundamental problem facing database implementers is: how can XML Schemes be mapped to relational (and object-relational) database without losing schema semantics or data-fidelity? In this paper, we present the Oracle XML DB solution for a flexible mapping of XML Schemas to object-relational database. It preserves document fidelity, including ordering, namespaces, comments, processing instructions etc., and handles all the XML Schema semantics including cyclic definitions, dervations (extension and restriction), and wildcards. We also discuss various query and update optimiations that involve rewriting XPath operations to directly operate on the underlying relational data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xml schema

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: charles e. campbell , andrew eisenberg , jim melton
",n
"LEFT id: NA
RIGHT id: 2061

LEFT text: We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive filters for continuous queries over distributed data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chris olston , jing jiang , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1992

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 1951

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining quantitative association rules in large relational tables

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 85

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: engineering federated information systems : report of eefis '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. conrad , w. hasselbring , u. hohenstein , r.-d . kutsche , m. roantree , g. saake , f. saltor
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 599

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules. This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides SQL-like operator, named MINE RULE, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 1102

LEFT text: We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: obtaining complete answers from incomplete databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy
",n
"LEFT id: NA
RIGHT id: 1130

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",n
"LEFT id: NA
RIGHT id: 1462

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 261

LEFT text: The Semantic Web is a vision the idea of having data on the Web defined and linked in such a way that it can be used by machines not just for display purposes but for automation, integration and reuse of data across various applications. Technically, however, there is a widespread misconception that the Semantic Web is primarily a rehash of existing AI and database work focused on encoding knowledge representation formalisms in markup languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler, and Moran seek to dispel this notion by presenting the broad dimensions of this emerging Semantic Web and the multi-disciplinary technological underpinnings like machine learning, information retrieval, service-oriented architectures, and grid computing, thus combining the informational and computational aspects needed to realize the full potential of the Semantic Web vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: monitoring xml data on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: benjamin nguyen , serge abiteboul , gr &#233; gory cobena , miha &#237; preda
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: We consider the problem of evaluating large numbers of XPath filters, each with many predicates, on a stream of XML documents. The solution we propose is to lazily construct a single deterministic pushdown automata, called the XPush Machine from the given XPath fllters. We describe a number of optimization techniques to make the lazy XPush machine more efficient, both in terms of space and time. The combination of these optimizations results in high, sustained throughput. For example, if the total number of atomic predicates in the filters is up to 200000, then the throughput is at least 0.5 MB/sec: it increases to 4.5 MB/sec when each fllter contains a single predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1434

LEFT text: One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scientific databases - state of the art and future directions

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria zemankova , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 85

LEFT text: There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: engineering federated information systems : report of eefis '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. conrad , w. hasselbring , u. hohenstein , r.-d . kutsche , m. roantree , g. saake , f. saltor
",n
"LEFT id: NA
RIGHT id: 1961

LEFT text: The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: birch : an efficient data clustering method for very large databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tian zhang , raghu ramakrishnan , miron livny
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",y
"LEFT id: NA
RIGHT id: 1317

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 2111

LEFT text: Our goal is to enhance multidimensional database systems with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. In this paper, we present a key component of our system that characterizes the information content of a cell based on a user's prior familiarity with the cube and provides a context-sensitive exploration of the cube. There are three main modules of this component. A Tracker, that continuously tracks the parts of the cube that a user has visited. A Modeler, that pieces together the information in the visited parts to model the user's expected values in the unvisited parts. An Informer, that processes user's queries about the most informative unvisited parts of the cube. The mathematical basis for the expected value modeling is provided by the classical maximum entropy principle. Accordingly, the expected values are computed so as to agree with every value that is already visited while reducing assumptions about unvisited values to the minimum by maximizing their entropy. The most informative values are defined as those that bring the new expected values closest to the actual values. We believe and prove through experiments that such a user-in-the-loop exploration will enable much faster assimilation of all significant information in the data compared to existing manual explorations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: user-cognizant multidimensional analysis

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sunita sarawagi
",y
"LEFT id: NA
RIGHT id: 1679

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dblearn : a system prototype for knowledge discovery in relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu , yue huang , yandong cai , nick cercone
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1901

LEFT text: Water has an outstanding importance for the life on earth. From this results the necessity for the monitoring and interpretation of mari ne data. For that, the visual analysis is a suitable and effective tool, whereby special demands arise from the heterogeneity of data (different data type s, different data sources), the quality of data (missing values, incorrect values ), and the large quantity of data. The visualization of marine data is particularly import ant within both their geographic context and their temporal course. First, this paper introduces a classification for the visualization of spatial and ti me related data, which is not only appropriate for marine data. Following special visual ization and interaction techniques for marine data are discussed. Thereby we do not raise the claim, to create new visualization paradigms. Rather we want to show solution concepts and special methods using well known paradigms for a special and complex application area, but also to address limits of the visualization paradigms in these applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a framework for information visualisation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jessie b. kennedy , kenneth j. mitchell , peter j. barclay
",n
"LEFT id: NA
RIGHT id: 1397

LEFT text: In this paper we present FeedbackBypass, a new approach to interactive similarity query processing. It complements the role of relevance feedback engines by storing and maintaining the query parameters determined with feedback loops over time, using a wavelet-based data structure (the Simplex Tree). For each query, a favorable set of query parameters can be determined and used to either “bypass” the feedback loop completely for already-seen queries, or to start the search process from a near-optimal configuration. FeedbackBypass can be combined well with all state-of-the-art relevance feedback techniques working in high-dimensional vector spaces. Its storage requirements scale linearly with the dimensionality of the query space, thus making even sophisticated query spaces amenable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: feedbackbypass : a new approach to interactive similarity query processing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ilaria bartolini , paolo ciaccia , florian waas
",y
"LEFT id: NA
RIGHT id: 994

LEFT text: The project MENTAS (Motor Development Assistant) -aims at realizing an interconnected, engineer-oriented development environment for a faster conception and comparative analysis of motors. In order to reach this goal, an integrated access to multi-vendor DBs is provided. In our exhibition we demonstrate how the interconnection of heterogeneous DBs in MENTAS works. After having analyzed the data models of each such DBs, we have brought the heterogeneous schemas into a global, virtual one, which contains just the data relevant for MENTAS. Finally, we apply a commercially available DB middleware solution to bridge the diverse ontologies and hence to cope with these heterogeneous schemas. Furthermore, we have designed a very friendly GUI in Java by means of which users are guided in the process of formulating SQL queries. We show how this interface allows users to issue SQL statements against any DBs incorporated in the federation, to navigate through heterogeneous DBs, and most importantly, to join and compare data in the DB federation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the heterogeneity problem and middleware technology : experiences with and performance of database gateways

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: fernando de ferreira rezende , klaudia hergula
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 2034

LEFT text: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient similarity search and classification via rank aggregation

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ronald fagin , ravi kumar , d. sivakumar
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 624

LEFT text: The integration of knowledge for multiple sources is an important aspect of automated reasoning systems. When different knowledge bases are used to store knowledge provided by multiple sources, we are faced with the problem of integrating multiple knowledge bases: Under these circumstances, we are also confronted with the prospect of inconsistency. In this paper we present a uniform theoretical framework, based on annotated logics, for amalgamating multiple knowledge bases when these knowledge bases (possibly) contain inconsistencies, uncertainties, and nonmonotonic modes of negation. We show that annotated logics may be used, with some modifications, to mediate between different knowledge bases. The multiple knowledge bases are amalgamated by a transformation of the individual knowledge bases into new annotated logic programs, together with the addition of a new axiom scheme. We characterize the declarative semantics of such amalgamated knowledge bases and study how the semantics of the amalgam is related to the semantics of the individual knowledge bases being combined.—Author's Abstract

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: updating knowledge bases while maintaining their consistency

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ernest teniente , antoni oliv &#233;
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 1542

LEFT text: This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 1790

LEFT text: Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5]. The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining and pruning optimizations of constrained associations rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: 1. Document Acquisition There is a broad spectrum of techniques how to acquire documents in such a way, that they are in computerreadable form and can be stored in a document base. This spectrum ranges from fully automatic at low cost via semiautomatic using tools like scanners and optical character recognition (OCR) to manual acquisition according to elaborate rules and regulations. The purpose of high quality document acquisition is to capture the structure and the semantic content of a document not as far as possible but as far as affordable. Presently the state of the art of semiautomatic acquisition of paper documents is scanning followed by OCR. This yields a facsimile image and the text content, but no structure and no real semantics. In many cases the text produced by OCR is low quality and must be corrected to be useful for effective retrieval. Various projects are under way to capture structure and semantics automatically [1,12], but they have not reached sufficient maturity to be used in production environments like libraries or businesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 754

LEFT text: Literature on information integration across databases tacitly assumes that the data in each database can be revealed to the other databases. However, there is an increasing need for sharing information across autonomous entities in such a way that no information apart from the answer to the query is revealed. We formalize the notion of minimal information sharing across private databases, and develop protocols for intersection, equijoin, intersection size, and equijoin size. We also show how new applications can be built using the proposed protocols.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: investigating xquery for querying across database object types

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nancy wiegand
",n
"LEFT id: NA
RIGHT id: 1207

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the design and implementation of k : a high-level knowledge-base programming language of osam * . kbms

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yuh-ming shyy , javier arroyo , stanley y.w. su , herman lam
",n
"LEFT id: NA
RIGHT id: 1562

LEFT text: A methodology of reengineering existing extended Entity-Relationship(EER) model to Object Modeling Technique(OMT) model is described. A set of translation rules from EER model to a generic Object-Oriented(OO) model of OMT methodology is devised. Such reengineering practices not only can provide us with significant insight to the ""interoperability"" between the OO and the traditional semantic modelling techniques, but also can lead us to the development of a practical design methodology for object-oriented databases(OODB).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mapping extended entity relationship model to object modeling technique

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joseph fong
",y
"LEFT id: NA
RIGHT id: 1915

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1653

LEFT text: Extended transaction model (ETM) is a powerful mechanism to ensure the consistency and reliability of complicated enterprise applications. However, there is few implementation of ETM in J2EE. The existing research is deficient in supporting range and requires some special database supporting. This paper explores the obstacle which prevents J2EE from supporting ETMs, and argues it is because of the limitation of J2EE XAResource interface and underlying databases. To overcome the obstacle, we propose a new approach, which processes concurrency control inside J2EE application server instead of in database. Furthermore, we implement TX/E service in JBoss to validate the approach, which is an enhanced J2EE transaction service supporting extended transaction models. Compared to existing work, TX/E supports user-defined transaction models and does not require any special database supporting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a survey and critique of advanced transaction models

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: c. mohan
",y
"LEFT id: NA
RIGHT id: 334

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable “views” of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 255

LEFT text: This paper addresses the effectiveness of two data mining techniques in analyzing and retrieving unknown behavior patterns from gigabytes of data collected in the health insurance industry. Specifically, an episode (claims) database for pathology services and a general practitioners database were used. Association rules were applied to the episode database; neural segmentation was applied to the overlaying of both databases. The results obtained from this study demonstrate the potential value of data mining in health insurance information systems, by detecting patterns in the ordering of pathology services and by classifying the general practitioners into groups reflecting the nature and style of their practices. The approach used led to results which could not have been obtained using conventional techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jiawei han
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: We describe the design and implementation of the Glue-Nail deductive database system. Nail is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code are both compiled into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm and supports well-founded models. The Glue compiler's static optimizer uses peephole techniques and data flow analysis to improve code.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: The abfity to optimize queries is considered one of the key enabling technologies for relational databases and is an active area in research and development. There continues to be work in discovering new transformations for SQL. The problem of building extensible query optimizers continues to draw a lot of attention, not only in academic research but also in commercial development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1275

LEFT text: For some multimedia applications, it has been found that domain objects cannot be represented as feature vectors in a multidimensional space. Instead, pair-wise distances between data objects are the only input. To support content-based retrieval, one approach maps each object to a k-dimensional (k-d) point and tries to preserve the distances among the points. Then, existing spatial access index methods such as the R-trees and KD-trees can support fast searching on the resulting k-d points. However, information loss is inevitable with such an approach since the distances between data objects can only be preserved to a certain extent. Here we investigate the use of a distance-based indexing method. In particular, we apply the vantage point tree (vp-tree) method. There are two important problems for the vp-tree method that warrant further investigation, the n-nearest neighbors search and the updating mechanisms. We study an n-nearest neighbors search algorithm for the vp-tree, which is shown by experiments to scale up well with the size of the dataset and the desired number of nearest neighbors, n. Experiments also show that the searching in the vp-tree is more efficient than that for the $R^*$-tree and the M-tree. Next, we propose solutions for the update problem for the vp-tree, and show by experiments that the algorithms are efficient and effective. Finally, we investigate the problem of selecting vantage-point, propose a few alternative methods, and study their impact on the number of distance computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: dynamic vp-tree indexing for n-nearest neighbor search given pair-wise distances

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ada wai-chee fu , polly mei-shuen chan , yin-ling cheung , yiu sang moon
",y
"LEFT id: NA
RIGHT id: 1189

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: fabio casati , ming-chien shan , dimitrios georgakopoulos
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",y
"LEFT id: NA
RIGHT id: 2261

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: Due to the advances in semiconductor manufacturing, the gap between main memory and secondary storage is constantly increasing. This becomes a significant performance bottleneck for Database Management Systems, which rely on secondary storage heavily to store large datasets. Recent advances in nanotechnology have led to the invention of alternative means for persistent storage. In particular, MicroElectroMechanical Systems (MEMS) based storage technology has emerged as the leading candidate for next generation storage systems. In order to integrate MEMS-based storage into conventional computing platform, new techniques are needed for I/O scheduling and data placement.    In the context of relational data, it has been observed that access to relations needs to be enabled in both row-wise as well as in columnwise fashions. In this paper, we exploit the physical characteristics of MEMS-based storage devices to develop a data placement scheme for relational data that enables retrieval in both row-wise and column-wise manner. We demonstrate that this data layout not only improves I/O utilization, but results in better cache performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 902

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a graphical query language for mobile information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ya-hui chang
",y
"LEFT id: NA
RIGHT id: 873

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1888

LEFT text: Visualizations embody design choices about data access, data transformation, visual representation, and interaction. To interpret a static visualization, a person must identify the correspondences between the visual representation and the underlying data. These correspondences become moving targets when a visualization is dynamic. Dynamics may be introduced in a visualization at any point in the analysis and visualization process. For example, the data itself may be streaming, shifting subsets may be selected, visual representations may be animated, and interaction may modify presentation. In this paper, we focus on the impact of dynamic data. We present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations. Visualization techniques are organized into categories at various levels of abstraction. The salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices. Examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability (and thus utility) of visualizations. The taxonomy presented provides a reference point for further exploration of dynamic data visualization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis e. ioannidis
",y
"LEFT id: NA
RIGHT id: 1785

LEFT text: The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 900

LEFT text: There has been much work on compressing database indexes, but less on compressing the data itself. We examine the performance gains to be made by compression outside the index. A novel compression algorithm is reported, which enables the processing of queries without decompressing data needed to perform join operations in a database built on a triple store. The results of modelling the performance of the database with and without compression are given and compared with other recent work in this area. It is found that for some applications, gains in performance of over 50% are achievable, and in OLTP-like situations, there are also gains to be made.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performing joins without decompression in a compressed database system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: s. j. o'connell , n. winterbottom
",y
"LEFT id: NA
RIGHT id: 441

LEFT text: Our method relies on randomizing techniques that compute small ""sketch"" summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. We also demonstrate how existing statistical information on the base data (e.g., histograms) can be used in the proposed framework to improve the quality of the approximation provided by our algorithms. The key idea is to intelligently partition the domain of the underlying attribute(s) and, thus, decompose the sketching problem in a way that provably tightens our guarantees. Results of our experimental study with real-life as well as synthetic data streams indicate that sketches provide significantly more accurate answers compared to histograms for aggregate queries. This is especially true when our domain partitioning methods are employed to further boast the accuracy of the final estimates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximating multi-dimensional aggregate range queries over real attributes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 268

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1839

LEFT text: A data warehouse is a repository of integrated information from distributed, autonomous, and possibly heterogeneous, sources. In effect, the warehouse stores one or more materialized views of the source data. The data is then readily available to user applications for querying and analysis. Figure 1 shows the basic architecture of a warehouse: data is collected from each source, integrated with data from other sources, and stored at the warehouse. Users then access the data directly from the warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line warehouse view maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dallan quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 849

LEFT text: This paper aims at classifying existing approaches which can be used to query heterogeneous data sources. We consider one of the approaches — the mediated query approach — in more detail and provide a classification framework for it as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what is the nearest neighbor in high dimensional spaces ?

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alexander hinneburg , charu c. aggarwal , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 473

LEFT text: Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: re-designing distance functions and distance-based applications for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 348

LEFT text: In this paper we discuss the benefits of workflow automation and we show why workflow is a key technology for building the foundation for ebusiness. We will demonstrate these concepts by presenting an example of a very successful ebusiness startup that has placed HP Changengine at the core of its e-business platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an active functionality service for e-business applications

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: m. cilia , a. p. buchmann
",n
"LEFT id: NA
RIGHT id: 1316

LEFT text: We present a change-centric method to manage versions in a Web WareHouse of XML data. The starting points is a sequence of snapshots of XML documents we obtain from the web. By running a di algorithm, we compute the changes between two consecutive versions. We then represent the sequence using a novel representation of changes based on completed deltas and persistent identi ers. We present the foundations of the logical representation and some aspects of the physical storage policy. The work presented here was developed in the context of the Xyleme project of massive XML warehouse for XML data from the Web. It has been implemented and tested. We brie y discuss the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: change-centric management of versions in an xml warehouse

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: am &#233; lie marian , serge abiteboul , gregory cobena , laurent mignet
",y
"LEFT id: NA
RIGHT id: 955

LEFT text: Similarity search is a very important operation in multimedia databases and other database applications involving complex objects, and involves finding objects in a data set S similar to a query object q, based on some similarity measure. In this article, we focus on methods for similarity search that make the general assumption that similarity is represented with a distance metric d. Existing methods for handling similarity search in this setting typically fall into one of two classes. The first directly indexes the objects based on distances (distance-based indexing), while the second is based on mapping to a vector space (mapping-based approach). The main part of this article is dedicated to a survey of distance-based indexing methods, but we also briefly outline how search occurs in mapping-based methods. We also present a general framework for performing search based on distances, and present algorithms for common types of queries that operate on an arbitrary ""search hierarchy."" These algorithms can be applied on each of the methods presented, provided a suitable search hierarchy is defined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: m-tree : an efficient access method for similarity search in metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo ciaccia , marco patella , pavel zezula
",n
"LEFT id: NA
RIGHT id: 1339

LEFT text: At present Web services are thought as the revolution of the next generation e-commerce and its technical architectures include UDDI, WSDL, SOAP, XML and so on. Web service discovery is one of the most important parts in the web service architectures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: are web services the next revolution in e-commerce ? ( panel )

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shalom tsur , serge abiteboul , rakesh agrawal , umeshwar dayal , johannes klein , gerhard weikum
",y
"LEFT id: NA
RIGHT id: 1052

LEFT text: In this paper, we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implemanting these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend that strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator TUR, resulting in a set of fixpoints for UR. The monotionicity of the operator is maintained nby explicitly representing the effect of asserting and retracting tuples in the database. A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relaions, i.e., those relations which are not update by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: We describe a system that supports arbitrarily complex SQL queries with ”uncertain” predicates. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is query evaluation. We describe an optimization algorithm that can compute eciently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any ecient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2037

LEFT text: In many applications from telephone fraud detection to network management, data arrives in a stream, and there is a need to maintain a variety of statistical summary information about a large number of customers in an online fashion. At present, such applications maintain basic aggregates such as running extrema values (MIN, MAX), averages, standard deviations, etc., that can be computed over data streams with limited space in a straightforward way. However, many applications require knowledge of more complex aggregates relating different attributes, so-called correlated aggregates. As an example, one might be interested in computing the percentage of international phone calls that are longer than the average duration of a domestic phone call. Exact computation of this aggregate requires multiple passes over the data stream, which is infeasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing set expressions over continuous update streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sumit ganguly , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 792

LEFT text: Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: high-performance extensible indexing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: marcel kornacker
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: During the past few years our research efforts have been inspired by two different needs. On one hand, the number of non-expert users accessing databases is growing apace. On the other, information systems will no longer be characterized by a single centralized architecture, but rather by several heterogeneous component systems. In order to address such needs we have designed a new query system with both user-oriented and multidatabase features. The system's main components are an adaptive visual interface, providing the user with different and interchangeable interaction modalities, and a “translation layer”, which creates and offers to the user the illusion of a single homogeneous schema out of several heterogeneous components. Both components are founded on a common ground, i.e. a formally defined and semantically rich data model, the Graph Model, and a minimal set of Graphical Primitives, in terms of which general query operations may be visually expressed. The Graph Model has a visual syntax, so that graphical operations can be applied on its components without unnecessary mappings, and an object-based semantics. The aim of this paper is twofold. We first present an overall view of the system architecture and then give a comprehensive description of the lower part of the system itself. In particular, we show how schemata expressed in different data models can be translated in terms of Graph Model, possibly by exploiting reverse engineering techniques. Moreover, we show how mappings can be established between well-known query languages and the Graphical Primitives. Finally, we describe in detail how queries expressed by using the Graphical Primitives can be translated in terms of relational expressions so to be processed by actual DBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",y
"LEFT id: NA
RIGHT id: 1686

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a tsql2 tutorial

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard thomas snodgrass , ilsoo ahn , gad ariav , don batory , james clifford , curtis e. dyreson , ramez elmasri , fabio grandi , christian s. jensen , wolfgang k &#228; fer , nick kline , krishna kulkarni , t. y. cliff leung , nikos lorentzos , john f. roddick , arie segev , michael d. soo , suryanarayana m. sripada
",n
"LEFT id: NA
RIGHT id: 1769

LEFT text: Fibonacci is an object-oriented database programming language characterized by static and strong typing, and by new mechanisms for modeling data-bases in terms of objects with roles, classes, and associations. A brief introduction to the language is provided to present those features, which are particularly suited to modeling complex databases. Examples of the use of Fibonacci are given with reference to the prototype implementation of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applications of java programming language to database management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bradley f. burton , victor w. marek
",n
"LEFT id: NA
RIGHT id: 1299

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: using semantic values to facilitate interoperability among heterogeneous information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore , michael siegel , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 1883

LEFT text: Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space. In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 731

LEFT text: B+-Trees have been traditionally optimized for I/O performance with disk pages as tree nodes. Recently, researchers have proposed new types of B+-Trees optimized for CPU cache performance in main memory environments, where the tree node sizes are one or a few cache lines. Unfortunately, due primarily to this large discrepancy in optimal node sizes, existing disk-optimized B+-Trees suffer from poor cache performance while cache-optimized B+-Trees exhibit poor disk performance. In this paper, we propose fractal prefetching B+-Trees (fpB+-Trees), which embed ""cache-optimized"" trees within ""disk-optimized"" trees, in order to optimize both cache and I/O performance. We design and evaluate two approaches to breaking disk pages into cache-optimized nodes: disk-first and cache-first. These approaches are somewhat biased in favor of maximizing disk and cache performance, respectively, as demonstrated by our results. Both implementations of fpB+-Trees achieve dramatically better cache performance than disk-optimized B+-Trees: a factor of 1.1-1.8 improvement for search, up to a factor of 4.2 improvement for range scans, and up to a 20-fold improvement for updates, all without significant degradation of I/O performance. In addition, fpB+-Trees accelerate I/O performance for range scans by using jump-pointer arrays to prefetch leaf pages, thereby achieving a speed-up of 2.5-5 on IBM's DB2 Universal Database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a compact b-tree

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter bumbulis , ivan t. bowman
",y
"LEFT id: NA
RIGHT id: 198

LEFT text: Information becomes a more and more valuable asset in today’s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome in the design and implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II , which is based on the wrapper-mediator approach. The main focus of this work lies on data model transformation and query processing. The following extensions to the AMOS II system are described in this thesis: • A framework for transforming various data models into the objectoriented model of AMOS II is presented. • The roles and tasks of wrappers are described. In particular their participation in query processing and query optimization is discussed. • A way for describing and utilizing the query capabilities of the different data sources is proposed. • Two different approaches to query processing over external data sources are developed and analyzed. All the proposed techniques are implemented in the AMOS II system, which runs on a Windows NT platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mind your vocabulary : query mapping across heterogeneous information sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chen-chuan k. chang , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 565

LEFT text: We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time ""twist"": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of ""big data"". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a ""big data"" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle ""big"" as well as ""fast"" data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research activities in database management and information retrieval at university of illinois at chicago

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: isabel cruz , ashfaq khokhar , bing liu , prasad sistla , ouri wolfson , clement yu
",n
"LEFT id: NA
RIGHT id: 767

LEFT text: National Tsing Hua University (NTttU) was founded in 1911 and is located in a suburb of the city of Hsinehu, Taiwan, about 50 miles southwest of Taipei, the capital city. Its Computer Science Department was established in 1977, and currently has 23 faculty members

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: going public : open-source databases and database research

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: philippe bonnet
",n
"LEFT id: NA
RIGHT id: 1775

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: predator : a resource for database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source's performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants mechanism, which shows how semantic information about data sources may be used to discover cached query results of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 1044

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the x-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1610

LEFT text: MTCache is a prototype midtier database caching solution for SQL server that achieves this transparency. It builds on SQL server's support for materialized views, distributed queries and replication. We describe MTCache and report experimental results on the TPC-W benchmark. The experiments show that a significant part of the query workload can be offloaded to cache servers, resulting in greatly improved scale-out on the read-dominated workloads of the benchmark. Replication overhead was small with an average replication delay of less than two seconds.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: wireless client/server computing for personal information services and applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ahmed elmagarmid , jin jing , tetsuya furukawa
",n
"LEFT id: NA
RIGHT id: 1698

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research perspectives for time series management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 1058

LEFT text: We present an optimization method and al gorithm designed for three objectives: physi cal data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and ""backchase"" with constraints (dependen cies). By using dictionaries (finite functions) in physical schemas we can capture with con straints useful access structures such as indexes, materialized views, source capabilities, access support relations, gmaps, etc. The search space for query plans is defined and enumerated in a novel manner: the chase phase rewrites the original query into a ""universal"" plan that integrates all the access structures and alternative pathways that are allowed by appli cable constraints. Then, the backchase phase produces optimal plans by eliminating various combinations of redundancies, again according to constraints. This method is applicable (sound) to a large class of queries, physical access structures, and semantic constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: physical data independence , constraints , and optimization with universal plans

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , lucian popa , val tannen
",y
"LEFT id: NA
RIGHT id: 1029

LEFT text: In this paper, our research objective is to develop a database virtualizat ion technique in order to let data analysts or other users who apply data mining methods to their jobs use all ubiquitous databases on the Internet as if they were recognized as a single database, thereby helping to reduce their workloads such as data collection from the Internet databases and data cleansing works. In this study, firstly we examine XML schema advantages and propose a database virtualizat ion method by which such ubiquitous databases as relational databases, object-oriented databases, and XML databases are accessed as if they all behave as a single database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: critical database technologies for high energy physics

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david m. malon , edward n. may
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 1936

LEFT text: In matching problems, given a pattern, a set of data objects and a distance metric, we find the distance between the pattern and one or more data objects. In discovery problems by contrast, given a set of objects, a metric, and a distance, we seek a pattern that matches many of those objects within the given distance. (So, discovery is a lot like data mining.) Our toolkit performs both matching and discovery with current targeted applications in molecular biology and document comparison.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: pattern matching and pattern discovery in scientific , program , and document databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jason t. l. wang , kaizhong zhang , dennis shasha
",y
"LEFT id: NA
RIGHT id: 947

LEFT text: Data warehouses and recording systems typically have a large continuous stream of incoming data, that must be stored in a manner suitable for future access. Access to stored records is usually based on a key. Organizing the data on disk as the data arrives using standard techniques would result in either (a) one or more I/OS to store each incoming record (to keep the data clustered by the key), which is too expensive when data arrival rates are very high, or (b) many I/OS to locate records for a particular customer (if data is stored clustered by arrival order). We study two techniques, inspired by external sorting algorithms, to store data incrementally as it arrives, simultaneously providing good performance for recording and querying. We present concurrency control and recovery schemes for both techniques. We show the benefits of our techniques both analytically and experimentally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental organization for data recording and warehousing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: h. v. jagadish , p. p. s. narayan , s. seshadri , s. sudarshan , rama kanneganti
",y
"LEFT id: NA
RIGHT id: 695

LEFT text: In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partial results for online query processing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1515

LEFT text: We describe a scheme to fragment and distribute centralized databases. ’ The problem is motivated by trends towards down-sizing and reorganization, reflecting actual, often distributed responsibilities within companies. A major practical requirement is that existing application code must be left unchanged. We present SQL extensions to specify ownership and data replication information declaratively. From this, a compiler generates triggers and view definitions that implement the distributed scheme, on top of a collection of local databases. Our strategy has been applied successfully at Telenor the Norwegian telephone comp.any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enhancing database correctness : a statistical approach

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-chi hou , zhongyang zhang
",n
"LEFT id: NA
RIGHT id: 1195

LEFT text: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing and optimization in oracle rdb

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: gennady antoshenkov , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 768

LEFT text: For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining distance-based outliers in large datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 2155

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: large databases for remote sensing and gis

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: a. r. dasgupta
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",y
"LEFT id: NA
RIGHT id: 200

LEFT text: We introduce a new algorithm (BOAT) for decision tree construction that improves upon earlier algorithms in both performance and functionality. BOAT constructs several levels of the tree in only two scans over the training database, resulting in an average performance gain of 300% over previous work. The key to this performance improvement is a novel optimistic approach to tree construction in which we construct an initial tree using a small subset of the data and refine it to arrive at the final tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: boat-optimistic decision tree construction

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: johannes gehrke , venkatesh ganti , raghu ramakrishnan , wei-yin loh
",y
"LEFT id: NA
RIGHT id: 1883

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",y
"LEFT id: NA
RIGHT id: 694

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting statistics on query expressions for optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 1506

LEFT text: Abstract.We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates, considering separatelycases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 413

LEFT text: Abstract Spatial-Query-by-Sketch is the design of a query language for geographic information systems. It allows a user to formulate a spatial query by drawing the desired configuration with a pen on a touch-sensitive computer screen and translates this sketch into a symbolic representation that can be processed against a geographic database. Since the configurations queried usually do not match exactly the sketch, it is necessary to relax the spatial constraints drawn. This paper describes the representation of a sketch and outlines the design of the constraint relaxation methods used during query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fact : a learning based web query processing system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: songting chen , yanlei diao , hongjun lu , zengping tian
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 562

LEFT text: National Tsing Hua University (NTttU) was founded in 1911 and is located in a suburb of the city of Hsinehu, Taiwan, about 50 miles southwest of Taipei, the capital city. Its Computer Science Department was established in 1977, and currently has 23 faculty members

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of illinois at urbana-champaign

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: m. winslett , k. chang , a. doan , j. han , c. zhai , y. zhou
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: We describe the TIGUKAT objectbase management system, which is under development at the Laboratory for Database Systems Research at the University of Alberta. TIGUKAT has a novel object model, whose identifying characteristics include a purely behavioral semantics and a uniform approach to objects. Everything in the system, including types, classes, collections, behaviors, and functions, as well as meta-information, is a first-class object with well-defined behavior. In this way, the model abstracts everything, including traditional structural notions such as instance variables, method implementation, and schema definition, into a uniform semantics of behaviors on objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 1871

LEFT text: Building such a database system requires fundamental changes in the architecture of the query processing engine; we present the system-level interfaces of PREDATOR that support E-ADTs, and describe the internal design details.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: predator : an or-dbms with enhanced data types

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: praveen seshadri , mark paskin
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 578

LEFT text: CORAL is a modular declarative query language/programming language that supports general Horn clauses with complex terms, set-grouping, aggregation, negation, and relations with tuples that contain (universally quantified) variables. Support for persistent relations is provided by using the EXODUS storage manager. A unique feature of CORAL is that it provides a wide range of evaluation strategies and allows users to optionally tailor execution of a program through high-level annotations. A CORAL program is organized as a collection of modules, and this structure is used as the basis for expressing control choices. CORAL has an interface to C++, and uses the class structure of C++ to provide extensibility. FinaUy, CORAL supports a command sublanguage, in which statements are evaluated in a user-specified order. The statements can be queries, updates, production-system style rules, or any command that can be typed in at the CORAL

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the coral deductive system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raghu ramakrishnan , divesh srivastava , s. sudarshan , praveen seshadri
",y
"LEFT id: NA
RIGHT id: 195

LEFT text: In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: join synopses for approximate query answering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 2126

LEFT text: We identify an emergent class of database systems that has not been dealt with extensively in the literature that we call ARCS (Active, Rapidly Changing data Systems) databases. These systems impose certain unique requirements on databases that monitor and control them. These requirements are such that traditional data and transaction management models appear inadequate. We present an analysis of data and transaction characteristics in ARCS systems and identify relevant research issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues for data communication in mobile ad-hoc network database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leslie d. fife , le gruenwald
",n
"LEFT id: NA
RIGHT id: 1587

LEFT text: A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel evaluation of multi-join queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: annita n. wilschut , jan flokstra , peter m. g. apers
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: This paper describes the XSB system, and its use as an in-memory deductive database engine. XSB began from a Prolog foundation, and traditional Prolog systems are known to have serious deficiencies when used as database systems. Accordingly, XSB has a fundamental bottom-up extension, introduced through tabling (or memoing)[4], which makes it appropriate as an underlying query engine for deductive database systems. Because it eliminates redundant computation, the tabling extension makes XSB able to compute all modularly stratified datalog programs finitely and with polynomial data complexity. For non-stratified programs, a meta-interpreter with the same properties is provided. In addition XSB significantly extends and improves the indexing capabilities over those of standard Prolog. Finally, its syntactic basis in HiLog [2], lends it flexibility for data modelling. The implementation of XSB derives from the WAM [25], the most common Prolog engine. XSB inherits the WAM's efficiency and can take advantage of extensive compiler technology developed for Prolog. As a result, performance comparisons indicate that XSB is significantly faster than other deductive database systems for a wide range of queries and stratified rule sets. XSB is under continuous development, and version 1.3 is available through anonymous ftp.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 26

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the aqua approximate query answering system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 1458

LEFT text: Parallel database systems have to support the effective parallelization of complex queries in multi-user mode, i.e. in combination with inter-query~mter-transaction parallelism. For this purpose, dynamic scheduling and load balancing strategies’ are necessary that umsider the current system state for dekrminhg the degree of intra-query parallelism and for selecting the processors for executing subqueries. We study these issues for parallel hash joinprocessing and show that the two subproblems should be addressed in au integrated way. Even more importantly, however, is the use of a multimannce load balancing approach that considers all potential bottleneck resources. in particular memory, disk and CPU. We discuss basic performance tradeoffs to consider and evalGate the performauce of several oad balancing strategies by means of a detailed simulation model. Simulation results will be analyzed for multiuser configurations with both homogeneous andheterogeneous (query/OLTP) workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic multi-resource load balancing in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: erhard rahm , robert marek
",y
"LEFT id: NA
RIGHT id: 597

LEFT text: In the typical database system, an execution is correct if it is equivalent to some serial execution. This criterion, called serializability, is unacceptable for new database applications which require long-duration transactions. We present a new transaction model which allows correctness criteria more suitable for these applications. This model combines three enhancements to the standard model: nested transactions, explicit predicates, and multiple versions. These features yield the name of the new model, nested transactions with predicates and versions, or NT/PV.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: transactional information systems : theory , algorithms , and the practice of concurrency control and recovery

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: marc h. scholl
",n
"LEFT id: NA
RIGHT id: 2129

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2286

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: re-tree : an efficient index structure for regular expressions

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chee-yong chan , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 2128

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1138

LEFT text: In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying heterogeneous information sources using source descriptions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy , anand rajaraman , joann j. ordille
",n
"LEFT id: NA
RIGHT id: 1924

LEFT text: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the semantics of now in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james clifford , curtis dyreson , tom &#225; s isakowitz , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1321

LEFT text: We present the design of ObjectGlobe, a distributed and open query processor for Internet data sources. Today, data is published on the Internet via Web servers which have, if at all, very localized query processing capabilities. The goal of the ObjectGlobe project is to establish an open marketplace in which data and query processing capabilities can be distributed and used by any kind of Internet application. Furthermore, ObjectGlobe integrates cycle providers (i.e., machines) which carry out query processing operators. The overall picture is to make it possible to execute a query with – in principle – unrelated query operators, cycle providers, and data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: hyperqueries : dynamic distributed query processing on the internet

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alfons kemper , christian wiesner
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 1301

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on completeness of historical relational query languages

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: james clifford , albert croker , alexander tuzhilin
",n
"LEFT id: NA
RIGHT id: 539

LEFT text: Abstract.We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates, considering separatelycases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1950

LEFT text: Traditional database systems provide a user with the ability to query and manipulate one database state, namely the current database state. However, in several emerging applications, the ability to analyze “what-if” scenarios in order to reason about the impact of an update (before committing that update) is of paramount importance. Example applications include hypothetical database access, active database management systems, and version management, to name a few. The central thesis of the Heraclitus paradigm is to provide flexible support for applications such as these by elevating deltas, which represent updates proposed against the current database state, to be first-class citizens. Heraclitus[Alg,C] is a database programming language that extends C to incorporate the relational algebra and deltas. Operators are provided that enable the programmer to explicitly construct, combine, and access deltas. Most interesting is the when operator, that supports hypothetical access to a delta: the expression E when &sgr; yields the value that side effect free expression E would have if the value of delta expression &sgr; were applied to the current database state. This article presents a broad overview of the philosophy underlying the Heraclitus paradigm, and describes the design and prototype implementation of Heraclitus[Alg, C]. A model-independent formalism for the Heraclitus paradigm is also presented. To illustrate the utility of Heraclitus, the article presents an in-depth discussion of how Heraclitus[Alg, C] can be used to specify, and thereby implement, a wide range of execution models for rule application in active databases; this includes both prominent execution models presented in the literature, and more recent “customized” execution models with novel features.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: heraclitus : elevating deltas to be first-class citizens in a database programming language

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: shahram ghandeharizadeh , richard hull , dean jacobs
",y
"LEFT id: NA
RIGHT id: 1782

LEFT text: We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient mid-query re-optimization of sub-optimal query execution plans

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 21

LEFT text: This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: versions and workspaces in microsoft repository

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: thomas bergstraesser , philip a. bernstein , shankar pal , david shutt
",y
"LEFT id: NA
RIGHT id: 540

LEFT text: In this paper we present FeedbackBypass, a new approach to interactive similarity query processing. It complements the role of relevance feedback engines by storing and maintaining the query parameters determined with feedback loops over time, using a wavelet-based data structure (the Simplex Tree). For each query, a favorable set of query parameters can be determined and used to either “bypass” the feedback loop completely for already-seen queries, or to start the search process from a near-optimal configuration. FeedbackBypass can be combined well with all state-of-the-art relevance feedback techniques working in high-dimensional vector spaces. Its storage requirements scale linearly with the dimensionality of the query space, thus making even sophisticated query spaces amenable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the cougar approach to in-network query processing in sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong yao , johannes gehrke
",n
"LEFT id: NA
RIGHT id: 1951

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining quantitative association rules in large relational tables

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 85

LEFT text: The Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS 2000) was held at the Crowne Plaza San Jose/Silicon Valley in Milpitas, California on June 8-9, 2000. The purpose of this workshop was to bring together leading practitioners, developers and researchers to explore the challenging technical issues and find feasible solutions for advancing the current state of the art in e-commerce and web-based information systems. In particular, the workshop was interested in the infrastructure issues to facilitate e-commerce and Web-based information systems.WECWIS 2000 was successful. There were three invited talks, one industrial panel discussion and six technical sessions. The keynote speech, ""The global trading web: A strategic vision for the Internet economy,"" was delivered by Dr. Jay M. Tenebaum, VP and Chief Scientist, Commerce One, Inc., on June 8 immediately following the opening remarks by the conference chair. The banquet address, ""Business issues in e-commerce,"" was delivered by Mr. Daniel Druker, General Manager, Hyperion e-Business Division. Finally, a lunch address, ""B2C, B2B, N2N, N2M: Why 2 is so instrumental?"" was delivered by Mr. Mstafa A. Syed, VP of Technology, VertialNet, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: engineering federated information systems : report of eefis '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. conrad , w. hasselbring , u. hohenstein , r.-d . kutsche , m. roantree , g. saake , f. saltor
",n
"LEFT id: NA
RIGHT id: 1891

LEFT text: With the growing importance of XML in data exchange, much research has been done in providing flexible query facilities to extract data from structured XML documents. In this paper, we propose ViST, a novel index structure for searching XML documents. By representing both XML documents and XML queries in structure-encoded sequences, we show that querying XML data is equivalent to finding subsequence matches. Unlike index methods that disassemble a query into multiple sub-queries, and then join the results of these sub-queries to provide the final answers, ViST uses tree structures as the basic unit of query to avoid expensive join operations. Furthermore, ViST provides a unified index on both content and structure of the XML documents, hence it has a performance advantage over methods indexing either just content or structure. ViST supports dynamic index update, and it relies solely on B+ Trees without using any specialized data structures that are not well supported by DBMSs. Our experiments show that ViST is effective, scalable, and efficient in supporting structural queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental data structures and algorithms for dynamic query interfaces

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: egemen tanin , richard beigel , ben shneiderman
",n
"LEFT id: NA
RIGHT id: 149

LEFT text: Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. Here we assume instead that the names are given in natural language text. We then propose a logic for database integration called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. An implemented data integration system based on WHIRL has been used to successfully integrate information from several dozen Web sites in two domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: providing database-like access to the web using queries based on textual similarity

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: william w. cohen
",y
"LEFT id: NA
RIGHT id: 2285

LEFT text: When an update to a view is requested by a user, there may be no unique way of up dating the stored relations in the database to realize the requested update. Chosing one of the alternatives for updating stored relations may not reflect the change that has actually taken place in the real world; in the presence of other derived views, the database may actually present a very wrong model of the world to the user. The problem is even more severe in the case of deductive databases. For avoiding this problem, we introduce a new notion of view updates, called cumulative updates. The key idea behind cumulative updates is that update mechanisms should wait for further update requests to resolve ambiguities. Equivalently, current update requests must also take into account previous requests made to the knowledge base. Cumulative updates, therefore, subsume conventional updates in which only the current update request is considered. In this paper, we motivate the need for cumulative updates and formally define the notion of such updates as well as the different classes therein. We then give methods for computing one particular class of cumulative updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cumulative updates

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: suryanarayana m. sripada , beat w &#252; thrich
",y
"LEFT id: NA
RIGHT id: 1900

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: pixel-oriented database visualizations

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: The extensions support new data types such as point, circle, etc., and functions such as confains, interval, text-contains, etc. Let the tables Policies (policy-id, name, address, location, vehicle-type, . . .) and Claims (policy-id, claim-tag, accident-date, accident-location, accident-report, . . .) represent the partial schema containing both SQL’92 and user defined data types (UDTs). Consider a scenario in a targeted marketing application that requires a mailing list of all customers within 5 miles of point L, who have insured a ‘sports utility vehicle’ and were involved in a ‘rear-ended’ accident in the past 3 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 66

LEFT text: Clustering is one of the most effective means to enhance the performance of object base applications. Consequently, many proposals exist for algorithms computing good object placements depending on the application profile. However, in an effective object base reorganization tool the clustering algorithm is only one constituent. In this paper, we report on our object base reorganization tool that covers all stages of reorganizing the objects: the application profile is determined by a monitoring tool, the object placement is computed from the monitored access statistics utilizing a variety of clustering algorithms and, finally, the reorganization tool restructures the object base accordingly. The costs as well as the effectiveness of these tools is quantitatively evaluated on the basis of the OO1-benchmark.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization in object databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mohana k. lakhamraju , rajeev rastogi , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1356

LEFT text: The ESPRIT Project DWQ (Foundations of Data Warehouse Quality) aimed at improving the quality of DW design and operation through systematic enrichment of the semantic foundations of data warehousing. Logic-based knowledge representation and reasoning techniques were developed to control accuracy, consistency, and completeness via advanced conceptual modeling techniques for source integration, data reconciliation, and multi-dimensional aggregation. This is complemented by quantitative optimization techniques for view materialization, optimizing timeliness and responsiveness without losing the semantic advantages from the conceptual approach. At the operational level, query rewriting and materialization refreshment algorithms exploit the knowledge developed at design time. The demonstration shows the interplay of these tools under a shared metadata repository, based on an example extracted from an application at Telecom Italia.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrency in the data warehouse

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: richard taylor
",n
"LEFT id: NA
RIGHT id: 668

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 695

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partial results for online query processing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 940

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries across diverse data sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laura m. haas , donald kossmann , edward l. wimmers , jun yang
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 1357

LEFT text: In this paper, we first introduce the database aspects of the groupware product Lotus Domino/Notes and then describe, in some more detail, many of the logging and recovery enhancements that were introduced in R5. We discuss briefly some of the changes that had to be made to the ARIES recovery method to accommodate the unique storage management characteristics of Notes. We also outline some of the on-going logging and locking work in the Dominotes project at the IBM Almaden Research Center.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evolution of groupware for business applications : a database perspective on lotus domino/notes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: c. mohan , ron barber , s. watts , a. somani , markos zaharioudakis
",y
"LEFT id: NA
RIGHT id: 1916

LEFT text: The DARPA Intelligent Integration of Information (I3) effort is based on the assumption that systems can easily exchange data. However, as a consequence of the rapid development of research, and prototype implementations, in this area, the initial outcome of this program appears to have been to produce a new set of systems. While they can perform certain advanced information integration tasks, they cannot easily communicate with each other.With a view to understanding and solving this problem, there was a group discussion at the DARPA Intelligent Integration of Information/Persistent Object Bases (I3/POB) meeting in San Diego, in January, 1996; and a further workshop was held on this topic at the University of Maryland in April, 1996. The list of participants is in Appendix A. The idea emerging from these meeting a was not to force all systems to communicate according to specified standards, but to agree on the following:&bull; A minimal core language, or Level 1 option, which would be a restriction of the object-oriented query language OQL, such that it will accept queries for relational databases. We recommend that all system components be able, at a minimum, to accept queries in this syntax, provided they address concepts (e.g., relations or classes, attributes or instance variables) known to that component. There must be a simple protocol to determine the schema of a system (its set of supported concepts).&bull; A simple format for representing answers. This could also be a fragment of OQL and will be included in the core language specification.&bull; A set of extensions, one of which could be full OQL, and would handle complex structures and abstract types (with methods). Other extensions will be needed to support rules (e.g., definitions of terms that can be shared among components), semistructured data (for self-describing objects), and shared code. A system component could support one or more of these extensions, independently, and there should be some simple protocol to determine the particular extensions that are supported.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mediator languages-a proposal for a standard : report of an i3/pob working group held at the university of maryland , april 12 and 13 , 1996

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter buneman , louiqa raschid , jeffrey ullman
",y
"LEFT id: NA
RIGHT id: 1613

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: view maintenance in mobile computing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ouri wolfson , prasad sistla , son dao , kailash narayanan , ramya raj
",n
"LEFT id: NA
RIGHT id: 420

LEFT text: Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a data model and data structures for moving objects databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: luca forlizzi , ralf hartmut g &#252; ting , enrico nardelli , markus schneider
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2151

LEFT text: Due to the recent growth of the World Wide Web, numerous spatio-temporal applications can obtain their required information from web sources. In this demonstration we show The WorldInfo Assistant, an application that extracts and integrates spatial, temporal and other information about dierent regions of the world from dierent web sources and databases. This application also provides integration of dierent vector data with the satellite images of dierent regions of the world. Finally, We demonstrate several approaches for ecient querying moving objects with predefined paths and schedules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information integration on the web : a view from ai and databases ( report on iiweb-03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: subbarao kambhampati , craig a. knoblock
",n
"LEFT id: NA
RIGHT id: 2218

LEFT text: The Web today consists exclusively of HTML documents designed for the human eye. While many of them are generated automatically by applications, it is difficult for other applbcations to read and process them. This may soon change, due to a series of new standards frorn the World Wide Web Consortium centered around XML (Extensible Markup Language). XML is designed to express the document content, while HTML expresses its presentation. In short, XML is a data exchange format, easily understood by applications. It enables data exchange on the Web, both intra-enterprise, across platforms (intranet), and inter-enterprise (internet). The focus of the Web shifts from document management to data management, and topics like queries, views, data warehouses, mediators, which were the domain of databases, become of interest to the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: managing uncertainty in sensor database

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: reynold cheng , sunil prabhakar
",n
"LEFT id: NA
RIGHT id: 228

LEFT text: Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the microsoft database research group

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: The wonderfully clean and beautiful scheme put ""on its head"" the world of query optimization I had assumed was the only one possible. In fact, this paper is all about questioning implicit assumptions behind classic query optimization. Is it always true that query-evaluation performance does not fluctuate during query execution?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 533

LEFT text: This second special issue provides a forum for topical issues that demonstrate the usefulness of PLS-SEM by piloting applications of this method in the field of strategic management with strong implications for strategic research and practice. As such, the special issue targets two audiences: academics involved in the fields of strategy and management, and practitioners such as consultants. The six articles in this issue are summarized in the following paragraphs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial directons

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1041

LEFT text:  In this paper we present a second enhancement: a single operator that lets the analyst get summarized reasons for drops or increases observed at an aggregated level. This eliminates the need to manually drill-down for such reasons. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design a dynamic programming algorithm that requires only one pass of the data improving significantly over our initial greedy algorithm that required multiple passes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: Abstract. Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",n
"LEFT id: NA
RIGHT id: 70

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: answering complex sql queries using automatic summary tables

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: markos zaharioudakis , roberta cochrane , george lapis , hamid pirahesh , monica urata
",n
"LEFT id: NA
RIGHT id: 118

LEFT text: We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative formalism for specifying these kinds of probabilistic information, and we propose algorithms for ordering the information sources. Finally, we discuss a preliminary experimental evaluation of these algorithms on the domain of bibliographic sources available on the WWW.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in global information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: a. m. ouksel , a. sheth
",n
"LEFT id: NA
RIGHT id: 720

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : database caching for web application servers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mehmet altinel , qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , bruce g. lindsay , honguk woo , larry brown
",n
"LEFT id: NA
RIGHT id: 548

LEFT text: Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - ""given the current information, it is possible that X will become true at time t"" - instead of no information at all.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: continuous queries over data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shivnath babu , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1447

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: procedures in object-oriented query languages

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kazimierz subieta , yahiko kambayashi , jacek leszczylowski
",n
"LEFT id: NA
RIGHT id: 2016

LEFT text: In this work, we address the efficient evaluation of XQuery expressions over continuous XML data streams, which is essential for a broad range of applications including monitoring systems and information dissemination systems. While previous work has shown that automata theory is suited for on-the-fly pattern retrieval over XML data streams, we find that automata-based approaches suffer from being not as flexibly optimizable as algebraic query systems. In fact, they enforce a rigid data-driven paradigm of execution. We thus now propose a unified query model to augment automata-style processing with algebra-based query optimization techniques. The proposed model has been successfully applied in the Raindrop stream processing system. Our experimental study confirms considerable performance gains with both established optimization techniques and our novel query rewrite rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rainbow : multi-xquery optimization using materialized xml views

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: xin zhang , katica dimitrova , ling wang , maged el sayed , brian murphy , bradford pielech , mukesh mulchandani , luping ding , elke a. rundensteiner
",y
"LEFT id: NA
RIGHT id: 858

LEFT text: Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance issues in incremental warehouse maintenance

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wilburt labio , jun yang , yingwei cui , hector garcia-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: Previous studies on mining association rules find rules at single concept level, however, mining association rules at multiple concept levels may lead to the discovery of more specific and concrete knowledge from data. In this study, a top-down progressive deepening method is developed for mining multiplelevel association rules from large transaction databases by extension of some existing association rule mining techniques. A group of variant algorithms are proposed based on the ways of sharing intermediate results, with the relative performance tested on different kinds of data. Relaxation of the rule conditions for finding “level-crossing” association rules is also discussed in the paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 513

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",y
"LEFT id: NA
RIGHT id: 1125

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 720

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : database caching for web application servers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mehmet altinel , qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , bruce g. lindsay , honguk woo , larry brown
",y
"LEFT id: NA
RIGHT id: 964

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast high-dimensional data search in incomplete databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: beng chin ooi , cheng hian goh , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 130

LEFT text: We address the development of a normalization theory for object-oriented data models that have common features to support objects. We first provide an extension of functional dependencies to cope with the richer semantics of relationships between objects, called path dependency, local dependency, and global dependency constraints. Using these dependency constraints, we provide normal forms for object-oriented data models based on the notions of user interpretation (user-specified dependency constraints) and object model. In constrast to conventional data models in which a normalized object has a unique interpretation, in object-oriented data models, an object may have many multiple interpretations that form the model for that object. An object will then be in a normal form if and only if the user's interpretation is derivable from the model of the object. Our normalization process is by nature iiterative, in which objects are restructured until their models reflect the user's interpretation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: object normal forms and dependency constraints for object-oriented schemata

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: zahir tari , john stokes , stefano spaccapietra
",y
"LEFT id: NA
RIGHT id: 347

LEFT text: To overcome current bottlenecks in business-to-business (B2B) electronic commerce, we need intelligent solutions for mechanizing the process of structuring, standardizing, aligning and personalizing data. This article surveys the overall content management process and discusses requirements for its scalable support.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1579

LEFT text: This is the seventh bibliography concerning temporal databases. In this bibliography, we collect 331 new temporal databases papers. Most of these papers were published in 1996-1997, some in 1995 and some will appear in 1997 or 1998.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography of benchmarks for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1542

LEFT text: The tutorial surveys state-of-the-art methods for storing and retrieving multimedia data from large databases. Records (= documents) may consist of formatted fields, text, images, voice, animation etc. .4 sample query that we would like to support is ‘in a collection of 2-d color images, find images that are similar to a sunset photograph’. Indexing for images and other media is a new, active area of research; the tutorial will present recent approaches and prototype systems, for 2-d and 3-d medical image databases, 2-d color image databases, and l-d time series databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",y
"LEFT id: NA
RIGHT id: 1833

LEFT text: The PBSM algorithm partitions the inputs into manageable chunks, and joins them using a computational geometry based plane-sweeping technique. This paper also presents a performance study comparing the the traditional indexed nested loops join algorithm, a spatial join algorithm based on joining spatial indices, and the PBSM algorithm. These comparisons are based on complete implementations of these algorithms in Paradise, a database system for handling GIS applications. Using real data sets, the performance study examines the behavior of these spatial join algorithms in a variety of situations, including the cases when both, one, or none of the inputs to the join have an suitable index. The study also examines the effect of clustering the join inputs on the performance of these join algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: size separation spatial join

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nick koudas , kenneth c. sevcik
",n
"LEFT id: NA
RIGHT id: 2266

LEFT text: A wide range of Web applications retrieve desired information from remote XML data sources across the Internet, which is usually costly due to transmission delays for large volumes of data. Therefore we propose to apply the ideas of semantic caching to XML query processing systems [2], in particular the XQuery engine. Semantic caching [3] implies view-based query answering and cache management. While it is well studied in the traditional database context, query containment for XQuery is left unexplored due to its complexity coming with the powerful expressiveness of hierarchy, recursion and result construction. We hence have developed the first solution for XQuery processing using cached views.We exploit the connections between XML and tree automata, and use subtype relations between two regular expression types to tackle the XQuery containment mapping problem. Inspired by XDuce [1], which explores the use of tree-automata-based regular expression types for XML processing, we have designed a containment mapping process to incorporate type inference and subtyping mechanisms provided by XDuce to establish containment mappings between regular-expression-type-based pattern variables of two queries. We have implemented a semantic caching system called XCache (see Figure 1), to realize the proposed containment and rewriting techniques for XQuery.The main modules of XCache include: (1) Query Decomposer. An input query is is decomposed into source-specific subqueries explicitly represented by matching patterns and return structures. (2) Query Pattern Register. By registering a few queries into semantic regions, we warm up XCache at its initialization phase. (3) Query Containment Mapper. The XDuce subtyper is incorporated into the containment mapper for establishing query containment mappings between variables of a new query and each cached query. (4) Query Rewriter. We implement the classical bucket algorithm and further apply heuristics to decide on an ""optimal"" rewriting plan if several valid ones exist. (5) Replacement Manager. We free space for new regions by both complete and partial replacement. (6) Region Coalescer. We apply a coalescing strategy to control the region granularity over time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: psoup : a system for streaming queries over streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sirish chandrasekaran , michael j. franklin
",n
"LEFT id: NA
RIGHT id: 638

LEFT text: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: themis : a database programming language handling integrity constraints

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: v &#233; ronique benzaken , anne doucet
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational model that would make it more appropriate for processing queries over XML documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1765

LEFT text: Scheduling query execution plans is an important component of query optimization in parallel database systems. The problem is particularly complex in a shared-nothing execution environment, where each system node represents a collection of time-shareable resources (e.g., CPU(s), disk(s), etc.) and communicates with other nodes only by message-passing. Significant research effort has concentrated on only a subset of the various forms of intra-query parallelism so that scheduling and synchronization is simplified. In addition, most previous work has focused its attention on one-dimensional models of parallel query scheduling, effectively ignoring the potential benefits of resource sharing. In this paper, we develop an approach that is more general in both directions, capturing all forms of intra-query parallelism and exploiting sharing of multi-dimensional resource nodes among concurrent plan operators. This allows scheduling a set of independent query tasks (i.e., operator pipelines) to be seen as an instance of the multi-dimensional bin-design problem. Using a novel quantification of coarse grain parallelism, we present a list scheduling heuristic algorithm that is provably near-optimal in the class of coarse grain parallel executions (with a worst-case performance ratio that depends on the number of resources per node and the granularity parameter). We then extend this algorithm to handle the operator precedence constraints in a bushy query plan by splitting the execution of the plan into synchronized phases. Preliminary performance results confirm the effectiveness of our scheduling algorithm compared both to previous approaches and the optimal solution. Finally, we present a technique that allows us to relax the coarse granularity restriction and obtain a list scheduling method that is provably near-optimal in the space of all possible parallel schedules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: t2 : a customizable parallel database for multi-dimensional data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chialin chang , anurag acharya , alan sussman , joel saltz
",n
"LEFT id: NA
RIGHT id: 15

LEFT text: In this paper, we describe an architecture for an open marketplace exploiting the workflow technology and the currently emerging data exchange and metadata representation standards on the Web. In this market architecture electronic commerce is realized through the adaptable workflow templates provided by the marketplace to its users. Having workflow templates for electronic commerce processes results in a component-based architecture where components can be agents (both buying and selling) as well as existing applications invoked by the workflows. Other advantages provided by the workflow technology are forward recovery, detailed logging of the processes through workflow history manager and being able to specify data and control flow among the workflow components.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a workflow-based electronic marketplace on the web

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: asuman dogac , ilker durusoy , sena arpinar , nesime tatbul , pinar koksal , ibrahim cingil , nazife dimililer
",y
"LEFT id: NA
RIGHT id: 793

LEFT text: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the ""top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating top-k selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1781

LEFT text: Bioinformatics, the discipline concerned with biological information management is essential in the post-genome era, where the complexity of data processing allows for contemporaneous multi level research including that at the genome level, transcriptome level, proteome level, the metabolome level, and the integration of these -omic studies towards gaining an understanding of biology at the systems level. This research is also having a major impact on disease research and drug discovery, particularly through pharmacogenomics studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: interaction of query evaluation and buffer management for information retrieval

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bj &#246; rn t. j &#243; nsson , michael j. franklin , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 205

LEFT text: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional selectivity estimation using compressed histogram information

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ju-hong lee , deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1041

LEFT text: Abstract.We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates, considering separatelycases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1694

LEFT text: Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative indepe...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data modelling in the large

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin bertram
",n
"LEFT id: NA
RIGHT id: 810

LEFT text: To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 243

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lore : a lightweight object repository for semistructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dallan quass , jennifer widom , roy goldman , kevin haas , qingshan luo , jason mchugh , svetlozar nestorov , anand rajaraman , hugo rivero , serge abiteboul , jeff ullman , janet wiener
",n
"LEFT id: NA
RIGHT id: 1947

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: solving satisfiability and implication problems in database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sha guo , wei sun , mark a. weiss
",n
"LEFT id: NA
RIGHT id: 1548

LEFT text: Data replication has recently become a topic of increased interest among customers. Several database vendors provide products that perform data replication, The capabilities of these products and the customer problems they solve vary widely. This talk starts by identifying some of the dimensions of the replication solution space including latency, concurrency, logical and physical units of replication, network link requirements, heterogeneity, replica topology, replica transparency, and data transformation requirements. Digital Equipment Corporation provides three products that allow customers to replicate data. The distributed, two-phase commit products allow customers to program and coordinate replicated updates. DECTM Reliable Transaction Router provides an OLTP environment with transactional data replication. Transactions succeed in the face of site and network failures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data extraction and transformation for the data warehouse

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: case squire
",n
"LEFT id: NA
RIGHT id: 2245

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: challenges for global information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova
",n
"LEFT id: NA
RIGHT id: 431

LEFT text: At Berkeley, we are developing TelegraphCQ [1, 2], a dataflow system for processing continuous queries over data streams. TelegraphCQ is based on a novel, highly-adaptive architecture supporting dynamic query workloads in volatile data streaming environments. In this demonstration we show our current version of TelegraphCQ, which we implemented by leveraging the code base of the open source PostgreSQL database system. Although TelegraphCQ differs significantly from a traditional database system, we found that a significant portion of the PostgreSQL code was easily reusable. We also found the extensibility features of PostgreSQL very useful, particularly its rich data types and the ability to load user-developed functions. Challenges: As discussed in [1], sharing and adaptivity are our main techniques for implementing a continuous query system. Doing this in the codebase of a conventional database posed a number of challenges:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: eddies : continuously adaptive query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ron avnur , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: Multiversion support for XML documents is needed in many critical applications, such as software configuration control, cooperative authoring, web information warehouses, and ”e-permanence” of web documents. In this paper, we introduce efficient and robust techniques for: (i) storing and retrieving; (ii) viewing and exchanging; and (iii) querying multiversion XML documents. We first discuss the limitations of traditional version control methods, such as RCS and SCCS, and then propose novel techniques that overcome their limitations. Initially, we focus on the problem of managing secondary storage efficiently, and introduce an edit-based versioning scheme that enhances RCS with an effective clustering policy based on the concept of page-usefulness. The new scheme drastically improves version retrieval at the expense of a small (linear) space overhead. However, the edit-based approach falls short of achieving objectives (ii) and (iii). Therefore, we introduce and investigate a second scheme, which is reference-based and preserves the structure of the original document. In the reference-based approach, a multiversion document can be represented as yet another XML document, which can be easily exchanged and viewed on the web; furthermore, simple queries are also expressed and supported well under this representation. To achieve objective (i), we extend the page-usefulness clustering technique to the reference-based scheme. After characterizing the asymptotic behavior of the new techniques proposed, the paper presents the results of an experimental study evaluating and comparing their performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 510

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a cost model for query processing in high dimensional data spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christian b &#246; hm
",n
"LEFT id: NA
RIGHT id: 865

LEFT text: We present the information mediator prototype called Kind, recently developed as part of an integrated Neuroscience workbench project at SDSC/UCSD within the NPACI project. The broad goal of the workbench is to serve as an environment where, among other tasks, the Neuroscientist can query a mediator to retrieve information from across a number of information sources, and use the results to perform her own analysis on the data. The Kind mediator is an instance of a novel model-centered mediator architecture that extends current XML-based mediator approaches by incorporating a semantic model of an information source as an integral part of the mediation process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a case-based approach to information integration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: maurizio panti , luca spalazzi , alberto giretti
",n
"LEFT id: NA
RIGHT id: 2097

LEFT text: We present the design of ObjectGlobe, a distributed and open query processor for Internet data sources. Today, data is published on the Internet via Web servers which have, if at all, very localized query processing capabilities. The goal of the ObjectGlobe project is to establish an open marketplace in which data and query processing capabilities can be distributed and used by any kind of Internet application. Furthermore, ObjectGlobe integrates cycle providers (i.e., machines) which carry out query processing operators. The overall picture is to make it possible to execute a query with – in principle – unrelated query operators, cycle providers, and data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: objectglobe : ubiquitous query processing on the internet

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: r. braumandl , m. keidl , a. kemper , d. kossmann , a. kreutz , s. seltzsam , k. stocker
",y
"LEFT id: NA
RIGHT id: 111

LEFT text: Several negative results are proved about the ability to type-check queries in the only existing proposed standard for object-oriented databases. The first of these negative results is that it is not possible to type-check OQL queries in the type system underlying the ODMG object model and its definition language ODL. The second negative result is that OQL queries cannot be type-checked in the type system of the Java binding of the ODMG standard either. A solution proposed in this paper is to extend the ODMG object model with explicit support for parametric polymorphism (universal type quantification). These results show that Java cannot be a viable database programming language unless extended with parametric polymorphism.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: type-checking oql queries in the odmg type systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: suad alag &#237; c
",y
"LEFT id: NA
RIGHT id: 428

LEFT text: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards self-tuning data placement in parallel database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal
",n
"LEFT id: NA
RIGHT id: 104

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 792

LEFT text: Today’s object-relational DBMSs (ORDBMSs) are designed to support novel application domains by providing an extensible architecture, supplemented by domain-specific database extensions supplied by external vendors. An important aspect of ORDBMSs is support for extensible indexing, which allows the core database server to be extended with external access methods (AMs). This paper describes a new approach to extensible indexing implemented in Informix Dynamic Server with Universal Data Option (IDS/UDO). The approach is is based on the generalized search tree, or GiST, which is a template index structure for abstract data types that supports an extensible set of queries. GiST encapsulates core database indexing functionality including search, update, concurrency control and recovery, and thereby relieves the external access method (AM) of the burden of dealing with these issues. The IDS/UDO implementation employs a newly designed GiST API that reduces the number of user defined function calls, which are typically expensive to execute, and at the same time makes GiST a more flexible data structure. Experiments show that GiST-based AM extensibility can offer substantially better performance than built-in AMs when indexing userdefined data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: high-performance extensible indexing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: marcel kornacker
",y
"LEFT id: NA
RIGHT id: 1881

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: semantics for update rule programs and implementation in a relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: louiqa raschid , jorge lobo
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: This paper describes the architecture of OPERA, a generic platform for building distributed systems over stand alone applications. The main contribution of this research effort. is t,o propose a “kernel” system providing the “essentials” for distributed processing and to show the important role database technology may play in supporting such functionality. These include a powerful process management environment. created as a generalization of workflow ideas and incorporating transactional notions such as spheres of isolation, atomicit.y, and persistence and a transactional engine enforcing correctness based on the nested and multi-level models. It also includes a tool-kit providing externalized database functionality enabling physical database design over heterogeneous data repositories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 147

LEFT text: In this paper we present a tool for enhanced exploration of OLAP data that is adaptive to a user’s prior knowledge of the data. The tool continuously keeps track of the parts of the cube that a user has visited. The information in these scattered visited parts of the cube is pieced together to form a model of the user’s expected values in the unvisited parts. The mathematical foundation for this modeling is provided by the classical Maximum Entropy principle. At any time, the user can query for the most surprising unvisited parts of the cube. The most surprising values are dened as those which if known to the user would bring the new expected values closest to the actual values. This process of updating the user’s context based on visited parts and querying for regions to explore further continues in a loop until the user’s mental model perfectly matches the actual cube.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multidimensional database system rasdaman

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. baumann , a. dehmel , p. furtado , r. ritsch , n. widmann
",n
"LEFT id: NA
RIGHT id: 431

LEFT text: Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: eddies : continuously adaptive query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ron avnur , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Active database systems have been a hot research topic for quite some years now. However, while “active functionality” has been claimed for many systems, and notions such as “active objects” or “events” are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of “active database management system” as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 447

LEFT text:  Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: privacy-preserving data mining

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 1009

LEFT text: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mindreader : querying databases through multiple examples

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yoshiharu ishikawa , ravishankar subramanya , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 952

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1772

LEFT text: Phrase matching is a common IR technique to search text and identify relevant documents in a document collection. Phrase matching in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for phrase matching in XML that permits dynamic specification of both the phrase to be matched and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 202

LEFT text: Datacube queries compute aggregates over database relations at a variety of granularities, and they constitute an important class of decision support queries. Real-world data is frequently sparse, and hence efficiently computing datacubes over large sparse relations is important. We show that current techniques for computing datacubes over sparse relations do not scale well with the number of CUBE BY attributes, especially when the relation is much larger than main memory. We propose a novel algorithm for the fast computation of datacubes over sparse relations, and demonstrate the efficiency of our algorithm using synthetic, benchmark and real-world data sets. When the relation fits in memory, our technique performs multiple in-memory sorts, and does not incur any I/O beyond the input of the relation and the output of the datacube itself. When the relation does not fit in memory, a divideand-conquer strategy divides the problem of computing the datacube into several simpler computations of sub-datacubes. Often, all but one of the sub-datacubes can be computed in memory and our in-memory solution applies. In that case, the total I/O overhead is linear in the number of CUBE BY attributes. We demonstrate with an implementation that the CPU cost of our algorithm is dominated by the I/O cost for sparse relations. ‘The research of Kenneth A.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate computation of multidimensional aggregates of sparse data using wavelets

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 2017

LEFT text: Recent work on querying data streams has focused on systems where newly arriving data is processed and continuously streamed to the user in real time. In many emerging applications, however, ad hoc queries and/or intermittent connectivity also require the processing of data that arrives prior to query submission or during a period of disconnection. For such applications, we have developed PSoup, a system that combines the processing of ad hoc and continuous queries by treating data and queries symmetrically, allowing new queries to be applied to old data and new data to be applied to old queries. PSoup also supports intermittent connectivity by separating the computation of query results from the delivery of those results. PSoup builds on adaptive query-processing techniques developed in the Telegraph project at UC Berkeley. In this paper, we describe PSoup and present experiments that demonstrate the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: timber : a native system for querying xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: stelios paparizos , shurug al-khalifa , adriane chapman , h. v. jagadish , laks v. s. lakshmanan , andrew nierman , jignesh m. patel , divesh srivastava , nuwee wiwatwattana , yuqing wu , cong yu
",n
"LEFT id: NA
RIGHT id: 934

LEFT text: Huge masses of digital data about products, customers and competitors have become available for companies in the services sector. In order to exploit its inherent (and often hidden) knowledge for improving business processes the application of data mining technology is the only way for reaching good and ef- cient results, as opposed to purely manual and interactive data exploration. This paper reports the rst steps of a pro ject initiated at Swiss Life for mining its data resources from the life insurance business. Based on the Data Warehouse MASY collecting all relevant data from the OLTP systems for the processing of private life insurance contracts, a Data Mining environment is set up which integrates a palette of tools for automatic data analysis, in particular machine learning approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining insurance data at swiss life

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: j &#246; rg-uwe kietz , ulrich reimer , martin staudt
",y
"LEFT id: NA
RIGHT id: 1353

LEFT text: We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1340

LEFT text: Introduction and Main Contributions Providing mechanisms that allow the user to retrieve desired multimedia information by their semantic content is now an important issue in multimedia databases. However, current prototypes (e.g. Oracle 8i interMedia and Informix Datablade Modules) index mostly only low-level features of multimedia objects. Therefore special techniques are needed for semantic indexing and retrieval of multimedia objects. In this context we present the SMOOTH system, a prototype of a distributed multimedia database system. It implements an integrated querying, annotating, and navigating framework relying on a generic video indexing model. The framework allows the structuring of videos into logical and physical units, and the annotation of these units by typed semantic objects. An index-database stores these structural and semantic information. We provide further a clear concept for capturing and querying the semantic content of multimedia objects, their correlation with low-level objects, as well as their spatio-temporal relationships.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: smooth - a distributed multimedia database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: harald kosch , l &#225; szl &#243; b &#246; sz &#246; rm &#233; nyi , alexander bachlechner , christian hanin , christian hofbauer , margit lang , carmen riedler , roland tusch
",y
"LEFT id: NA
RIGHT id: 379

LEFT text: DISIMA (Distributed Image Database Management System) is a research project under development at the University of Alberta. DISIMA implements a database approach to developing an image database system. Image contents are modeled using objectoriented paradigms while a declarative query language and a corresponding visual query language allow queries over syntactic and semantic features of images. The distributed and interoperable architecture is designed using common facilities as defined in the Object Management Architecture (OMA).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 1084

LEFT text: This chapter describes an efficient method for maintaining materialized views with non-distributive aggregate functions, even in the presence of super aggregates. Incremental view maintenance is an extremely important aspect of the modern database management systems. It enables the fast execution of complex queries without sacrificing the freshness of the data. However, the maintenance of views defined with non-distributive aggregate functions was not sufficiently explored. Incremental refresh has been studied in depth only for a subset of the aggregate functions. Materialized views, or automatic summary tables (ASTs), are increasingly being used to facilitate the analysis of the large amounts of data being collected in relational databases. The use of ASTs can significantly reduce the execution time of a query, often by orders of magnitude, which is particularly significant for databases with sizes in the terabyte to petabyte range, whose queries are designed by business intelligence tools or decision support systems. Such queries tend to be extremely complex, involving a large number of join and grouping operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 2054

LEFT text: We present the design and implementation of the XSQ system for querying streaming XML data using XPath 1.0. Using a clean design based on a hierarchical arrangement of pushdown transducers augmented with buffers, XSQ supports features such as multiple predicates, closures, and aggregation. XSQ not only provides high throughput, but is also memory efficient: It buffers only data that must be buffered by any streaming XPath processor. We also present an empirical study of the performance characteristics of XPath features, as embodied by XSQ and several other systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpath queries on streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: feng peng , sudarshan s. chawathe
",y
"LEFT id: NA
RIGHT id: 589

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1816

LEFT text: Data Mining places specific requirements on DBMS query performance that cannot be evaluated satisfactorily using existing OLAP benchmarks. The DD Benchmark - defined here - provides a practical case and yardstick to explore how well a DBMS is able to support Data Mining applications. It was derived from real-life data mining tasks performed by our Data SurveyorTM tool running on a variety of DBMS backends. We describe initial results obtained using both the Monet system and a relational DBMS product as backend.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the bucky object-relational benchmark

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. carey , david j. dewitt , jeffrey f. naughton , mohammad asgarian , paul brown , johannes e. gehrke , dhaval n. shah
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: The wonderfully clean and beautiful scheme put ""on its head"" the world of query optimization I had assumed was the only one possible. In fact, this paper is all about questioning implicit assumptions behind classic query optimization. Is it always true that query-evaluation performance does not fluctuate during query execution?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1868

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sentinel : an object-oriented dbms with event-based rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. chakravarthy
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: The 5th East European Conference ADBIS'2001 was organized by the Vilnius Gediminas Technical University, Institute of Mathematics and Informatics (Lithuania), Lithuanian Computer Society in cooperation with Moscow ACM SIGMOD Chapter and Law University of Lithuania in Vilnius, Lithuania, September 25-28, 2001. The call for papers attracted 82 submissions from 30 countries. The international program committee, consisting of 47 researchers from 21 countries, selected 25 papers for long presentations and 19 research communications for regular sessions. Additionally, 9 professional communications and reports have been selected for industrial sessions. The authors of accepted papers come from 29 countries, indicating the truly international recognition of the ADBIS conference series. The conference had 127 registered participants from 23 countries and included invited lectures, tutorials, regular sessions, and industrial sessions. This report describes the goals of the conference and summarizes the issues discussed during the sessions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",y
"LEFT id: NA
RIGHT id: 1962

LEFT text: Multimedia technology, global information infrastructures and other developments allow users to access more and more information sources of various types. However, the “technical” availability alone (by means of networks, WWW, mail systems, databases, etc.) is not sufficient for making meaningful and advanced use of all information available on-line. Therefore, the problem of effectively and efficiently accessing and querying heterogeneous and distributed data sources is an important research direction. This paper aims at classifying existing approaches which can be used to query heterogeneous data sources. We consider one of the approaches — the mediated query approach — in more detail and provide a classification framework for it as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 68

LEFT text: A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: density biased sampling : an improved method for data mining and clustering

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christopher r. palmer , christos faloutsos
",y
"LEFT id: NA
RIGHT id: 969

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design , implementation , and performance of the lham log-structured history data access method

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter muth , patrick e. o'neil , achim pick , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 995

LEFT text: Curated databases are databases that are populated and updated with a great deal of human effort. Most reference works that one traditionally found on the reference shelves of libraries -- dictionaries, encyclopedias, gazetteers etc. -- are now curated databases. Since it is now easy to publish databases on the web, there has been an explosion in the number of new curated databases used in scientific research. The value of curated databases lies in the organization and the quality of the data they contain. Like the paper reference works they have replaced, they usually represent the efforts of a dedicated group of people to produce a definitive description of some subject area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bridging heterogeneity : research and practice of database middleware technology

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: fernando de ferreira rezende , g &#252; nter sauter
",n
"LEFT id: NA
RIGHT id: 25

LEFT text: Users of the Web are overloaded with information. This medium is “polluted” with redundant, erroneous and low quality information. A WWW survey of 11,700 users conducted from April 10 to May 10, 1996[1] indicates that 30.31% of the users report “finding known info” is their problem and 27.80% of the users report organizing collected information as their problem. An empirical study[2] on users’ revisitation patterns to WWW pages found that 58% of an individual’s pages are revisits. With these study results, we believe the Web users would like to build and organize a larger collection of bookmarks for future references than they can reasonably maintain now.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: powerbookmarks : a system for personalizable web information organization , sharing , and management

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: wen-syan li , quoc vu , edward chang , divyakant agrawal , kyoji hirata , sougata mukherjea , yi-leh wu , corey bufi , chen-chuan kevin chang , yoshinori hara , reiko ito , yutaka kimura , kezuyuki shimazu , yukiyoshi saito
",y
"LEFT id: NA
RIGHT id: 1371

LEFT text: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing multi-feature queries for image databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ulrich g &#252; ntzer , wolf-tilo balke , werner kie &#223; ling
",n
"LEFT id: NA
RIGHT id: 138

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: multiview access protocols for large-scale replication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: xiangning liu , abdelsalam helal , weimin du
",n
"LEFT id: NA
RIGHT id: 793

LEFT text: We develop a new schema for unstructured data. Traditional schemas resemble the type systems of programming languages. For unstructured data, however, the underlying type may be much less constrained and hence an alternative way of expressing constraints on the data is needed. Here, we propose that both data and schema be represented as edge-labeled graphs. We develop notions of conformance between a graph database and a graph schema and show that there is a natural and efficiently computable ordering on graph schemas. We then examine certain subclasses of schemas and show that schemas are closed under query applications. Finally, we discuss how they may be used in query decomposition and optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating top-k selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 2227

LEFT text: New types of data processing applications are no longer satisfied with the capabilities offered by the relational data model. One example of this phenomenon is the growing use of the Internet as a source of data. The data on the Internet is inherently non-relational. As a result, demand developed for database management systems natively built on advanced data models. The semantic binary data model (Rishe, 1992), satisfies the criteria for the models required for today’s applications by providing the ability to build rich schemas with arbitrarily flexible relationships between objects. In this paper, we discuss a new design for a semantic database management system which is based on the semantic binary data model. Our challenge was to design and implement a database engine which, while being native to the model, is reasonably efficient on a wide variety of industrial applications, and which surpasses relational systems in performance and flexibility on those applications that require non-relational modelling. Special attention is given to multi-platform support by the semantic database engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic integration in heterogeneous databases using neural networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 2153

LEFT text: In this White Paper, produced as a result of discussions at the OOPSLA 2002 Workshop on Agent-Oriented Methodologies, we outline the current state of play of agent-oriented methodologies, how they might be integrated into an underlying, metamodel-based framework, and what the research community needs to do to make their products acceptable to industry. We conclude with an invitation to the community.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: agent-oriented software engineering report on the 4th aose workshop ( aose 2003 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: paolo giorgini
",y
"LEFT id: NA
RIGHT id: 307

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view selection and maintenance using multi-query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hoshi mistry , prasan roy , s. sudarshan , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 67

LEFT text: High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: finding generalized projected clusters in high dimensional spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",y
"LEFT id: NA
RIGHT id: 2

LEFT text: The Carnot research project [CARN, WOEL93] at MCC was initiated in 1990 with the goal of addressing the problem of logically unifying physically-distributed, enterprisewide, heterogeneous information. A prototype has been implemented that provides services for enterprise modeling and model integration to create au enterprise-wide view, semantic expansion of queries on the view to queries on individual resources, and interresource consistency management. Carnot also includes technology for 3D visualization of large information spaecs, knowledge discovery in databases, and software application design recovery. The Camot prototype software has been used by the sponsors of the Carnot project to develop a number of applications. These applications have included worldtow management, heterogeneous database access, knowledge discovery in large databases, and integrated access to both text databases and structured databases from a single initial query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: world wide database-integrating the web , corba and databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: athman bouguettaya , boualem benatallah , lily hendra , james beard , kevin smith , mourad quzzani
",n
"LEFT id: NA
RIGHT id: 219

LEFT text: Spatial joins are one of the most important operations for combining spatial objects of several relations. In this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year's conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidate which is handled by the following two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaining candidates has to be tested against the join predicate. The time required for computing spatial join predicates can essentially be reduced when objects are adequately organized in main memory. In our approach, objects are first decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of spatial join algorithms for processing multiple inputs

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: nikos mamoulis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1520

LEFT text: In order to access information from a variety of heterogeneous information sources, one has to be able to translate queries and data from one data model into another. This functionality is provided by so-called (source) wrappers [4,8] which convert queries into one or more commands/queries understandable by the underlying source and transform the native results into a format understood by the application. As part of the TSIMMIS project [1, 6] we have developed hard-coded wrappers for a variety of sources (e.g., Sybase DBMS, WWW pages, etc.) including legacy systems (Folio). However, anyone who has built a wrapper before can attest that a lot of effort goes into developing and writing such a wrapper. In situations where it is important or desirable to gain access to new sources quickly, this is a major drawback. Furthermore, we have also observed that only a relatively small part of the code deals with the specific access details of the source. The rest of the code is either common among wrappers or implements query and data transformation that could be expressed in a high level, declarative fashion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: information translation , mediation , and mosaic-based browsing in the tsimmis system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joachim hammer , h &#233; ctor garc &#237; a-molina , kelly ireland , yannis papakonstantinou , jeffrey ullman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1965

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: performance tradeoffs for client-server query processing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. franklin , bj &#246; rn th &#243; r j &#243; nsson , donald kossmann
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: This paper investigates the problem of incremental joins of multiple ranked data sets when the join condition is a list of arbitrary user-defined predicates on the input tuples. This problem arises in many important applications dealing with ordered inputs and multiple ranked data sets, and requiring the top solutions. We use multimedia applications as the motivating examples but the problem is equally applicable to traditional database applications involving optimal resource allocation, scheduling, decision making, ranking, etc. We propose an algorithm that enables querying of ordered data sets by imposing arbitrary userdefined join predicates. The basic version of the algorithm does not use any random access but a variation can exploit available indexes for efficient random access based on the join predicates. A special case includes the join scenario considered by Fagin [1] for joins based on identical keys, and in that case, our algorithms perform as efficiently as Fagin’s. Our main contribution, however, is the generalization to join scenarios that were previously unsupported, including cases where random access in the algorithm is not possible due to lack of unique keys. In addition, can support multiple join levels, or nested join hierarchies, which are the norm for modeling multimedia data. We also give -approximation versions of both of the above algorithms. Finally, we give strong optimality results for some of the proposed algorithms, and we study their performance empirically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 738

LEFT text: As object technology is adopted by software systems for analysis and design, language, GUI, and frameworks, the database community also is working to support objects, and to develop standards for that support. A key benefit of object technology is the ability for different objects and object tools to interoperate, so it's critical that such DBMS object standards interoperate with those of the rest of the object world. Starting with a discussion of the new issues objects bring to query standards, we present the efforts of various groups relevant to this, including ODMG, OMG, ANSI X3H2 (SQL3), and recent merger efforts feeding into SQL3. What's different with Objects? 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: standards

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: andrew eisenberg
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 1271

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: one-dimensional and multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 512

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: ROLEX is a research system for closely coupled XML-relational interoperation [2]. Whereas typical XML-based applications interoperate with existing relational databases via a “shred-and-publish” approach, the ROLEX system seeks to provide direct access to relational data via XML interfaces at the speed of cached XML data. To achieve this, ROLEX is integrated tightly with both the DBMS and the application through a standard interface supported by most XML parsers, the Document Object Model (DOM). Thus, in general, an application need not be modified to be used with ROLEX.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1785

LEFT text: Many applications require the management of spatial data. Clustering large spatial databases is an important problem which tries to find the densely populated regions in the feature space to be used in data mining, knowledge discovery, or efficient information retrieval. A good clustering approach should be efficient and detect clusters of arbitrary shape. It must be insensitive to the outliers (noise) and the order of input data. We propose WaveCluster, a novel clustering approach based on wavelet transforms, which satisfies all the above requirements. Using multiresolution property of wavelet transforms, we can effectively identify arbitrary shape clusters at different degrees of accuracy. We also demonstrate that WaveCluster is highly efficient in terms of time complexity. Experimental results on very large data sets are presented which show the efficiency and effectiveness of the proposed approach compared to the other recent clustering methods. This research is supported by Xerox Corporation. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",y
"LEFT id: NA
RIGHT id: 297

LEFT text: In this paper we present a method for automatically segmenting unformatted text records into structured elements. Several useful data sources today are human-generated as continuous text whereas convenient usage requires the data to be organized as structured records. A prime motivation is the warehouse address cleaning problem of transforming dirty addresses stored in large corporate databases as a single text field into subfields like “City” and “Street”. Existing tools rely on hand-tuned, domain-specific rule-based systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automatic segmentation of text into structured records

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: vinayak borkar , kaustubh deshmukh , sunita sarawagi
",y
"LEFT id: NA
RIGHT id: 299

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: independence is good : dependency-based histogram synopses for high-dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amol deshpande , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 1159

LEFT text: EOS employs the multigranularity two version two phase locking protocol, that allows many readers and one writer to access the same item simultaneously. The option to switch to simple 2PL is also available. EOS uses a write-ahead redo-only logging scheme that offers short logs, fast recovery from system failures, and non-blocking checkpoints. Also, configuration files are provided that can be edited by users to customize and tune EOS performance. Finally, the EOS architecture has been designed to be extensible. Users may define hook functions to be executed when certain primitive events occur. This allows controlled access to a number of entry points in the system without compromising modularity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 829

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extendi...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: o-o , what have they done to db2 ?

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael j. carey , donald d. chamberlin , srinivasa narayanan , bennet vance , doug doole , serge rielau , richard swagerman , nelson mendon &#231; a mattos
",y
"LEFT id: NA
RIGHT id: 1829

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",y
"LEFT id: NA
RIGHT id: 336

LEFT text: We introduce a new algorithm to compute the spatial join of two or more spatial data sets, when indexes are not available on them. Size Separation Spatial Join (S<3J<) imposes a hierarchical decomposition of the data space and, in contrast with previous approaches, requires no replication of entities from the input data sets. Thus its execution time depends only on the sizes of the joined data sets. We describe S<3J< and present an analytical evaluation of its I/O and processor requirements comparing them with those of previously proposed algorithms for the same problem. We show that S<3J< has relatively simple cost estimation formulas that can be exploited by a query optimizer. S<3J< can be efficiently implemented using software already present in many relational systems. In addition, we introduce Dynamic Spatial Bitmaps< (DSB), a new technique that enables S<3J< to dynamically or statically exploit bitmap query processing techniques. Finally, we present experimental results for a prototype implementation of S<3J< involving real and synthetic data sets for a variety of data distributions. Our experimental results are consistent with our analytical observations and demonstrate the performance benefits of S<3J< over alternative approaches that have been proposed recently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: multiway spatial joins

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nikos mamoulis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1889

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tiziana catarci , isabel f. cruz
",n
"LEFT id: NA
RIGHT id: 687

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: statistical synopses for graph-structured xml databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: neoklis polyzotis , minos garofalakis
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1669

LEFT text: When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: relaxed transaction processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munindar p. singh , christine tomlinson , darrell woelk
",n
"LEFT id: NA
RIGHT id: 1292

LEFT text: We present new techniques for supervised wrapper generation and automated web information extraction, and a system called Lixto implementing these techniques. Our system can generate wrappers which translate relevant pieces of HTML pages into XML. Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface. In this convenient user-interface very expressive extraction programs can be created. Internally, this functionality is reected by the new logicbased declarative language Elog. Users never have to deal with Elog and even familiarity with HTML is not required. Lixto can be used to create an \XML-Companion"" for an HTML web page with changing content, containing the continually updated XML translation of the relevant information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supervised wrapper generation with lixto

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: robert baumgartner , sergio flesca , georg gottlob
",n
"LEFT id: NA
RIGHT id: 2075

LEFT text: Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments where data is updated, not only information provided by v...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the computation of relational view complements

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jens lechtenb &#246; rger , gottfried vossen
",y
"LEFT id: NA
RIGHT id: 1149

LEFT text: Access control models, such as the ones supported by commercial DBMSs,  are not yet able to fully meet many application needs. An important requirement derives from the temporal dimension that permissions have in many real-world situations. Permissions are often limited in time or may hold only for specific periods of time. In this article, we present an access control model in which periodic temporal intervals are associated with authorizations. An authorization is automatically granted in the specified intervals and revoked when such intervals expire. Deductive temporal rules with periodicity and order constraints are provided to derive new authorizations based on the presence or absence of other authorizations in specific periods of time. We provide a solution to the problem of  ensuring the uniqueness of the global set of valid authorizations derivable at each instant, and we propose an algorithm to compute this set. Moreover, we address issues related to the efficiency of access control by adopting a materialization approach. The resulting model provides a high degree of flexibility and supports the specification of several protection requirements that cannot be expressed in traditional access control models.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting periodic authorizations and temporal reasoning in database access control

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: elisa bertino , claudio bettini , elena ferrari , pierangela samarati
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: We present an optimization method and al gorithm designed for three objectives: physi cal data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and ""backchase"" with constraints (dependen cies). By using dictionaries (finite functions) in physical schemas we can capture with con straints useful access structures such as indexes, materialized views, source capabilities, access support relations, gmaps, etc. The search space for query plans is defined and enumerated in a novel manner: the chase phase rewrites the original query into a ""universal"" plan that integrates all the access structures and alternative pathways that are allowed by appli cable constraints. Then, the backchase phase produces optimal plans by eliminating various combinations of redundancies, again according to constraints. This method is applicable (sound) to a large class of queries, physical access structures, and semantic constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 1392

LEFT text: Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of wavelet-based histograms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: There has been much research on various aspects of Approximate Query Processing (AQP), such as different sampling strategies, error estimation mechanisms, and various types of data synopses. However, many subtle challenges arise when building an actual AQP engine that can be deployed and used by real world applications. These subtleties are often ignored (or at least not elaborated) by the theoretical literature and academic prototypes alike. For the first time to the best of our knowledge, in this article, we focus on these subtle challenges that one must address when designing an AQP system. Our intention for this article is to serve as a handbook listing critical design choices that database practitioners must be aware of when building or using an AQP system, not to prescribe a specific solution to each challenge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",y
"LEFT id: NA
RIGHT id: 1307

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 1041

LEFT text: Our goal is to enhance multidimensional database systems with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. In this paper, we present a key component of our system that characterizes the information content of a cell based on a user's prior familiarity with the cube and provides a context-sensitive exploration of the cube. There are three main modules of this component. A Tracker, that continuously tracks the parts of the cube that a user has visited. A Modeler, that pieces together the information in the visited parts to model the user's expected values in the unvisited parts. An Informer, that processes user's queries about the most informative unvisited parts of the cube. The mathematical basis for the expected value modeling is provided by the classical maximum entropy principle. Accordingly, the expected values are computed so as to agree with every value that is already visited while reducing assumptions about unvisited values to the minimum by maximizing their entropy. The most informative values are defined as those that bring the new expected values closest to the actual values. We believe and prove through experiments that such a user-in-the-loop exploration will enable much faster assimilation of all significant information in the data compared to existing manual explorations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1089

LEFT text: XML - the eXtensible Markup Language has recently emerged as a new standard for data representation and exchange on the Interact. It is believed that it will become a universal format for data exchange on the Web and that in the near future we will find vast amounts of documents in XML format on the Web. As a result, it has become crucial to address the question of how large collections of XML documents can be sorted and retrieved efficiently and effectively.To date, most work on storing, indexing, querying, and searching documents in XML has stemmed from the database community's work on semi-structured data. An alternative approach, that has received less attention to date, is to view XML documents as a collection of text documents with additional tags and relations between these tags. IR techniques have traditionally been applied to search large sets of textual data and should thus be extended to encode the structure and semantics inherent in XML documents. Integrating IR and XML search techniques will enable more sophisticated search on the structure as well as the content of these documents, while leveraging the success of IR techniques in document similarity ranking and keyword search.The SIGIR workshop on XML and information retrieval was held July 28th, in Athens Greece. The goal of the workshop was to bring together researchers and practitioners interested in XML and IR to discuss and define the most relevant topics in the relation between these two technologies, present recent results, and propose future directions for research. The topics for discussion included:&bull; How to extend IR technologies to search XML documents&bull; How to integrate XML structure in IR indexing structures&bull; How to query XML documents both on content and structure&bull; How to introduce the semantics inherent in XML into the search process&bull; How to adopt database indexing techniques in an IR frameworkThe opening session of the workshop consisted of a survey of search engines for XML documents. This was followed by three technical sessions: query languages, retrieval algorithms, and IR systems for XML documents. The final talk of the day, ""Searching Annotated Language Resources in XML"", by Nancy Ide was given from the perspective of potential users of XML search systems and opened many topics for discussion. The workshop was concluded with a panel discussion where the panelists outlined their vision of the future of XML search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information retrieval from an incomplete data cube

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: curtis e. dyreson
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1235

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient schemes for managing multiversionxml documents

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s.-y . chien , v. j. tsotras , c. zaniolo
",n
"LEFT id: NA
RIGHT id: 1353

LEFT text: Businessestoday need to interrelate data stored in diverse systems with differing capabilities, ideally via a single high-level query interface. We present the design of a query optimizer for Garlic [C 95], a middleware system designedto integrate data from a broad range of data sources with very different query capabilities. Garlic’s optimizer extends the rule-based approach of [Loh88] to work in a heterogeneous environment, by defining generic rules for the middleware and using wrapper-provided rules to encapsulate the capabilities of each data source. This approach offers great advantages in terms of plan quality, extensibility to new sources, incremental implementationof rules for new sources, and the ability to express the capabilities of a diverse set of sources. We describe the design and implementationof this optimizer, and illustrate its actions through an example.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: Privacy – the control over one’s personal data – and security – the attempted access to data by unauthorized others – are two critical problems for both e-commerce consumers and sites alike. Without either, consumers will not visit or shop at a site, nor can sites function effectively without considering both. This chapter reviews the current state of the art and the relevance for privacy and security respectively. We examine privacy from social psychological, organizational, technical, regulatory, and economic perspectives. We then examine security from technical, social and organizational, and economic perspectives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",y
"LEFT id: NA
RIGHT id: 245

LEFT text: Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multiview project : object-oriented view technology and applications

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: e. a. rundensteiner , h. a. kuno , y.-g . ra , v. crestana-taube , m. c. jones , p. j. marron
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 1951

LEFT text: We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be ""10% of married people between age 50 and 60 have at least 2 cars"". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a ""greater-than-expected-value"" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining quantitative association rules in large relational tables

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",y
"LEFT id: NA
RIGHT id: 193

LEFT text: Descriptions of new indexing techniques are a common outcome of database research, but these descriptions are sometimes marred by poor methodology and a lack of comparison to other schemes. In this paper we describe a framework for presentation and comparison of indexing schemes that we believe sets a minimum standard for development and dissemination of research results in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: random sampling techniques for space efficient online computation of order statistics of large datasets

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: gurmeet singh manku , sridhar rajagopalan , bruce g. lindsay
",n
"LEFT id: NA
RIGHT id: 685

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: time-parameterized queries in spatio-temporal databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: We propose a framework for integrating data from multiple relational sources into an XML document that both conforms to a given DTD and satisfies predefined XML constraints. The framework is based on a specification language, AIG, that extends a DTD by (1) associating element types with semantic attributes (inherited and synthesized, inspired by the corresponding notions from Attribute Grammars), (2) computing these attributes via parameterized SQL queries over multiple data sources, and (3) incorporating XML keys and inclusion constraints. The novelty of AIG consists in semantic attributes and their dependency relations for controlling context-dependent, DTD-directed construction of XML documents, as well as for checking XML constraints in parallel with document-generation. We also present cost-based optimization techniques for efficiently evaluating AIGs, including algorithms for merging queries and for scheduling queries on multiple data sources. This provides a new grammar-based approach for data integration under both syntactic and semantic constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: Specifically, we use state-of-the-art concepts from morphology, n;,mely the ‘pattern spectrum’ of a shape, to map each shape to a point in n-dimensional space. FollowingThis text is a guide to the foundations of method engineering, a developing field concerned with the definition of techniques for designing software systems. The approach is based on metamodeling, the construction of a model about a collection of other models. The book applies the metamodeling approach in five case studies, each describing a solution to a problem in a specific domain. Suitable for classroom use, the book is also useful as a reference for practitioners. The book first presents the theoretical basis of metamodeling for method engineering, discussing information modeling, the potential of metamodeling for software systems development, and the introduction of the metamodeling tool ConceptBase. , we organize the n-d points in an R-tree. We show that the L, (= max) norm in the n-d space lower-bounds the actual distance. This guarantees no false dismissals for range queries. In addition, we present a nearest neighbor algorithm that also guarantees no false dismissals.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 493

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental maintenance of recursive views using relational calculus/sql

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: guozhu dong , jianwen su
",n
"LEFT id: NA
RIGHT id: 1847

LEFT text: The optimization capabilities of RDBMSs make them attractive for executing data transformations. However, despite the fact that many useful data transformations can be expressed as relational queries, an important class of data transformations that produce several output tuples for a single input tuple cannot be expressed in that way. To overcome this limitation, we propose to extend Relational Algebra with a new operator named data mapper. In this paper, we formalize the data mapper operator and investigate some of its properties. We then propose a set of algebraic rewriting rules that enable the logical optimization of expressions with mappers and prove their correctness. Finally, we experimentally study the proposed optimizations and identify the key factors that influence the optimization gains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: languages for multi-database interoperability

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri
",y
"LEFT id: NA
RIGHT id: 1205

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1699

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a hypertext query language for images

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: li yang
",n
"LEFT id: NA
RIGHT id: 384

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lambda-db : an odmg-based object-oriented dbms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: Microsoft’s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David’s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch’s vision, and, indeed, in Microsoft’s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch’s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital’s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft’s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1293

LEFT text: Predicting query execution time is crucial for many database management tasks including admission control, query scheduling, and progress monitoring. While a number of recent papers have explored this problem, the bulk of the existing work either considers prediction for a single query, or prediction for a static workload of concurrent queries, where by ""static"" we mean that the queries to be run are fixed and known. In this paper, we consider the more general problem of dynamic concurrent workloads. Unlike most previous work on query execution time prediction, our proposed framework is based on analytic modeling rather than machine learning. We first use the optimizer's cost model to estimate the I/O and CPU requirements for each pipeline of each query in isolation, and then use a combination queueing model and buffer pool model that merges the I/O and CPU requests from concurrent queries to predict running times. We compare the proposed approach with a machine-learning based approach that is a variant of previous work. Our experiments show that our analytic-model based approach can lead to competitive and often better prediction accuracy than its machine-learning based counterpart.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic pipeline scheduling for improving interactive query performance

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tolga urhan , michael j. franklin
",y
"LEFT id: NA
RIGHT id: 691

LEFT text: A workflow history manager maintains the information essential for workflow monitoring and data mining as well as for recovery and authorization purposes.Certain characteristics of workflow systems like the necessity to run these systems on heterogeneous, autonomous and distributed environments and the nature of data, prevent history management in workflows to be handled by the classical data management techniques like distributed DBMSs. We further demonstrate that multi-database query processing techniques are also not appropriate for the problem at hand.In this paper, we describe history management, i.e., the structure of the history and querying of the history, in a fully distributed workflow architecture realized in conformance with Object Management Architecture (OMA) of OMG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: workflow management with service quality guarantees

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: michael gillmann , gerhard weikum , wolfgang wonner
",n
"LEFT id: NA
RIGHT id: 190

LEFT text: We map an object model to a commercial relational multi-processor database system using replication and view materialisation to provide fast retrieval. To speed up complex update operations, we exploit intra-transaction parallelism by breaking such an operation down into shorter relational operations which are executed as parallel subtransactions of the update transaction. To ensure the correctness and recoverability of the operation’s execution, we use multi-level transactions. In addition, we minimise the resulting overhead for the logging of the compensating inverse operation required by the multi-level concept by logging the compensation for non-derived data only. In particular, we concentrate on the novel application of multi-level transaction management to efficiently maintain the replicated data and materialised views. We present a prototype implementation and give a performance evaluation of an exemplary set-oriented update statement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on parallel processing of aggregate and scalar functions in object-relational dbms

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael jaedicke , bernhard mitschang
",n
"LEFT id: NA
RIGHT id: 26

LEFT text: QBIC (Query By Image Content) is a prototype software system for image retrieval developed at the IBM Almaden Research Center. It allows a user to query an image collection using features of image content – colors, textures, shapes, locations, and layout of images and image objects. For example, a user can query for images with a green background that contain a round red object in the upper left. The queries are formed graphically a query for red objects can be specified by selecting the color red from a color wheel, a texture query can be specified by selecting from a palette of textures, a query for a shape can be specified by drawing the shape on a” blackboard”, and so on. Retrievals are based on similarity, not exact match, computed from nuHarry Road, San Jose, CA 95120

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the aqua approximate query answering system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: Rapid growth in the volume of documents, their diversity, and terminological variations render federated digital libraries increasingly difficult to manage. Suitable abstraction mechanisms are required to construct meaningful and scalable document clusters, forming a cross-digital library information space for browsing and semantic searching. This paper addresses the above issues, proposes a distributed semantic framework that achieves a logical partitioning of the information space according to topic areas, and provides facilities to contextualize and landscape the available document sets in subject-specific categories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 780

LEFT text: XKeyword provides efficient keyword proximity queries on large XML graph databases. A query is simply a list of keywords and does not require any schema or query language knowledge for its formulation. XKeyword is built on a relational database and, hence, can accommodate very large graphs. Query evaluation is optimized by using the graph's schema. In particular, XKeyword consists of two stages. In the preprocessing stage a set of keyword indices are built along with indexed path relations that describe particular patterns of paths in the graph. In the query processing stage plans are developed that use a near optimal set of path relations to efficiently locate the keyword query results. The results are presented graphically using the novel idea of interactive result graphs, which are populated on-demand according to the user's navigation and allow efficient information discovery. We provide theoretical and experimental points for the selection of the appropriate set of precomputed path relations. We also propose and experimentally evaluate algorithms to minimize the number of queries sent to the database to output the top-K results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 754

LEFT text: This paper describes the design and implementation of PEST0 (Portable Explorer of Snuctured Objects), a user interface that supports browsing and querying of object databases. PEST0 allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (“query-in-place”) that presents querying as a natural extension of browsing. PEST0 is designed to be portable to any object database system that supports a high-level query language; in addition, PEST0 is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text). uniformly and manipulated using an object-oriented dialect of SQL. One component of this project, which is joint work between IBM Almaden and the University of Wisconsin, is the development of a graphical user interface called PEST0 (Portable Explorer of STructured Objects). We refer to the PEST0 interface as a query/browser, as it marries navigational object browsing’ with declarative querying; it integrates browsing and querying via a “query-in-place” paradigm that provides a powerful yet natural user interface for exploring the contents of object databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: investigating xquery for querying across database object types

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nancy wiegand
",n
"LEFT id: NA
RIGHT id: 2022

LEFT text: Derived data is maintained in a database system to correlate and summarize base data which records real world facts. As base data changes, derived data needs to be recomputed. This is often implemented by writing active rules that are triggered by changes to base data. In a system with rapidly changing base data, a database with a standard rule system may consume most of its resources running rules to recompute data. This paper presents the rule system implemented as part of the STandard Real-time Information Processor (STRIP). The STRIP rule system is an extension of SQL3-type rules that allows groups of rule actions to be batched together to reduce the total recomputation load on the system. In this paper we describe the syntax and semantics of the STRIP rule system, present an example set of rules to maintain stock index and theoretical option prices in a program trading application, and report the results of experiments performed on the running system. The experiments verify that STRIP's rules allow much more efficient derived data maintenance than conventional rules without batching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lockx : a system for efficiently querying secure xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sungran cho , sihem amer-yahia , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 464

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql and management of external data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jim melton , jan-eike michels , vanja josifovski , krishna kulkarni , peter schwarz , kathy zeidenstein
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 932

LEFT text: In this paper, we define and examine a particular class of queries called group queries. Group queries are natural queries in many decisionsupport applications. The main characteristic of a group query is that it can be executed in a groupby-group fashion. In other words, the underlying relation(s) can be partitioned (based on some set of attributes) into disjoint groups, and each group can be processed separately. We give a syntactic criterion to identify these queries and prove its sufficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: groupwise processing of relational queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",y
"LEFT id: NA
RIGHT id: 1534

LEFT text: In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX has to be updated every time an object is updated. This has previously been shown to be a potential bottleneck, and in this paper, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 402

LEFT text: The learning-enhanced relevance feedback has been one of the most active research areas in content-based image retrieval in recent years. However, few methods using the relevance feedback are currently available to process relatively complex queries on large image databases. In the case of complex image queries, the feature space and the distance function of the user's perception are usually different from those of the system. This difference leads to the representation of a query with multiple clusters (i.e., regions) in the feature space. Therefore, it is necessary to handle disjunctive queries in the feature space.In this paper, we propose a new content-based image retrieval method using adaptive classification and cluster-merging to find multiple clusters of a complex image query. When the measures of a retrieval method are invariant under linear transformations, the method can achieve the same retrieval quality regardless of the shapes of clusters of a query. Our method achieves the same high retrieval quality regardless of the shapes of clusters of a query since it uses such measures. Extensive experiments show that the result of our method converges to the user's true information need fast, and the retrieval quality of our method is about 22% in recall and 20% in precision better than that of the query expansion approach, and about 34% in recall and about 33% in precision better than that of the query point movement approach, in MARS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spire : a progressive content-based spatial image retrieval engine

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: chung-sheng li , lawrence d. bergman , yuan-chi chang , vittorio castelli , john r. smith
",n
"LEFT id: NA
RIGHT id: 2177

LEFT text: Ultrahigh storage densities of up to 1 Tb/in2. or more can be achieved by using local-probe techniques to write, read back, and erase data in very thin polymer films. The thermomechanical scanning-probe-based data-storage concept, internally dubbed ""millipede"", combines ultrahigh density, small form factor, and high data rates. High data rates are achieved by parallel operation of large 2D arrays with thousands micro/nanomechanical cantilevers/tips that can be batch-fabricated by silicon surface-micromachining techniques. The inherent parallelism, the ultrahigh areal densities and the small form factor may open up new perspectives and opportunities for application in areas beyond those envisaged today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a requirement-based approach to data modeling and re-engineering

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alice h. muntz , christian t. ramiller
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 1378

LEFT text: This demonstration illustrates how a comprehensive database reconciliation tool can provide the ability to characterize data-quality and data-reconciliation issues in complex real-world applications. Telcordia’s data reconciliation and data quality analysis tool includes rapid generation of appropriate pre-processing and matching rules applied to a training set created from samples of the data. Once tuned, the appropriate rules can be applied efficiently to the complete data sets. The tool uses a modular JavaBeans-based architecture that allows for customized matching functions and iterative runs that build upon previously learned information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: telcordia 's database reconciliation and data quality analysis tool

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: francesco caruso , munir cochinwala , uma ganapathy , gail lalk , paolo missier
",y
"LEFT id: NA
RIGHT id: 1602

LEFT text: In July of this year, the American and International committees responsible for the SQL standard finalized the specification for new binding style called the Call Level Interface (SQL/CLI)[2]. This new binding style is an addendum to the existing SQL Standard [1], and

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql/cli-a new binding style for sql

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: murali venkatrao , michael pizzo
",y
"LEFT id: NA
RIGHT id: 510

LEFT text: In this paper we introduce generalized projections (G P an extension of duplicateeliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinctand duplicate-preserving projections in a common unified framework. Using G P s we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a cost model for query processing in high dimensional data spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christian b &#246; hm
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 82

LEFT text: The SIT-IN (acronym for Integrated Territorial Information System, in Italian) system integrates a historical database, providing information about the temporal evolution of territorial administrative partitions; the Institute's GIS, providing the cartography of the Italian territory down to the census tract level of detail; a statistical data warehouse, providing spatiotemporal data from a number of di erent surveys; and nally an address normalizing/geo-matching system, providing information about the limits of census tracts (e.g. portions of streets or the sides of town squares).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: the sift information dissemination system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tak w. yan , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 2248

LEFT text: This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sence that it changes the replication scheme of the object i.e., the set of processors at which the object inreplicated) as changes occur in the read-write patern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of ta distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorith.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: are quorums an alternative for data replication ?

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ricardo jim &#233; nez-peris , m. pati &#241; o-mart &#237; nez , gustavo alonso , bettina kemme
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: As teachers, if we believe content knowledge matters the most for successful instruction, we may not understand that getting to know students and discovering their strengths as learners are equally important. Teachers are instructional islands with a lot of content to share but perhaps unconnected to the learners that make up the classroom. If as teachers, we describe students by saying, “she is a math wizard,” “she is a science ace,” or “he is a sponge for historical facts,” we can communicate a lot about students with minimal language. These metaphors help us make comparisons that evoke multiple layers of meaning, and yet thinking metaphorically is also an aspect of everyday life. Cognitive scientists Lakoff and Johnson (2008) argued that our conceptualizations of the world around us are metaphorical and provided examples of metaphors such as “time is money” and suggested that the way we construe argument is conceived in metaphors of war when we “attack a position,” for example, to support a philosophical claim. From an educational philosophy perspective, Greene contended learning is a landscape (1973) and teachers are philosophers working to help learners resist the forces that limit and oppress them (1988) to attain freedom to think for themselves. These theorists recognized the epistemological power of metaphor and challenged us to see its educational potential. Comparisons through metaphoric thinking afford different perspectives and open imaginative possibilities, challenging us to see familiar relationships in new ways. Metaphors can push us to think about teacher education differently as well and move beyond familiar views of clinical experiences, teacher interns, teacher preparation programs, and who we are as educators to see these concepts more complexly. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1517

LEFT text: This paper proposes the use of repetitive broadcast as a way of augmenting the memory hierarchy of clients in an asymmetric communication environment. We describe a new technique called ""Broadcast Disks"" for structuring the broadcast in a way that provides improved performance for non-uniformly accessed data. The Broadcast Disk superimposes multiple disks spinning at different speeds on a single broadcast channel--in effect creating an arbitrarily fine-grained memory hierarchy. In addition to proposing and defining the mechanism, a main result of this work is that exploiting the potential of the broadcast structure requires a re-evaluation of basic cache management policies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: broadcast disks : data management for asymmetric communication environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: swarup acharya , rafael alonso , michael franklin , stanley zdonik
",y
"LEFT id: NA
RIGHT id: 1896

LEFT text: Over the last few years, there have been at least two dramatic changes in the way computers are used. The first has its origin in the fact that computers have become more and more connected to each other. The second was triggered by the increasing miniaturization and affordability of hardware components and power supplies, together with the development of wireless communication paths. These two trends combined have allowed the development of powerful, yet comparatively low-priced, portable computers. In spite of these changes, little attention has been given to reaching a common consensus and to the development of a strong infrastructure in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 2062

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic xml documents with distribution and replication

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: serge abiteboul , angela bonifati , gr &#233; gory cob &#233; na , ioana manolescu , tova milo
",y
"LEFT id: NA
RIGHT id: 1376

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating the ub-tree into a database system kernel

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: frank ramsak , volker markl , robert fenk , martin zirkel , klaus elhardt , rudolf bayer
",n
"LEFT id: NA
RIGHT id: 1326

LEFT text: We consider the problem of evaluating a large number of XPath expressions on an XML stream. Our main contribution consists in showing that Deterministic Finite Automata (DFA) can be used effectively for this problem: in our experiments we achieve a throughput of about 5.4MB/s, independent of the number of XPath expressions (up to 1,000,000 in our tests). The major problem we face is that of the size of the DFA. Since the number of states grows exponentially with the number of XPath expressions, it was previously believed that DFAs cannot be used to process large sets of expressions. We make a theoretical analysis of the number of states in the DFA resulting from XPath expressions, and consider both the case when it is constructed eagerly, and when it is constructed lazily. Our analysis indicates that, when the automaton is constructed lazily, and under certain assumptions about the structure of the input XML data, the number of states in the lazy DFA is manageable. We also validate experimentally our findings, on both synthetic and real XML data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query engines for web-accessible xml data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonidas fegaras , ramez elmasri
",n
"LEFT id: NA
RIGHT id: 1647

LEFT text: Our GC uses a new synchronization mechanism (mechanism that allows the GC to operate concurrently with ordinary users of the database), called CC-consistent cuts. A GC-consistent cut is a set of virtual copies of database pages. The copies are taken at times such that an object may appear as garbage in the cut only if it is garbage in the system. Our GC examines the copies, instead of the real database, in order to determine which objects are garbage. More sophisticated GCs can execute concurrently with the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partition selection policies in object database garbage collection

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jonathan e. cook , alexander l. wolf , benjamin g. zorn
",n
"LEFT id: NA
RIGHT id: 1225

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 1631

LEFT text: This paper describes the design and implementation of an OODBMS, namely the METU Object-Oriented DBMS (MOOD). MOOD [Dog 94b] is developed on the Exodus Storage Manager (ESM) [ESM 92] and therefore some of the kernel functions like storage management, concurrency control, backup and recovery of data were readily available through ESM. In addition ESM has a client-server architecture and each MOOD process is a client application in ESM. The kernel functions provided by MOOD are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management. SQL statements are interpreted whereas functions (which have been previously compiled with C++) within SQL statements are dynamically linked and executed. A query optimizer is implemented by using the Volcano Query Optimizer Generator. A graphical user interface, namely Mood-View [Arp 93a, Arp 93b], is developed using Motif. MoodView displays both the schema information and the query results graphically. Additionally it is possible to update the database schema and to traverse the references in query results graphically.The system is coded in GNU C++ on Sun Sparc 2 workstations. MOOD has a SQL-like object-oriented query language, namely MOODSQL [Ozk 93b, Dog 94c]. MOOD type system is derived from C++, thus eliminating the impedance mismatch between MOOD and C++. The users can also access the MOOD Kernel from their application programs written in C++. For this purpose MOOD Kernel defines a class named UserRequest that contains a method for the execution of MOODSQL statements. The MOOD source code is available both for anonymous ftp users from ftp.cs.wisc.edu and for the WWW users from the site http://www.srdc.metu.edu.tr along with its related documents.In MOOD, each object is given a unique Object Identifier (OID) at object creation time by the ESM which is the disk start address of the object returned by the ESM. Object encapsulation is considered in two parts, method encapsulation and attribute encapsulation. These encapsulation properties are similar to the public and private declarations of C++.Methods can be defined in C++ by users to manipulate user defined classes and after compilation, they are dynamically linked and executed during the interpretation of SQL statements. This late binding facility is essential since database environments enforce run-time modification of schema and objects. With our approach, the interpretation of functions are avoided thus increasing the efficiency of the system. Dynamic linking primitives are implemented by the use of the shared object facility of SunOS [Sun 90]. Overloading is realized by making use of the signature concept of C++.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: odmg-93 : a standard for object-oriented dbmss

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. g. g. cattell
",n
"LEFT id: NA
RIGHT id: 463

LEFT text: Expert database systems extend the functionality of conventional database systems by providing a facility for creating and automatically executing Condition-Action rules. While Condition-Action rules in database systems are very powerful, they also can be very difficult to program, due to the unstructured and unpredictable nature of rule processing. We provide methods for static analysis of Condition-Action rules; our methods determine whether a given rule set is guaranteed to terminate, and whether rule execution is confluent (has a guaranteed unique final state). Our methods are based on previous methods for analyzing rules in active database systems. We improve considerably on the previous methods by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not. Our improved analysis is based on a ``propagation'''' algorithm, which uses a formal approach based on an extended relational algebra to accurately determine when the action of one rule can affect the condition of another. Our algebraic approach yields methods that are applicable to a broad class of expert database rule languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an algebraic approach to static analysis of active database rules

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: elena baralis , jennifer widom
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: Electronic commerce systems (retail, auction, etc.) are good examples of data-based systems that operate under correctness and resilience requirements of a transactional nature but go beyond conventional databases, as they are formed by the aggregation of heterogeneous, autonomous components. We introduce a framework to specify, analyze and reason about the behavior of such systems, focusing on how they are designed to make consistent progress in spite of failures. The contributions are: (a) the introduction of the Guarantee abstraction to deal with transactional applications; (b) a framework based on guarantees and protocols to specify the behaviors of systems and their components and reason about the properties of systems and their components; and (c) application of the framework to a common e-commerce scenario. The framework allows the hierarchical composition of transactional systems and their properties, as well as the proofs of these properties: we specify a system's behavior at its most abstract level, and proceed to decompose the specification mirroring the structure of the system's components, considering the role of guarantee-preserving component systems and recovery in each case. In particular we show how the lower-level properties are supported by the component systems, which we also characterize within the same framework.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",y
"LEFT id: NA
RIGHT id: 1519

LEFT text: This study presents a modified B2B CRM using the Genetic algorithm and Data Mining Techniques to improve decision making. The model classifies consumers into consumers of Repeat and Shop-and-Go. Modified data mining C5.0 and the Genetic algorithm was employed to optimize rules generated by the decision tree algorithm. The findings showed that the proposed model allocates resources effectively to the most profitable customers’ decisions. The output metrics are machine time, calibration graph, and ROC curve. In comparison with the conventional C5.0, k-NN, and Support Vector Machine, the proposed model has greater accuracy of 89.3 percent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an effective hash-based algorithm for mining association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jong soo park , ming-syan chen , philip s. yu
",n
"LEFT id: NA
RIGHT id: 237

LEFT text: In this paper we propose techniques that solve the problem by performing a single query for the whole input segment. As a result the cost, depending on the query and dataset characteristics, may drop by orders of magnitude. In addition, we propose analytical models for the expected size of the output, as well as, the cost of query processing, and extend out techniques to several variations of the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: enhanced nearest neighbour search on the r-tree

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: king lum cheung , ada wai-chee fu
",n
"LEFT id: NA
RIGHT id: 2081

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qc-trees : an efficient summary structure for semantic olap

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , jian pei , yan zhao
",n
"LEFT id: NA
RIGHT id: 1661

LEFT text: In this article, we present an extended relational algebra with universally or existentially quantified classes as attribute values. The proposed extension can greatly enhance the expressive power of relational systems, and significantly reduce the size of a database, at small additional computational cost. We also show how the proposed extensions can be built on top of a standard relational database system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: the incinerate data model

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: h. v. jagadish
",y
"LEFT id: NA
RIGHT id: 240

LEFT text: Previous studies on mining association rules find rules at single concept level, however, mining association rules at multiple concept levels may lead to the discovery of more specific and concrete knowledge from data. In this study, a top-down progressive deepening method is developed for mining multiplelevel association rules from large transaction databases by extension of some existing association rule mining techniques. A group of variant algorithms are proposed based on the ways of sharing intermediate results, with the relative performance tested on different kinds of data. Relaxation of the rule conditions for finding “level-crossing” association rules is also discussed in the paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbminer : interactive mining of multiple-level knowledge in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jaiwei han , youngjian fu , wei wang , jenny chiang , osmar r. za &#239; ane , krzysztof koperski
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",y
"LEFT id: NA
RIGHT id: 1985

LEFT text: Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices.Our first contribution is a practical scheme that models magic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM's DB2 C/S V2 database system. Our performance measurements demonstrate that the cost-based magic optimization technique performs well, and that without it, several poor decisions could be made.Our second contribution is a formal algebraic model of magic sets rewriting, based on an extension of the multiset relational algebra, which cleanly defines the search space and can be used in a rule-based optimizer. We introduce the multiset &theta;-semijoin operator, and derive equivalence rules involving this operator. We demonstrate that magic sets rewriting for non-recursive SQL queries can be modeled as a sequential composition of these equivalence rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rule languages and internal algebras for rule-based optimizers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mitch cherniack , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: The fourth International Conference on Flexible Query Answering Systems (FQAS'2000) was held at the Academy of Sciences in Warsaw, Poland on October, 25-27, 2000. This series of conferences was launched in 1994 by Troels Andreasen, Henning Christiansen and Henrik Larsen from Roskilde University in Denmark, who have been the main driving force behind this series ever since. The previous FQAS events were held in Denmark in 1994, 1996, mad 1998. The next conference in this series will return to Denmark in 2002.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1131

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage estimation for multidimensional aggregates in the presence of hierarchies

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton , karthikeyan ramasamy
",n
"LEFT id: NA
RIGHT id: 1429

LEFT text: According to Michael Stonebraker’s experiences, if you select a group of well-known experts in database research and ask them to identify the domains where meeting use+ requirements is of primary importance and at the ‘same time more significant advances are needed and more research should be promoted, user interfaces comes out as number one in the list. This has been the case over the last decade. Despite such a strong incentive, research on user interfaces seems to remain marginal within the database community. Part of this community considers that this is a domain for development, not for research. Often researchers feel that the specification of an user interface is not much more than assembling widgets in some order. Significantly, the VLDB 94 program offers no contribution on user interfaces. This panel will try to investigate the reasons for such a gap between discourse and practice, and look for remedies. It will start by setting the scene:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: user interfaces ; who cares ?

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: stefano spaccapietra
",y
"LEFT id: NA
RIGHT id: 1259

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic heterogeneity resolution in federated databases by metadata implantation and stepwise evolution

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: goksel aslan , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 763

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database tuning : principles , experiments , and troubleshooting techniques ( part i )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dennis shasha , philippe bonnet
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 512

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 568

LEFT text: Benchmarks belong to the very standard repertory of tools deployed in database development. Assessing the capabilities of a system, analyzing actual and potential bottlenecks, and, naturally, comparing the pros and cons of different systems architectures have become indispensable tasks as databases management systems grow in complexity and capacity. In the course of the development of XML databases the need for a benchmark framework has become more and more evident: a great many different ways to store XML data have been suggested in the past, each with its genuine advantages, disadvantages and consequences that propagate through the layers of a complex database system and need to be carefully considered. The different storage schemes render the query characteristics of the data variably different. However, no conclusive methodology for assessing these differences is available to date.In this paper, we outline desiderata for a benchmark for XML databases drawing from our own experience of developing an XML repository, involvement in the definition of the standard query language, and experience with standard benchmarks for relational databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: why and how to benchmark xml databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: albrecht schmidt , florian waas , martin kersten , daniela florescu , michael j. carey , ioana manolescu , ralph busse
",y
"LEFT id: NA
RIGHT id: 2117

LEFT text: Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: issues in data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lukasz golab , m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 1230

LEFT text: The amount of scientific and technical information is growing exponentially. As a result, the scientific community has been overwhelmed by the information published in number of new books, journal articles, and conference proceedings. In addition to increasing number of publications, advances in information technology have dramatically reduced the barriers in electronic publishing and distribution of information over networks virtually anywhere in the world. As a result, the scientific community is facing the problem of locating relevant or interesting information. To address the problem of information overload and to sift all available information sources for useful information, recommender systems or filtering systems have emerged. Generally, recommender systems are used online to suggest items that users find interesting, thereby, benefiting both the user and merchant. Recommender systems benefit the user by making him suggestions on items that he is likely to purchase and the business by increase of sales. Filtering information or generation of recommendatio ns by the recommender systems mimic the process of information retrieval systems by incorporating advanced profile building techniques, item/user representation techniques, filtering and recommendation techniques, and profile adaptation techniques. This paper addresses the application domain analysis, functional classification, advantages and disadvantages of various filtering and recommender systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: functional-join processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: r. braumandl , j. claussen , a. kemper , d. kossmann
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: • "" … make a computer so imbedded, so fitting, so natural, that we use it without even thinking about it. "" • "" Ubiquitous (pervasive) computing is roughly the opposite of virtual reality. Where virtual reality puts people inside a computer-generated world, ubiquitous computing forces the computer to live out here in the world with people. "" – Mark Weiser, Xerox PARC

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 1474

LEFT text: We describe a scheme to fragment and distribute centralized databases. ’ The problem is motivated by trends towards down-sizing and reorganization, reflecting actual, often distributed responsibilities within companies. A major practical requirement is that existing application code must be left unchanged. We present SQL extensions to specify ownership and data replication information declaratively. From this, a compiler generates triggers and view definitions that implement the distributed scheme, on top of a collection of local databases. Our strategy has been applied successfully at Telenor the Norwegian telephone comp.any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: providing database migration tools - a practicioner 's approach

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: andreas meier
",n
"LEFT id: NA
RIGHT id: 944

LEFT text: A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a generic approach to bulk loading multidimensional index structures

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger , peter widmayer
",n
"LEFT id: NA
RIGHT id: 1661

LEFT text: Publisher Summary This chapter explores the basic functionality of the volume data model. It considers examples from biology and volumes of measurement from quantum physics. The data model is similar in the two cases, but the operations that are commonly used are quite different. In the second case, for instance, one is often interested in conditions on the behavior of local differential operators, such as zero-flow surfaces, while in the biological case one has a mix of value conditions and geometric condition. With these two application fields, this chapter highlights the generality and flexibility of the model. The system is based on a commercial database, augmented with specialized functions to manipulate the volume data model. The chapter demonstrates various operations specifying queries both using a graphical user interface and entering them directly in structured query language (SQL) augmented with the volume algebra operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: the incinerate data model

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: h. v. jagadish
",n
"LEFT id: NA
RIGHT id: 1276

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: unql : a query language and algebra for semistructured data based on structural recursion

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter buneman , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 2197

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xxl - a library approach to supporting efficient implementations of advanced database queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jochen van den bercken , bj &#246; rn blohsfeld , jens-peter dittrich , j &#252; rgen kr &#228; mer , tobias sch &#228; fer , martin schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1722

LEFT text: We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is “find the libraries that are within 10 miles of schools”. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call “pair-count exponent” (PC). We show that this law also holds for self-spatial-joins (“find schools within 5 miles of other schools”) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy). In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial joins using seeded trees

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules. This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides SQL-like operator, named MINE RULE, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 535

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1414

LEFT text: DataMine is a statistical database mining system with strong emphasis on interactiveness and nice graphical representation of information produced. It also supports an offline mode of discovery, and provides an extensive API which allows users to write ""mining applications"" just as easily as routine database applications, The central idea is to perform discovery with a ""human in the loop"" guiding the system using his initial hypothesis and the feedback from the system. Users can pose a rule-query against a rulebase and the system can generate all rules matching their query. The rulebase could either be pregenerated (using offline mode) or could be realized in real-time as the discovery progresses.Rules generated by the system are of the form:Body -&gt; Consequentwhere Body is a conjunction of the elementary predicates of the form (A=a), where A is an attribute and a is a value from the attribute domain of A. Consequent is a single elementary predicate. Each rule can have several parameters like support, confidence, atypicality, color etc. (the definitions have been left out) which can also be used by the user in framing the rule query.For continuous attributes, the system also allows the user some control in deciding how they are discretized. It also allows for the creation of extra attributes at run time which can then be used in queries like the rest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: potter 's wheel : an interactive data cleaning system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: This paper presents an approach that preserves the semi-atomicity (a weaker form of atomicity) of flexible transactions, allowing local sites to autonomously maintain serializability and recoverability. We offer a fundamental characterization of the flexible transaction model and precisely define the semi-atomicity. We investigate the commit dependencies among the subtransactions of a flexible transaction. These dependencies are used to control the commitment order of the subtransactions. We next identify those restrictions that must be placed upon a flexible transaction to ensure the maintenance of its semi-atomicity. As atomicity is a restrictive criterion, semi-atomicity enhances the class of executable global transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1359

LEFT text: Replication is a key mechanism to achieve scalability and fault-tolerance in databases. Its importance has recently been further increased because of the role it plays in achieving elasticity at the database layer. In database replication, the biggest challenge lies in the trade-off between performance and consistency. A decade ago, performance could only be achieved through lazy replication at the expense of transactional guarantees. The strong consistency of eager approaches came with a high cost in terms of reduced performance and limited scalability. Postgres-R combined results from distributed systems and databases to develop a replication solution that provided both scalability and strong consistency. The use of group communication primitives with strong ordering and delivery guarantees together with optimized transaction handling (tailored locking, transferring logs instead of re-executing updates, keeping the message overhead per transaction constant) were a drastic departure from the state-of-the-art at the time. Ten years later, these techniques are widely used in a variety of contexts but particularly in cloud computing scenarios. In this paper we review the original motivation for Postgres-R and discuss how the ideas behind the design have evolved over the years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: do n't be lazy , be consistent : postgres-r , a new way to implement database replication

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina kemme , gustavo alonso
",y
"LEFT id: NA
RIGHT id: 1542

LEFT text: Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: Expectations for change are ubiquitous in education. Teachers who complete education programs and enter classrooms are encouraged to embrace change as new professionals. Instructional change is expected as a result of professional development when teachers examine their instructional practices and reflect on student learning. Two scholars whose work is foundational in teacher education, Lortie (2002) and Britzman (2003) challenged researchers and educators to consider how teachers and teacher education changes and resists change. Lortie (2002), in his sociological study of the profession, speculated on changes and resistance in teaching and argued that the two would “interact contrapuntally” (p. 219). In other words, these two forces exist in tension so that sustained change would often be thwarted. In a similar vein, Britzman’s (2003) critical analysis of learning to teach examined discourses and discursive structures of teacher preparation and noted contradictions and struggles in the lived experiences of teacher candidates. Near the conclusion of her analysis, she wrote, we should “attend to the possible and acknowledge the uncertainty of our educational lives” (p. 241). Possibilities and uncertainties continue to inform the work of educating teacher candidates and supporting inservice teachers as agents of innovation in schools. To be sure, schools and teaching have changed in terms of student demographics, accountability, standardization, and testing since these two studies were published. And yet, the articles in this issue suggest that exploring sources, motivation, and possible outcomes of change, as well as the nature of resistance, remain paramount. In this issue, one pedagogical innovations article and four empirical studies articles explore change and consider how resistance informs teacher education. In an argument for including data literacy in teacher preparation, Reeves describes six hours of classroom-based training with four cohorts of preservice elementary teachers that he claims will help them more deeply understand how data literacy can inform pedagogy. Reeves’ study is grounded in current research indicating that there is an increased expectation for teachers to use data to make instructional decisions. He contends that teacher preparation programs should include instruction about data literacy. This change could help preservice teachers enter classrooms with detailed knowledge about how data can productively inform their instruction. Reeves recognizes that such a change will require thoughtful consideration about when such instruction should be part of curricula; that these shifts will demand new resources; and, resistance to adding another facet to teacher education programs is possible. Reinhardt investigates how six cooperating teachers understand their roles within contexts of clinical experiences using Vygotskian theory. Her qualitative study focused on four female and two male teacher mentors in a diverse school district and examined how the cooperating teachers understand their role in a clinical experience as contrasted with the reality of teachers’ everyday work. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1583

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at university of hagen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes
",n
"LEFT id: NA
RIGHT id: 1247

LEFT text: Building a data-intensive web site is a complex task. Ad hoc rapid prototyping approaches easily lead to unsatisfactory results, e.g. poor maintainability and extensibility. To address this problem, a number of model-based approaches have been proposed, which attempt to simplify the design and development of data-intensive web sites. However, these approaches typically lack expressive meta-models and, as a result, suffer from a number of limitations, e.g. the lack of appropriate support for the creation of complex user interfaces, for the specification of layouts and presentation styles, and for customization. In this paper we describe a new software tool OntoWeaver, which uses ontologies to drive the design and development of data-intensive web sites. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: building and customizing data-intensive web sites using weave

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",y
"LEFT id: NA
RIGHT id: 1407

LEFT text: When querying a temporal database, a user often makes certain semantic assumptions on stored temporal data. This paper formalizes and studies two types of semantic assumptions: point-based and interval-based. The point-based assumptions include those assumptions that use interpolation methods, while the interval-based assumptions include those that involve different temporal types (time granularities). Each assumption is viewed as a way to derive certain implicit data from the explicit data stored in the database. The database system must use all explicit as well as (possibly infinite) implicit data to answer user queries. This paper introduces a new method to facilitate such query evaluations. A user query is translated into a system query such that the answer of this system query over the explicit data is the same as that of the user query over the explicit and the implicit data. The paper gives such a translation procedure and studies the properties (safety in particular) of user queries and system queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: transaction timestamping in ( temporal ) databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian s. jensen , david b. lomet
",n
"LEFT id: NA
RIGHT id: 883

LEFT text: The number, size, and user population of bibliographic and full-text document databases are rapidly growing. With a high document arrival rate, it becomes essential for users of such databases to have access to the very latest documents; yet the high document arrival rate also makes it difficult for users to keep themselves updated. It is desirable to allow users to submit profiles, i.e., queries that are constantly evaluated, so that they will be automatically informed of new additions that may be of interest. Such service is traditionally called Selective Dissemination of Information (SDI).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents for selective dissemination of information

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mehmet altinel , michael j. franklin
",n
"LEFT id: NA
RIGHT id: 238

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on knowledge representation meets databases ( krdb ' 98 )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alex borgida , vinay k. chaudhri , martin staudt
",n
"LEFT id: NA
RIGHT id: 582

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: versioning and configuration management in an object-oriented data model

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1876

LEFT text: In a 'shared-nothing' parallel computer, each processor has its own memory and disks and processors communicate by passing messages through an interconnect. Many academic researchers, and some vendors, assert that shared-nothingness is the 'consensus' architecture for parallel DBMSs. This alleged consensus is used as a justification for simulation models, algorithms, research prototypes and even marketing campaigns. We argue that shared-nothingness is no longer the consensus hardware architecture and that hardware resource sharing is a poor basis for categorising parallel DBMS software architectures if one wishes to compare the performance characteristics of parallel DBMS products.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: much ado about shared-nothing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael g. norman , thomas zurek , peter thanisch
",y
"LEFT id: NA
RIGHT id: 759

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 444

LEFT text: We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for mining outliers from large data sets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2123

LEFT text: To benefit from mature database technology RDF stores are built on top of relational databases and SPARQL queries are mapped into SQL. Using a shared-nothing computer cluster is a way to achieve scalability by carrying out query processing on top of large RDF datasets in a distributed fashion. Aiming to this the current paper elaborates on the impact of relational schema design when queries are mapped into Apache Spark SQL. A single triple table, a set of tables resulting from partitioning by predicate, a single wide table covering all properties, and a set of tables based on the application model specification called domain-dependent-schema, are the considered designs. For each of the mentioned approaches, the rows of the corresponding tables are stored in the distributed file system HDFS using the columnar-store Parquet. Experiments using standard benchmarks demonstrate that the single wide property table approach, despite its simplicity, is superior to other approaches. Further experiments demonstrate that this single table approach continues to be attractive even when repartitioning by key (RDF subject) is applied before executing queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xpath processing in a nutshell

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: georg gottlob , christoph koch , reinhard pichler
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 817

LEFT text: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: probabilistic optimization of top n queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: donko donjerkovic , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1866

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 768

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining distance-based outliers in large datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",y
"LEFT id: NA
RIGHT id: 176

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: java and relational databases ( tutorial ) : sqlj

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gray clossman , phil shaw , mark hapner , johannes klein , richard pledereder , brian becker
",n
"LEFT id: NA
RIGHT id: 1347

LEFT text: In this paper, we propose two update propagation strategies that improve freshness. Both of them use immediate propagation: updates to a primary copy are propagated towards a slave node as soon as they are detected at the master node without waiting for the commitment of the update transaction. Our performance study shows that our strategies can improve data freshness by up to five times compared with the deferred approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: update propagation strategies for improving the quality of data on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alexandros labrinidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: Due to organizational or operational constraints, the diverse data sources that an enterprise uses do not generally lend themselves to being fully replicated or completely consolidated under a single database, hence the increased demand for data interchange and for federated access to distributed sources. IBM has ongoing work in information integration technology that enables integrated, real-time access to traditional and emerging data sources, transforms information to meet the needs of business analysts, and manages data placement for performance, currency, and availability leading to fast, constant, and easy access for customer e-business solutions. IBM's Information Integration infrastructure today supports SQL—a mature, powerful query language—plus a number of SQL extensions in support of XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 183

LEFT text: The distinctions among the protocols in terms of performance are significant. For example, an offered load where 70% - 80% of transactions under the global locking protocol were aborted, only 10% of transactions were aborted under the protocols based on the replication graph. The results of the study suggest that protocols based on a replication graph offer practical techniques for replica management. However, it also shows that performance deteriorates rapidly and dramatically when transaction throughput reaches a saturation point.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: replication , consistency , and practicality : are these mutually exclusive ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: todd anderson , yuri breitbart , henry f. korth , avishai wool
",y
"LEFT id: NA
RIGHT id: 1784

LEFT text: Tree pattern is at the core of XML queries. The tree patterns in XML queries typically contain redundancies, especially when broad integrity constraints (ICs) are present and considered. Apparently, tree pattern minimization has great significance for efficient XML query processing. Although various minimization schemes/algorithms have been proposed, none of them can exploit broad ICs for thoroughly minimizing the tree patterns in XML queries. The purpose of this research is to develop an innovative minimization scheme and provide a novel implementation algorithm.Design/methodology/approach – Query augmentation/expansion was taken as a necessary first‐step by most prior approaches to acquire XML query pattern minimization under the presence of certain ICs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficiently mining long patterns from databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roberto j. bayardo , jr.
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 819

LEFT text: Data management in a networked world presents us with some of the same challenges that we’ve seen in the past, but emphasizes our ability to deal with scale, in that there are several orders of magnitude more database users, and database sizes are rising more quickly than Moore’s law. We have considerably less control over the structure of the data than in the past and must efficiently operate over poorly or weakly specified schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: networked data management design points

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: james r. hamilton
",y
"LEFT id: NA
RIGHT id: 1921

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 1954

LEFT text: This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql query optimization : reordering for a general class of queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 548

LEFT text: We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: continuous queries over data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shivnath babu , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 185

LEFT text: A software architecture is presented that allows client application programs to interact with a DBMS server in a flexible and powerful way, using either direct, volatile messages, or messages sent via recoverable queues. Normal requests from clients to the server and replies from the server to clients can be transmitted using direct or recoverable messages. In addition, an application event notification mechanism is provided, whereby client applications running anywhere on the network can register for events, and when those events are raised, the clients are notified. A novel parameter passing mechanism allows a set of tuples to be included in an event notification. The event mechanism is particularly useful in an active DBMS, where events can be raised by triggers to signal running application programs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient transparent application recovery in client-server information systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 757

LEFT text: Recent demands for querying big data have revealed various shortcomings of traditional database systems. This, in turn, has led to the emergency of a new kind of query mode, approximate query.Online aggregation is a sample-based technology for approximate querying. It becomes quite indispensable in the era of information explosion today. Online aggregation continuously gives an approximate result with some error estimation (usually confidence interval) until all data are processed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: software as a service : asp and asp aggregation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: christoph bussler
",n
"LEFT id: NA
RIGHT id: 1861

LEFT text: A special-purpose extension of the Entity-Relationship model for the needs of conceptual modeling of geographic applications, called the Geo-ER Model, is presented. Handling properties associated to objects not because of the objects' nature but because of the objects' position, calls for dealing -at the semantic modeling level-with space, location and dimensionality of objects, spatial relationships, space-depending attributes, and scale and generalization of representations. In order to accomplish this in the framework of ER and its derivatives, we introduce special entity sets, relationships, and add new constructs. The rationale as well as examples of usage of the Geo-ER model from actual projects are presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an extended entity-relationship model for geographic applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thanasis hadzilacos , nectaria tryfona
",y
"LEFT id: NA
RIGHT id: 567

LEFT text: XML has established itself over-the recent years as THE standard for representing data in scientific and business applications. Starting out as a standard data exchange format for the Web, it has become instrumental in the development of electronic commerce applications and online information services, and draws in its tailwind a multitude of standardization efforts for all kinds of applications. Documents are not only used for representing multimedia information content but also for many other purposes, like the representation of meta-information and the specification of component interfaces, protocols, and processes. As a consequence, the amount of XML data being stored and processed is large and will be increasing at an astonishing rate. This has caused XML data management to become a focus of research efforts in the database conmmnity. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advanced xml data processing : guest editor 's introduction

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",y
"LEFT id: NA
RIGHT id: 2173

LEFT text: In this paper, we define and examine a particular class of queries called group queries. Group queries are natural queries in many decisionsupport applications. The main characteristic of a group query is that it can be executed in a groupby-group fashion. In other words, the underlying relation(s) can be partitioned (based on some set of attributes) into disjoint groups, and each group can be processed separately. We give a syntactic criterion to identify these queries and prove its sufficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: preference formulas in relational queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jan chomicki
",n
"LEFT id: NA
RIGHT id: 1455

LEFT text: In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",y
"LEFT id: NA
RIGHT id: 240

LEFT text: DISCOVER operates on relational databases and facilitates information discovery on them by allowing its user to issue keyword queries without any knowledge of the database schema or of SQL. DISCOVER returns qualified joining networks of tuples, that is, sets of tuples that are associated because they join on their primary and foreign keys and collectively contain all the keywords of the query. DISCOVER proceeds in two steps. First the Candidate Network Generator generates all candidate networks of relations, that is, join expressions that generate the joining networks of tuples. Then the Plan Generator builds plans for the efficient evaluation of the set of candidate networks, exploiting the opportunities to reuse common subexpressions of the candidate networks.    We prove that DISCOVER finds without redundancy all relevant candidate networks, whose size can be data bound, by exploiting the structure of the schema. We prove that the selection of the optimal execution plan (way to reuse common subexpressions) is NP-complete. We provide a greedy algorithm and we show that it provides near-optimal plan execution time cost. Our experimentation also provides hints on tuning the greedy algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbminer : interactive mining of multiple-level knowledge in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jaiwei han , youngjian fu , wei wang , jenny chiang , osmar r. za &#239; ane , krzysztof koperski
",n
"LEFT id: NA
RIGHT id: 1959

LEFT text: The Teradata Multimedia Object Manager is a general-purpose content analysis multimedia server designed for symmetric multiprocessing and massively parallel processing environments. The Multimedia Object Manager defines and manipulates user-defined functions (UDFs), which are invoked in parallel to analyze or manipulate the contents of multimedia objects. Several computationally intensive applications of this technology, which use large persistent datasets, include fingerprint matching, signature verification, face recognition, and speech recognition/translation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fault-tolerant architectures for continuous media servers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: banu &#214; zden , rajeev rastogi , prashant shenoy , avi silberschatz
",n
"LEFT id: NA
RIGHT id: 113

LEFT text: Database management is one of the main areas of research of the School of Computer Science at The University of Oklahoma (OU). The objective of the database research team at OU (OUDB) is to help solve the many issues and challenges facing the database research community, especially with respect to emerging technology. Currently, many projects are being conducted in the following areas: real-time databases, object-oriented databases, mobile databases, multimedia databases, data mining and data warehouses. These projects have been funded by federal and state agencies as well as private industries such as National Science Foundation, the U.S. Department of Education, Oklahoma State Department of Environmental Quality, and Objectivity, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the asilomar report on database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phil bernstein , michael brodie , stefano ceri , david dewitt , mike franklin , hector garcia-molina , jim gray , jerry held , joe hellerstein , h. v. jagadish , michael lesk , dave maier , jeff naughton , hamid pirahesh , mike stonebraker , jeff ullman
",n
"LEFT id: NA
RIGHT id: 1399

LEFT text: Building a data-intensive web site is a complex task. Ad hoc rapid prototyping approaches easily lead to unsatisfactory results, e.g. poor maintainability and extensibility. To address this problem, a number of model-based approaches have been proposed, which attempt to simplify the design and development of data-intensive web sites. However, these approaches typically lack expressive meta-models and, as a result, suffer from a number of limitations, e.g. the lack of appropriate support for the creation of complex user interfaces, for the specification of layouts and presentation styles, and for customization. In this paper we describe a new software tool OntoWeaver, which uses ontologies to drive the design and development of data-intensive web sites. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching strategies for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1210

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a configurable type hierarchy index for oodb

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas a. mueck , martin l. polaschek
",y
"LEFT id: NA
RIGHT id: 1855

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization at the crossroads

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 243

LEFT text: QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lore : a lightweight object repository for semistructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dallan quass , jennifer widom , roy goldman , kevin haas , qingshan luo , jason mchugh , svetlozar nestorov , anand rajaraman , hugo rivero , serge abiteboul , jeff ullman , janet wiener
",n
"LEFT id: NA
RIGHT id: 2117

LEFT text: One of the fundamental aspects of information and database systems is that they change. Moreover, in so doing they evolve, although the manner and quality of this evolution is highly dependent on the mechanisms in place to handle it. While changes in data are handled well, changes in other aspects, such as structure, rules, constraints, the model, etc., are handled to varying levels of sophistication and completeness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: issues in data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lukasz golab , m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: A matrix comprising a water-insoluble beta -1,3-glucan gel in the shape of beads with diameters within the range of about 5 to 1000 mu is prepared by, for example, dispersing an alkaline aqueous solution of a water-soluble beta -1,3-glucan in a water-immiscible organic solvent, and adding an organic acid to the resultant dispersion. The matrix is useful as carrier materials for immobilized enzymes, affinity chromatography, gel filtration, ion exchange and other applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 1462

LEFT text: We consider the view data lineageproblem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 984

LEFT text: We consider the problem of mapping data in peer-to-peer data-sharing systems. Such systems often rely on the use of mapping tables listing pairs of corresponding values to search for data residing in different peers. In this paper, we address semantic and algorithmic issues related to the use of mapping tables. We begin by arguing why mapping tables are appropriate for data mapping in a peer-to-peer environment. We discuss alternative semantics for these tables and we present a language that allows the user to specify mapping tables under different semantics. Then, we show that by treating mapping tables as constraints (called mapping constraints) on the exchange of information between peers it is possible to reason about them. We motivate why reasoning capabilities are needed to manage mapping tables and show the importance of inferring new mapping tables from existing ones. We study the complexity of this problem and we propose an efficient algorithm for its solution. Finally, we present an implementation along with experimental results that show that mapping tables may be managed efficiently in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the discovery of interesting patterns in association rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , sameer mahajan , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 1518

LEFT text: Implementing crash recovery in an Object-Oriented Database System (OODBMS) raises several challenging issues for performance that are not present in traditional DBMSs. These performance concerns result both from significant architectural differences between OODBMSs and traditional database systems and differences in OODBMS's target applications. This paper compares the performance of several alternative approaches to implementing crash recovery in an OODBMS based on a client-server architecture. The four basic recovery techniques examined in the paper are termed page differencing, sub-page differencing, whole-page logging, and redo-at-server. All of the recovery techniques were implemented in the context of QuickStore, a memory-mapped store built using the EXODUS Storage Manager, and their performance is compared using the OO7 database benchmark. The results of the performance study show that the techniques based on differencing generally provide superior performance to whole-page logging.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementing crash recovery in quickstore : a performance study

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",y
"LEFT id: NA
RIGHT id: 1787

LEFT text: There are two key motivations for this work. First, the implementation of object-oriented databases has grown to a significant number. Second, there has been a need for integrated access of information from multiple data sources. The multidatabase system has been proposed as a solution for integrated access of data from multiple distributed, heterogeneous, and autonomous database systems. To present a single database illusion to its users, a multidatabase system maintains a single global database schema, which is the integration of all component database schemas and against which its users will issue queries and updates. Many approaches to schema integration have been proposed in the literature. Most of the previous approaches are concerned with relational databases. In this paper, we propose an approach to the integration of database schemas between object-oriented databases in a multidatabase system environment. The underlying principle of our approach is to facilitate the automation of the schema integration process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 7

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the jungle database search engine

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael b &#246; hlen , linas bukauskas , curtis dyreson
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 2114

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: global transaction support for workflow management systems : from formal specification to practical implementation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: paul grefen , jochem vonk , peter apers
",n
"LEFT id: NA
RIGHT id: 1768

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: materialized views and data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1687

LEFT text: This paper attempts a comprehensive study of deadlock detection in distributed database systems. First, the two predominant deadlock models in these systems and the four different distributed deadlock detection approaches are discussed. Afterwards, a new deadlock detection algorithm is presented. The algorithm is based on dynamically creating deadlock detection agents (DDAs), each being responsible for detecting deadlocks in one connected component of the global wait-for-graph (WFG). The DDA scheme is a “self-tuning” system: after an initial warm-up phase, dedicated DDAs will be formed for “centers of locality”, i.e., parts of the system where many conflicts occur. A dynamic shift in locality of the distributed system will be responded to by automatically creating new DDAs while the obsolete ones terminate. In this paper, we also compare the most competitive representative of each class of algorithms suitable for distributed database systems based on a simulation model, and point out their relative strengths and weaknesses. The extensive experiments we carried out indicate that our newly proposed deadlock detection algorithm outperforms the other algorithms in the vast majority of configurations and workloads and, in contrast to all other algorithms, is very robust with respect to differing load and access profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performance evaluation of a new distributed deadlock detection algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chim-fu yeung , sheung-lun hung , kam-yiu lam
",n
"LEFT id: NA
RIGHT id: 1890

LEFT text: In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: spotfire : an information exploration environment

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: christopher ahlberg
",y
"LEFT id: NA
RIGHT id: 704

LEFT text: Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hd-eye : visual clustering of high dimensional data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim , markus wawryniuk
",y
"LEFT id: NA
RIGHT id: 4

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1896

LEFT text: EFIS 2000 was held at Dublin City University in June 2000. The principal aim of this third workshop was to bring together new insights from academic research with industry-driven developments and perspectives in the area of federated information systems. This report describes the observations of the workshop together with the outcome and future research possibilities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 870

LEFT text: User-defined Aggregates (UDAs) provide a versatile mechanism for extending the power and applicability of Object-Relational Databases (O-R DBs). In this paper, we describe the AXL system that supports an SQLbased language for introducing new UDAs. AXL is easy to learn and use for database programmers because it preserves the constructs, programming paradigm and data types of SQL (whereas there is an ‘impedance mismatch’ between SQL and the procedural languages of user-defined functions currently used in O-R DBs). AXL will also inherit the benefits of database query languages, such as scalability, data independence and parallelizability. In this paper, we show that, while adding only minimal extensions to SQL, AXL is very powerful and capable of expressing complex algorithms efficiently. We demonstrate this by coding data mining functions and other advanced applications that, previously, had been a major problem for SQL databases. Due to its flexibility, SQL-compatibility and ease of use, the AXL approach offers a better extensibility mechanism, in several application domains, than the function libraries now offered by commercial O-R DBs under names such as Datablades or DB-Extenders. † Los Angeles CA 90095, USA, hxwang|zaniolo@cs.ucla.edu 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using sql to build new aggregates and extenders for object - relational systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: haixun wang , carlo zaniolo
",y
"LEFT id: NA
RIGHT id: 448

LEFT text: Information Dissemination applications are gaining increasing popularity due to dramatic improvements in communications bandwidth and ubiquity. The sheer volume of data available necessitates the use of selective approaches to dissemination in order to avoid overwhelming users with unnecessaryinformation. Existing mechanisms for selective dissemination typically rely on simple keyword matching or “bag of words” information retrieval techniques. The advent of XML as a standard for information exchangeand the development of query languages for XML data enables the development of more sophisticated filtering mechanisms that take structure information into account. We have developed several index organizations and search algorithms for performing efficient filtering of XML documents for large-scale information dissemination systems. In this paper we describe these techniques and examine their performance across a range of document, workload, and scale scenarios.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: using quantitative information for efficient association rule generation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: b. p &#244; ssas , m. carvalho , r. resende , w. meita , jr.
",n
"LEFT id: NA
RIGHT id: 499

LEFT text: This paper studies the Candy model, a marked point process introduced by Stoica et al. (2000). We prove Ruelle and local stability, investigate its Markov properties, and discuss how the model may be sampled. Finally, we consider estimation of the model parameters and present some examples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop on performance and architecture of web servers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: krishna kant , prasant mohapatra
",n
"LEFT id: NA
RIGHT id: 1201

LEFT text: Whereas serializability captures database consistency requirements and transaction correctness properties via a single notion, recent research has attempted to come up with correctness criteria that view these two types of requirements independently. The search for more flexible correctness criteria is partily motivated by the introduction of new transaction models that extend the traditional atomic transaction model. These extensions came about because the atomic transaction model in conjunction with serializability is found to be very constraining when used in advanced applications (e.g., design databases) that function in distributed, cooperative, and heterogeneous environments. In this article we develop a taxonomy of various correctness criteria that focus on database consistency requirements and transaction correctness properties from the viewpoint of what the different dimensions of these two are. This taxonomy allows us to categorize correctness criteria that have been proposed in the literature. To help in this categorization, we have applied a uniform specification technique, based on ACTA, to express the various criteria. Such a categorization helps shed light on the similarities and differences between different criteria and places them in perspective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a taxonomy of correctness criteria in database applications

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , panos k. chrysanthis
",y
"LEFT id: NA
RIGHT id: 2029

LEFT text: Query optimization is a computationally intensive process, especially for the complex queries that are typical in current data warehousing and mining applications. The inherent overheads of query optimization are compounded by the fact that a new query is typically optimized afresh, providing no opportunity to amortize these overheads over prior optimizations. While current commercial query optimizers do provide facilities for reusing execution plans generated for earlier queries (e.g. ""stored outlines"" in Oracle 9i), the query matching is extremely restrictive-only if the incoming query has a close textual resemblance with one of the stored queries is the associated plan re-used to execute the new query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: plastic : reducing query optimization overheads through plan recycling

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vibhuti s. sengar , jayant r. haritsa
",y
"LEFT id: NA
RIGHT id: 1488

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: versant replication : supporting fault-tolerant object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: yuh-ming shyy , h. stephen au-yeung , c. p. chou
",n
"LEFT id: NA
RIGHT id: 1641

LEFT text: Archiving is important for scientific data, where it is necessary to record all past versions of a database in order to verify findings based upon a specific version. Much scientific data is held in a hierachical format and has a key structure that provides a canonical identification for each element of the hierarchy. In this article, we exploit these properties to develop an archiving technique that is both efficient in its use of space and preserves the continuity of elements through versions of the database, something that is not provided by traditional minimum-edit-distance diff approaches. The approach also uses timestamps. All versions of the data are merged into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. By identifying the semantic continuity of elements and merging them into one data structure, our technique is capable of providing meaningful change descriptions, the archive allows us to easily answer certain temporal queries such as retrieval of any specific version from the archive and finding the history of an element.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as an efficient deductive database engine

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 17

LEFT text: The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an xjml-based wrapper generator for web information extraction

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ling liu , wei han , david buttler , calton pu , wei tang
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",y
"LEFT id: NA
RIGHT id: 2019

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1599

LEFT text: A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: nearest neighbor queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: nick roussopoulos , stephen kelley , fr &#233; d &#233; ric vincent
",y
"LEFT id: NA
RIGHT id: 1239

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 1023

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",y
"LEFT id: NA
RIGHT id: 2239

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 2047

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting structured data from web pages

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arvind arasu , hector garcia-molina , stanford university
",n
"LEFT id: NA
RIGHT id: 578

LEFT text: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the coral deductive system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raghu ramakrishnan , divesh srivastava , s. sudarshan , praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 1863

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quasi-cubes : exploiting approximations in multidimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , mark sullivan
",n
"LEFT id: NA
RIGHT id: 1676

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as a deductive database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 1704

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database research group at eth zurich

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: moira c. norrie , stephen m. blott , hans-j &#246; rg schek , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: Active databases and real-time databases have been important areas of research in the recent past. It has been recognized that many benefits can be gained by integrating active and real-time database technologies. However, there has not been much work done in the area of transaction processing in active real-time databases. This paper deals with an important aspect of transaction processing in active real-time databases, namely the problem of assigning priorities to transactions. In these systems, time-constrained transactions trigger other transactions during their execution. We present three policies for assigning priorities to parent, immediate and deferred transactions executing on a multiprocessor system and then evaluate the policies through simulation. The policies use different amounts of semantic information about transactions to assign the priorities. The simulator has been validated against the results of earlier published studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 445

LEFT text: Abstract. Our aim is to develop new database technologies for the approximate matching of unstructured string data using indexes. We explore the potential of the suffix tree data structure in this context. We present a new method of building suffix trees, allowing us to build trees in excess of RAM size, which has hitherto not been possible. We show that this method performs in practice as well as the O(n) method of Ukkonen [70]. Using this method we build indexes for 200 Mb of protein and 300 Mbp of DNA, whose disk-image exceeds the available RAM. We show experimentally that suffix trees can be effectively used in approximate string matching with biological data. For a range of query lengths and error bounds the suffix tree reduces the size of the unoptimised O(mn) dynamic programming calculation required in the evaluation of string similarity, and the gain from indexing increases with index size. In the indexes we built this reduction is significant, and less than 0.3% of the expected matrix is evaluated. We detail the requirements for further database and algorithmic research to support efficient use of large suffix indexes in biological applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and cost-effective techniques for browsing and indexing large video databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghwan oh , kien a. hua
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: We present an approach to database interoperation that exploits the semantic information provided by integrity constraints defined on the component databases. We identify two roles of integrity constraints in database interoperation. First, a set of integrity constraints describing valid states of the integrated view can be derived from the constraints defined on the underlying databases. Moreover, local integrity constraints can be used as a semantic check on the validity of the specification of the integrated view. We illustrate our ideas in the context of an instance-based database interoperation paradigm, where objects rather than classes are the unit of integration. We introduce the notions of objectivity and subjectivity as an indication of whether a constraint is valid beyond the context of a specific database, and demonstrate the impact of these notions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 1126

LEFT text: One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are “imported” into the mediator. These semantic ids can then be used to specify how various objects are combined or merged into objects “exported” by the mediator. In this paper we also discuss the implementation of a mediation system based on these principles. In particular, we present key optimization techniques that significantly reduce the processing costs associated with information fusion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: object fusion in mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , serge abiteboul , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 109

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: specification and implementation of exceptions in workflow management systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi
",n
"LEFT id: NA
RIGHT id: 1565

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. dogac , c. dengi , e. kilic , g. ozhan , f. ozcan , s. nural , c. evrendilek , u. halici , b. arpinar , p. koksal , n. kesim , s. mancuhan
",y
"LEFT id: NA
RIGHT id: 407

LEFT text: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: image mining in iris : integrated retinal information system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wynne hsu , mong li lee , kheng guan goh
",n
"LEFT id: NA
RIGHT id: 176

LEFT text: Object-relational database systems, a.k.a. “universal servers,” are emerging as the next major generation of commercial database system technology. Products from relational DBMS vendors including IBM, Informix, Oracle, UniSQL, and others, include object-relational features today, and all of the major vendors appear to be on course to delivering full object-relational support in their products over the next few years. In addition, the SQL3 standard is rapidly solidifying in this area. The goal of this tutorial is to explain what the key features are of object-relational database systems, review what today's products provide, and then look ahead to where these systems are heading. The presentation will be aimed at general SIGMOD audience, and should therefore be appropriate for users, practitioners, and/or researchers who want to learn about object-relational database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: java and relational databases ( tutorial ) : sqlj

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gray clossman , phil shaw , mark hapner , johannes klein , richard pledereder , brian becker
",n
"LEFT id: NA
RIGHT id: 512

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1441

LEFT text: During the past few years our research efforts have been inspired by two different needs. On one hand, the number of non-expert users accessing databases is growing apace. On the other, information systems will no longer be characterized by a single centralized architecture, but rather by several heterogeneous component systems. In order to address such needs we have designed a new query system with both user-oriented and multidatabase features. The system's main components are an adaptive visual interface, providing the user with different and interchangeable interaction modalities, and a “translation layer”, which creates and offers to the user the illusion of a single homogeneous schema out of several heterogeneous components. Both components are founded on a common ground, i.e. a formally defined and semantically rich data model, the Graph Model, and a minimal set of Graphical Primitives, in terms of which general query operations may be visually expressed. The Graph Model has a visual syntax, so that graphical operations can be applied on its components without unnecessary mappings, and an object-based semantics. The aim of this paper is twofold. We first present an overall view of the system architecture and then give a comprehensive description of the lower part of the system itself. In particular, we show how schemata expressed in different data models can be translated in terms of Graph Model, possibly by exploiting reverse engineering techniques. Moreover, we show how mappings can be established between well-known query languages and the Graphical Primitives. Finally, we describe in detail how queries expressed by using the Graphical Primitives can be translated in terms of relational expressions so to be processed by actual DBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a scalable architecture for autonomous heterogeneous database interactions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: steven milliner , athman bouguettaya , mike p. papazoglou
",n
"LEFT id: NA
RIGHT id: 1794

LEFT text: Despite decades of research on AQP (approximate query processing), our understanding of sample-based joins has remained limited and, to some extent, even superficial. The common belief in the community is that joining random samples is futile. This belief is largely based on an early result showing that the join of two uniform samples is not an independent sample of the original join, and that it leads to quadratically fewer output tuples. Unfortunately, this early result has little applicability to the key questions practitioners face. For example, the success metric is often the final approximation's accuracy, rather than output cardinality. Moreover, there are many non-uniform sampling strategies that one can employ. Is sampling for joins still futile in all of these settings? If not, what is the best sampling strategy in each case? To the best of our knowledge, there is no formal study answering these questions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: new sampling-based summary statistics for improving approximate query answers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias
",n
"LEFT id: NA
RIGHT id: 499

LEFT text: Personalization, advertising, and the sheer volume of online data generate a staggering amount of dynamic web content. In addition to web caching, View Materialization has been shown to accelerate the generation of dynamic web content. View materialization is an attractive solution as it decouples the serving of access requests from the handling of updates. In the context of the Web, selecting which views to materialize must be decided online and needs to consider both performance and data freshness, which we refer to as the Online View Selection problem. In this paper, we define data freshness metrics, provide an adaptive algorithm for the online view selection problem, and present experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop on performance and architecture of web servers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: krishna kant , prasant mohapatra
",n
"LEFT id: NA
RIGHT id: 287

LEFT text: Database systems offer efficient and reliable technology to query structured data. However, because of the explosion of the World Wide Web [11], an increasing amount of information is stored in repositories organized according to less rigid structures, usually as hypertextual documents, and data access is based on browsing and information retrieval techniques. Since browsing and search engines present important limitations [8], several query languages [19, 20, 23] for the Web have been recently proposed. These approaches are mainly based on a loose notion of structure, and tend to see the Web as a huge collection of unstructured objects, organized as a graph. Clearly, traditional database techniques are of little use in this field, and new techniques need to be developed. In this paper, we present the approach to the management of Web data as attacked in the ArtANEUS project carried out by the database group at Universith di l=toma Tre. Our approach is based on a generalization of the notion of view to the Web framework. In fact, in traditional databases, views represent an essential tool for restructuring and integrating da ta to be presented to the user. Since the Web is becoming a major computing platform and a uniform interface for sharing data, we believe that also in this field a sophisticate view mechanism is needed, with novel features due to the semi-structured nature of the Web. First, in this context, restructuring and presenting da ta under different perspectives requires the generation of derived Web hypertexts, in order to re-organize and re-use portions of the Web. To do this, da ta from existing Web sites must be extracted, and then queried and integrated in order to build new hypertexts, i.e., hypertextual views over the original sites; these manipulations can be better attained in a more structured framework, in which traditional database technology can be leveraged to analyze and correlate information. Therefore, there seem to be different view levels in this framework: (i) at the first level, da ta are extracted from the sites of interest and given a database structure, which represents a first structured view over the original semi-structured data; (ii) then, further database views can be built by means of reorganizations and integrations based on traditional database techniques; (iii) finally, a derived hypertext can be generated offering an alternative or integrated hypertextual view over the original sites. In the process, data go from a loosely structured organizat ion-the Web pages-to a very structured onethe database--and then again to Web structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vqbd : exploring semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , thomas baby , jihwang yoo
",n
"LEFT id: NA
RIGHT id: 857

LEFT text: Bioinformatics, the discipline concerned with biological information management is essential in the post-genome era, where the complexity of data processing allows for contemporaneous multi level research including that at the genome level, transcriptome level, proteome level, the metabolome level, and the integration of these -omic studies towards gaining an understanding of biology at the systems level. This research is also having a major impact on disease research and drug discovery, particularly through pharmacogenomics studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database platform for bioinformatics

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sandeepan banerjee
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda — broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 2262

LEFT text: Support vector machines (SVMs) have shown superb performance for text classification tasks. They are accurate, robust, and quick to apply to test instances. Their only potential drawback is their training time and memory requirement. For n training instances held in memory, the best-known SVM implementations take time proportional to na, where a is typically between 1.8 and 2.1. SVMs have been trained on data sets with several thousand instances, but Web directories today contain millions of instances that are valuable for mapping billions of Web pages into Yahoo!-like directories. We present SIMPL, a nearly linear-time classification algorithm that mimics the strengths of SVMs while avoiding the training bottleneck.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: fast and accurate text classification via multiple linear discriminant projections

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: soumen chakrabarti , shourya roy , mahesh v. soundalgekar
",n
"LEFT id: NA
RIGHT id: 2214

LEFT text: Our system architecture to manage sensor data is described. Our data mining applications require past history of the sensor data. Therefore, unlike most present systems that focus on streaming data, and cache a small window of historic data, we store the entire historic data. Several interesting problems arise in these scenarios. We study two of them: (a) Given that a sensor can send data corresponding to its current configuration at any particular instant, how do we define the data that should be stored in the database? (b) Sensors try to minimize the amount of data transmitted. Also there could be data loss in the network. So the data stored will have lots of ""holes"". In this case, how can an application make sense of the stored data? In this paper, we describe our approach to solve these problems that enables an application to recreate the environment that generated the data as precisely as possible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: understanding the semantics of sensor data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: murali mani
",y
"LEFT id: NA
RIGHT id: 1688

LEFT text: Many requests for proposals have been issued since the last issue of this column appeared six months ago. We first briefly touch upon some recent developments along the policy/legislation front concerning NSF, ARPA, and HPCC. We then recap the recent requests for proposals from ARPA, NSF, Air Force, NASA, and Army.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: jumping on the nii bandwagon

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: xiaolei qian
",y
"LEFT id: NA
RIGHT id: 848

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 246

LEFT text: This paper describes GnatDb, which is an embedded database system that provides protection against both accidental and malicious corruption of data. GnatDb is designed to run on a wide range of appliances, some of which have very limited resources. Therefore, its design is heavily driven by the need to reduce resource consumption. GnatDb employs atomic and durable updates to protect the data against accidental corruption. It prevents malicious corruption of the data using standard cryptographic techniques that leverage the underlying log-structured storage model. We show that the total memory consumption of GnatDb, which includes the code footprint, the stack and the heap, does not exceed 11 KB, while its performance on a typical appliance platform remains at an acceptable level.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac , ugur halici , ebru kilic , gokhan ozhan , fatma ozcan , sena nural , cevdet dengi , sema mancuhan , budak arpinar , pinar koksal , cem evrendilek
",n
"LEFT id: NA
RIGHT id: 655

LEFT text: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 18th british national conference on databases ( bncod )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: carole goble , brian read
",n
"LEFT id: NA
RIGHT id: 1911

LEFT text: This paper covers what we at NCR have learned about the TPC-D benchmark as we executed and published our first set of volume points for the Teradata Database. Areas where customers should read the Full Disclosure Report carefully are pointed out as well as the weaknesses in the benchmark relative to real customer applications. The key execution and optimization elements of the Teradata Database and the 5100 WorldMark platform that contribute to our published results are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: tpc-d-the challenges , issues and results

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramesh bhashyam
",n
"LEFT id: NA
RIGHT id: 402

LEFT text: In this demo, we will show the implementation of a content-based SPatial Image Retrieval Engine (SPIRE) for multimodal unstructured data. This architecture provides a framework for retrieving multi-modal data including image, image sequence, time series and parametric data from large archives. Dramatic speedup (from a factor of 4 to 35) has been achieved for many search operations such as template matching, texture feature extraction. This framework has been applied and validated in solar flares and petroleum exploration in which spatial and spatial-temporal phenomena are located.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spire : a progressive content-based spatial image retrieval engine

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: chung-sheng li , lawrence d. bergman , yuan-chi chang , vittorio castelli , john r. smith
",y
"LEFT id: NA
RIGHT id: 1125

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1572

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and user testing of a multi-paradigm query interface to an object-oriented database

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: dac khoa doan , norman w. paton , alistair kilgour
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 432

LEFT text: In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dlfm : a transactional resource manager

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hui-i hsiao , inderpal narang
",n
"LEFT id: NA
RIGHT id: 1864

LEFT text: A data warehouse is a repository of integrated information from distributed, autonomous, and possibly heterogeneous, sources. In effect, the warehouse stores one or more materialized views of the source data. The data is then readily available to user applications for querying and analysis. Figure 1 shows the basic architecture of a warehouse: data is collected from each source, integrated with data from other sources, and stored at the warehouse. Users then access the data directly from the warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the whips prototype for data warehouse creation and maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wilburt j. labio , yue zhuge , janet l. wiener , himanshu gupta , h &#233; ctor garc &#237; a-molina , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1183

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1282

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: surfing wavelets on streams : one-pass summaries for approximate aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anna c. gilbert , yannis kotidis , s. muthukrishnan , martin strauss
",y
"LEFT id: NA
RIGHT id: 706

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbxplorer : enabling keyword search over relational databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 1342

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing : taming the terabytes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: minos n. garofalakis , phillip b. gibbon
",n
"LEFT id: NA
RIGHT id: 506

LEFT text: The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a survey of logical models for olap databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: panos vassiliadis , timos sellis
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 178

LEFT text: There has been a lot of talk about how the Internet is going to change the world economy. Companies will come together in a “plug and play” fashion to form trading partner networks. Virtual companies will be established and new business models can be created based on access to information and agents that can carry it around the world using computer networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: electronic commerce : tutorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nabil r. adam , yelena yesha
",n
"LEFT id: NA
RIGHT id: 413

LEFT text: Publisher Summary This chapter provides a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages. Stream data are also generated naturally by web services, in which loosely coupled systems interact by exchanging high volumes of business data tagged in XML, forming continuous XML data streams. A central aspect of web services is the ability to efficiently operate on these XML data streams executing queries to continuously match, extract, and transform parts of the XML data stream to drive legacy back-end business applications. Manipulating stream data presents many technical challenges, which are just beginning to be addressed in the database, systems, algorithms, networking, and other computer science communities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fact : a learning based web query processing system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: songting chen , yanlei diao , hongjun lu , zengping tian
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: With the growing popularity of the internet and the World Wide Web (Web), there is a fast growing demand for access to database management systems (DBMS) from the Web. We describe here techniques that we invented to bridge the gap between HTML, the standard markup language of the Web, and SQL, the standard query language used to access relational DBMS. We propose a flexible general purpose variable substitution mechanism that provides cross-language variable substitution between HTML input and SQL query strings as well as between SQL result rows and HTML output thus enabling the application developer to use the full capabilities of HTML for creation of query forms and reports, and SQL for queries and updates. The cross-language variable substitution mechanism has been used in the design and implementation of a system called DB2 WWW Connection that enables quick and easy construction of applications that access relational DBMS data from the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1065

LEFT text: In this paper we propose a distributed case-based approach to the problem of rewriting queries. According to this approach we use a case memory instead of static views, i.e. views that are deened a priori. As a consequence, the mediated schema is dynamically updated, strongly innuenced by the queries submitted by a consumer. This approach allows a mediator to face systems where consumers may change their customization needs and information sources may become unavailable, may be added, or may modify their schemas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sting : a statistical information grid approach to spatial data mining

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wei wang , jiong yang , richard r. muntz
",n
"LEFT id: NA
RIGHT id: 1080

LEFT text: The current integrated developments in network and computing give rise to a technical infrastructure for the information society which one may variously circumscribe by terms such as ubiquitous computing, telepresence and the network as one giant global database. The paper applies to the network the metaphor of global database, and subsumes the aspects of ubiquity and telepresence under it. It should then be possible to preserve many of the existing database techniques and to concentrate on adjusting these to the network information infrastructure. The paper explores four challenges for adjustment: interoperability due to heterogeneous data repositories, proactivity due to autonomy of data sources, interactiveness due to the need of short-term and task-specific interaction and cooperation, and legacy due to the fitting of old systems to the networked environment. Based on several application projects and exemplary solutions, the paper claims as its experiences that objectorientation provides a natural framework for meeting the challenges, but must also draw on the combined resources of databases, data communications, and software engineering. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the network as a global database : challenges of interoperability , proactivity , interactiveness , legacy

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter c. lockemann , ulrike k &#246; lsch , arne koschel , ralf kramer , ralf nikolai , mechtild wallrath , hans-dirk walter
",y
"LEFT id: NA
RIGHT id: 1719

LEFT text: Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: SEMCOG Approach We argue that image retrieval based on either approach alone is not sufficient in terms of modeling and query specification flexibility y. We also argue that a visual query interface which is capability of visualizing target images is essential. SEMCOG[l] (SEMantics and COGnitionbased image retrieval) aims at integrating semautics and cognition-based approaches to give users a greater flexibility to pose queries. SEMCOG’S image matching is based on objects in the images rather than the whole images. In SEMCOG, a query “Retrieve all images in which there is a man to the right of a car and the man looks like this image” can be posed using combinations of semantics and visual expressions. The queries are posed in the way of specifying image objects and their layouts using a visual query interface, IFQ (In Frame Query), rather than complicated multimedia database query languages. The user’s query can be simplified as a mental model shown at the top of Figure 1. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1793

LEFT text: As databases get widely deployed, it becomes increasingly important to reduce the overhead of database administration. An important aspect of data administration that critically influences performance is the ability to select indexes for a database. In order to decide the right indexes for a database, it is crucial for the database administrator (DBA) to be able to perform a quantitative analysis of the existing indexes. Furthermore, the DBA should have the ability to propose hypothetical (“what-if”) indexes and quantitatively analyze their impact on performance of the system. Such impact analysis may consist of analyzing workloads over the database, estimating changes in the cost of a workload, and studying index usage while taking into account projected changes in the sizes of the database tables. In this paper we describe a novel index analysis utility that we have prototyped for Microsoft SQL Server 7.0. We describe the interfaces exposed by this utility that can be leveraged by a variety of front-end tools and sketch important aspects of the user interfaces enabled by the utility. We also discuss the implementation techniques for efficiently supporting “what-if” indexes. Our framework can be extended to incorporate analysis of other aspects of physical database design.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: autoadmin what-if index analysis utility

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: surajit chaudhuri , vivek narasayya
",y
"LEFT id: NA
RIGHT id: 226

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1213

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: priority assignment in real-time active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rajendran m. sivasankaran , john a. stankovic , don towsley , bhaskar purimetla , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 791

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation of two semantic query optimization techniques in db2 universal database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: qi cheng , jarek gryz , fred koo , t. y. cliff leung , linqi liu , xiaoyan qian , k. bernhard schiefer
",n
"LEFT id: NA
RIGHT id: 1475

LEFT text: Bulk loading refers to the process of creating an index from scratch for a given data set. This problem is well understood for B-trees, but so far, non-traditional index structures received modest attention. We are particularly interested in fast generic bulk loading techniques whose implementations only employ a small interface that is satisfied by a broad class of index structures. Generic techniques are very attractive to extensible database systems since different user-implemented index structures implementing that small interface can be bulk-loaded without any modification of the generic code. The main contribution of the paper is the proposal of two new generic and conceptually simple bulk loading algorithms. These algorithms recursively partition the input by using a main-memory index of the same type as the target index to be build. In contrast to previous generic bulk loading algorithms, the implementation of our new algorithms turns out to be much easier. Another advantage is that our new algorithms possess fewer parameters whose settings have to be taken into consideration. An experimental performance comparison is presented where different bulk loading algorithms are investigated in a system-like scenario. Our experiments are unique in the sense that we examine the same code for different index structures (R-tree and Slim-tree). The results consistently indicate that our new algorithms outperform asymptotically worst-case optimal competitors. Moreover, the search quality of the target index will be better when our new bulk loading algorithms are used.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a performance evaluation of oid mapping techniques

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: andr &#233; eickler , carsten andreas gerlhof , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1224

LEFT text: Over the last few years, the information technology industry has witnessed revolutions in multiple dimensions. Increasing ubiquitous sources of data have posed two connected challenges to data management solutions -- processing unprecedented volumes of data, and providing ad-hoc real-time analysis in mainstream production data stores without compromising regular transactional workload performance. In parallel, computer hardware systems are scaling out elastically, scaling up in the number of processors and cores, and increasing main memory capacity extensively. The data processing challenges combined with the rapid advancement of hardware systems has necessitated the evolution of a new breed of main-memory databases optimized for mixed OLTAP environments and designed to scale.    The Oracle RDBMS In-memory Option (DBIM) is an industry-first distributed dual format architecture that allows a database object to be stored in columnar format in main memory highly optimized to break performance barriers in analytic query workloads, simultaneously maintaining transactional consistency with the corresponding OLTP optimized row-major format persisted in storage and accessed through database buffer cache. In this paper, we present the distributed, highly-available, and fault-tolerant architecture of the Oracle DBIM that enables the RDBMS to transparently scale out in a database cluster, both in terms of memory capacity and query processing throughput. We believe that the architecture is unique among all mainstream in-memory databases. It allows complete application-transparent, extremely scalable and automated distribution of Oracle RDBMS objects in-memory across a cluster, as well as across multiple NUMA nodes within a single server. It seamlessly provides distribution awareness to the Oracle SQL execution framework through affinitized fault-tolerant parallel execution within and across servers without explicit optimizer plan changes or query rewrites.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the lham log-structured history data access method

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter muth , patrick o'neil , achim pick , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 345

LEFT text: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the ""top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the design and performance evaluation of alternative xml storage strategies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: feng tian , david j. dewitt , jianjun chen , chun zhang
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: Both databases and knowledge bases are used to represent the relevant parts of an application domain, and to allow convenient access to the stored information. Research in knowledge representation (KR) originally concentrated on expressive formalisms with sophisticated reasoning services, usually under the assumption that the size of the knowledge base (KB) was relatively small. In contrast, database (DB) research was concerned with e ciently storing, retrieving, and sharing large amounts of simple data, but the languages for describing schema informationwere rather simple, and reasoning about the schema played only a minor role.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 931

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient testing of high performance transaction processing systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. wildfogel , ramana yerneni
",n
"LEFT id: NA
RIGHT id: 1031

LEFT text: A data warehouse materializes views derived from data that may not reside at the warehouse. Maintaining these views effciently in response to base updates is diffcult, since it may involve querying external sources where the base data reside. This paper considers the problem of view self-maintenance, where the views are maintained without using all the base data. Without full use of the base data, however, maintaining a view unambiguously is not always possible. Thus, the two critical questions that must be addressed are to determine, in a given situation, whether a view is maintainable, and how to maintain it. W e provide algorithms that answer these questions for a general class of views, and for an important subclass, generate SQL queries that test whether a view is self-maintainable and update the view if it is. We improve significantly on previous work by solving the view self-maintenance problem in the presence of multiple views, with optional access to a subset of the base data, and under arbitrary mixes of insertions and deletions. We provide better insight into the problem by showing that view self-maintainability can be reduced to the problem of deciding query containment. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",y
"LEFT id: NA
RIGHT id: 956

LEFT text: Abstract. Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating sql databases with content-specific search engines

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan de &#223; loch , nelson mendon &#231; a mattos
",n
"LEFT id: NA
RIGHT id: 1413

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2054

LEFT text: Central to any XML query language is a path language such as XPath which operates on the tree structure of the XML document. We demonstrate in this paper that the tree structure can be effectively compressed and manipulated using techniques derived from symbolic model checking. Specifically, we show first that succinct representations of document tree structures based on sharing subtrees are highly effective. Second, we show that compressed structures can be queried directly and efficiently through a process of manipulating selections of nodes and partial decompression. We study both the theoretical and experimental properties of this technique and provide algorithms for querying our compressed instances using node-selecting path query languages such as XPath.    We believe the ability to store and manipulate large portions of the structure of very large XML documents in main memory is crucial to the development of efficient, scalable native XML databases and query engines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpath queries on streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: feng peng , sudarshan s. chawathe
",n
"LEFT id: NA
RIGHT id: 20

LEFT text: In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the need for distributed asynchronous transactions

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: lyman do , prabhu ram , pamela drew
",n
"LEFT id: NA
RIGHT id: 1371

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing multi-feature queries for image databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ulrich g &#252; ntzer , wolf-tilo balke , werner kie &#223; ling
",y
"LEFT id: NA
RIGHT id: 681

LEFT text: XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate xml joins

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: sudipto guha , h. v. jagadish , nick koudas , divesh srivastava , ting yu
",y
"LEFT id: NA
RIGHT id: 945

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a region splitting strategy for physical database design of multidimensional file organizations

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jong-hak lee , young-koo lee , kyu-young whang , il-yeol song
",n
"LEFT id: NA
RIGHT id: 850

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the a-tree : an index structure for high-dimensional spaces using relative approximation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",y
"LEFT id: NA
RIGHT id: 927

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management. Even though the idea of randomized linear projections (a.k.a., sketches) was known for some time in the domain of functional analysis (dating back to the famous Johnson-Lindenstrauss Lemma), Alon, Matias, and Szegedy were the first to exploit sketches for small-space data-stream computation, through the use of limited-independence random variates that can be constructed in small space and time. Of course, in addition to small-space sketching, the AMS paper also makes a number of other fundamental contributions in data streaming, including practical approximation algorithms for other frequency moments (e.g., the number of distinct values in a stream), as well as several inapproximability results (i.e., lower bounds) based on beautiful communication-complexity arguments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: I'm happy to be able to share with you the following three reminiscences. I continue to invite unsolicited contributions. See http://www.acm.org/sigmod/record/author.html for submission guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1146

LEFT text: Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 1295

LEFT text: The SIT-IN (acronym for Integrated Territorial Information System, in Italian) system integrates a historical database, providing information about the temporal evolution of territorial administrative partitions; the Institute's GIS, providing the cartography of the Italian territory down to the census tract level of detail; a statistical data warehouse, providing spatiotemporal data from a number of di erent surveys; and nally an address normalizing/geo-matching system, providing information about the limits of census tracts (e.g. portions of streets or the sides of town squares).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the worlinfo assistant : spatio-temporal information integration on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jos &#233; luis ambite , craig a. knoblock , mohammad r. kolahdouzan , maria muslea , cyrus shahabi , snehal thakkar
",n
"LEFT id: NA
RIGHT id: 1090

LEFT text: PAPER NO. 1077 The focus of this paper is on the characterization of the skewness of an attribute-value distribution and on the extrapolations for interesting parameters. We provide eeective schemes for obtaining estimates about either its statistics or subsets/supersets of the relation. We assume an 80/20 law, and speciically, a p=(1 ? p) law. This law gives a distribution which is commonly known in the fractals literature as`multifractal'. We show how to estimate p from the given information ((rst few multiplicities, and a few moments), and present the results of our experimentations on real data. Our results demonstrate that schemes based on our multifractal assumption consistently outperforms those schemes based on the uniformity assumption, which are commonly used in current DBMSs. Moreover, our schemes can be used to provide estimates for supersets of a relation, which the uniformity assumption based schemes can not not provide at all.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: modeling skewed distribution using multifractals and the ` 80-20 ' law

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: christos faloutsos , yossi matias , abraham silberschatz
",y
"LEFT id: NA
RIGHT id: 1738

LEFT text: In this article, we analyze several quorum types in order to better understand their behavior in practice. The results obtained challenge many of the assumptions behind quorum based replication. Our evaluation indicates that the conventional read-one/write-all-available approach is the best choice for a large range of applications requiring data replication. We believe this is an important result for anybody developing code for computing clusters as the read-one/write-all-available strategy is much simpler to implement and more flexible than quorum-based approaches. In this article, we show that, in addition, it is also the best choice using a number of other selection criteria.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data replication for mobile computers

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: yixiu huang , prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 530

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1122

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: calibrating the query optimizer cost model of iro-db , an object-oriented federated database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , fei sha , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 192

LEFT text: A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. It is not even known whether it is possible to generate a sample of a join tree without first evaluating the join tree completely. We undertake a detailed study of this problem and attempt to analyze it in a variety of settings. We present theoretical results explaining the difficulty of this problem and setting limits on the efficiency that can be achieved. Based on new insights into the interaction between join and sampling, we develop join sampling techniques for the settings where our negative results do not apply. Our new sampling algorithms are significantly more efficient than those known earlier. We present experimental evaluation of our techniques on Microsoft's SQL Server 7.0.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on random sampling over joins

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , rajeev motwani , vivek narasayya
",y
"LEFT id: NA
RIGHT id: 1898

LEFT text: We describe the TIGUKAT objectbase management system, which is under development at the Laboratory for Database Systems Research at the University of Alberta. TIGUKAT has a novel object model, whose identifying characteristics include a purely behavioral semantics and a uniform approach to objects. Everything in the system, including types, classes, collections, behaviors, and functions, as well as meta-information, is a first-class object with well-defined behavior. In this way, the model abstracts everything, including traditional structural notions such as instance variables, method implementation, and schema definition, into a uniform semantics of behaviors on objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 686

LEFT text: To take advantage of this batch processing in achieving fast response time, this paper uses prediction methods to predict future values. FFT is used to compute the cross correlations of the predicted series (with the values that have already arrived) and the database patterns, and to obtain predicted distances between the incoming time series at many future time positions and the database patterns. When the actual data value arrives, the prediction error together with the predicted distances is used to filter out patterns that are not possible to be the nearest or near neighbors, which provides fast responses. Experiments show that with reasonable prediction errors, the performance gain is significant.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: continually evaluating similarity-based pattern queries on a streaming time series

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: like gao , x. sean wang
",y
"LEFT id: NA
RIGHT id: 1068

LEFT text: In this paper, we propose a bulk-algebra, TIX, and describe how it can be used as a basis for integrating information retrieval techniques into a standard pipelined database query evaluation engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining deviants in a time series database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , s. muthukrishnan
",n
"LEFT id: NA
RIGHT id: 2290

LEFT text: This second special issue provides a forum for topical issues that demonstrate the usefulness of PLS-SEM by piloting applications of this method in the field of strategic management with strong implications for strategic research and practice. As such, the special issue targets two audiences: academics involved in the fields of strategy and management, and practitioners such as consultants. The six articles in this issue are summarized in the following paragraphs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1932

LEFT text: This paper presents general algorithms for concurrency control in tree-based access methods as well as a recovery protocol and a mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data structure, an index structure supporting an extensible set of queries and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods. The concurrency control protocol is based on an extension of the link technique originally developed for B-trees, and completely avoids holding node locks during I/Os. Repeatable read isolation is achieved with a novel combination of predicate locks and two-phase locking of data records. To our knowledge, this is the first time that isolation issues have been addressed outside the context of B-trees. A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the algorithms developed here deviate from their B-tree counterparts. An implementation of GiSTs emulating B-trees in DB2/Common Server is underway.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: concurrency and recovery in generalized search trees

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marcel kornacker , c. mohan , joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 1654

LEFT text: Databases are replicated to improve performance and availability. The notion of correctness that has commonly been adopted for concurrent access by transactions to shared, possibly replicated, data is serializability. However, serializability may be impractical in high-performance applications since it imposes too stringent a restriction on concurrency. When serializability is relaxed, the integrity constraints describing the data may be violated. By allowing bounded violations of the integrity constraints, however, we are able to increase the concurrency of transactions that execute in a replicated environment. In this article, we introduce the notion of an N-ignorant transaction, which is a transaction that may be ignorant of the results of at most N prior transactions, which is a transaction that may be ignorant of the results of at most N prior transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: bounded ignorance : a technique for increasing concurrency in a replicated system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narayanan krishnakumar , arthur j. bernstein
",y
"LEFT id: NA
RIGHT id: 1038

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: geo/environmental and medical data management in the rasdaman system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter baumann , paula furtado , roland ritsch , norbert widmann
",n
"LEFT id: NA
RIGHT id: 1185

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 2200

LEFT text: Sensor networking technologies have developed very rapidly in the last ten years. In many situations, high quality multimedia streams may be required for providing detailed information of the hot spots in a large scale network. With the limited capabilities of sensor node and sensor network, it is very difficult to support multimedia streams in current sensor network structure. In this paper, we propose to enhance the sensor network by deploying limited number of mobile ""swarms"". The swarm nodes have much higher capabilities than the sensor nodes in terms of both hardware functionalities and networking capabilities. The mobile swarms can be directed to the hot spots in the sensor network to provide detailed information of the intended area. With the help of mobile swarms, high quality of multimedia streams can be supported in the large scale sensor network without too much cost. The wireless backbone network for connecting different swarms and the routing schemes for supporting such a unified architecture is also discussed and verified via simulations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: multimedia streaming in large-scale sensor networks with mobile swarms

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: mario gerla , kaixin xu
",y
"LEFT id: NA
RIGHT id: 1471

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: The experimental results show that distributed commit processing can have considerably more influence than distributed data processing on the throughput performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best transaction throughput performance for a variety of workloads and system configurations. In fact, OPT's peak throughput is often close to the upper bound on achievable performance. Even more interestingly, a three-phase (i.e., non-blocking) version of OPT provides better peak throughput performance than all of the standard two-phase (i.e., blocking protocols evaluated in our study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 565

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research activities in database management and information retrieval at university of illinois at chicago

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: isabel cruz , ashfaq khokhar , bing liu , prasad sistla , ouri wolfson , clement yu
",n
"LEFT id: NA
RIGHT id: 886

LEFT text: Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integration of data mining with database technology

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: amir netz , surajit chaudhuri , jeff bernhardt , usama m. fayyad
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: I'm happy to be able to share with you the following three reminiscences. I continue to invite unsolicited contributions. See http://www.acm.org/sigmod/record/author.html for submission guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: world wide database-integrating the web , corba and databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: athman bouguettaya , boualem benatallah , lily hendra , james beard , kevin smith , mourad quzzani
",y
"LEFT id: NA
RIGHT id: 1477

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bypassing joins in disjunctive queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael steinbrunn , klaus peithner , guido moerkotte , alfons kemper
",n
"LEFT id: NA
RIGHT id: 232

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: safe query languages for constraint databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter z. revesz
",n
"LEFT id: NA
RIGHT id: 535

LEFT text: Many emerging application domains require database systems to support efficient access over highly multidimensional datasets. The current state-of-the-art technique to indexing high dimensional data is to first reduce the dimensionality of the data using Principal Component Analysis and then indexing the reduceddimensionality space using a multidimensional index structure. The above technique, referred to as global dimensionality reduction (GDR), works well when the data set is globally correlated, i.e. most of the variation in the data can be captured by a few dimensions. In practice, datasets are often not globally correlated. In such cases, reducing the data dimensionality using GDR causes significant loss of distance information resulting in a large number of false positives and hence a high query cost. Even when a global correlation does not exist, there may exist subsets of data that are locally correlated. In this paper, we propose a technique called Local Dimensionality Reduction (LDR) that tries to find local correlations in the data and performs dimensionality reduction on the locally correlated clusters of data individually. We develop an index structure that exploits the correlated clusters to efficiently support point, range and k-nearest neighbor queries over high dimensional datasets. Our experiments on synthetic as well as real-life datasets show that our technique (1) reduces the dimensionality of the data with significantly lower loss in distance information compared to GDR and (2) significantly outperforms the GDR, original space indexing and linear scan techniques in terms of the query cost for both synthetic and real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 756

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database tuning : principles , experiments , and troubleshooting techniques ( part ii )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dennis shasha , philippe bonnet
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 294

LEFT text: The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: probe , count , and classify : categorizing hidden web databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: panagiotis g. ipeirotis , luis gravano , mehran sahami
",y
"LEFT id: NA
RIGHT id: 208

LEFT text: We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast algorithms for projected clustering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: charu c. aggarwal , joel l. wolf , philip s. yu , cecilia procopiuc , jong soo park
",n
"LEFT id: NA
RIGHT id: 460

LEFT text: Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (XopY) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ∈{<, ≤, =, ≠, >, ≥). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ≤, =, ≥, >} and {<, ≤, =, ≠, ≥, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from <archive.fiu.edu>.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a new approach to developing and implementing eager database replication protocols

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina kemme , gustavo alonso
",n
"LEFT id: NA
RIGHT id: 786

LEFT text: While the desire to support fast, ad hoc query processing for large data warehouses has motivated the recent introduction of many new indexing structures, with a few notable exceptions (namely, the LSM-Tree and the Stepped Merge Method) little attention has been given to developing new indexing schemes that allow fast insertions. Since additions to a large warehouse may number in the millions per day, indices that require a disk seek (or even a significant fraction of a seek) per insertion are not acceptable. In this paper, we offer an alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequent insertions. The Y-tree is a new indexing structure supporting both point and range queries over a single attribute, with retrieval performance comparable to the B+-tree. For processing insertions, however, the Y-tree may exhibit a speedup of 100 times over batched insertions into a B+-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: curio : a novel solution for efficient storage and indexing in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: anindya datta , krithi ramamritham , helen m. thomas
",n
"LEFT id: NA
RIGHT id: 1681

LEFT text: Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in databases for arcs : active rapidly changing data systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: anindya datta
",n
"LEFT id: NA
RIGHT id: 1869

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mentor workbench for enterprise-wide workflow management

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dirk wodtke , jeanine weissenfels , gerhard weikum , angelika kotz dittrich , peter muth
",n
"LEFT id: NA
RIGHT id: 1557

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a structured approach for the definition of the semantics of active databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: piero fraternali , letizia tanca
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 255

LEFT text: Data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. We provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. Several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. Techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. Potential data mining applications and some research issues will also be discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jiawei han
",y
"LEFT id: NA
RIGHT id: 1455

LEFT text: In the relational model the order of fetching data does not affect query correctness. This flexibility is exploited in query optimization by statically reordering data accesses. However, once a query is optimized, it is executed in a fixed order in most systems, with the result that data requests are made in a fixed order. Only limited forms of runtime reordering can be provided by low-level device managers. More aggressive reordering strategies are essential in scenarios where the latency of access to data objects varies widely and dynamically, as in tertiary devices. This paper presents such a strategy. Our key innovation is to exploit dynamic reordering to match execution order to the optimal data fetch order, in all parts of the plan-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1074

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1130

LEFT text: In this paper we propose techniques that solve the problem by performing a single query for the whole input segment. As a result the cost, depending on the query and dataset characteristics, may drop by orders of magnitude. In addition, we propose analytical models for the expected size of the output, as well as, the cost of query processing, and extend out techniques to several variations of the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Active database systems have been a hot research topic for quite some years now. However, while “active functionality” has been claimed for many systems, and notions such as “active objects” or “events” are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of “active database management system” as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",y
"LEFT id: NA
RIGHT id: 1751

LEFT text: In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1506

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: In addition to facilitating querying over the Web, XML query languages may provide high level constructs for useful facilities in traditional DBMSs that do not currently exist. In particular, current DBMS query languages do not allow querying across database object types to yield heterogeneous results. This paper motivates the usefulness of heterogeneous querying in traditional DBMSs and investigates XQuery, an emerging standard for XML query languages, to express such queries. The usefulness of querying and storing heterogeneous types is also applied to XML data within a Web information system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 516

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1560

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",n
"LEFT id: NA
RIGHT id: 1560

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1770

LEFT text: While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unbundling active functionality

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: stella gatziu , arne koschel , g &#252; nter von b &#252; ltzingsloewen , hans fritschi
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1743

LEFT text: This paper presents an approach that preserves the semi-atomicity (a weaker form of atomicity) of flexible transactions, allowing local sites to autonomously maintain serializability and recoverability. We offer a fundamental characterization of the flexible transaction model and precisely define the semi-atomicity. We investigate the commit dependencies among the subtransactions of a flexible transaction. These dependencies are used to control the commitment order of the subtransactions. We next identify those restrictions that must be placed upon a flexible transaction to ensure the maintenance of its semi-atomicity. As atomicity is a restrictive criterion, semi-atomicity enhances the class of executable global transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ensuring relaxed atomicity for flexible transactions in multidatabase systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: aidong zhang , marian nodine , bharat bhargava , omran bukhres
",y
"LEFT id: NA
RIGHT id: 1410

LEFT text: We present an approach to searching genetic DNA sequences using an adaptation of the sufx tree data structure deployed on the general purpose persistent Java platform, PJama. Our implementation technique is novel, in that it allows us to build su x trees on disk for arbitrarily large sequences, for instance for the longest human chromosome consisting of 263 million letters. We propose to use such indexes as an alternative to the current practice of serial scanning. We describe our tree creation algorithm, analyse the performance of our index, and discuss the interplay of the data structure with object store architectures. Early measurements are presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database index to large biological sequences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ela hunt , malcolm p. atkinson , robert w. irving
",y
"LEFT id: NA
RIGHT id: 646

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 388

LEFT text: This chapter reveals that to date grid middleware has focused principally on the basic issues of storage, computation, and resource management needed to make a global scientific community's information and tools accessible. However, from an e-Science viewpoint, the purpose of the grid is to deliver a collaborative and supportive environment that allows geographically distributed scientists to achieve research goals more effectively. Such an environment is likely to hide many aspects of a grid middleware, and to exploit both generic and application-specific service discovery, invocation, and composition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data access ( tutorial session )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , anand deshpande
",n
"LEFT id: NA
RIGHT id: 744

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1042

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a new sql-like operator for mining association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rosa meo , giuseppe psaila , stefano ceri
",n
"LEFT id: NA
RIGHT id: 1960

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization of sparsely-populated b + - trees

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: chendong zou , betty salzberg
",n
"LEFT id: NA
RIGHT id: 1333

LEFT text: An increasing number of database applications demands high availability combined with online scalability and soft real-time transaction response. This means that scaling must be done online and non-blocking. This paper extends the primary/hot standby approach to high availability with online scaling operations. The challenges are to do this without degrading the response time and throughput of transactions and to support high availability throughout the scaling period. We measure the impact of online scaling on response time and throughput using di erent scheduling schemes. We also show some of the recovery problems that appear in this setting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: online scaling in a highly available database

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: svein erik bratsberg , rune humborstad
",y
"LEFT id: NA
RIGHT id: 1114

LEFT text: Concurrency control is essential to the correct functioning of a database due to the need for correct, reproducible results. For this reason, and because concurrency control is a well-formulated problem, there has developed an enormous body of literature studying the performance of concurrency control algorithms. Most of this literature uses either analytic modeling or random number-driven simulation, and explicitly or implicitly makes certain assumptions about the behavior of transactions and the patterns by which they set and unset locks. Because of the difficulty of collecting suitable measurements, there have been only a few studies which use trace-driven simulation, and still less study directed toward the characterization of concurrency control behavior of real workloads. In this paper, we present a study of three database workloads, all taken from IBM DB2 relational database systems running commercial applications in a production environment. This study considers topics such as frequency of locking and unlocking, deadlock and blocking, duration of locks, types of locks, correlations between applications of lock types, two-phase versus non-two-phase locking, when locks are held and released, etc. In each case, we evaluate the behavior of the workload relative to the assumptions commonly made in the research literature and discuss the extent to which those assumptions may or may not lead to erroneous conclusions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 2006

LEFT text: We have been developing a mobile passenger support system for public transport. Passengers can make their travel plans and purchase necessary tickets by accessing databases via the system. After starting the travel, a mobile terminal checks the travel schedule of its user by accessing several databases and gathering various kinds of information. In this application field, many kinds of data must be handled. Examples of such data are route information, fare information, area map, station map, planned operation schedule, real-time operation schedule, vehicle facilities and so on. Depending on the user's situation, different information should be supplied and personalized. In this paper, we propose a new mechanism to support passengers using the multi-channel data communication environments. On the other hand, transport systerns can gather information about situations and demands of users and modify their services offered for the users. We also describe a prototype system developed for visually handicapped passengers and the results of tests in an actual railway station.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of electronic tickets and personal guide system for public transport using mobile terminals

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: koichi goto , yahiko kambayashi
",n
"LEFT id: NA
RIGHT id: 1023

LEFT text: In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: In this paper, we first focus our attention on the question of how much space remains for performance improvement over current association rule mining algorithms. Our strategy is to compare their performance against an “Oracle algorithm” that knows in advance the identities of all frequent itemsets in the database and only needs to gather their actual supports to complete the mining process. Our experimental results show that current mining algorithms do not perform uniformly well with respect to the Oracle for all database characteristics and support thresholds. In many cases there is a substantial gap between the Oracle’s performance and that of the current mining algorithms. Second, we present a new mining algorithm, called ARMOR, that is constructed by making minimal changes to the Oracle algorithm. ARMOR consistently performs within a factor of two of the Oracle on both real and synthetic datasets over practical ranges of support specifications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 117

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use relevant information. In the Ecobase project, we address these problems in the context of several environmental applications in Brazil and Europe. We propose a distributed architecture for environmental information systems (EIS) based on the Le Select middleware developed at INRIA. In this paper, we present this architecture and its capabilities, and discuss the lessons learned and open issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of environmental models for application to global information systems and decision-making

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: d. scott mackay
",n
"LEFT id: NA
RIGHT id: 2269

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementing lazy database updates for an object database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari
",n
"LEFT id: NA
RIGHT id: 767

LEFT text: There are a number of database systems available free of charge for the research community, with complete access to the source code. Some of these systems result from completed research projects, others have been developed outside the research community. How can the database community best take advantage of these publically available systems? The most widely used open-source database is MySQL. Their objective is to become the 'best and most used database in the world'.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: going public : open-source databases and database research

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: philippe bonnet
",y
"LEFT id: NA
RIGHT id: 1939

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 1393

LEFT text: Due to organizational or operational constraints, the diverse data sources that an enterprise uses do not generally lend themselves to being fully replicated or completely consolidated under a single database, hence the increased demand for data interchange and for federated access to distributed sources. IBM has ongoing work in information integration technology that enables integrated, real-time access to traditional and emerging data sources, transforms information to meet the needs of business analysts, and manages data placement for performance, currency, and availability leading to fast, constant, and easy access for customer e-business solutions. IBM's Information Integration infrastructure today supports SQL—a mature, powerful query language—plus a number of SQL extensions in support of XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information integration : the momis project demonstration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , silvana castano , alberto corni , r. guidetti , g. malvezzi , michele melchiori , maurizio vincini
",n
"LEFT id: NA
RIGHT id: 2076

LEFT text: We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating keyword search queries over hierarchical XML documents, as opposed to (conceptually) flat HTML documents, introduces many new challenges. First, XML keyword search queries do not always return entire documents, but can return deeply nested XML elements that contain the desired keywords. Second, the nested structure of XML implies that the notion of ranking is no longer at the granularity of a document, but at the granularity of an XML element. Finally, the notion of keyword proximity is more complex in the hierarchical XML data model. In this paper, we present the XRANK system that is designed to handle these novel features of XML keyword search. Our experimental results show that XRANK offers both space and performance benefits when compared with existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xrank : ranked keyword search over xml documents

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lin guo , feng shao , chavdar botev , jayavel shanmugasundaram
",y
"LEFT id: NA
RIGHT id: 1313

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1881

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: semantics for update rule programs and implementation in a relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: louiqa raschid , jorge lobo
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: Arrays are a common and important class of data. At present, database systems do not provide adequate array support: arrays can neither be easily defined nor conveniently manipulated. Further, array manipulations are not optimized. This paper describes a language called the Array Manipulation Language (AML), for expressing array manipulations, and a collection of optimization techniques for AML expressions.In the AML framework for array manipulation, arbitrary externally-defined functions can be applied to arrays in a structured manner. AML can be adapted to different application domains by choosing appropriate external function definitions. This paper concentrates on arrays occurring in databases of digital images such as satellite or medical images.AML queries can be treated declaratively and subjected to rewrite optimizations. Rewriting minimizes the number of applications of potentially costly external functions required to compute a query result. AML queries can also be optimized for space. Query results are generated a piece at a time by pipelined execution plans, and the amount of memory required by a plan depends on the order in which pieces are generated. An optimizer can consider generating the pieces of the query result in a variety of orders, and can efficiently choose orders that require less space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1823

LEFT text: In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infosleuth : agent-based semantic integration of information in open and dynamic environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. bayardo , jr. , w. bohrer , r. brice , a. cichocki , j. fowler , a. helal , v. kashyap , t. ksiezyk , g. martin , m. nodine , m. rashid , m. rusinkiewicz , r. shea , c. unnikrishnan , a. unruh , d. woelk
",n
"LEFT id: NA
RIGHT id: 711

LEFT text: In this demo, we show how database-style declarative queries can be executed over data streaming from sensor networks. Our demo consists of two major components: a set of Berkeley TinyOS battery-powered, wireless sensor ""motes"" (see Figure 1) that produce and process data, and a desktop-based query processor which parses queries, distributes them over motes, and collects and displays answers. Specifically, we allow conference attendees standing at our query processing workstation to query a number of motes distributed throughout the demo space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distributing queries over low-power wireless sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 1499

LEFT text: Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-oriented , rapid application development in a pc database environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate fox development team microsoft
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2225

LEFT text: Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLAHANS which is based on randomized search. We also develop two spatial data mining algorithms that use CLAHANS. Our analysis and experiments show that with the assistance of CLAHANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms. Furthermore, experiments conducted to compare the performance of CLAHANS with that of existing clustering methods show that CLAHANS is the most efficient.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient and effective clustering methods for spatial data mining

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raymond t. ng , jiawei han
",y
"LEFT id: NA
RIGHT id: 1870

LEFT text: Four degradation patterns were determined objectively through the clustering approach, each of which showed similar trends and characteristics. Based on the distribution of clusters, interval 2 was determined to have the worst overall condition. The LSTM network was used for performance prediction in each cluster. Compared with the traditional multilayer neural network, the LSTM also exhibited good prediction effectiveness. The predicted data are well consistent with the observed data with correlation coefficient R2 equaling to 88.4%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1693

LEFT text: This paper introduces a new efficient join algorithm to increase the speed of the join relational operation. Using a divide and conquer strategy, stack oriented filter technique in the new join algorithm filters unwanted tuples as early as possible while none of the currently existing join algorithms takes advantage of any filtering concept. Other join algorithms may carry the unnecessary tuples up to the last moment of join attribute comparisons.Four join algorithms are described and discussed in this paper: the nested-loop join algorithm, the sort-merge join algorithm, the hash join algorithm, and the new join algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a new join algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dong keun shin , arnold charles meltzer
",y
"LEFT id: NA
RIGHT id: 1399

LEFT text: Abstract. In meta-searchers accessing distributed Web-based information repositories, performance is a major issue. Efficient query processing requires an appropriate caching mechanism. Unfortunately, standard page-based as well as tuple-based caching mechanisms designed for conventional databases are not efficient on the Web, where keyword-based querying is often the only way to retrieve data. In this work, we study the problem of semantic caching of Web queries and develop a caching mechanism for conjunctive Web queries based on signature files. Our algorithms cope with both relations of semantic containment and intersection between a query and the corresponding cache items. We also develop the cache replacement strategy to treat situations when cached items differ in size and contribution when providing partial query answers. We report results of experiments and show how the caching mechanism is realized in the Knowledge Broker system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching strategies for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",y
"LEFT id: NA
RIGHT id: 599

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 452

LEFT text: Database systems have enjoyed a tremendous market because they have served many applications really well -- transaction processing in the beginning, and then decision support. Today, with over 200% cumulative growth rate in certain segments of E-Commerce, it is clear that this new class of applications will be a strong driver for databases to grow, commercially, as well as from a Research perspective. This paper outlines some of the issues that I have learnt in dealing with E-Commerce applications that may well be the focus of some of the research in database systems over the course of next few years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: moving up the food chain : supporting e-commerce applications on databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: anant jhingran
",y
"LEFT id: NA
RIGHT id: 1794

LEFT text: A method for generating an approximate answer in response to a query to a database in which an SQL query Q for operating on a relation R in a database is received. Relation R has an associated histogram H. The SQL query Q is translated to be a query Q′ for operating on histogram H. Translated query Q′ is executed on histogram H for obtaining a result histogram. The result histogram is expanded into a relation having tuples containing approximate attribute values.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: new sampling-based summary statistics for improving approximate query answers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1174

LEFT text: Three designs of hierarchical locking suitable for B-tree indexes are explored in detail and their advantages and disadvantages compared. Traditional hierarchies include index, leaf page, and key range or key value. Alternatively, locks on separator keys in interior B-tree pages can protect key ranges of different sizes. Finally, for keys consisting of multiple columns, key prefixes of different sizes permit a third form of hierarchical locking. Each of these approaches requires appropriate implementation techniques. The techniques explored here include node splitting and merging, lock escalation and lock de-escalation, and online changes in the granularity of locking. Those techniques are the first designs permitting introduction and removal of levels in a lock hierarchy on demand and without disrupting transaction or query processing. In addition, a simplification of traditional key range locking is introduced that applies principled hierarchical locking to keys in B-tree leaves. This new method of key range locking avoids counter-intuitive lock modes used in today’s highperformance database systems. Nonetheless, it increases concurrency among operations on individual keys and records beyond that enabled by traditional lock modes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the hb $ ^ \ \ pi $ - tree : a multi-attribute index supporting concurrency , recovery and node consolidation

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: georgios evangelidis , david lomet , betty salzberg
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: We follow the stack-baaed approach to query languages which is a new formal and intellectual paradigm for integrating querying and programming for object-oriented databases. Queries are considered generalized programing expressions which may be used within macroscopic imperative statements, such as creating, updating, inserting, and deleting data objects. Queries may be also used as procedures’ parameters, as well as determine the output from functional procedures (SQL-like views). The semantics, including generalized query operators (selection, projection, navigation, join, quantifiers, etc.), is defined in terms of operations on two stacks. The environment stack deals with the scope control and binding names.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1455

LEFT text: In this paper we introduce generalized projections (G P an extension of duplicateeliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinctand duplicate-preserving projections in a common unified framework. Using G P s we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: Recently, several query languages have been proposed for querying information sources whose data is not constrained by a schema, or whose schema is unknown. Examples include: LOREL (for querying data combined from several heterogeneous sources), W3QS (for querying the World Wide Web); and UnQL (for querying unstructured data). The natural data model for such languages is that of a rooted, labeled graph. Their main novelty is the ability to express queries which traverse arbitrarily long paths in the graph, typically described by a regular expression. Such queries however may prove difficult to evaluate in the case when the data is distributed on severalsites, with many edges going between sites. A typical case is that of a collection of WWW sites, with links pointing freely from one site to another (even forming cycles). A naive query shipping strategy may force the query to migrate back and forth between the various sites, leading to poor performance (or even non-termination). We present a technique for query decomposition, under which the query is shipped exactly once to every site, computed locally, then the local results are shipped to the client, and assembled here into the final result. This technique is efficient, in that (a) only data which is part of the final result is shipped from the data sites to the client site, and (b) the total work done locally at all sites does not exceed that needed for computing the (unoptimized) query on a centralized version of the database. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 1445

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bigsur : a system for the management of earth science data

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: paul brown , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 695

LEFT text: The current main memory (DRAM) access speeds lag far behind CPU speeds. Cache memory, made of static RAM, is being used in today’s architectures to bridge this gap. It provides access latencies of 2-4 processor cycles, in contrast to main memory which requires 15-25 cycles. Therefore, the performance of the CPU depends upon how well the cache can be utilized. We show that there are significant benefits in redesigning our traditional query processing algorithms so that they can make better use of the cache. The new algorithms run 8%-200% faster than the traditional ones.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partial results for online query processing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 269

LEFT text: Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enabling dynamic content caching for database-driven web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , wen-syan li , qiong luo , wang-pin hsiung , divyakant agrawal
",y
"LEFT id: NA
RIGHT id: 990

LEFT text: The paper describes the ARANEUS Wel-Base Management System [l, 5, 4, 61, a system developed at Universitb di Roma Tre, which represents a proposal towards the definition of a new kind of data-repository, designed to manage Web data in the database style. We call a WebBase a collection of data of heterogeneous nature, and more specifically: (i) highly structured data, such as the ones typically stored in relational or objectoriented database systems; (G) semistructured data, in the Web style.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 711

LEFT text: Recent developments in processor, memory and radio technology have enabled wireless sensor networks which are deployed to collect useful information from an area of interest. The sensed data must be gathered and transmitted to a base station where it is further processed for end-user queries. Since the network consists of low-cost nodes with limited battery power, power efficient methods must be employed for data gathering and aggregation in order to achieve long network lifetimes.In an environment where in a round of communication each of the sensor nodes has data to send to a base station, it is important to minimize the total energy consumed by the system in a round so that the system lifetime is maximized. With the use of data fusion and aggregation techniques, while minimizing the total energy per round, if power consumption per node can be balanced as well, a near optimal data gathering and routing scheme can be achieved in terms of network lifetime.So far, besides the conventional protocol of direct transmission, two elegant protocols called LEACH and PEGASIS have been proposed to maximize the lifetime of a sensor network. In this paper, we propose two new algorithms under name PEDAP (Power Efficient Data gathering and Aggregation Protocol), which are near optimal minimum spanning tree based routing schemes, where one of them is the power-aware version of the other. Our simulation results show that our algorithms perform well both in systems where base station is far away from and where it is in the center of the field. PEDAP achieves between 4x to 20x improvement in network lifetime compared with LEACH, and about three times improvement compared with PEGASIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distributing queries over low-power wireless sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1841

LEFT text: A data warehouse provides information for analytical processing, decision making and data mining tools. As the concept of real-time enterprise evolves, the synchronism between transactional data and data warehouses, statically implemented, has been redefined. Traditional data warehouse systems have static structures of their schemas and relationships between data, and therefore are not able to support any dynamics in their structure and content. Their data is only periodically updated because they are not prepared for continuous data integration. For real-time enterprises with needs in decision support purposes, real-time data warehouses seem to be very promising. In this paper we present a methodology on how to adapt data warehouse schemas and user-end OLAP queries for efficiently supporting real-time data integration. To accomplish this, we use techniques such as table structure replication and query predicate restrictions for selecting data, to enable continuously loading data in the data warehouse with minimum impact in query execution time. We demonstrate the efficiency of the method by analyzing its impact in query performance using benchmark TPC-H executing query workloads while simultaneously performing continuous data integration at various insertion time rates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mdm : a multiple-data model tool for the management of heterogeneous database schemes

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , riccardo torlone
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: The focus of our work is to design and build a dynamic data distribution system that is coherence-preserving, i.e. the delivered data must preserve associated coherence requirements (the user-specified bound on tolerable imprecision) and resilient to failures. To this end, we consider a system in which a set of repositories cooperate with each other and the sources, forming a peer-to-peer network. In this system, necessary changes are pushed to the users so that they are automatically informed, about changes of interest. We present techniques 1) to determine when to push an update from one repository to another for coherence maintenance, 2) to construct an efficient dissemination tree for propagating changes from sources to cooperating repositories, and 3) to make the system resilient to failures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 75

LEFT text: Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best exploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection. We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents. At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention, and keeps only the most reliable ones for the next iteration.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xtract : a system for extracting document type descriptors from xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: minos garofalakis , aristides gionis , rajeev rastogi , s. seshadri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 1850

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-relational database systems ( tutorial ) : principles , products and challenges

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. carey , nelson m. mattos , anil k. nori
",n
"LEFT id: NA
RIGHT id: 2290

LEFT text: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1146

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1274

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: view management in multimedia databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , eric lemar , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1057

LEFT text: Three topic areas relevant to the database community are identi ed. First is user-centered information analysis environments for correlation and manipulation of multimedia and complex information resources based on semantic content, visualizing complex and abstract information spaces, value-based ltering, and search, retrieval, and manipulation of multimedia and complex documents. Second is scalable, secure, and interoperable information repositories supporting a wide range of information resources and services. Issues to be addressed include: registration and security of information resources and services, access control and rights management, automatic classi cation and federation, and distributed service quality assurance facilities. Third is the intelligent integration of information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information management for genome level bioinformatics

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: norman w. paton , carole a. goble
",n
"LEFT id: NA
RIGHT id: 1918

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management for earth system science

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james frew , jeff dozier
",n
"LEFT id: NA
RIGHT id: 2001

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: gigascope : a stream database for network applications

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chuck cranor , theodore johnson , oliver spataschek , vladislav shkapenyuk
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 1800

LEFT text:  In this paper we present a second enhancement: a single operator that lets the analyst get summarized reasons for drops or increases observed at an aggregated level. This eliminates the need to manually drill-down for such reasons. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design a dynamic programming algorithm that requires only one pass of the data improving significantly over our initial greedy algorithm that required multiple passes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: caching multidimensional queries using chunks

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: prasad m. deshpande , karthikeyan ramasamy , amit shukla , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 380

LEFT text: We believe that the greatest growth potential for soft real-time databases is not as isolated monolithic databases but as components in open systems consisting of many heterogenous databases. In such environments, the flexibility to deal with unpredictable situations and the ability to cooperate with other databases (often non-real-time databases) is just as important as the guarantee of stringent timing constraints. In this paper, we describe a database designed explicitly for heterogeneous environments, the STanford Real-time Information Processor (STRIP). STRIP, which runs on standard Posix Unix, is a soft real-time main memory database with special facilities for importing and exporting data as well as handling derived data. We will describe the architecture of STRIP, its unique features, and its potential uses in overall system architectures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: overview of the stanford real-time information processor ( strip )

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: brad adelberg , ben kao , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 1748

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extracting entity profiles from semistructured information spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: robert a. nado , scott b. huffman
",n
"LEFT id: NA
RIGHT id: 482

LEFT text: The OOPSLA '97 Workshop on Experiences Using Object Data Management in the Real-World was held at the Cobb Galleria Centre in Atlanta, Georgia on Monday 6 October 1997. This report summarises some of the commercial case-study presentations made by workshop participants.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and management of data warehouses report on the dmdw '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stella gatziu , manfred jeusfeld , martin staudt , yannis vassiliou
",n
"LEFT id: NA
RIGHT id: 501

LEFT text: The 5th East European Conference ADBIS'2001 was organized by the Vilnius Gediminas Technical University, Institute of Mathematics and Informatics (Lithuania), Lithuanian Computer Society in cooperation with Moscow ACM SIGMOD Chapter and Law University of Lithuania in Vilnius, Lithuania, September 25-28, 2001. The call for papers attracted 82 submissions from 30 countries. The international program committee, consisting of 47 researchers from 21 countries, selected 25 papers for long presentations and 19 research communications for regular sessions. Additionally, 9 professional communications and reports have been selected for industrial sessions. The authors of accepted papers come from 29 countries, indicating the truly international recognition of the ADBIS conference series. The conference had 127 registered participants from 23 countries and included invited lectures, tutorials, regular sessions, and industrial sessions. This report describes the goals of the conference and summarizes the issues discussed during the sessions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on second international workshop on advanced issues of e-commerce and web-based information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1814

LEFT text: One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: devise : integrated querying and visual exploration of large datasets

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: m. livny , r. ramakrishnan , k. beyer , g. chen , d. donjerkovic , s. lawande , j. myllymaki , k. wenger
",n
"LEFT id: NA
RIGHT id: 315

LEFT text: Answering aggregate queries like SUM, COUNT, MIN, MAX, AVG in an approximate manner is often desirable when the exact answer is not needed or too costly to compute. We present an algorithm for answering such queries in multi-dimensional databases, using selective traversal of a Multi-Resolution Aggregate (MRA) tree structure storing point data. Our approach provides 100% intervals of confidence on the value of the aggregate and works iteratively, coming up with improving quality answers, until some error requirement is satisfied or time constraint as reached. Using the same technique we can also answer aggregate queries exactly and our experiments indicate that even for exact answering the proposed data structure and algorithm are very fast.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: progressive approximate aggregate queries with a multi-resolution tree structure

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: iosif lazaridis , sharad mehrotra
",y
"LEFT id: NA
RIGHT id: 2234

LEFT text: P2P computing has become an extremely popular topic in computer science. It affects diverse areas such as networking, distributed systems, information systems, algorithms and databases. The P2P paradigm introduces an architectural principle “replacing” the paradigm of client-server computing. It is based on the concepts of decentralization and resource sharing. By avoiding central bottlenecks and distributing workload it facilitates the deployment of applications at a global scale. The use of the P2P paradigm as a practical

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guest editor 's introduction

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",y
"LEFT id: NA
RIGHT id: 1771

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 715

LEFT text: A wide range of Web applications retrieve desired information from remote XML data sources across the Internet, which is usually costly due to transmission delays for large volumes of data. Therefore we propose to apply the ideas of semantic caching to XML query processing systems [2], in particular the XQuery engine. Semantic caching [3] implies view-based query answering and cache management. While it is well studied in the traditional database context, query containment for XQuery is left unexplored due to its complexity coming with the powerful expressiveness of hierarchy, recursion and result construction. We hence have developed the first solution for XQuery processing using cached views.We exploit the connections between XML and tree automata, and use subtype relations between two regular expression types to tackle the XQuery containment mapping problem. Inspired by XDuce [1], which explores the use of tree-automata-based regular expression types for XML processing, we have designed a containment mapping process to incorporate type inference and subtyping mechanisms provided by XDuce to establish containment mappings between regular-expression-type-based pattern variables of two queries. We have implemented a semantic caching system called XCache (see Figure 1), to realize the proposed containment and rewriting techniques for XQuery.The main modules of XCache include: (1) Query Decomposer. An input query is is decomposed into source-specific subqueries explicitly represented by matching patterns and return structures. (2) Query Pattern Register. By registering a few queries into semantic regions, we warm up XCache at its initialization phase. (3) Query Containment Mapper. The XDuce subtyper is incorporated into the containment mapper for establishing query containment mappings between variables of a new query and each cached query. (4) Query Rewriter. We implement the classical bucket algorithm and further apply heuristics to decide on an ""optimal"" rewriting plan if several valid ones exist. (5) Replacement Manager. We free space for new regions by both complete and partial replacement. (6) Region Coalescer. We apply a coalescing strategy to control the region granularity over time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xcache : a semantic caching system for xml queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: li chen , elke a. rundensteiner , song wang
",y
"LEFT id: NA
RIGHT id: 1078

LEFT text: Abstract.Data placement in shared-nothing database systems has been studied extensively in the past and various placement algorithms have been proposed. However, there is no consensus on the most efficient data placement algorithm and placement is still performed manually by a database administrator with periodic reorganization to correct mistakes. This paper presents the first comprehensive simulation study of data placement issues in a shared-nothing system. The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using taxonomy , discriminants , and signatures for navigating in text databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: soumen chakrabarti , byron dom , rakesh agrawal , prabhakar raghavan
",y
"LEFT id: NA
RIGHT id: 342

LEFT text: Unfortunately, there is very little money to support the health promotion initiatives in this plan. Health promotion receives very little of the $17 billion NIH research budget and few health promotion procedures are covered by the $400+ billion spent annually for Medicare and Medicaid. The Office of Health Promotion and Disease Prevention has a budget so small that very few health promotion professionals ever encounter this office directly during their careers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1279

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of navigation behaviour in web sites integrating multiple information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina berendt , myra spiliopoulou
",n
"LEFT id: NA
RIGHT id: 497

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tracing the lineage of view data in a warehousing environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: Our hypothesis is that keyphrases that are automatically extracted from documents can support this aim. We report on a user study that compared how accurately users categorized result documents on small screens when the document surrogates consisted of either keyphrases only, or document titles. We found no significant performance differences between the two conditions. In addition to these encouraging results, keyphrases have the benefit that they can be extracted and presented when no other document metadata can be identified.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 809

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 670

LEFT text: Due to organizational or operational constraints, the diverse data sources that an enterprise uses do not generally lend themselves to being fully replicated or completely consolidated under a single database, hence the increased demand for data interchange and for federated access to distributed sources. IBM has ongoing work in information integration technology that enables integrated, real-time access to traditional and emerging data sources, transforms information to meet the needs of business analysts, and manages data placement for performance, currency, and availability leading to fast, constant, and easy access for customer e-business solutions. IBM's Information Integration infrastructure today supports SQL—a mature, powerful query language—plus a number of SQL extensions in support of XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: supply chain infrastructures : system integration and information sharing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: michael o. ball , meng ma , louiqa raschid , zhengying zhao
",n
"LEFT id: NA
RIGHT id: 2144

LEFT text: The emergence and growing popularity of Internet-based electronic market-places, in their various forms, has raised the challenge to explore genericity in market design. In this paper we present a domain-specific software architecture that delineates the abstract components of a generic market and specifies control and data-flow constraints between them, and a framework that allows convenient pluggability of components that implement specific market policies. The framework was realized in the GEM system. GEM provides infrastructure services that allow market designers to focus solely on market-issues. In addition, it allows dynamic (re)configuration of components.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: oracle industrial exhibit

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: amy pogue
",n
"LEFT id: NA
RIGHT id: 49

LEFT text: Several organizations have developed very large market basket databases for the maintenance of customer transactions. New applications, e.g., Web recommendation systems, present the requirement for processing similarity queries in market basket databases. In this paper, we propose a novel scheme for similarity search queries in basket data. We develop a new representation method, which, in contrast to existing approaches, is proven to provide correct results. New algorithms are proposed for the processing of similarity queries. Extensive experimental results, for a variety of factors, illustrate the superiority of the proposed scheme over the state-of-the-art method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 2236

LEFT text: The North Carolina Department of Health and Human Services, through the Division of Public Health and the Division of Aging and Adult Services, has adopted an evidence-based self-management curriculum called Living Healthy in NC that uses peer-to-peer learning to improve the ability of persons to manage their diseases, including diabetes, and to prevent or slow the progression of chronic conditions. The program is based on Stanford University’s Chronic Disease Self-Management Program (CDSMP) and is being implemented in North Carolina through broad and diverse partnerships within and between multiple systems. Kate Lorig and colleagues at the Stanford Patient Education Research Center created and evaluated the CDSMP in the early 1990s, recognizing that physician care is only part of the disease-management process and that persons with chronic conditions must be good self-managers 24 hours a day, 7 days a week.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: peer-to-peer research at stanford

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: mayank bawa , brian f. cooper , arturo crespo , neil daswani , prasanna ganesan , hector garcia-molina , sepandar kamvar , sergio marti , mario schlosser , qi sun , patrick vinograd , beverly yang
",y
"LEFT id: NA
RIGHT id: 1998

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 1828

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: association rules over interval data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. miller , y. yang
",n
"LEFT id: NA
RIGHT id: 1663

LEFT text: Many database applications require the storage and manipulation of different versions of data objects. To satisfy the diverse needs of these applications, current database systems support versioning at a very low level. This article demonstrates that application-independent versioning can be supported at a significantly higher level. In particular, we extend the EXTRA data model and EXCESS query language so that configurations can be specified conceptually and non-procedurally. We also show how version sets can be viewed multidimensionally, thereby allowing configurations to be expressed at a higher level of abstraction. The resulting model integrates and generalizes ideas in CAD systems, CASE systems, and temporal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: implementation aspects of an object-oriented dbms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: asuman dogac , mehmet altinel , cetin ozkan , ilker durusoy
",n
"LEFT id: NA
RIGHT id: 473

LEFT text: Parallel implementations based on OpenMP or MapReduce also adopt the pruning policy and do not solve the problem thoroughly. In this context, taking into account features of document datasets, we propose 2Step-SSJ, which solves the document similarity self-join in CUDA environment on GPUs. 2Step-SSJ performs the similarity self-join in two steps, i.e., similarity computing on the inverted list and similarity computing on the forward list, which compromises between the memory visiting and dot-product computation. The experimental results show that 2Step-SSJ could solve the problem much faster than existing methods on three benchmark text corpora, achieving the speedup of 2x-23x against the state-of-the-art parallel algorithm in general, while keep a relatively stable running time with different values of the threshold.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: re-designing distance functions and distance-based applications for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 1030

LEFT text: In the relational model the order of fetching data does not affect query correctness. This flexibility is exploited in query optimization by statically reordering data accesses. However, once a query is optimized, it is executed in a fixed order in most systems, with the result that data requests are made in a fixed order. Only limited forms of runtime reordering can be provided by low-level device managers. More aggressive reordering strategies are essential in scenarios where the latency of access to data objects varies widely and dynamically, as in tertiary devices. This paper presents such a strategy. Our key innovation is to exploit dynamic reordering to match execution order to the optimal data fetch order, in all parts of the plan-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 959

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caprera : an activity framework for transaction processing on wide-area networks

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: suresh kumar , eng-kee kwang , divyakant agrawal
",y
"LEFT id: NA
RIGHT id: 968

LEFT text: Abstract. Association Rule Mining algorithms operate on a data matrix (e.g., customers  $times$ products) to derive association rules [AIS93b, SA96]. We propose a new paradigm, namely, Ratio Rules, which are quantifiable in that we can measure the “goodness” of a set of discovered rules. We also propose the “guessing error” as a measure of the “goodness”, that is, the root-mean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can “guess” the amount spent on butter. Thus, unlike association rules, Ratio Rules can perform a variety of important tasks such as forecasting, answering “what-if” scenarios, detecting outliers, and visualizing the data. Moreover, we show that we can compute Ratio Rules in a single pass over the data set with small memory requirements (a few small matrices), in contrast to association rule mining methods which require multiple passes and/or large memory. Experiments on several real data sets (e.g., basketball and baseball statistics, biological data) demonstrate that the proposed method: (a) leads to rules that make sense; (b) can find large itemsets in binary matrices, even in the presence of noise; and (c) consistently achieves a “guessing error” of up to 5 times less than using straightforward column averages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: ratio rules : a new paradigm for fast , quantifiable data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: flip korn , alexandros labrinidis , yannis kotidis , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 775

LEFT text: Real-world entities are inherently spatially and temporally referenced, and database applications increasingly exploit databases that record the past, present, and anticipatedfu tu locations of entities, e.g., the residences ofcuEERRx7 obtained by the geo-coding of addresses. Indices that efficiently suient quient on the spatio-temporal extents ofsuE entities are needed. However, past indexing research has progressed in largely separate spatial and temporal streams. Adding time dimensions to spatial indices, as if time were a spatial dimension, neither suther7 nor exploits the special properties of time. On the other hand, temporal indices are generally not amenable to extension with spatial dimensions. This paper proposes the first efficient and versatile index for a general class of spatio-temporal data: the discretely changing spatial aspect of an object may be a point or may have an extent; both transaction time and valid time are su;wkP-7y and a generalized notion of thecu;kx: time, now, is accommodated for both temporal dimensions. The index is based on the R # -tree and provides means of prioritizing space versu time, which enables it to adapt to spatially and temporally restrictivequ@;-P7 Performance experiments are reported that evalu-; pertinent aspects of the index.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: r-tree based indexing of now-relative bitemporal data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: rasa bliujute , christian s. jensen , simonas saltenis , giedrius slivinskas
",n
"LEFT id: NA
RIGHT id: 1663

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of round-trips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose the use of the context in which an object is loaded as a predictor of future accesses, where a context can be a stored collection of relationships, a query result, or a complex object. When an object O's state is loaded, similar state for other objects in O's context is prefetched. We present a design for maintaining context and for using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe several variations of the optimization: selectively applying the technique based on application and database characteristics, using application-supplied performance hints, using concurrent database queries to support asynchronous prefetch, prefetching across relationship paths, and delayed prefetch to save database round-trips.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: implementation aspects of an object-oriented dbms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: asuman dogac , mehmet altinel , cetin ozkan , ilker durusoy
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1067

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: watchman : a data warehouse intelligent cache manager

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter scheuermann , junho shim , radek vingralek
",n
"LEFT id: NA
RIGHT id: 301

LEFT text: When querying a temporal database, a user often makes certain semantic assumptions on stored temporal data. This paper formalizes and studies two types of semantic assumptions: point-based and interval-based. The point-based assumptions include those assumptions that use interpolation methods, while the interval-based assumptions include those that involve different temporal types (time granularities). Each assumption is viewed as a way to derive certain implicit data from the explicit data stored in the database. The database system must use all explicit as well as (possibly infinite) implicit data to answer user queries. This paper introduces a new method to facilitate such query evaluations. A user query is translated into a system query such that the answer of this system query over the explicit data is the same as that of the user query over the explicit and the implicit data. The paper gives such a translation procedure and studies the properties (safety in particular) of user queries and system queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptable query optimization and evaluation in temporal middleware

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 237

LEFT text: Multimedia databases usually deal with huge amounts of data and it is necessary to have an indexing structure such that efficient retrieval of data can be provided. R-Tree with its variations, is a commonly cited indexing method. In this paper we propose an improved nearest neighbor search algorithm on the R-tree and its variants. The improvement lies in the removal of two hueristics that have been used in previous R*-tree work, which we prove cannot improve on the pruning power during a search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: enhanced nearest neighbour search on the r-tree

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: king lum cheung , ada wai-chee fu
",y
"LEFT id: NA
RIGHT id: 1162

LEFT text: A data warehouse provides information for analytical processing, decision making and data mining tools. As the concept of real-time enterprise evolves, the synchronism between transactional data and data warehouses, statically implemented, has been redefined. Traditional data warehouse systems have static structures of their schemas and relationships between data, and therefore are not able to support any dynamics in their structure and content. Their data is only periodically updated because they are not prepared for continuous data integration. For real-time enterprises with needs in decision support purposes, real-time data warehouses seem to be very promising. In this paper we present a methodology on how to adapt data warehouse schemas and user-end OLAP queries for efficiently supporting real-time data integration. To accomplish this, we use techniques such as table structure replication and query predicate restrictions for selecting data, to enable continuously loading data in the data warehouse with minimum impact in query execution time. We demonstrate the efficiency of the method by analyzing its impact in query performance using benchmark TPC-H executing query workloads while simultaneously performing continuous data integration at various insertion time rates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source's performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants mechanism, which shows how semantic information about data sources may be used to discover cached query results of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",y
"LEFT id: NA
RIGHT id: 1649

LEFT text: Numerous proposals for extending the relational data model to incorporate the temporal dimension of data have appeared in the past several years. These proposals have differed considerably in the w...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 2267

LEFT text: Online alternative finance is rapidly growing worldwide while driving financial inclusion thanks to the growing body of data made available by the expansion of cyberspace and the advancement of machine learning and other technology. The epicenter of the growth is China, but online alternative finance is also continuing to grow in developed countries, such as the United States and the United Kingdom, through the creation of new market segments and a shift from some existing financial institutions. The sort of information used for loan screening in the case of online alternative finance is different from that used in the case of finance provided by existing financial institutions. There are even cases where credit history information is not used. Even when credit history information is used, the range of additional data used for loan screening is expanding to include information related to electronic commerce (EC) sites, in the case of corporate borrowers, and detailed information on personality and academic achievement and the history of residential moving, in the case of individual borrowers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an overview of repository technology

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: philip a. bernstein , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 1664

LEFT text: Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mosaico-a system for conceptual modeling and rapid prototyping of object-oriented database application

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. missikoff , m. toiati
",n
"LEFT id: NA
RIGHT id: 372

LEFT text:  The database systems have nowadays an increasingly important role in the knowledge-based society, in which computers have penetrated all fields of activity and the Internet tends to develop worldwide. In the current informatics context, the development of the applications with databases is the work of the specialists. Using databases, reach a database from various applications, and also some of related concepts, have become accessible to all categories of IT users. This paper aims to summarize the curricular area regarding the fundamental database systems issues, which are necessary in order to train specialists in economic informatics higher education. The database systems integrate and interfere with several informatics technologies and therefore are more difficult to understand and use. Thus, students should know already a set of minimum, mandatory concepts and their practical implementation: computer systems, programming techniques, programming languages, data structures. The article also presents the actual trends in the evolution of the database systems, in the context of economic informatics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: New types of data processing applications are no longer satisfied with the capabilities offered by the relational data model. One example of this phenomenon is the growing use of the Internet as a source of data. The data on the Internet is inherently non-relational. As a result, demand developed for database management systems natively built on advanced data models. The semantic binary data model (Rishe, 1992), satisfies the criteria for the models required for today’s applications by providing the ability to build rich schemas with arbitrarily flexible relationships between objects. In this paper, we discuss a new design for a semantic database management system which is based on the semantic binary data model. Our challenge was to design and implement a database engine which, while being native to the model, is reasonably efficient on a wide variety of industrial applications, and which surpasses relational systems in performance and flexibility on those applications that require non-relational modelling. Special attention is given to multi-platform support by the semantic database engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 1195

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing and optimization in oracle rdb

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: gennady antoshenkov , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 1191

LEFT text: Efficient user-adaptable similarity search more and more increases in its importance for multimedia and spatial database systems. As a general similarity model for multi-dimensional vectors that is adaptable to application requirements and user preferences, we use quadratic form distance functions which have been successfully applied to color histograms in image databases [Fal+ 94]. The components aij of the matrix A denote similarity of the components i and j of the vectors. Beyond the Euclidean distance which produces spherical query ranges, the similarity distance defines a new query type, the ellipsoid query. We present new algorithms to efficiently support ellipsoid query processing for various user-defined similarity matrices on existing precomputed indexes. By adapting techniques for reducing the dimensionality and employing a multi-step query processing architecture, the method is extended to high-dimensional data spaces. In particular, from our algorithm to reduce the similarity matrix, we obtain the greatest lowerbounding similarity function thus guaranteeing no false drops. We implemented our algorithms in C++ and tested them on an image database containing 12,000 color histograms. The experiments demonstrate the flexibility of our method in conjunction with a high selectivity and efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient similarity search for market basket data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexandros nanopoulos , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 334

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1967

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the dangers of replication and a solution

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jim gray , pat helland , patrick o'neil , dennis shasha
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 622

LEFT text: The Grid is an emerging platform to support on-demand ""virtual organisations"" for coordinated resource sharing and problem solving on a global scale. The application thrust is large-scale scientific endeavour, and the scale and complexity of scientific data presents challenges for databases. The Grid is beginning to exploit technologies developed for Web Services and to realise its potential it also stands to benefit from Semantic Web technologies; conversely, the Grid and its scientific users provide application pull which will benefit the Semantic Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the &#961; operator : discovering and ranking associations on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kemafor anyanwu , amit sheth
",n
"LEFT id: NA
RIGHT id: 1954

LEFT text: Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql query optimization : reordering for a general class of queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 152

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the araneus web-based management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. mecca , p. atzeni , a. masci , g. sindoni , p. merialdo
",n
"LEFT id: NA
RIGHT id: 1427

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 1416

LEFT text: There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sit-in : a real-life spatio-temporal information system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giuseppe sindoni , leonardo tininini , amedea ambrosetti , cristina bedeschi , stefano de francisci , orietta gargano , rossella molinaro , mario paolucci , paola patteri , pina ticca
",n
"LEFT id: NA
RIGHT id: 2075

LEFT text: Numerous proposals for extending the relational data model to incorporate the temporal dimension of data have appeared in the past several years. These proposals have differed considerably in the w...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the computation of relational view complements

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jens lechtenb &#246; rger , gottfried vossen
",n
"LEFT id: NA
RIGHT id: 1250

LEFT text: Many commercial database systems use some form of statistics, typically histograms, to summarize the contents of relations and permit efficient estimation of required quantities. While there has been considerable work done on identifying good histograms for the estimation of query-result sizes, little attention has been paid to the estimation of the data distribution of the result, which is of importance in query optimization. In this paper, we prove that the optimal histogram for estimating the size of the result of a join operator is optimal for estimating its data distribution as well. We also study the effectiveness of these optimal histograms in the context of an important application that requires estimates for the data distribution of a query result: load-balancing for parallel Hybrid hash joins. We derive a cost formula to capture the effect of data skew in both the input and output relations on the load and use the optimal histograms to estimate this cost most accurately. We have developed and implemented a load balancing algorithm using these histograms on a simulator for the Gamma parallel database system. The experiments establish the superiority of this approach compared to earlier ones in handling all kinds and levels of skew while incurring negligible overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data partitioning and load balancing in parallel disk systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter scheuermann , gerhard weikum , peter zabback
",n
"LEFT id: NA
RIGHT id: 279

LEFT text: We describe a system that supports arbitrarily complex SQL queries with ”uncertain” predicates. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is query evaluation. We describe an optimization algorithm that can compute eciently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any ecient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: orthogonal optimization of subqueries and aggregation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , milind joshi
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: I'm happy to be able to share with you the following three reminiscences. I continue to invite unsolicited contributions. See http://www.acm.org/sigmod/record/author.html for submission guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: In this paper, we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implemanting these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend that strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator TUR, resulting in a set of fixpoints for UR. The monotionicity of the operator is maintained nby explicitly representing the effect of asserting and retracting tuples in the database. A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relaions, i.e., those relations which are not update by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: We demonstrate a visual based XML-Relational database system where XML data is managed by commercial RDBMS. A query interface enables users to form path expression based queries against stored data visually. Statistics about data and a special path directory are used to rewrite path expression based queries into efficient SQL statements involving less number of joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1965

LEFT text: The construction of high-performance database systems that combine the best aspects of the relational and object-oriented approaches requires the design of client-server architectures that can fully exploit client and server resources in a flexible manner. The two predominant paradigms for client-server query execution are data-shipping and query-shipping We first define these policies in terms of the restrictions they place on operator site selection during query optimization. We then investigate the performance tradeoffs between them for bulk query processing. While each strategy has advantages, neither one on its own is efficient across a wide range of circumstances. We describe and evaluate a more flexible policy called hybrid-shipping, which can execute queries at clients, servers, or any combination of the two. Hybrid-shipping is shown to at least match the best of the two ""pure"" policies, and in some situations, to perform better than both. The implementation of hybrid-shipping raises a number of difficult problems for query optimization. We describe an initial investigation into the use of a 2-step query optimization strategy as a way of addressing these issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: performance tradeoffs for client-server query processing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. franklin , bj &#246; rn th &#243; r j &#243; nsson , donald kossmann
",y
"LEFT id: NA
RIGHT id: 104

LEFT text: In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2146

LEFT text: Classification is an important data mining problem. Although classification is a wellstudied problem, most of the current classification algorithms require that all or a portion of the the entire dataset remain permanently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algorithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data mining.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dms : a parallel data mining server

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: felicity george
",y
"LEFT id: NA
RIGHT id: 1945

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1613

LEFT text: Digital libraries bring about the integration, management, and communication of gigabytes of multimedia data in a distributed environment. Digital library systems currently envision users as being static when they access information. But it is expected in the near future that tens of millions of users will have access to a digital library through wireless access. Providing digital library services to users whose location is constantly changing, whose network connections are through a wireless medium, and whose computing power is low necessitates modifications to existing digital library systems. In this paper, we identify the issues that arise when users are mobile, classify queries that are specific to mobile users and introduce an architecture that supports flexible and transparent access to digital libraries for mobile users. The main features of the architecture include a layered data representation, support of adaptability, dual broadcast and on demand querying, caching, and mobile-specific user interfaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: view maintenance in mobile computing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ouri wolfson , prasad sistla , son dao , kailash narayanan , ramya raj
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 582

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: versioning and configuration management in an object-oriented data model

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore
",n
"LEFT id: NA
RIGHT id: 906

LEFT text: Everyone knows the small-world phenomenon: soon after meeting a stranger, we are surprised to discover that we have a mutual friend, or we are connected through a short chain of acquaintances. In his book, Duncan Watts uses this intriguing phenomenon--colloquially called ""six degrees of separation""--as a prelude to a more general exploration: under what conditions can a small world arise in any kind of network?The networks of this story are everywhere: the brain is a network of neurons; organisations are people networks; the global economy is a network of national economies, which are networks of markets, which are in turn networks of interacting producers and consumers. Food webs, ecosystems, and the Internet can all be represented as networks, as can strategies for solving a problem, topics in a conversation, and even words in a language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: small worlds : the dynamics of networks between order and randomness

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jie wu , duncan j. watts
",y
"LEFT id: NA
RIGHT id: 899

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in information managment at dublin city university

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mark roantree , alan f. smeaton
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 2152

LEFT text: The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: Deduplication, a key operation in integrating data from multiple sources, is a time-consuming, labor-intensive and domain-specific operation. We present our design of ALIAS that uses a novel approach to ease this task by limiting the manual effort to inputing simple, domain-specific attribute similarity functions and interactively labeling a small number of record pairs. We describe how active learning is useful in selecting informative examples of duplicates and nonduplicates that can be used to train a deduplication function. ALIAS provides mechanism for efficiently applying the function on large lists of records using a novel cluster-based execution model.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Environmental Management Information Systems (EMIS) are socio-technological systems used as business applications to gather, process, and provide environmental information, inside companies and in exchange with other actors in industry. They help to identify environmental impacts and support measures avoiding these impacts or reducing them. EMIS provide the necessary information support for decision making in companies. Hence, EMIS can be viewed as certain Information Systems (IS) usually implemented in companies as a part of their Environmental Management Systems (EMS). In order to give a tangible example with practical implications, the developments that EMIS have passed the last years are described along the field of “online communication and sustainability reporting” and illustrated by a case study. This area represents an emerging digital and fully ICT (information and communication technologies) supported approach within EMIS, using current internet technologies and services. It makes clear the array of capabilities of latest EMIS to be exploited for the improvement of advanced environmental and sustainability management, finally to the benefit for companies and their various stakeholders. The case study describes the concept and implementation of a software tool with shopping cart functionality providing sustainability reports a la carte so that stakeholders (i.e. users, readers) can create their own report on the fly, exactly meeting their detailed information needs and preferred media out from a single publishing database. This software tool which represents a module of a comprehensive EMIS is implemented as a web-based ICT application. Its performance goes beyond the leadingedge approach of O2 who provides a personalized reporting feature on its website that could be regarded as best practice and pioneering effort in sustainability online reporting, so far.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 320

LEFT text: Database management is one of the main areas of research of the School of Computer Science at The University of Oklahoma (OU). The objective of the database research team at OU (OUDB) is to help solve the many issues and challenges facing the database research community, especially with respect to emerging technology. Currently, many projects are being conducted in the following areas: real-time databases, object-oriented databases, mobile databases, multimedia databases, data mining and data warehouses. These projects have been funded by federal and state agencies as well as private industries such as National Science Foundation, the U.S. Department of Education, Oklahoma State Department of Environmental Quality, and Objectivity, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the indian institute of technology , bombay

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 590

LEFT text: The principal objective of this paper has been to suppress this drawback while conserving the strong correctness of 2LSR executions We propose defining precisely the notion of value dependencies, and managing them so as not to impose the LDP property.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 599

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: The world today is characterised by the proliferation of information sources available through media such as the WWW, databases, semi-structured files (e.g. XML documents), etc. Nevertheless, this information is usually scattered, heterogeneous and weakly structured, so it is difficult to process it automatically. DENODO Corporation has developed a mediator system for the construction of semi-structured and structured data integration applications. This system has already been used in the construction of several applications on the Internet and in corporate environments, which are currently deployed at several important Internet audience sites and large sized business corporations. In this extended abstract, we present an overview of the system and we put forward some conclusions arising from our experience in building real-world data integration applications, focusing in some challenges we believe require more attention from the research community.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 2001

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: gigascope : a stream database for network applications

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chuck cranor , theodore johnson , oliver spataschek , vladislav shkapenyuk
",n
"LEFT id: NA
RIGHT id: 462

LEFT text: The size of The Boeing Company posts some stringent requirements on data warehouse design and implementation. We summarize four interesting and challenging issues in developing very large scale data warehouses, namely failure recovery, incremental update maintenance, cost model for schema design and query optimization, and metadata definition and management. For each issue, we give the reasons we think it is important but not well-addressed in research literature and commercial products, and our current research to solve it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: knowledge discovery in data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: themistoklis palpanas
",n
"LEFT id: NA
RIGHT id: 1660

LEFT text: An electronic dictionary system (EDS) is developed with object-oriented database techniques based on ObjectStore. The EDS is composed of two parts: the Database Building Program (DBP), and the Database Querying Program (DQP). DBP reads in a dictionary encoded in SGML tags, and builds a database composed of a collection of trees which holds dictionary entries, and several lists which contain items of various lexical categories. With text exchangeability introduced by the SGML, DBP is able to accommodate dictionaries of different languages with different structures, after easy modification of a configuration file. The tree model, the Category Lists, and an optimization procedure enables DQP to quickly accomplish complicated queries, including context requirements, via simple SQL-like syntax and straightforward search methods. Results show that compared with relational database, DQP enjoys much higher speed and flexibility.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: application of oodb and sgml techniques in text database : an electronic dictionary system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jian zhang
",y
"LEFT id: NA
RIGHT id: 342

LEFT text: A number of researchers have become interested in the design of global-scale networked systems and applications. Our thesis here is that the database community's principles and technologies have an important role to play in the design of these systems. The point of departure is at the roots of database research: we generalize Codd's notion of data independence to physical environments beyond storage systems. We note analogies between the development of database indexes and the new generation of structured peer-to-peer networks. We illustrate the emergence of data independence in networks by surveying a number of recent network facilities and applications, seen through a database lens. We present a sampling of database query processing techniques that can contribute in this arena, and discuss methods for adoption of these technologies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 418

LEFT text: Decision support applications are growing in popularity as more business data is kept on-line. Such applications typically include complex SQL queries that can test a query optimizer's ability to produce an efficient access plan. Many access plan strategies exploit the physical ordering of data provided by indexes or sorting. Sorting is an expensive operation, however. Therefore, it is imperative that sorting is optimized in some way or avoided all together. Toward that goal, this paper describes novel optimization techniques for pushing down sorts in joins, minimizing the number of sorting columns, and detecting when sorting can be avoided because of predicates, keys, or indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the onion technique : indexing for linear optimization queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yuan-chi chang , lawrence bergman , vittorio castelli , chung-sheng li , ming-ling lo , john r. smith
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 1349

LEFT text: E-business sites are increasingly utilizing dynamic web pages since they enable a much wider range of interaction than static HTML pages can provide. Dynamic page generation technologies allow a Web site to generate pages at run-time, based on various parameters. Delaying content decisions until run-time a ords a Web site signi cant exibility in customizing page content, thereby enriching users' Web experiences. At the same time, however, dynamic page generation technologies have resulted in serious performance problems due to the increased load placed on the server-side infrastructure. Consequently, end users experience increased response times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a comparative study of alternative middle tier caching solutions to support dynamic web content acceleration

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anindya datta , kaushik dutta , helen m. thomas , debra e. vandermeer , krithi ramamritham , dan fishman
",y
"LEFT id: NA
RIGHT id: 1254

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 540

LEFT text: The widespread distribution and availability of small-scale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.In this paper, we introduce the Cougar approach to tasking sensor networks through declarative queries. Given a user query, a query optimizer generates an efficient query plan for in-network query processing, which can vastly reduce resource usage and thus extend the lifetime of a sensor network. In addition, since queries are asked in a declarative language, the user is shielded from the physical characteristics of the network. We give a short overview of sensor networks, propose a natural architecture for a data management system for sensor networks, and describe open research problems in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the cougar approach to in-network query processing in sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong yao , johannes gehrke
",y
"LEFT id: NA
RIGHT id: 304

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 1896

LEFT text: The Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS 2000) was held at the Crowne Plaza San Jose/Silicon Valley in Milpitas, California on June 8-9, 2000. The purpose of this workshop was to bring together leading practitioners, developers and researchers to explore the challenging technical issues and find feasible solutions for advancing the current state of the art in e-commerce and web-based information systems. In particular, the workshop was interested in the infrastructure issues to facilitate e-commerce and Web-based information systems.WECWIS 2000 was successful. There were three invited talks, one industrial panel discussion and six technical sessions. The keynote speech, ""The global trading web: A strategic vision for the Internet economy,"" was delivered by Dr. Jay M. Tenebaum, VP and Chief Scientist, Commerce One, Inc., on June 8 immediately following the opening remarks by the conference chair. The banquet address, ""Business issues in e-commerce,"" was delivered by Mr. Daniel Druker, General Manager, Hyperion e-Business Division. Finally, a lunch address, ""B2C, B2B, N2N, N2M: Why 2 is so instrumental?"" was delivered by Mr. Mstafa A. Syed, VP of Technology, VertialNet, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 26

LEFT text: In large data recording and warehousing environments, it is often advantageous to provide fast, approximate answers to queries, whenever possible. Before DBMSs providing highly-accurate approximate answers can become a reality, many new techniques for summarizing data and for estimating answers from summarized data must be developed. This paper introduces two new sampling-based summary statistics, concise samples and counting samples, and presents new techniques for their fast incremental maintenance regardless of the data distribution. We quantify their advantages over standard sample views in terms of the number of additional sample points for the same view size, and hence in providing more accurate query answers. Finally, we consider their application to providing fast approximate answers to hot list queries. Our algorithms maintain their accuracy in the presence of ongoing insertions to the data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the aqua approximate query answering system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1151

LEFT text: Connectivity products are finally available to provide the “highways” between computers containing data. IBM has provided strong validation of the concept with their “Information Warehouse.” DBMS vendors are providing gateways into their products, and SQL is being retrofitted on many older DBMSs to make it easier to access data from standard 4GL products and application development systems. The next step needed for data integration is to provide (1) a common data dictionary with a conceptual schema across the data to mask the many differences that occur when databases are developed independently and (2) a server that can access and integrate the databases using information from the data dictionary. In this article, we discuss InterViso, one of the first commercial federated database products.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: practical issues with commercial use of federated databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jim kleewein
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: The wonderfully clean and beautiful scheme put ""on its head"" the world of query optimization I had assumed was the only one possible. In fact, this paper is all about questioning implicit assumptions behind classic query optimization. Is it always true that query-evaluation performance does not fluctuate during query execution?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: Active databases and real-time databases have been important areas of research in the recent past. It has been recognized that many benefits can be gained by integrating active and real-time database technologies. However, there has not been much work done in the area of transaction processing in active real-time databases. This paper deals with an important aspect of transaction processing in active real-time databases, namely the problem of assigning priorities to transactions. In these systems, time-constrained transactions trigger other transactions during their execution. We present three policies for assigning priorities to parent, immediate and deferred transactions executing on a multiprocessor system and then evaluate the policies through simulation. The policies use different amounts of semantic information about transactions to assign the priorities. The simulator has been validated against the results of earlier published studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",n
"LEFT id: NA
RIGHT id: 1704

LEFT text: Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database research group at eth zurich

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: moira c. norrie , stephen m. blott , hans-j &#246; rg schek , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 581

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the glue-nail deductive database system : design , implementation , and evaluation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marcia a. derr , shinichi morishita , geoffrey phipps
",n
"LEFT id: NA
RIGHT id: 578

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the coral deductive system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raghu ramakrishnan , divesh srivastava , s. sudarshan , praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1047

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering queries with aggregation using views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: divesh srivastava , shaul dar , h. v. jagadish , alon y. levy
",n
"LEFT id: NA
RIGHT id: 783

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aqua : a fast decision support systems using approximate query answers

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1613

LEFT text: Materialized views in data warehouses are maintained incrementally, for reasons of efficiency, to present the latest updates to the users. These views are used by many warehouse readers (users) to execute OLAP queries by running several reader sessions and these views are maintained periodically by maintenance transactions. Therefore, there is an inherent problem of maintaining these views while the reader sessions continue to receive consistent data from these views. In this paper, we discuss a method that allows warehouse maintenance transactions to run concurrently with the reader sessions. Concurrency allows the readers to read the data from the views while the maintenance transaction updates these views. In our proposed method we create additional versions of views dynamically that contain only the modified tuples of the views and provide a mechanism to collapse these versions into the views periodically when there are no reader sessions accessing the views. These versions allow the reader sessions to access the old and the new information. The collapsing of the views is done by a low-priority process executing periodically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: view maintenance in mobile computing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ouri wolfson , prasad sistla , son dao , kailash narayanan , ramya raj
",n
"LEFT id: NA
RIGHT id: 488

LEFT text: MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mlpq/gis constraint database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang
",y
"LEFT id: NA
RIGHT id: 859

LEFT text: We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: research directions in biodiversity informatics

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: john l. schnase
",n
"LEFT id: NA
RIGHT id: 995

LEFT text: The project MENTAS (Motor Development Assistant) -aims at realizing an interconnected, engineer-oriented development environment for a faster conception and comparative analysis of motors. In order to reach this goal, an integrated access to multi-vendor DBs is provided. In our exhibition we demonstrate how the interconnection of heterogeneous DBs in MENTAS works. After having analyzed the data models of each such DBs, we have brought the heterogeneous schemas into a global, virtual one, which contains just the data relevant for MENTAS. Finally, we apply a commercially available DB middleware solution to bridge the diverse ontologies and hence to cope with these heterogeneous schemas. Furthermore, we have designed a very friendly GUI in Java by means of which users are guided in the process of formulating SQL queries. We show how this interface allows users to issue SQL statements against any DBs incorporated in the federation, to navigate through heterogeneous DBs, and most importantly, to join and compare data in the DB federation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bridging heterogeneity : research and practice of database middleware technology

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: fernando de ferreira rezende , g &#252; nter sauter
",y
"LEFT id: NA
RIGHT id: 1408

LEFT text: Traditional indexes aim at optimizing the node accesses during query processing, which, however, does not necessarily minimize the total cost due to the possibly large number of random accesses. In this paper, we propose a general framework for adaptive indexes that improve overall query cost. The performance gain is achieved by allowing index nodes to contain a variable number of disk pages. Update algorithms dynamically re-structure adaptive indexes depending on the data and query characteristics. Extensive experiments show that adaptive B- and R-trees significantly outperform their conventional counterparts, while incurring minimal update overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1605

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at national technical university of athens ( ntua )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate national technical univ. of athens
",n
"LEFT id: NA
RIGHT id: 339

LEFT text: In this article we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time, DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently or can be bundled within a multiquery expression. In the latter case, we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a case for dynamic view management

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",y
"LEFT id: NA
RIGHT id: 1199

LEFT text: We propose a novel index structure, the A-tree (approximation tree), for similarity searches in high-dimensional data. The basic idea of the A-tree is the introduction of virtual bounding rectangles (VBRs) which contain and approximate MBRs or data objects. VBRs can be represented quite compactly and thus affect the tree configuration both quantitatively and qualitatively. First, since tree nodes can contain a large number of VBR entries, fanout becomes large, which increases search speed. More importantly, we have a free hand in arranging MBRs and VBRs in the tree nodes. Each A-tree node contains an MBR and its children VBRs. Therefore, by fetching an A-tree node, we can obtain information on the exact position of a parent MBR and the approximate position of its children. We have performed experiments using both synthetic and real data sets. For the real data sets, the A-tree outperforms the SR-tree and the VA-file in all dimensionalities up to 64 dimensions, which is the highest dimension in our experiments. Additionally, we propose a cost model for the A-tree. We verify the validity of the cost model for synthetic and real data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: indexing of now-relative spatio-bitemporal data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: simonas &#352; altenis , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1693

LEFT text: Recently, Haas and Hellerstein proposed the hash ripple join algorithm in the context of online aggregation. Although the algorithm rapidly gives a good estimate for many join-aggregate problem instances, the convergence can be slow if the number of tuples that satisfy the join predicate is small or if there are many groups in the output. Furthermore, if memory overflows (for example, because the user allows the algorithm to run to completion for an exact answer), the algorithm degenerates to block ripple join and performance suffers. In this paper, we build on the work of Haas and Hellerstein and propose a new algorithm that (a) combines parallelism with sampling to speed convergence, and (b) maintains good performance in the presence of memory overflow. Results from a prototype implementation in a parallel DBMS show that its rate of convergence scales with the number of processors, and that when allowed to run to completion, even in the presence of memory overflow, it is competitive with the traditional parallel hybrid hash join algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a new join algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dong keun shin , arnold charles meltzer
",n
"LEFT id: NA
RIGHT id: 1410

LEFT text: Abstract. Our aim is to develop new database technologies for the approximate matching of unstructured string data using indexes. We explore the potential of the suffix tree data structure in this context. We present a new method of building suffix trees, allowing us to build trees in excess of RAM size, which has hitherto not been possible. We show that this method performs in practice as well as the O(n) method of Ukkonen [70]. Using this method we build indexes for 200 Mb of protein and 300 Mbp of DNA, whose disk-image exceeds the available RAM. We show experimentally that suffix trees can be effectively used in approximate string matching with biological data. For a range of query lengths and error bounds the suffix tree reduces the size of the unoptimised O(mn) dynamic programming calculation required in the evaluation of string similarity, and the gain from indexing increases with index size. In the indexes we built this reduction is significant, and less than 0.3% of the expected matrix is evaluated. We detail the requirements for further database and algorithmic research to support efficient use of large suffix indexes in biological applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database index to large biological sequences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ela hunt , malcolm p. atkinson , robert w. irving
",n
"LEFT id: NA
RIGHT id: 658

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: databases and transaction processing : an application-oriented approach

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: philip m. lewis , arthur bernstein , michael kifer
",n
"LEFT id: NA
RIGHT id: 110

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distance browsing in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",y
"LEFT id: NA
RIGHT id: 1146

LEFT text: MineSetTM is a highly integrated suite of client-server tools for the high-end mining and visualization of very large enterprise databases. MineSet represents the confluence of several important software and hardware technologies: data mining algorithms, fast multiprocessing database servers, novel techniques for interactive 3-D data visualization, and powerful graphics workstations. MineSet provides integrated facilities for the extraction of data from varied sources, algorithms for mining the extracted data, and tools for the 3-D visualization of results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",y
"LEFT id: NA
RIGHT id: 344

LEFT text: XML has established itself over-the recent years as THE standard for representing data in scientific and business applications. Starting out as a standard data exchange format for the Web, it has become instrumental in the development of electronic commerce applications and online information services, and draws in its tailwind a multitude of standardization efforts for all kinds of applications. Documents are not only used for representing multimedia information content but also for many other purposes, like the representation of meta-information and the specification of component interfaces, protocols, and processes. As a consequence, the amount of XML data being stored and processed is large and will be increasing at an astonishing rate. This has caused XML data management to become a focus of research efforts in the database conmmnity. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce : guest editor 's introduction

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 955

LEFT text: Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically diffcult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT { Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT's outperform previous data structures in a number of applications. Keywords { near neighbor, metric space, approximate queries, data mining, Dirichlet domains, Voronoi regions

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: m-tree : an efficient access method for similarity search in metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo ciaccia , marco patella , pavel zezula
",n
"LEFT id: NA
RIGHT id: 514

LEFT text: Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: clustering validity checking methods : part ii

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: maria halkidi , yannis batistakis , michalis vazirgiannis
",y
"LEFT id: NA
RIGHT id: 501

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on second international workshop on advanced issues of e-commerce and web-based information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 345

LEFT text: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the design and performance evaluation of alternative xml storage strategies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: feng tian , david j. dewitt , jianjun chen , chun zhang
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be ""10% of married people between age 50 and 60 have at least 2 cars"". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a ""greater-than-expected-value"" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 2061

LEFT text: We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive filters for continuous queries over distributed data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chris olston , jing jiang , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1543

LEFT text: TPC Benchmark&trade; C (TPC-C) is the modern standard for measuring OLTP performance. Running TPC-C, Tandem demonstrated a massively parallel configuration of 112 CPUs which achieved ten times higher performance than any other system previously measured (and today is still better by a factor of five). This result qualifies as the largest industry-standard benchmark ever run.This paper briefly describes how the benchmark was configured and the results which were obtained.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: order-of-magnitude advantage on tpc-c through massive parallelism

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: charles levine
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 318

LEFT text: This paper presents a simulation study of a video-on-demand system. We present video server algorithms for real-time disk scheduling, prefetching, and buffer pool management. The performance of these algorithms is compared against the performance of simpler algorithms such as elevator and round-robin disk scheduling and global LRU buffer pool management. Finally, we show that the SPIFFI video-on-demand system scales nearly linearly as the number of disks, videos, and terminals is increased.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic buffer allocation in video-on-demand systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sang-ho lee , kyu-young whang , yang-sae moon , il-yeol song
",n
"LEFT id: NA
RIGHT id: 722

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: web caching for database applications with oracle web cache

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jesse anton , lawrence jacobs , xiang liu , jordan parker , zheng zeng , tie zhong
",n
"LEFT id: NA
RIGHT id: 462

LEFT text: Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention, because the physical plans---unaware of each other---compete for access to the underlying I/O and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly when multiple complex queries run at the same time.    We describe an augmentation of traditional query engines that improves join throughput in large-scale concurrent data warehouses. In contrast to the conventional query-at-a-time model, our approach employs a single physical plan that can share I/O, computation, and tuple storage across all in-flight join queries. We use an ""always-on"" pipeline of non-blocking operators, coupled with a controller that continuously examines the current query mix and performs run-time optimizations. Our design allows the query engine to scale gracefully to large data sets, provide predictable execution times, and reduce contention. In our empirical evaluation, we found that our prototype outperforms conventional commercial systems by an order of magnitude for tens to hundreds of concurrent queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: knowledge discovery in data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: themistoklis palpanas
",n
"LEFT id: NA
RIGHT id: 647

LEFT text: We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: continuously adaptive continuous queries over streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , mehul shah , joseph m. hellerstein , vijayshankar raman
",y
"LEFT id: NA
RIGHT id: 2009

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qxtract : a building block for efficient information extraction from text databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: eugene agichtein , luis gravano
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: In this paper, we describe a novel Web query processing approach with learning capabilities. Under this approach, user queries are in the form of keywords and search engines are employed to find URLs of Web sites that might contain the required information. The first few URLs are presented to the user for browsing. Meanwhile, the query processor learns both the information required by the user and the way that the user navigates through hyperlinks to locate such information. With the learned knowledge, it processes the rest URLs and produces precise query results in the form of segments of Web pages without user involvement. The preliminary experimental results indicate that the approach can process a range of Web queries with satisfactory performance. The architecture of such a query processor, techniques of modeling HTML pages, and knowledge for query processing are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 391

LEFT text: EAEcient support for set-valued attributes is likely to grow in importance as object-relational database systems, which either support set-valued attributes or propose to do so soon, begin to replace their purely relational predecessors. One of the most interesting and challenging operations on set-valued attributes is the set containment join, because it provides a concise and elegant way to express otherwise complex queries. Unfortunately, evaluating these joins is diAEcult, and naive approaches lead to algorithms that are very expensive. In this paper, we develop a new partition based algorithm for set containment joins: the Partitioning Set Join Algorithm (PSJ), which uses a replicating multilevel partitioning scheme based on a combination of set elements and signatures. We present a detailed performance study with a complete implementation in the Paradise object-relational database system. Our results show that PSJ outperforms previously proposed set join algorithms over a wide range of data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management in ecommerce ( tutorial session ) : the good , the bad , and the ugly

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: avigdor gal
",n
"LEFT id: NA
RIGHT id: 1044

LEFT text: To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the x-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 2146

LEFT text:  Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dms : a parallel data mining server

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: felicity george
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: The OASIS Prototype is under development at Dublin City University in Ireland. We describe a multi-database architecture which uses the ODMG model as a canonical model and describe an extention for construction of virtual schemas within the multidatabase system. The OMG model is used to provide a standard distribution layer for data from local databases. This takes the form of CORBA objects representing export schemas from separate data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 672

LEFT text: Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: clustering by pattern similarity in large data sets

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: haixun wang , wei wang , jiong yang , philip s. yu
",y
"LEFT id: NA
RIGHT id: 1389

LEFT text: Personalization is here defined as a process of changing a system to increase its personal relevance. This may have a work or social motivation. A taxonomy of motivations is developed and illustrated by application to mobile phones and e-commerce Web pages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: demonstration : enabling scalable online personalization on the web

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik dutta , anindya datta , debra e. vandermeer , krithi ramamritham , helen m. thomas
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 676

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation for spatio-temporal queries to moving objects

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong-jin choi , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: In this paper, we first focus our attention on the question of how much space remains for performance improvement over current association rule mining algorithms. Our strategy is to compare their performance against an “Oracle algorithm” that knows in advance the identities of all frequent itemsets in the database and only needs to gather their actual supports to complete the mining process. Our experimental results show that current mining algorithms do not perform uniformly well with respect to the Oracle for all database characteristics and support thresholds. In many cases there is a substantial gap between the Oracle’s performance and that of the current mining algorithms. Second, we present a new mining algorithm, called ARMOR, that is constructed by making minimal changes to the Oracle algorithm. ARMOR consistently performs within a factor of two of the Oracle on both real and synthetic datasets over practical ranges of support specifications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 1749

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating dynamically-fetched external information into a dbms for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1774

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the second ieee metadata conference ( metadata ' 97 )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: ron musick , chris miller
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 422

LEFT text: A spatial distance join is a relatively new type of operation introduced for spatial and multimedia database applications. Additional requirements for ranking and stopping cardinality are often combined with the spatial distance join in on-line query processing or internet search environments. These requirements pose new challenges as well as opportunities for more efficient processing of spatial distance join queries. In this paper, we first present an efficient k-distance join algorithm that uses spatial indexes such as R-trees. Bi-directional node expansion and plane-sweeping techniques are used for fast pruning of distant pairs, and the plane-sweeping is further optimized by novel strategies for selecting a sweeping axis and direction.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive multi-stage distance join processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hyoseop shin , bongki moon , sukho lee
",y
"LEFT id: NA
RIGHT id: 99

LEFT text: Building a data-intensive web site is a complex task. Ad hoc rapid prototyping approaches easily lead to unsatisfactory results, e.g. poor maintainability and extensibility. To address this problem, a number of model-based approaches have been proposed, which attempt to simplify the design and development of data-intensive web sites. However, these approaches typically lack expressive meta-models and, as a result, suffer from a number of limitations, e.g. the lack of appropriate support for the creation of complex user interfaces, for the specification of layouts and presentation styles, and for customization. In this paper we describe a new software tool OntoWeaver, which uses ontologies to drive the design and development of data-intensive web sites. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design principles for data-intensive web sites

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 1659

LEFT text: Galax is a light-weight, portable, open-source implementation of XQuery 1.0. Started in December 2000 as a small prototype designed to test the XQuery static type system, Galax has now become a solid implementation, aiming at full conformance with the family of XQuery 1.0 specifications. Because of its completeness and open architecture, Galax also turns out to be a very convenient platform for researchers interested in experimenting with XQuery optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: implementing deletion in b + - trees

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jan jannink
",n
"LEFT id: NA
RIGHT id: 1040

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating triggers and declarative constraints in sql database sytems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: roberta cochrane , hamid pirahesh , nelson mendon &#231; a mattos
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 58

LEFT text: Summary: The requeening process was investigated under emergency conditions in honey bee colonies (Apis mellifera L.). The progression of queen cell construction was closely monitored after removal of the mother queen, and the newly-emerged queens were measured for several physical traits to quantify their reproductive potential (= quality). The results suggest that workers regulate the queen rearing process by differentially constructing cells. Workers built different numbers of queens cells from different ages of brood and non-randomly destroyed over half (53%) of the initiated cells before their emergence. For those queens whose cells were not torn down, the variation in reproductive quality was limited, varying only slightly among age groups for queen size. Several hypotheses are discussed which might explain the adaptive benefit of worker regulation during queen rearing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: honey , i shrunk the database : footprint , mobility , and beyond

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: praveen seshadri
",y
"LEFT id: NA
RIGHT id: 1989

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view maintenance and integrity constraint checking : trading space for time

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 1981

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional resource scheduling for parallel queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1345

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching technologies for web applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 1279

LEFT text: The analysis of web usage has mostly focused on sites composed of conventional static pages. However, huge amounts of information available in the web come from databases or other data collections and are presented to the users in the form of dynamically generated pages. The query interfaces of such sites allow the specification of many search criteria. Their generated results support navigation to pages of results combining cross-linked data from many sources. For the analysis of visitor navigation behaviour in such web sites, we propose the web usage miner (WUM), which discovers navigation patterns subject to advanced statistical and structural constraints. Since our objective is the discovery of interesting navigation patterns, we do not focus on accesses to individual pages. Instead, we construct conceptual hierarchies that reflect the query capabilities used in the production of those pages. Our experiments with a real web site that integrates data from multiple databases, the German SchulWeb, demonstrate the appropriateness of WUM in discovering navigation patterns and show how those discoveries can help in assessing and improving the quality of the site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of navigation behaviour in web sites integrating multiple information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina berendt , myra spiliopoulou
",y
"LEFT id: NA
RIGHT id: 2289

LEFT text: In this work, we devise and evaluate control strategies for combining two potentially powerful buffer management techniques in object bases: (1) buffer pool segmentation with segment-specific replacement criteria and (2) dual buffering consisting of copying objects from pages into object buffers. We distinguish two dimensions for exerting control on the buffer pool: (1) the copying time determines when objects are copied from their memory-resident home page and (2) the relocation time determines the occasion on which a (copied) object is transferred back into its home page. Along both dimensions, we distinguish an eager and a lazy strategy. Our extensive experimental results indicate that a lazy object copying combined with an eager relocation strategy is almost always superior and significantly outperforms page-based buffering in most applications. 1 Introduction In the Eighties, object-oriented database systems emerged as the potential next-generation database technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dual-buffering strategies in object bases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann
",y
"LEFT id: NA
RIGHT id: 2191

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on processing xml in ldap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pedro jos &#233; marr &#243; n , georg lausen
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: With the increased emphasis on healthcare worldwide, the issue of being able to efficiently and effectively manage large amount of patient information in diverse medium becomes critical. In this work, we will demonstrate how advanced database technologies are used in RETINA, an integrated system for the screening and management of diabetic patients. RETINA captures the profile and retinal images of diabetic patients and automatically processes the retina fundus images to extract interesting features. Given the wealth of information acquired, we employ novel techniques to determine the risk profile of patients for better patient care management and to target significant subpopulations for more detailed studies. The results of such studies can be used to introduce effective preventive measures for the targeted sub-populations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1455

LEFT text: The experimental results show that distributed commit processing can have considerably more influence than distributed data processing on the throughput performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best transaction throughput performance for a variety of workloads and system configurations. In fact, OPT's peak throughput is often close to the upper bound on achievable performance. Even more interestingly, a three-phase (i.e., non-blocking) version of OPT provides better peak throughput performance than all of the standard two-phase (i.e., blocking protocols evaluated in our study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 2188

LEFT text: The Semantic Web is a vision the idea of having data on the Web defined and linked in such a way that it can be used by machines not just for display purposes but for automation, integration and reuse of data across various applications. Technically, however, there is a widespread misconception that the Semantic Web is primarily a rehash of existing AI and database work focused on encoding knowledge representation formalisms in markup languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler, and Moran seek to dispel this notion by presenting the broad dimensions of this emerging Semantic Web and the multi-disciplinary technological underpinnings like machine learning, information retrieval, service-oriented architectures, and grid computing, thus combining the informational and computational aspects needed to realize the full potential of the Semantic Web vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: learning to match ontologies on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: anhai doan , jayant madhavan , robin dhamankar , pedro domingos , alon halevy
",n
"LEFT id: NA
RIGHT id: 1395

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: similarity search for adaptive ellipsoid queries using spatial transformation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , ryoji kataoka , shunsuke uemura
",y
"LEFT id: NA
RIGHT id: 218

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 2110

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic integrity support in sql :1999 and commercial ( object - ) relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: can t &#252; rker , michael gertz
",n
"LEFT id: NA
RIGHT id: 1566

LEFT text: Although “now<” is expressed in SQL and CURRENT_TIMESTAMP within queries, this value cannot be stored in the database. How ever, this notion of an ever-increasing current-time value has been reflected in some temporal data models by inclusion of database-resident variables, such as “now<” “until-changed,< ” “**,” “@,” and “-”. Time variables are very desirable, but their used also leads to a new type of database, consisting of tuples with variables, termed a variable database.<

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on the issue of valid time ( s ) in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stavros kokkotos , efstathios v. ioannidis , themis panayiotopoulos , constantine d. spyropoulos
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 238

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on knowledge representation meets databases ( krdb ' 98 )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alex borgida , vinay k. chaudhri , martin staudt
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1958

LEFT text: In this paper, we study how to find such optimal schedules. In particular, we consider two optimization criteria: (i) one based on maximizing the number of piggybacked clips, and (ii) the other based on maximizing the impact on buffer space. We show that the optimal schedule under the first criterion is equivalent to a maximum matching in a suitably defined bipartite graph, and that under the second criterion, the optimal schedule is equivalent to a maximum matching in a suitably defined weighted bipartite graph.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries over multimedia repositories

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 355

LEFT text: In this paper a new colour space for content based image retrieval is presented, which is based upon psychophysical research into human perception. It provides both the ability to measure similarity and determine dissimilarity, using fuzzy logic and psychologically based set theoretic similarity measurement. These properties are shown to be equal or superior to conventional colour spaces. Example applications are also demonstrated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: pbir - perception-based image retrieval

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: edward chang , kwang-ting cheng , lihyuarn l. chang
",y
"LEFT id: NA
RIGHT id: 51

LEFT text: Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: walrus : a similarity retrieval algorithm for image databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: apostol natsev , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 950

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: parallel algorithms for high-dimensional similarity joins for data mining applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: john c. shafer , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: In these environments, the best retrieval performance can be achieved only if the data is clustered on the tertiary storage by all searchable attributes of the events. Since the number of these attributes is high, the underlying data-management facility must be able to cope with extremely large volumes and very high dimensionalities of data at the same time. The proposed indexing technique is designed to facilitate both clustering and efficient retrieval of high-dimensional data on tertiary storage. The structure uses an original space-partitioning scheme, which has numerous advantages over other space-partitioning techniques. While the main objective of the design is to support high-energy physics experiments, the proposed solution is appropriate for many other scientific applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 825

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: miro web : integrating multiple data sources through semistructured data types

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: luc bouganim , tatiana chan-sine-ying , tuyet-tram dang-ngoc , jean-luc darroux , georges gardarin , fei sha
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1922

LEFT text: Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an adaptive data replication algorithm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ouri wolfson , sushil jajodia , yixiu huang
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: We propose and evaluate two indexing schemes for improving the efficiency of data retrieval in high-dimensional databases that are incomplete. These schemes are novel in that the search keys may contain missing attribute values. The first is a multi-dimensional index structure, called the Bitstring-augmented R-tree (BR-tree), whereas the second comprises a family of multiple one-dimensional one-attribute (MOSAIC) indexes. Our results show that both schemes can be superior over exhaustive search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1870

LEFT text: XKeyword provides efficient keyword proximity queries on large XML graph databases. A query is simply a list of keywords and does not require any schema or query language knowledge for its formulation. XKeyword is built on a relational database and, hence, can accommodate very large graphs. Query evaluation is optimized by using the graph's schema. In particular, XKeyword consists of two stages. In the preprocessing stage a set of keyword indices are built along with indexed path relations that describe particular patterns of paths in the graph. In the query processing stage plans are developed that use a near optimal set of path relations to efficiently locate the keyword query results. The results are presented graphically using the novel idea of interactive result graphs, which are populated on-demand according to the user's navigation and allow efficient information discovery. We provide theoretical and experimental points for the selection of the appropriate set of precomputed path relations. We also propose and experimentally evaluate algorithms to minimize the number of queries sent to the database to output the top-K results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1038

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: geo/environmental and medical data management in the rasdaman system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter baumann , paula furtado , roland ritsch , norbert widmann
",n
"LEFT id: NA
RIGHT id: 1123

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use relevant information. In the Ecobase project, we address these problems in the context of several environmental applications in Brazil and Europe. We propose a distributed architecture for environmental information systems (EIS) based on the Le Select middleware developed at INRIA. In this paper, we present this architecture and its capabilities, and discuss the lessons learned and open issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the structured information manager : a database system for sgml documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ron sacks-davis
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: The processing of spatial joins can be greatly improved by the use of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of nonintersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. This article presents a polygon approximation for spatial join processing which we call four-colors raster signature (4CRS). The performance of a filter using this approximation was evaluated with real world data sets. The results showed that our approach, when compared to other approaches presented in the related literature, reduced the inconclusive answers by a factor of more than two. As a result, the need for retrieving the representation of polygons and carrying out exact geometry tests is reduced by a factor of more than two, as well. A Raster Approximation for the Processing of Spatial Joins

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 1848

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data warehousing and olap for decision support

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 150

LEFT text: Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system's performance and scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ariadne : a system for constructing mediators for internet sources

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jos &#233; luis ambite , naveen ashish , greg barish , craig a. knoblock , steven minton , pragnesh j. modi , ion muslea , andrew philpot , sheila tejada
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1720

LEFT text: In this paper, we explore the execution of pipelined hash joins in a multiprocessor-based database system. To improve the query execution, an innovative approach on query execution tree selection is proposed to exploit segmented right-deep trees, which are bushy trees of right-deep subtrees. We first derive an analytical model for the execution of a pipeline segment, and then, in light of the model, develop heuristic schemes to determine the query execution plan based on a segmented right-deep tree so that the query can be efficiently executed. As shown by our simulation, the proposed approach, without incurring additional overhead on plan execution, possesses more flexibility in query plan generation, and leads to query plans of significantly better performance than those achievable by the previous schemes using right-deep trees.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on parallel execution of multiple pipelined hash joins

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: hui-i hsiao , ming-syan chen , philip s. yu
",y
"LEFT id: NA
RIGHT id: 1137

LEFT text: The primary session of the workshop took place the morning of the first day. In this session, each of the participants had up to 10 min to deliver a brief message, using just one slide. Researchers were asked to answer the question: ‘In your view, what is the most urgent, unsolved question/issue in verbal lie detection?’ Similarly, practitioners were asked: ‘As a practitioner, what question/issue do you wish verbal lie detection research would address?’ The issues raised served as the basis for the discussions that were held throughout the workshop. The current paper first presents the urgent, unsolved issues raised by the workshop group members in the main session, followed by a message to researchers in the field, designed to deliver the insights, decisions, and conclusions resulting from the discussions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tpc-d : the challenges , issues and results

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramesh bhashyam
",n
"LEFT id: NA
RIGHT id: 1036

LEFT text: Our algorithm has the following characteristics: (1) It requires only one pass over the data; (2) It is deterministic; (3) It produces good lower and upper bounds of the true values of the quantiles; (4) It requires no a priori knowledge of the distribution of the data set; (5) It has a scalable parallel formulation; (6) Extra time and memory for computing additional quantiles (beyond the first one) are constant per quantile. We present experimental results on the IBM SP-2. The experimental results show that the algorithm is indeed robust and does not depend on the distribution of the data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a one-pass algorithm for accurately estimating quantiles for disk-resident data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: khaled alsabti , sanjay ranka , vineet singh
",y
"LEFT id: NA
RIGHT id: 1604

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data and knowledge base research at hong kong university of science and technology

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: p. drew , b. hamidzadeh , k. karlapalem , a. kean , d. lee , q. li , f. lochovsky , c. d. shum , b. wuthrich
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",n
"LEFT id: NA
RIGHT id: 201

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of constrained frequent set queries with 2-variable constraints

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , raymond ng , jiawei han , alex pang
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: A prototyped data mining system, DBLearn, has been developed, which efficiently and effectively extracts different kinds of knowledge rules from relational databases. It has the following features: high level learning interfaces, tightly integrated with commercial relational database systems, automatic refinement of concept hierarchies, efficient discovery algorithms and good performance. Substantial extensions of its knowledge discovery power towards knowledge mining in object-oriented, deductive and spatial databases are under research and development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 891

LEFT text: Large herbivores can shape young forest stands and determine the successional trajectory of forested ecosystems by selectively browsing palatable species at the sapling stage. Moose (Alces alces) is the dominant vertebrate herbivore in Fennoscandian boreal forests, and high population densities have raised concerns about potential negative effects on ecosystem functioning and properties including biological diversity and timber production. We used 31 herbivore exclosures in Norway to investigate how forests developed after clear-cutting with or without moose present. We tested how tree demography, abundances of understory plant functional groups, community composition, and plant diversity (including bryophytes) across multiple scales varied with moose exclusion. After seven years, the exclosures were dominated by deciduous trees, including many large rowan (Sorbus aucuparia) individuals, a functionally important keystone species. In contrast, the open plots subject to moose impacts (browsing, trampling, defecation) were dominated by economically important coniferous trees and there was next to no rowan recruitment to taller height classes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: contrast plots and p-sphere trees : space vs. time in nearest neighbour searches

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jonathan goldstein , raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 308

LEFT text: Histograms are frequently used to represent the distribution of data values in an attribute of a relation. Most previous work has focused on identifying the optimal histogram (given a limited number of buckets) for a single attribute independent of other attributes/histograms. In this paper, we propose the idea of global optimization of histograms, i.e., single-attribute histograms for a set of attributes are optimized collectively so as to minimize the overall error in using the histograms. The idea is to allocate more buckets to histograms whose attributes are more frequently used and/or distributions are highly skewed. While the accuracy of some histograms is penalized (being assigned fewer buckets), we expect the global error to be low compared to the traditional method (of allocating equal number of buckets to each histogram).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: global optimization of histograms

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: h. v. jagadish , hui jin , beng chin ooi , kian-lee tan
",y
"LEFT id: NA
RIGHT id: 1712

LEFT text: Semantic data modelling I is the established method for the requirements definition and the conceptual specification of application systems. In large projects and especially in enterprise data models the cost of creating a data model amount to a large proportion of the overall cost. On the other hand there is a general pressure to reduce the cost of data modelling for application systems to harness the skyrocketing costs of data processing in a colnpany. The standard textbook modelling process calls for the modelling of single entities to represent simple facts and combining these into a model in a bottom up fashion: 'An entity is a concept, person, thing

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data modeling of time-based media

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: simon gibbs , christian breiteneder , dennis tsichritzis
",n
"LEFT id: NA
RIGHT id: 1116

LEFT text: Developments in high-throughput sequencing (HTS) result in an exponential increase in the amount of data generated by sequencing experiments, an increase in the complexity of bioinformatics analysis reporting and an increase in the types of data generated. These increases in volume, diversity and complexity of the data generated and their analysis expose the necessity of a structured and standardized reporting template. BioCompute Objects (BCOs) provide the requisite support for communication of HTS data analysis that includes support for workflow, as well as data, curation, accessibility and reproducibility of communication. BCOs standardize how researchers report provenance and the established verification and validation protocols used in workflows while also being robust enough to convey content integration or curation in knowledge bases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database de-centralization - a practical approach

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: tor didriksen , c &#233; sar a. galindo-legaria , eirik dahle
",n
"LEFT id: NA
RIGHT id: 664

LEFT text: This second editionsystematically introduces the notion of ontologies to the non-expert reader and demonstrates in detail how to apply this conceptual framework for improved intranet retrieval of corporate information and knowledge and for enhanced Internet-based electronic commerce. He also describes ontology languages (XML, RDF, and OWL) and ontology tools, and the application of ontologies. In addition to structural improvements, the second edition covers recent developments relating to the Semantic Web, and emerging web-based standard languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: business data management for business-to-business electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: christoph quix , mareike schoop , manfred jeusfeld
",n
"LEFT id: NA
RIGHT id: 123

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unpacking the semantics of source and usage to perform semantic reconciliation in large-scale information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ken smith , leo obrst
",y
"LEFT id: NA
RIGHT id: 458

LEFT text: Introduction to Constraint Databases comprehensively covers both constraint-database theory and several sample systems. The book reveals how constraint databases bring together techniques from a variety of fields, such as logic and model thoery, algebraic and computational geometry, and symbolic computation, to the design and analysis of data models and query languages. Constraint databases are shown to be powerful and simple tools for data modeling and querying in application areas---such as environmental modeling, bioinformatics, and computer vision---that are not suitable for relational databases. Specific applications are examined in geographic information systems, spatiotemporal data management, linear programming, genome databases, model checking of automata, and other areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraint databases : a tutorial introduction

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jan van den bussche
",n
"LEFT id: NA
RIGHT id: 1147

LEFT text: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient snapshot differential algorithms for data warehousing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: wilburt labio , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 221

LEFT text: This paper describes an efficient optimistic concurrency control scheme for use in distributed database systems in which objects are cached and manipulated at client machines while persistent storage and transactional support are provided by servers. The scheme provides both serializability and external consistency for committed transactions; it uses loosely synchronized clocks to achieve global serialization. It stores only a single version of each object, and avoids maintaining any concurrency control information on a per-object basis; instead, it tracks recent invalidations on a per-client basis, an approach that has low in-memory space overhead and no per-object disk overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient concurrency control in multidimensional access methods

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 410

LEFT text: We have implemented a compressor (XMilI) and decompressor (XDemill) for XML data, to be used in data exchange and archiving, which can be downloaded from http://www.research.att.com/sw/tools/xmill. XMill compresses about twice as good as gzip, at about the same speed. It does not need a DTD in order to compress, and preserves the input XML file faithfully, including element order, attributes order, PI 's, comments, the DTD, etc. A novelty in XMill is that it allows users to combine existing compressors in order to compress heterogeneous XML data: by default it uses zlib , a library function implementing gz ip ' s functionality, and includes some standard compression techniques for simple data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ajax : an extensible data cleaning tool

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: helena galhardas , daniela florescu , dennis shasha , eric simon
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: Many papers have examined how to efficiently export a materialized view but to our knowledge none have studied how to efficiently import one. To import a view, i.e., to install a stream of updates, a real-time database system must process new updates in a timely fashion to keep the database ""fresh,"" but at the same time must process transactions and ensure they meet their time constraints. In this paper, we discuss the various properties of updates and views (including staleness) that affect this tradeoff. We also examine, through simulation, four algorithms for scheduling transactions and installing updates in a soft real-time database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 863

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic access : semantic interface for querying databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: naphtali rishe , jun yuan , rukshan athauda , shu-ching chen , xiaoling lu , xiaobin ma , alexander vaschillo , artyom shaposhnikov , dmitry vasilevsky
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: The Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS 2000) was held at the Crowne Plaza San Jose/Silicon Valley in Milpitas, California on June 8-9, 2000. The purpose of this workshop was to bring together leading practitioners, developers and researchers to explore the challenging technical issues and find feasible solutions for advancing the current state of the art in e-commerce and web-based information systems. In particular, the workshop was interested in the infrastructure issues to facilitate e-commerce and Web-based information systems.WECWIS 2000 was successful. There were three invited talks, one industrial panel discussion and six technical sessions. The keynote speech, ""The global trading web: A strategic vision for the Internet economy,"" was delivered by Dr. Jay M. Tenebaum, VP and Chief Scientist, Commerce One, Inc., on June 8 immediately following the opening remarks by the conference chair. The banquet address, ""Business issues in e-commerce,"" was delivered by Mr. Daniel Druker, General Manager, Hyperion e-Business Division. Finally, a lunch address, ""B2C, B2B, N2N, N2M: Why 2 is so instrumental?"" was delivered by Mr. Mstafa A. Syed, VP of Technology, VertialNet, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 655

LEFT text: The fourth International Conference on Flexible Query Answering Systems (FQAS'2000) was held at the Academy of Sciences in Warsaw, Poland on October, 25-27, 2000. This series of conferences was launched in 1994 by Troels Andreasen, Henning Christiansen and Henrik Larsen from Roskilde University in Denmark, who have been the main driving force behind this series ever since. The previous FQAS events were held in Denmark in 1994, 1996, mad 1998. The next conference in this series will return to Denmark in 2002.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 18th british national conference on databases ( bncod )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: carole goble , brian read
",n
"LEFT id: NA
RIGHT id: 473

LEFT text: Outlier detection is an integral component of statistical modelling and estimation. For high-dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. The cut-off value is obtained from the asymptotic distribution of the distance, which enables us to control the Type I error and deliver robust outlier detection. Simulation studies show that the proposed method behaves well for high-dimensional data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: re-designing distance functions and distance-based applications for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 1416

LEFT text: The SIT-IN (acronym for Integrated Territorial Information System, in Italian) system integrates a historical database, providing information about the temporal evolution of territorial administrative partitions; the Institute's GIS, providing the cartography of the Italian territory down to the census tract level of detail; a statistical data warehouse, providing spatiotemporal data from a number of di erent surveys; and nally an address normalizing/geo-matching system, providing information about the limits of census tracts (e.g. portions of streets or the sides of town squares).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sit-in : a real-life spatio-temporal information system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giuseppe sindoni , leonardo tininini , amedea ambrosetti , cristina bedeschi , stefano de francisci , orietta gargano , rossella molinaro , mario paolucci , paola patteri , pina ticca
",y
"LEFT id: NA
RIGHT id: 558

LEFT text: Many decision support systems, which utilize association rules for discovering interesting patterns, require the discovery of association rules that vary over time. Such rules describe complicated temporal patterns such as events that occur on the “first working day of every month.” In this paper, we study the problem of discovering how association rules vary over time. In particular, we introduce the idea of using a calendar algebra to describe complicated temporal phenomena of interest to the user. We then present algorithms for discovering calendric association rules, which are association rules that follow the patterns set forth in the user supplied calendar expressions. We devise various optimizations that speed up the discovery of calendric association rules. We show, through an extensive series of experiments, that these optimization techniques provide performance benefits ranging from to over a less sophisticated algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: using unknowns to prevent discovery of association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: y &#252; cel saygin , vassilios s. verykios , chris clifton
",n
"LEFT id: NA
RIGHT id: 1550

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: one size fits all database architectures do not work for dss

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: clark d. french
",n
"LEFT id: NA
RIGHT id: 177

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: high-dimensional index structures database support for next decade 's applications ( tutorial )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: Recently there has been an increasing interest in supporting bulk operations on multidimensional index structures. Bulk loading refers to the process of creating an initial index structure for a presumably very large data set. In this paper, we present a generic algorithm for bulk loading which is applicable to a broad class of index structures. Our approach differs completely from previous ones for the following reasons. First, sorting multidimensional data according to a predefined global ordering is completely avoided. Instead, our approach is based on the standard routines for splitting and merging pages which are already fully implemented in the corresponding index structure. Second, in contrast to inserting records one by one, our approach is based on the idea of inserting multiple records simultaneously. As an example we demonstrate in this paper how to apply our technique to the R-tree family. For R-trees we show that the I/O performance of our generic algorithm meets the lower bound of external sorting. Empirical results demonstrate that performance improvements are also achieved in practice without sacrificing query performance

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: Panel Abstract This panel addresses a very important area that is often neglected or overlooked by database systems, database applications developers and data warehouse designers, namely storage. We propose to inform, discuss and debate the use of “Active Storage Hierarchy” in database systems and applications. By active storage hierarchy we mean a database system that uses all storage media (i.e. optical, tape, and disk) to store and retrieve data and not just disk. We will examine, discuss and debate how active storage compares and/or complements what is known in the database research community as “Active Disks” [RGF 98] and other emerging diskcentric storage paradigms. The presentations and analysis will span current real products, emerging technology to active (and visionary) research in several related areas, like storage technology, storage systems, federated databases and database system uses of storage. Panel Format • Overview of Storage Technology, Current and Future Commercial Products. • Overview of Database Research and Commercial Database Product Plans • Overview and sample case studies of current and emerging applications that do and/or will in the future exploit costeffective storage hierarchy. • Discussion and debate on feasibility and future (visionary) use of storage systems in database applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",n
"LEFT id: NA
RIGHT id: 481

LEFT text: Given a set of objects S, a spatio-temporal window query q retrieves the objects of S that will intersect the window during the (future) interval qT. A nearest neighbor query q retrieves the objects of S closest to q during qT. Given a threshold d, a spatio-temporal join retrieves the pairs of objects from two datasets that will come within distance d from each other during qT. In this article, we present probabilistic cost models that estimate the selectivity of spatio-temporal window queries and joins, and the expected distance between a query and its nearest neighbor(s). Our models capture any query/object mobility combination (moving queries, moving objects or both) and any data type (points and rectangles) in arbitrary dimensionality. In addition, we develop specialized spatio-temporal histograms, which take into account both location and velocity information, and can be incrementally maintained. Extensive performance evaluation verifies that the proposed techniques produce highly accurate estimation on both uniform and non-uniform data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: comparative analysis of five xml query languages

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: angela bonifati , stefano ceri
",n
"LEFT id: NA
RIGHT id: 744

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1745

LEFT text: The discussions during the panel stayed largely but not entirely focused on the question of active database research issues from the application perspective. There were nine panelists. Each panelist was asked to prepare brief answers to a set of questions. The sets of answers were discussed by all participants, and finally a number of more general issues were discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in federated database systems : report of efdbs '97 workshop

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. conrad , b. eaglestone , w. hasselbring , m. roantree , m. sch &#246; hoff , m. str &#228; ssler , m. vermeer , f. saltor
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1737

LEFT text: In the mobile wireless computing environment of the future a large number of users equipped with low powered palm-top machines will query databases over the wireless communication channels. Palmtop based units will often be disconnected for prolonged periods of time due to the battery power saving measures; palmtops will also frequencly relocate between different cells and connect to different data servers at different times. Caching of frequently accessed data items will be an important technique that will reduce contention on the narrow bandwidth wireless channel. However, cache invalidation strategies will be severely affected by the disconnection and mobility of the clients. The server may no longer know which clients are currently residing under its cell and which of them are   currently on. We propose a taxonomy of different cache invalidation strategies and study the impact of client's disconnection times on their performance. We determine that for the units which are often disconnected (sleepers) the best cache invalidation strategy is based on signatures previously used for efficient file comparison. On the other hand, for units which are connected most of the time (workaholics), the best cache invalidation strategy is based on the periodic broadcast of changed data items.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sleepers and workaholics : caching strategies in mobile environments

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , tomasz imieli &#324; ski
",n
"LEFT id: NA
RIGHT id: 886

LEFT text: Curated databases are databases that are populated and updated with a great deal of human effort. Most reference works that one traditionally found on the reference shelves of libraries -- dictionaries, encyclopedias, gazetteers etc. -- are now curated databases. Since it is now easy to publish databases on the web, there has been an explosion in the number of new curated databases used in scientific research. The value of curated databases lies in the organization and the quality of the data they contain. Like the paper reference works they have replaced, they usually represent the efforts of a dedicated group of people to produce a definitive description of some subject area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integration of data mining with database technology

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: amir netz , surajit chaudhuri , jeff bernhardt , usama m. fayyad
",n
"LEFT id: NA
RIGHT id: 1216

LEFT text: Wireless and mobile computing have advanced significantly in the last decade. In particular, we now face the challenge to spontaneously establish wireless self-organizing networks, such as ad hoc, disruption-tolerant, sensor, and wireless mesh networks. These spontaneous self-organizing networks have been the focus of intensive research activity in recent years. Spontaneous networks arise from the cooperation of mobile devices in an ad hoc fashion requiring no previous infrastructure in place. A key point to couple research and real-life applications in this context is to understand how mobility (of devices, users, and applications) impacts practical networking aspects

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo atzeni , alberto o. mendelzon
",n
"LEFT id: NA
RIGHT id: 173

LEFT text: With an explosion of data on the web, consistent data access to diverse data sources has become a challenging task. In this tutorial will present topics of interest to database researchers and developers building: interoperable middle-ware, gateways, distributed heterogeneous query processors, federated databases, data source wrappers, mediators, and DBMS extensions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql open heterogeneous data access

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: berthold reinwald , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 732

LEFT text: We present algorithms for performing backup and recovery of the DBMS data in a coordinated fashion with the files on the file servers. Our algorithms for coordinated backup and recovery have been implemented in the IBM DB2/DataLinks product. We also propose an efficient solution to the problem of maintaining consistency between the content of a file and the associated meta-data stored in the DBMS from a reader's point of view without holding long duration locks on meta-data tables. In the model, an object is directly accessed and edited in-place through normal file system APIs using a reference obtained via an SQL Query on the database. To relate file modifications to meta-data updates, the user issues an update through the DBMS, and commits both file and meta-data updates together.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: coordinating backup/recovery and data consistency between database and file systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: suparna bhattacharya , c. mohan , karen w. brannon , inderpal narang , hui-i hsiao , mahadevan subramanian
",y
"LEFT id: NA
RIGHT id: 50

LEFT text: In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table, which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a new method for similarity indexing of market basket data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: charu c. aggarwal , joel l. wolf , philip s. yu
",y
"LEFT id: NA
RIGHT id: 1640

LEFT text: Compensation-based query processing has been proposed in order to avoid lock contention between updating transactions and ad-hoc queries. This paper presents an algorithm based on undo /no-redo compensation. A query will read an inconsistent version of the database, but updates made by concurrent transactions are later undone to make the query result transaction-consistent. By processing the database internal log to obtain information on concurrent updates, queries impose no extra work on updating transactions. A simulation study shows that response times for query execution is significantly improved compared to the earlier compensation-based algorithms. Compared to executing queries with no consistency requirements, the algorithm gives only a small increase in query response times, while the effects on transaction response times are negligible. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 358

LEFT text: A prototyped data mining system, DBLearn, has been developed, which efficiently and effectively extracts different kinds of knowledge rules from relational databases. It has the following features: high level learning interfaces, tightly integrated with commercial relational database systems, automatic refinement of concept hierarchies, efficient discovery algorithms and good performance. Substantial extensions of its knowledge discovery power towards knowledge mining in object-oriented, deductive and spatial databases are under research and development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dna-miner : a system prototype for mining dna sequences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jiawei han , hasan jamil , ying lu , liangyou chen , yaqin liao , jian pei
",n
"LEFT id: NA
RIGHT id: 976

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: filtering with approximate predicates

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: narayanan shivakumar , hector garcia-molina , chandra chekuri
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: Multiversion access methods have been emerged in the literature primarily to support queries on a transaction-time database where records are never physically deleted. For a popular class of efficient methods (including the multiversion Btree), data records and index entries are occasionally duplicated to separate data according to time. In this paper, we present techniques for improving query processing in multiversion access methods. In particular, we address the problem of avoiding duplicates in the response sets. We first discuss traditional approaches that eliminate duplicates using hashing and sorting. Next, we propose two new algorithms for avoiding duplicates without using additional data structures. The one performs queries in a depth-first order starting from a root, whereas the other exploits links between data pages. These methods are discussed in full details and their main properties are identitied. Preliminary performance results confirm the advantages of these methods in comparison to traditional ones according to CPU-time, disk accesses and storage.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1703

LEFT text: In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a consensus glossary of temporal database concepts

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: curtis dyreson , fabio grandi , wolfgang k &#228; fer , nick kline , nikos lorentzos , yannis mitsopoulos , angelo montanari , daniel nonen , elisa peressi , barbara pernici , john f. roddick , nandlal l. sarda , maria rita scalas , arie segev , richard thomas snodgrass , mike d. soo , abdullah tansel , paolo tiberio , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: Conceptual data modeling for complex applications, such as multimedia and spatiotemporal applications, often results in large, complicated and difficult-to-comprehend diagrams. One reason for this is that these diagrams frequently involve repetition of autonomous, semantically meaningful parts that capture similar situations and characteristics. By recognizing such parts and treating them as units, it is possible to simplify the diagrams, as well as the conceptual modeling process. We propose to capture autonomous and semantically meaningful excerpts of diagrams that occur frequently as modeling patterns. Specifically, the paper concerns modeling patterns for conceptual design of spatiotemporal databases. Based on requirements drawn from real applications, it presents a set of modeling patterns that capture spatial, temporal, and spatiotemporal aspects. To facilitate the conceptual design process, these patterns are abbreviated by corresponding spatial, temporal, and spatiotemporal pattern abstractions, termed components. The result is more elegant and less-detailed diagrams that are easier to comprehend, but yet semantically rich. The Entity-Relationship model serves as the context for this study. An extensive example from a real cadastral application illustrates the benefits of using a component-based conceptual model.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1526

LEFT text: This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efendi : federated database system of cadlab

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: e. radeke , r. b &#246; ttger , b. burkert , y. engel , g. kachel , s. kolmschlag , d. nolte
",n
"LEFT id: NA
RIGHT id: 1462

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 638

LEFT text: Fibonacci is an object-oriented database programming language characterized by static and strong typing, and by new mechanisms for modeling data-bases in terms of objects with roles, classes, and associations. A brief introduction to the language is provided to present those features, which are particularly suited to modeling complex databases. Examples of the use of Fibonacci are given with reference to the prototype implementation of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: themis : a database programming language handling integrity constraints

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: v &#233; ronique benzaken , anne doucet
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Flexible methods supporting the data interchange between autonomous information systems are important for today's increasingly heterogeneous enterprise IT infrastructures. Updates, insertions, and deletions of data objects in autonomous information systems often have to trigger data changes in other autonomous systems, even if the distributed systems are not integrated into a global schema. We suggest a solution to this problem based on the propagation and transformation of data using several XML technologies. Our prototype manages dependencies between the schemas of distributed data sources and allows to define and process arbitrary actions on changed data by manipulating all dependent data sources. The prototype comprises a propagation engine that interprets scripts based on a workflow specification language, a data dependency specification tool, a system administration tool, and a repository that stores all relevant information for these tools.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 1439

LEFT text: Predictive queries over spatio-temporal data proved to be vital in many location-based services including traffic management, ride sharing, and advertising. In the last few years, one of the most exciting work on spatio-temporal data management is about predictive queries. In this paper, we review the current research trends and present their related applications in the field of predictive spatio-temporal queries processing. Then, we discuss some basic challenges arising from new opportunities and open problems. The goal of this paper is to catch the interesting areas and future work under the umbrella of predictive queries over spatio-temporal data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: similarity based retrieval of pictures using indices on spatial relationships

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , clement t. yu , chengwen liu , king liu
",y
"LEFT id: NA
RIGHT id: 375

LEFT text: While much recent work has focussed on the performance of transaction systems where individual transactions have deadlines, our research addresses the semantics of data usage in real-time applications and its integration with real-time resource management, in particular, the timeless value of real-time data and the inherent path and not state-based constraints on concurrency control. Central to our research is the idea of similarity which is a reflexive, symmetric relation over the domain of a data object. By exploiting the similarity relation, we propose a class of efficient data-access policies for real-time data objects. We shall also discuss the design of a distributed real-time data-access interface. Our goal is to build a database facility which can support predictable real-time applications involving high-speed communication, information access, and multimedia.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: real-time database - similarity and resource scheduling

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tei-wei kuo , aloysius k. mok
",y
"LEFT id: NA
RIGHT id: 1866

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 2003

LEFT text: While content-based image retrieval (CBIR) is an expanding field, and new approaches to ever more effective retrieval are frequently proposed, relatively little attention has so far been paid to the process of evaluating the effectiveness of CBIR methods. Most of the reported evaluations use standard IR evaluation methodologies, with little consideration of their statistical significance or appropriateness for CBIR, which makes it difficult to assess the precise impact of individual methods. In this paper, we present a new approach for evaluating CBIR systems which provides both efficient and statistically-sound performance evaluation. The approach is based on stratified sampling, and provides a significant improvement over existing evaluation approaches. Comprehensive experiments using our approach to evaluate a range of CBIR methods have shown that the approach reduces not only the estimation error, but also reduces the size of the test data set required to achieve specific estimation error levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cmvf : a novel dimension reduction scheme for efficient indexing in a large image database

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jialie shen , anne h. h. ngu , john shepherd , du q. huynh , quan z. sheng
",n
"LEFT id: NA
RIGHT id: 2040

LEFT text: In this paper, we define and examine a particular class of queries called group queries. Group queries are natural queries in many decisionsupport applications. The main characteristic of a group query is that it can be executed in a groupby-group fashion. In other words, the underlying relation(s) can be partitioned (based on some set of attributes) into disjoint groups, and each group can be processed separately. We give a syntactic criterion to identify these queries and prove its sufficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream processing of xpath queries with predicates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ashish kumar gupta , dan suciu
",n
"LEFT id: NA
RIGHT id: 912

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international conference on ontologies , databases and applications of semantics ( odbase ) : part of the federated conference on the move to meaningful internet systems 2002

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: The second international workshop on semantic Web technologies for health data management aimed at putting together an interdisciplinary audience that is interested in the fields of semantic web, data management and health informatics to discuss the challenges in health-care data management and to propose new solutions for the next generation data-driven health-care systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerge. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 166

LEFT text: Customer retention” is an important real-world problem in many sales and services related industries today. This work illustrates how we can integrate the various techniques of data-mining, such as decision-tree induction, deviation analysis and multiple concept-level association rules to form an intuitive and novel approach to gauging customer's loyalty and predicting their likelihood of defection. Immediate action taken against these “early-warnings” is often the key to the eventual retention or loss of the customers involved.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a data mining application : customer retention at the port of singapore authority ( psa )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: kiansing ng , huan liu , hweebong kwah
",y
"LEFT id: NA
RIGHT id: 1297

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: automated resolution of semantic heterogeneity in multidatabases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. w. bright , a. r. hurson , s. pakzad
",n
"LEFT id: NA
RIGHT id: 619

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1854

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: delaunay : a database visualization system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang
",n
"LEFT id: NA
RIGHT id: 276

LEFT text: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting constraint-like data characterizations in query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: parke godfrey , jarek gryz , calisto zuzarte
",n
"LEFT id: NA
RIGHT id: 706

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbxplorer : enabling keyword search over relational databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: Introduction and Main Contributions Providing mechanisms that allow the user to retrieve desired multimedia information by their semantic content is now an important issue in multimedia databases. However, current prototypes (e.g. Oracle 8i interMedia and Informix Datablade Modules) index mostly only low-level features of multimedia objects. Therefore special techniques are needed for semantic indexing and retrieval of multimedia objects. In this context we present the SMOOTH system, a prototype of a distributed multimedia database system. It implements an integrated querying, annotating, and navigating framework relying on a generic video indexing model. The framework allows the structuring of videos into logical and physical units, and the annotation of these units by typed semantic objects. An index-database stores these structural and semantic information. We provide further a clear concept for capturing and querying the semantic content of multimedia objects, their correlation with low-level objects, as well as their spatio-temporal relationships.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 1958

LEFT text: Web repositories, such as the Stanford WebBase repository, manage large heterogeneous collections of Web pages and associated indexes. For effective analysis and mining, these repositories must provide a declarative query interface that supports complex expressive Web queries. Such queries have two key characteristics: (i) They view a Web repository simultaneously as a collection of text documents, as a navigable directed graph, and as a set of relational tables storing properties of Web pages (length, URL, title, etc.). (ii) The queries employ application-specific ranking and ordering relationships over pages and links to filter out and retrieve only the ""best"" query results. In this paper, we model a Web repository in terms of ""Web relations"" and describe an algebra for expressing complex Web queries. Our algebra extends traditional relational operators as well as graph navigation operators to uniformly handle plain, ranked, and ordered Web relations. In addition, we present an overview of the cost-based optimizer and execution engine that we have developed, to efficiently execute Web queries over large repositories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries over multimedia repositories

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1896

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 1146

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2040

LEFT text: We present the design and implementation of the XSQ system for querying streaming XML data using XPath 1.0. Using a clean design based on a hierarchical arrangement of pushdown transducers augmented with buffers, XSQ supports features such as multiple predicates, closures, and aggregation. XSQ not only provides high throughput, but is also memory efficient: It buffers only data that must be buffered by any streaming XPath processor. We also present an empirical study of the performance characteristics of XPath features, as embodied by XSQ and several other systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream processing of xpath queries with predicates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ashish kumar gupta , dan suciu
",n
"LEFT id: NA
RIGHT id: 1836

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distance-based indexing for high-dimensional metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 807

LEFT text: The dynamic load balancing strategies for parallel association rule mining are proposed under heterogeneous PC cluster environment. PC cluster is recently regarded as one of the most promising platforms for heavy data intensive applications, such as decision support query processing and data mining. The development period of PC hardware is becoming extremely short, which results in heterogeneous system, where the clock cycle of CPU, the performance/capacity of disk drives, etc are di erent among component PC's. Heterogeneity is inevitable. Basically, current algorithms assume the homogeneity. Thus if we naively apply them to heterogeneous system, its performance is far below expectation. We need some new methodologies to handle heterogeneity. In this paper, we propose the new dynamic load balancing methods for association rule mining, which works under heterogeneous system. Two strategies, called candidate migration and transaction migration are proposed. Initially rst one is invoked. When the load imbalance cannot be resolved with the rst method, the second one is employed, which is costly but more e ective for strong imbalance. We have implemented them on the PC cluster system with two di erent types of PCs: one with Pentium Pro, the other one with Pentium II. The experimental results confirm that the proposed approach can very e ectively balance the workload among heterogeneous PCs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing for parallel association rule mining on heterogenous pc cluster systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: masahisa tamura , masaru kitsuregawa
",y
"LEFT id: NA
RIGHT id: 1990

LEFT text: This study presents a modified B2B CRM using the Genetic algorithm and Data Mining Techniques to improve decision making. The model classifies consumers into consumers of Repeat and Shop-and-Go. Modified data mining C5.0 and the Genetic algorithm was employed to optimize rules generated by the decision tree algorithm. The findings showed that the proposed model allocates resources effectively to the most profitable customers’ decisions. The output metrics are machine time, calibration graph, and ROC curve. In comparison with the conventional C5.0, k-NN, and Support Vector Machine, the proposed model has greater accuracy of 89.3 percent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based optimization for magic : algebra and implementation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , joseph m. hellerstein , hamid pirahesh , t. y. cliff leung , raghu ramakrishnan , divesh srivastava , peter j. stuckey , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: HYPERQUERY is a hypertext query language for object-oriented pictorial database systems. First, we discuss object calculus based on term rewriting. Then, example queries are used to illustrate language facilities. This query language has been designed with a flavor similar to QBE as the highly nonprocedural and conversational language for object-oriented pictorial database management system OISDBS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1808

LEFT text: The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a few one-pass clustering algorithms have been developed for the data stream problem. Although such methods address the scalability issues of the clustering problem, they are generally blind to the evolution of the data and do not address the following issues: (1) The quality of the clusters is poor when the data evolves considerably over time. (2) A data stream clustering algorithm requires much greater functionality in discovering and exploring clusters over different portions of the stream.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for implementing hypothetical queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: timothy griffin , richard hull
",n
"LEFT id: NA
RIGHT id: 445

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and cost-effective techniques for browsing and indexing large video databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghwan oh , kien a. hua
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: This second special issue provides a forum for topical issues that demonstrate the usefulness of PLS-SEM by piloting applications of this method in the field of strategic management with strong implications for strategic research and practice. As such, the special issue targets two audiences: academics involved in the fields of strategy and management, and practitioners such as consultants. The six articles in this issue are summarized in the following paragraphs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 902

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a graphical query language for mobile information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ya-hui chang
",n
"LEFT id: NA
RIGHT id: 1709

LEFT text: ACTA is a comprehensive transaction framework that facilitates the formal description of properties of extended transaction models. Specifically, using ACTA, one can specify and reason about (1) the effects of transactions on objects and (2) the interactions between transactions. This article presents ACTA as a tool for the synthesis of extended transaction models, one which supports the development and analysis of new extended transaction models in a systematic manner. Here, this is demonstrated by deriving new transaction definitions (1) by modifying the specifications of existing transaction models, (2) by combining the specifications of existing models, and (3) by starting from first principles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: synthesis of extended transaction models using acta

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: panos k. chrysanthis , krithi ramamritham
",y
"LEFT id: NA
RIGHT id: 1650

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 82

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: the sift information dissemination system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tak w. yan , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 935

LEFT text: Publisher Summary This chapter explores the basic functionality of the volume data model. It considers examples from biology and volumes of measurement from quantum physics. The data model is similar in the two cases, but the operations that are commonly used are quite different. In the second case, for instance, one is often interested in conditions on the behavior of local differential operators, such as zero-flow surfaces, while in the biological case one has a mix of value conditions and geometric condition. With these two application fields, this chapter highlights the generality and flexibility of the model. The system is based on a commercial database, augmented with specialized functions to manipulate the volume data model. The chapter demonstrates various operations specifying queries both using a graphical user interface and entering them directly in structured query language (SQL) augmented with the volume algebra operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a language for manipulating arrays

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",y
"LEFT id: NA
RIGHT id: 204

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an efficient bitmap encoding scheme for selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chee-yong chan , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1773

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 2109

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems, such as query optimization, the maintenance of physical data independence, data integration and data warehousing. This article surveys the theoretical issues concerning the problem of answering queries using views

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 1620

LEFT text: On-Line Analytical Processing (OLAP) and Data Warehousing are decision support technologies. Their goal is to enable enterprises to gain competitive advantage by exploiting the ever-growing amount of data that is collected and stored in corporate databases and files for better and faster decision making. Over the past few years, these technologies have experienced explosive growth, both in the number of products and services offered, and in the extent of coverage in the trade press. Vendors, including all database companies, are paying increasing attention to all aspects of decision support.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dec data distributor : for data replication and data warehousing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel j. dietterich
",n
"LEFT id: NA
RIGHT id: 1187

LEFT text: The Senior Therapist's Grandiosity: Clinical and Ethical Consequences of Merging Multiple Roles"" is the second paper by ROBERT S. PEPPER, C.S.W., Ph.D. to appear in this Journal on this very important topic in the contemporary practice of psychotherapy. He notes that some senior therapists engage in multiple roles with grandiosity and other unresolved narcissistic pathology, doing harm to their patients, while violating professional and ethical codes of conduct.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an authorization system for digital libraries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: e. ferrari , n.r. adam , v. atluri , e. bertino , u. capuozzo
",y
"LEFT id: NA
RIGHT id: 533

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial directons

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1866

LEFT text: In the era of electronic publishing, there is a need for a comprehensive Web Site Management System (WSMS) that provides an end-to-end solution ranging from integration of web sites to re-structuring and maintenance of new customized web views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: Rules in active database systems can be very difficult to program due to the unstructured and unpredictable nature of rule processing. We provide static analysis techniques for predicting whether a given rule set is guaranteed to terminate and whether rule execution is confluent (guaranteed to have a unique final state). Our methods are based on previous techniques for analyzing rules in active database systems. We improve considerably on the previous techniques by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not make this determination.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: This paper abstracts from the underlying platforms and instead considers the requirements to CRM solutions for the various communication channels, in order to devise a uniform and corporate-wide data architecture for an omni-channel customer view to maximize the business clients' value in customer retention and customer centric analytics. Especially, online customer segmentation integrating channel usage and preferences is presented as a very promising means for constructing a self-energising information loop which will lead to highly improved customer service along the whole customer journey.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 1612

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: managing video data in a mobile environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rafael alonso , yuh-lin chang , liviu iftode , v. s. mani
",n
"LEFT id: NA
RIGHT id: 1999

LEFT text: The paper presents a systematic review of the relative efficacy of traditional listing and the USPS address list as sampling frames for national probability samples of households. NORC and ISR collaborated to compare these two national area-probability sampling frames for household surveys. We conducted this comparison in an ongoing survey operation which combines the current wave of the HRS with the first wave of NSHAP. Since 2000, survey samplers have been exploring the potential of the USPS address lists to serve as a sampling frame for probability samples from the general population. We report the relative coverage properties of the two frames, as well as predictors of the coverage and performance of the USPS frame. The research provides insight into the coverage and cost/benefit trade-offs that researchers can expect from traditionally listed frames and USPS address databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: is gui programming a database research problem ?

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: nita goyal , charles hoch , ravi krishnamurthy , brian meckler , michael suckow
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1643

LEFT text: In this paper, we propose a bulk-algebra, TIX, and describe how it can be used as a basis for integrating information retrieval techniques into a standard pipelined database query evaluation engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast subsequence matching in time-series databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christos faloutsos , m. ranganathan , yannis manolopoulos
",y
"LEFT id: NA
RIGHT id: 971

LEFT text: DataJoiner (DJ) is a heterogeneous database system that provides a single database image of multiple databases. It provides transparent access to tables at remote databases through user defined aliases (nicknames) that can be accessed as if they were local tables. DJ is also a fully functional relational database system. A couple of salient features of the DataJoiner query optimizer are: (1) A query submitted to DataJoiner is optimized using a cost model that takes into account the remote optimizer’s capabilities in addition to the remote query processing capabilities and (2) If a remote database system lacks some functionality (eg: sorting), DataJoiner compensates for it. In this paper, we present the design of the Datajoiner query optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: heterogeneous database query optimization in db2 universal datajoiner

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shivakumar venkataraman , tian zhang
",y
"LEFT id: NA
RIGHT id: 77

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mocha : a self-extensible database middleware system for distributed data sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1850

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-relational database systems ( tutorial ) : principles , products and challenges

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. carey , nelson m. mattos , anil k. nori
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 923

LEFT text: Whereas physical database tuning has received a lot of attention over the last decade, logical database tuning seems to be under-studied. We have developed a project called DBA Companion devoted to the understanding of logical database constraints from which logical database tuning can be achieved.In this setting, two main data mining issues need to be addressed: the first one is the design of efficient algorithms for functional dependencies and inclusion dependencies inference and the second one is about the interestingness of the discovered knowledge. In this paper, we point out some relationships between database analysis and data mining. In this setting, we sketch the underlying themes of our approach. Some database applications that could benefit from our project are also described, including logical database tuning.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: analysis of existing databases at the logical level : the dba companion project

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: fabien de marchi , st &#233; phane lopes , jean-marc petit , farouk toumani
",y
"LEFT id: NA
RIGHT id: 173

LEFT text: We deal first with the case of perfectly declustered queries, i.e., queries which retrieve a fixed proportion of the answer from each disk. We show that the fraction of the dataset which must be allocated to each disk is affected by both the relative speed and capacity of the disk. Furthermore, the hierarchical structure of most distributed systems, where groups of disks are placed in servers, imposes further complications due to variations . in server and network bandwidths which may affect the actual achievable transfer rates. We propose an algorithm which determines the fraction of the dataset which must be loaded on each disk. The algorithm may be tailored to find disk loading for minimal response time for a given database size, or to compute a system profile showing the optimal loading of the disks for all possible ranges of database sizes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql open heterogeneous data access

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: berthold reinwald , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1427

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 1388

LEFT text: We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: biodiversity informatics infrastructure : an information commons for the biodiversity community

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: gladys a. cotter , barbara t. bauldock
",y
"LEFT id: NA
RIGHT id: 2290

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1155

LEFT text: This paper presents general algorithms for concurrency control in tree-based access methods as well as a recovery protocol and a mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data structure, an index structure supporting an extensible set of queries and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods. The concurrency control protocol is based on an extension of the link technique originally developed for B-trees, and completely avoids holding node locks during I/Os. Repeatable read isolation is achieved with a novel combination of predicate locks and two-phase locking of data records. To our knowledge, this is the first time that isolation issues have been addressed outside the context of B-trees. A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the algorithms developed here deviate from their B-tree counterparts. An implementation of GiSTs emulating B-trees in DB2/Common Server is underway.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency and recovery for index trees

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david lomet , betty salzberg
",n
"LEFT id: NA
RIGHT id: 1855

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization at the crossroads

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: Query optimizers normally compile queries into one optimal plan by assuming complete knowledge of all cost parameters such as selectivity and resource availability. The execution of such plans could be sub-optimal when cost parameters are either unknown at compile time or change significantly between compile time and runtime [Loh89, GrW89]. Parametric query optimization [INS+92, CG94, GK94] optimizes a query into a number of candidate plans, each optimal for some region of the parameter space. In this paper, we present parametric query optimization algorithms. Our approach is based on the property that for linear cost functions, each parametric optimal plan is optimal in a convex polyhedral region of the parameter space. This property is used to optimize linear and non-linear cost functions. We also analyze the expected sizes of the parametric optimal set of plans and the number of plans produced by the Cole and Graefe algorithm [CG94].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: ROLEX is a research system for closely coupled XML-relational interoperation [2]. Whereas typical XML-based applications interoperate with existing relational databases via a “shred-and-publish” approach, the ROLEX system seeks to provide direct access to relational data via XML interfaces at the speed of cached XML data. To achieve this, ROLEX is integrated tightly with both the DBMS and the application through a standard interface supported by most XML parsers, the Document Object Model (DOM). Thus, in general, an application need not be modified to be used with ROLEX.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 846

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: active views for electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: serge abiteboul , bernd amann , sophie cluet , adi eyal , laurent mignet , tova milo
",n
"LEFT id: NA
RIGHT id: 1270

LEFT text: The Teradata Multimedia Object Manager is a general-purpose content analysis multimedia server designed for symmetric multiprocessing and massively parallel processing environments. The Multimedia Object Manager defines and manipulates user-defined functions (UDFs), which are invoked in parallel to analyze or manipulate the contents of multimedia objects. Several computationally intensive applications of this technology, which use large persistent datasets, include fingerprint matching, signature verification, face recognition, and speech recognition/translation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exploiting early sorting and early partitioning for decision support query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: j. claussen , a. kemper , d. kossmann , c. wiesner
",n
"LEFT id: NA
RIGHT id: 191

LEFT text: The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure and portable database extensibility

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael godfrey , tobias mayr , praveen seshadri , thorsten von eicken
",y
"LEFT id: NA
RIGHT id: 1976

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: The design of a distributed deductive database system differs from the design of conventional non-distributed deductive database systems in that it requires design of distribution of both the database and rulebase. In this paper, we address the rule allocation problem. We consider minimisation of data communic& tion cost during rule execution as a primary basis for rule allocation. The rule allocation problem can be stated in terms of a directed acyclic graph, where nodes represent rules or relations, and edges represent either dependencies between rules or usage of relations by rules. The arcs are given weights representing volume of data that need to flow between the connected nodes. We show that rule allocation problem is NP-complete. Next, we propose a heuristic for nonreplicated allocation based on successively combining adjacent nodes for placement at same site which are connected by highest weight edge, and study its performance vib*vis the enumerative algorithm for optimal allocation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1144

LEFT text: Multiversion access methods have been emerged in the literature primarily to support queries on a transaction-time database where records are never physically deleted. For a popular class of efficient methods (including the multiversion Btree), data records and index entries are occasionally duplicated to separate data according to time. In this paper, we present techniques for improving query processing in multiversion access methods. In particular, we address the problem of avoiding duplicates in the response sets. We first discuss traditional approaches that eliminate duplicates using hashing and sorting. Next, we propose two new algorithms for avoiding duplicates without using additional data structures. The one performs queries in a depth-first order starting from a root, whereas the other exploits links between data pages. These methods are discussed in full details and their main properties are identitied. Preliminary performance results confirm the advantages of these methods in comparison to traditional ones according to CPU-time, disk accesses and storage.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing techniques for multiversion access methods

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger
",y
"LEFT id: NA
RIGHT id: 1234

LEFT text: Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tree pattern query minimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s. amer-yahia , s. cho , l. v. s. lakshmanan , d. srivastava
",n
"LEFT id: NA
RIGHT id: 2128

LEFT text: Amongst the wide range of parking solutions that can contribute to reduce parking problems or regulate parking activities, e Parking looks at developing and applying an innovative e-business application for parking space optimization. The purpose of this paper is to present the innovative e-business platform that has been deve loped, from a technical point of view, by the University of Zurich. The ideas are coming from a transcross European consortium within the framework of the IST Information Society Technologies of the 5th framework program. E-Parking provides a database-centered Web application solution based on our proposed conceptual model CIA (Channel, Integration, Application) for Web applications. The WAP, WEB and Bluetooth communication channels enable drivers to obtain early information on available parking space, make a reservation, access the reserved place and pay for the service booked. In reaching this goal, the innovative solutions seek to benefit all social segments, to optimize existing parking resources, and to contribute to achieving a more sustainable urban transport, reducing congestion and pollution.t

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 2291

LEFT text: A central development in the database area concerns tools that allow non expert users to understand and easily extract information from a database. Fourth generation query languages, although non-procedural, are not friendly enough for a casual user who must know both the logical structure of the database and the syntax and semantics of the DBMS query language. Instead, recently proposed visual systems which allow a user to extract information by means of interactive graphical commands, have not yet been able to combine ease of use and high expressive power.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: graphdb : modeling and querying graphs in databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 316

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems, such as query optimization, the maintenance of physical data independence, data integration and data warehousing. This article surveys the theoretical issues concerning the problem of answering queries using views

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: generating efficient plans for queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: foto n. afrati , chen li , jeffrey d. ullman
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 470

LEFT text: Datacube queries compute aggregates over database relations at a variety of granularities, and they constitute an important class of decision support queries. Real-world data is frequently sparse, and hence efficiently computing datacubes over large sparse relations is important. We show that current techniques for computing datacubes over sparse relations do not scale well with the number of CUBE BY attributes, especially when the relation is much larger than main memory. We propose a novel algorithm for the fast computation of datacubes over sparse relations, and demonstrate the efficiency of our algorithm using synthetic, benchmark and real-world data sets. When the relation fits in memory, our technique performs multiple in-memory sorts, and does not incur any I/O beyond the input of the relation and the output of the datacube itself. When the relation does not fit in memory, a divideand-conquer strategy divides the problem of computing the datacube into several simpler computations of sub-datacubes. Often, all but one of the sub-datacubes can be computed in memory and our in-memory solution applies. In that case, the total I/O overhead is linear in the number of CUBE BY attributes. We demonstrate with an implementation that the CPU cost of our algorithm is dominated by the I/O cost for sparse relations. ‘The research of Kenneth A.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient computation of iceberg cubes with complex measures

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jiawei han , jian pei , guozhu dong , ke wang
",n
"LEFT id: NA
RIGHT id: 1972

LEFT text: This paper presents an innovative partitionbased time join strategy for temporal databases where time is represented by time intervals. The proposed method maps time intervals to points in a two dimensional space and partitions the space into subspaces. Tupies of a temporal relation are clustered into partitions based on the mapping in the space. As a result, when two temporal relations are to be joined over the time attribute, a partition in one relation only needs to be compared with a predetermined set of partitions of the other relation. The mapping scheme and the join algorithms are described. The use of spatial indexing techniques to support direct access to the stored partitions is discussed. The results of a preliminary performance study indicate the efficiency of the proposed method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partition based spatial-merge join

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jignesh m. patel , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: This second special issue provides a forum for topical issues that demonstrate the usefulness of PLS-SEM by piloting applications of this method in the field of strategic management with strong implications for strategic research and practice. As such, the special issue targets two audiences: academics involved in the fields of strategy and management, and practitioners such as consultants. The six articles in this issue are summarized in the following paragraphs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: We consider an optimization technique for deductive and relational databases. The optimization technique is an extension of the magic templates rewriting, and it can improve the performance of query evaluation by not materializing the extension of intermediate views. Standard relational techniques, such as unfolding embedded view definitions, do not apply to recursively defined views, and so alternative techniques are necessary. We demonstrate the correctness of our rewriting. We define a class of “nonrepeating” view definitions, and show that for certain queries our rewriting performs at least as well as magic templates on nonrepeating views, and often much better. A syntactically recognizable property, called “weak right-linearity”, is proposed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1655

LEFT text: Gifford’s basic Quorum Consensus algorithm for data replication is generalized to accommodate nested transactions and transaction failures (aborts), A formal description of the generalized algorithm is presented using the new Lynch-Merritt inputoutput automaton model for nested transaction systems. This formal description is used to construct a complete (yet simple) proof of correctness that uses standard assertional techniques and is based on a natural correctness condition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: quorum consensus in nested-transaction systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kenneth j. goldman , nancy lynch
",y
"LEFT id: NA
RIGHT id: 441

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximating multi-dimensional aggregate range queries over real attributes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",y
"LEFT id: NA
RIGHT id: 1601

LEFT text: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1866

LEFT text: In order to cope with the dynamic scenario of fast changing business requirements enterprises have embraced web technologies to manage their business processes. However, the ability to integrate business processes like procurement, customer relationship management, finance, human resources and manufacturing in a typical supply chain on the web is a challenging task. Today’s virtual enterprises need to integrate different workflows within and across enterprises efficiently so as to provide seamless services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1299

LEFT text: HTTPHypertext Transfer Protocolrequests, Java virtual machines embedded in Web browsers to run applets, or remote procedure call protocols), researchers and practitioners alike are coming to realize that any technology for information services should tackle head-on the problem of semantic interoperability, i.e. the capability of an application to exchange data and activate data manipulation functions by utilizing its domain model. Therefore, architecture for information services should first and foremost handle the semantic issues in providing information services. Within this architecture, tools for semantic understanding of heterogeneous, distributed, and autonomously evolving data sources should be developed in order to provide a transparent representation of the domain regardless of the underlying data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: using semantic values to facilitate interoperability among heterogeneous information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore , michael siegel , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 77

LEFT text: Querying large numbers of data sources is gaining importance due to increasing numbers of independent data providers. One of the key challenges is executing queries on all relevant information sources in a scalable fashion and retrieving fresh results. The key to scalability is to send queries only to the relevant servers and avoid wasting resources on data sources which will not provide any results. Thus, a catalog service, which would determine the relevant data sources given a query, is an essential component in efficiently processing queries in a distributed environment. This paper proposes a catalog framework which is distributed across the data sources themselves and does not require any central infrastructure. As new data sources become available, they automatically become part of the catalog service infrastructure, which allows scalability to large numbers of nodes. Furthermore, we propose techniques for workload adaptability. Using simulation and real-world data we show that our approach is valid and can scale to thousands of data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mocha : a self-extensible database middleware system for distributed data sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 137

LEFT text: DTL’s DataSpot is a database publishing tool that enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation. DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a hyperbase. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and userdefined associations, and creates the hyperbase. The DataSpot Search Server performs searches and navigation against the hyperbase, returning answers to the user either in HTML pages or through an object API. The DataSpot product has been successfilly deployed in diverse application areas including electronic catalogs, yellow pages, classified ads, help desks and finance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dtl 's dataspot : database exploration as easy as browsing the web ...

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shaul dar , gadi entin , shai geva , eran palmon
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 2191

LEFT text: Abstract. Inter-object references are one of the key concepts of object-relational and object-oriented database systems. In this work, we investigate alternative techniques to implement inter-object references and make the best use of them in query processing, i.e., in evaluating functional joins. We will give a comprehensive overview and performance evaluation of all known techniques for simple (single-valued) as well as multi-valued functional joins. Furthermore, we will describe special order-preserving\/ functional-join techniques that are particularly attractive for decision support queries that require ordered results. While most of the presentation of this paper is focused on object-relational and object-oriented database systems, some of the results can also be applied to plain relational databases because index nested-loop joins\/ along key/foreign-key relationships, as they are frequently found in relational databases, are just one particular way to execute a functional join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on processing xml in ldap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pedro jos &#233; marr &#243; n , georg lausen
",n
"LEFT id: NA
RIGHT id: 592

LEFT text: Regular readers of this column will have become familiar with database language SQL -- indeed, most readers are already familiar with it. We have also discussed the fact that the SQL standard is being published in multiple parts and have even discussed one of those parts in some detail[l].Another standard, based on SQL and its structured user-defined types[2], has been developed and published by the International Organization for Standardization (ISO). This standard, like SQL, is divided into multiple parts (more independent than the parts of SQL, in fact). Some parts of this other standard, known as SQL/MM, have already been published and are currently in revision, while others are still in preparation for initial publication.In this issue, we introduce SQL/MM and review each of its parts, necessarily at a high level.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql multimedia and application packages ( sql/mm )

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jim melton , andrew eisenberg
",y
"LEFT id: NA
RIGHT id: 1649

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 2114

LEFT text: In this paper we describe a Patricia tree-based B-tree variant suitable for OLTP. In this variant, each page of the B-tree contains a local Patricia tree instead of the usual sorted array of keys. It has been implemented in iAnywhere ASA Version 8.0. Preliminary experience has shown that these indexes can provide significant space and performance benefits over existing ASA indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: global transaction support for workflow management systems : from formal specification to practical implementation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: paul grefen , jochem vonk , peter apers
",y
"LEFT id: NA
RIGHT id: 852

LEFT text: Aggregation along hierarchies is a critical summary technique in a large variety of on-line applications including decision support and network management (e.g., IP clustering, denial-of-service attack monitoring). Despite the amount of recent study that has been dedicated to online aggregation on sets (e.g., quantiles, hot items), surprisingly little attention has been paid to summarizing hierarchical structure in stream data.    The problem we study in this paper is that of finding Hierarchical Heavy Hitters (HHH): given a hierarchy and a fraction φ, we want to find all HHH nodes that have a total number of descendants in the data stream no smaller than φ of the total number of elements in the data stream, after discounting the descendant nodes that are HHH nodes. The resulting summary gives a topological ""cartogram"" of the hierarchical data. We present deterministic and randomized algorithms for finding HHHs, which builds upon existing techniques by incorporating the hierarchy into the algorithms. Our experiments demonstrate several factors of improvement in accuracy over the straightforward approach, which is due to making algorithms hierarchy-aware.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: comparing hierarchical data in external memory

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe
",n
"LEFT id: NA
RIGHT id: 1506

LEFT text: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 1998

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 964

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast high-dimensional data search in incomplete databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: beng chin ooi , cheng hian goh , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",y
"LEFT id: NA
RIGHT id: 2078

LEFT text: In this paper, we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. Our estimation algorithms are probabilistic in nature and rely on a novel, hash-based synopsis data structure, termed ""2-level hash sketch. We demonstrate how our 2-level hash sketch synopses can be used to provide low-error, high-confidence estimates for the cardinality of set expressions (including operators such as set union, intersection, and difference) over continuous update streams, using only small space and small processing time per update. Furthermore, our estimators never require rescanning or resampling of past stream items, regardless of the number of deletions in the stream. We also present lower bounds for the problem, demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. Preliminary experimental results verify the effectiveness of our approach

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1768

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: materialized views and data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1996

LEFT text: Descriptions of new indexing techniques are a common outcome of database research, but these descriptions are sometimes marred by poor methodology and a lack of comparison to other schemes. In this paper we describe a framework for presentation and comparison of indexing schemes that we believe sets a minimum standard for development and dissemination of research results in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guidelines for presentation and comparison of indexing techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: justin zobel , alistair moffat , kotagiri ramamohanarao
",y
"LEFT id: NA
RIGHT id: 513

LEFT text: A book title cannot be more timely or accurate. Information rules society and it always has. The key difference is, that in our generation, the manner in which information is managed is more apparent to the everyday person and as more information becomes readily available the curse is that information can overload and intimidate us with little or no effort. Prior to the personal computer the everyday person could more easily manage the flow—such is not the case today. Throw into this fray the fact that information is a force in economics and the everyday person may become bewildered and perplexed. Many of these concerns are addressed in this excellent new book that focuses on the information economy and its effect on society and culture. In ten engaging chapters, key concepts such as pricing, versioning, rights management, recognizing and managing lock-in, networks, cooperation and compatibility, standards, and information policy are dissected, discussed, and explained. Most chapters end with lessons that reflect key points made in the chapter. The first chapter presents the foundation of the thesis of the book—the material is relatively general in nature—and sets the stage for the following nine interesting chapters. In discussing pricing, the authors cite the case of Encyclopedia Britannica and its inability to compete with the more popular and less expensive Microsoft product, Encarta. An associated concept, “versioning” is discussed and the authors show how a business can offer information products in different versions for differing markets to the benefit of the bottom line. The heady and confusion issue of copyright management, especially as related to internet economy is examined in chapter four of the book. Another issue of concern, lock-in, which results from switching from one technology to another, is discussed in chapters five and six. In chapter seven the authors discuss how the old industrial economy was driven by economies of scale whereas the information economy is driven by economics of networks. The last three chapters push the envelope and advise the reader how to affect real changes in their relationship with the information economy. The last chapter is key in that it discusses current government information policies in light of advice provided earlier in the book. This book may be one of the best to examine the theory and implications of the information economy. Although written by heavyweights in the field of economics and information management, the authors present a well written and thoughtful treatment of a subject that non-academics and academics alike should enjoy and refer to often. More importantly, this book offers direct advice that could well affect the bottom line of many entrepreneurs and existing companies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",y
"LEFT id: NA
RIGHT id: 1217

LEFT text: Businessestoday need to interrelate data stored in diverse systems with differing capabilities, ideally via a single high-level query interface. We present the design of a query optimizer for Garlic [C 95], a middleware system designedto integrate data from a broad range of data sources with very different query capabilities. Garlic’s optimizer extends the rule-based approach of [Loh88] to work in a heterogeneous environment, by defining generic rules for the middleware and using wrapper-provided rules to encapsulate the capabilities of each data source. This approach offers great advantages in terms of plan quality, extensibility to new sources, incremental implementationof rules for new sources, and the ability to express the capabilities of a diverse set of sources. We describe the design and implementationof this optimizer, and illustrate its actions through an example.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 1981

LEFT text: An important requirement for multimedia presentations is the ability to compose new multimedia objects from the existing ones using temporal relationships. When compositions of continuous media objects are specified dynamically, the task of displaying these objects poses new challenges. These challenges are addressed in this paper. We show that in the case of a single composite object retrieval, a prefetching technique, simple sliding, provides an approach to reduce latency and buffering requirements. We extend this prefetching technique to the problem of retrieving multiple composite objects simultaneously. This new technique is termed buffered sliding. We consider several variants of the buffered sliding algorithm. A simulationbased study is used to compare their usage pattern of available memory and in determining their relative merits in reducing latency and increasing disk bandwidth utilization. *Research supported in part by the National Science Foundation under grants IRI-9203389, IRI-9258362 (NY1 award), and CDA-9216321, and a Hewlett-Packard unrestricted cash/equipment gift. Permission to copy without fee all OT part of this material is granted provided that the copies are not made OT distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, OT to republish, Tequires a fee and/or special permission from the Endowment. Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 An important requirement for multimedia information systems is the ability to compose new multimedia objects from the existing multimedia objects [LG91]. Temporal primitives (e.g., before, after, overlaps [All83]) p rovide one of the most powerful and natural ways of authoring composition. Such composition is necessary in the domain of electronic publishing, computer music, news editing and many other applications. In this paper, we investigate how a multimedia storage system can display a composite object. We focus on composite objects that are authored dynamically. To illustrate an example environment, consider a TVnews editor preparing to present new footage on unrest in Bosnia. He requires background material to provide the audience with a context. He considers playing a sequence of clips one after another from different footage taken at different times to author a thirty second presentation. He may decide to accompany a footage with appropriate music in parts (i.e., music overlaps video). He may conclude his presentatiod with split windows that concurrently display short clips that leave us with the images of diverse scenes in Bosnia. During editing of such a presentation, he would try several possible composition, possibly picking different sets of clips or music from the repository. Surely, the editor would like to display his composition during the authoring process to evaluate his choice.’ Thus, the process of editing a news story consisted of specifying composite objects using temporal relationships and then displaying those. Note that displaying atomic objects of highbandwidth continuous media objects, such as video (requiring no composition) is a challenging task in itself. Video clips require a continuous bandwidth for their display. For example, the bandwidth re-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional resource scheduling for parallel queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: This paper describes the ACM Multimedia '94 Conference Workshop on Multimedia Database Management Systems held on 21 October 1994 in San Francisco, California. The workshop consisted of four sessions: designing multimedia database management systems, video and continuous media service, multimedia storage and retrieval management, and miscellaneous topics in multimedia data management. The workshop concluded with a discussion session on directions for multimedia database management. Twenty-eight participants from U.S.A., U.K., Germany, Norway, and Egypt attended the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 488

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mlpq/gis constraint database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang
",n
"LEFT id: NA
RIGHT id: 2024

LEFT text: We propose a data model and query language that integrates an explicit modeling and querying of graphs smoothly into a standard database environment. For standard applications, some key features of object-oriented modeling are offered such as object classes organized into a hierarchy, object identity, and attributes referencing objects. Querying can be done in a familiar style with a derive statement that can be used like a select ... from ... where. On the other hand, the model allows for an explicit representation of graphs by partitioning object classes into simple classes, link classes, and path classes whose objects can be viewed as nodes, edges, and explicitly stored paths of a graph (which is the whole database instance). For querying graphs, the derive statement has an extended meaning in that it allows one to refer to subgraphs of the database graph. A powerful rewrite operation is offered for the manipulation of heterogeneous sequences of objects which often occur as a result of accessing the database graph. Additionally there are special graph operations like determining a shortest path or a subgraph and the model is extensible by such operations. Besides being attractive for standard applications, the model permits a natural representation and sophisticated querying of networks, in particular of spatially embedded networks like highways, public transport, etc. This work was supported by the ESPRIT Basic Research Project 6881 AMUSING

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: panel : querying networked databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1123

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the structured information manager : a database system for sgml documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ron sacks-davis
",n
"LEFT id: NA
RIGHT id: 780

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1854

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: delaunay : a database visualization system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1795

LEFT text: Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (XopY) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ∈{<, ≤, =, ≠, >, ≥). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ≤, =, ≥, >} and {<, ≤, =, ≠, ≥, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from <archive.fiu.edu>.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integrating association rule mining with relational database systems : alternatives and implications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sunita sarawagi , shiby thomas , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1389

LEFT text: Softwaresysteme, die ihre Services an Charakteristika individueller Benutzer anpassen haben sich bereits als effektiver und/oder benutzerfreundlicher als statische Systeme in mehreren Anwendungsdomanen erwiesen. Um solche Anpassungsleistungen anbieten zu konnen, greifen benutzeradaptive Systeme auf Modelle von Benutzercharakteristika zuruck. Der Aufbau und die Verwaltung dieser Modelle wird durch dezidierte Benutzermodellierungskomponenten vorgenommen. Ein wichtiger Zweig der Benutzermodellierungsforschung beschaftigt sich mit der Entwicklung sogenannter ?Benutzermodellierungs-Shells?, d.h. generischen Benutzermodellierungssystemen, die die Entwicklung anwendungsspezifischer Benutzermodellierungskomponenten erleichtern. Die Bestimmung des Leistungsumfangs dieser generischen Benutzermodellierungssysteme und deren Dienste bzw. Funktionalitaten wurde bisher in den meisten Fallen intuitiv vorgenommen und/oder aus Beschreibungen weniger benutzeradaptiver Systeme in der Literatur abgeleitet. In der jungeren Vergangenheit fuhrte der Trend zur Personalisierung im World Wide Web zur Entwicklung mehrerer kommerzieller Benutzermodellierungsserver. Die fur diese Systeme als wichtig erachteten Eigenschaften stehen im krassen Gegensatz zu denen, die bei der Entwicklung der Benutzermodellierungs-Shells im Vordergrund standen und umgekehrt. Vor diesem Hintergrund ist das Ziel dieser Dissertation (i) Anforderungen an Benutzermodellierungsserver aus einer multi-disziplinaren wissenschaftlichen und einer einsatzorientierten (kommerziellen) Perspektive zu analysieren, (ii) einen Server zu entwerfen und zu implementieren, der diesen Anforderungen genugt, und (iii) die Performanz und Skalierbarkeit dieses Servers unter der Arbeitslast kleinerer und mittlerer Einsatzumgebungen gegen die diesbezuglichen Anforderungen zu uberprufen. Um dieses Ziel zu erreichen, verfolgen wir einen anforderungszentrierten Ansatz, der auf Erfahrungen aus verschiedenen Forschungsbereichen aufbaut. Wir entwickeln eine generische Architektur fur einen Benutzermodellierungsserver, die aus einem Serverkern fur das Datenmanagement und modular hinzufugbaren Benutzermodellierungskomponenten besteht, von denen jede eine wichtige Benutzermodellierungstechnik implementiert. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: demonstration : enabling scalable online personalization on the web

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik dutta , anindya datta , debra e. vandermeer , krithi ramamritham , helen m. thomas
",n
"LEFT id: NA
RIGHT id: 2057

LEFT text: Abstract. XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on relational support for xml publishing : beyond sorting and tagging

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: surajit chaudhuri , raghav kaushik , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 1791

LEFT text: In this paper, we first focus our attention on the question of how much space remains for performance improvement over current association rule mining algorithms. Our strategy is to compare their performance against an “Oracle algorithm” that knows in advance the identities of all frequent itemsets in the database and only needs to gather their actual supports to complete the mining process. Our experimental results show that current mining algorithms do not perform uniformly well with respect to the Oracle for all database characteristics and support thresholds. In many cases there is a substantial gap between the Oracle’s performance and that of the current mining algorithms. Second, we present a new mining algorithm, called ARMOR, that is constructed by making minimal changes to the Oracle algorithm. ARMOR consistently performs within a factor of two of the Oracle on both real and synthetic datasets over practical ranges of support specifications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query flocks : a generalization of association-rule mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: dick tsur , jeffrey d. ullman , serge abiteboul , chris clifton , rajeev motwani , svetlozar nestorov , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: This tutorial presents the primary constructs of the consensus temporal query language TSQL2 via a media planning scenario. Media planning is a series of decisions involved in the delivery of a promotional message via mass media. We will follow the planning of a particular advertising campaign. We introduce the scenario by identifying the marketing objective. The media plan involves placing commercials, and is recorded in a temporal database. The media plan must then be evaluated; we show how TSQL2 can be used to derive information from the stored data. We then give examples that illustrate storing and querying indeterminate information, comparing multiple versions of the media plan, accommodating changes to the schema, and vacuuming a temporal database of old data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1512

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic assumptions and query evaluation in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: claudio bettini , x. sean wang , elisa bertino , sushil jajodia
",n
"LEFT id: NA
RIGHT id: 1374

LEFT text: This article concentrates on query unnesting (also known as query decorrelation), an optimization that, even though it improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature, and is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of our method is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform method of unnesting queries, regardless of their type of nesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries on compressed bitmaps

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sihem amer-yahia , theodore johnson
",n
"LEFT id: NA
RIGHT id: 2111

LEFT text: In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: user-cognizant multidimensional analysis

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1928

LEFT text: In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database buffer size investigation for oltp workloads

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thin-fong tsuei , allan n. packer , keng-tai ko
",y
"LEFT id: NA
RIGHT id: 809

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 430

LEFT text: Tree pattern is at the core of XML queries. The tree patterns in XML queries typically contain redundancies, especially when broad integrity constraints (ICs) are present and considered. Apparently, tree pattern minimization has great significance for efficient XML query processing. Although various minimization schemes/algorithms have been proposed, none of them can exploit broad ICs for thoroughly minimizing the tree patterns in XML queries. The purpose of this research is to develop an innovative minimization scheme and provide a novel implementation algorithm.Design/methodology/approach – Query augmentation/expansion was taken as a necessary first‐step by most prior approaches to acquire XML query pattern minimization under the presence of certain ICs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and extensible algorithms for multi query optimization

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: prasan roy , s. seshadri , s. sudarshan , siddhesh bhobe
",n
"LEFT id: NA
RIGHT id: 986

LEFT text: Recently, several query languages have been proposed for querying information sources whose data is not constrained by a schema, or whose schema is unknown. Examples include: LOREL (for querying data combined from several heterogeneous sources), W3QS (for querying the World Wide Web); and UnQL (for querying unstructured data). The natural data model for such languages is that of a rooted, labeled graph. Their main novelty is the ability to express queries which traverse arbitrarily long paths in the graph, typically described by a regular expression. Such queries however may prove difficult to evaluate in the case when the data is distributed on severalsites, with many edges going between sites. A typical case is that of a collection of WWW sites, with links pointing freely from one site to another (even forming cycles). A naive query shipping strategy may force the query to migrate back and forth between the various sites, leading to poor performance (or even non-termination). We present a technique for query decomposition, under which the query is shipped exactly once to every site, computed locally, then the local results are shipped to the client, and assembled here into the final result. This technique is efficient, in that (a) only data which is part of the final result is shipped from the data sites to the client site, and (b) the total work done locally at all sites does not exceed that needed for computing the (unoptimized) query on a centralized version of the database. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated with a database system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 1114

LEFT text:  The database systems have nowadays an increasingly important role in the knowledge-based society, in which computers have penetrated all fields of activity and the Internet tends to develop worldwide. In the current informatics context, the development of the applications with databases is the work of the specialists. Using databases, reach a database from various applications, and also some of related concepts, have become accessible to all categories of IT users. This paper aims to summarize the curricular area regarding the fundamental database systems issues, which are necessary in order to train specialists in economic informatics higher education. The database systems integrate and interfere with several informatics technologies and therefore are more difficult to understand and use. Thus, students should know already a set of minimum, mandatory concepts and their practical implementation: computer systems, programming techniques, programming languages, data structures. The article also presents the actual trends in the evolution of the database systems, in the context of economic informatics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 1567

LEFT text: This article describes a novel way of combining data mining techniques on Internet data in order to discover actionable marketing intelligence in electronic commerce scenarios. The data that is considered not only covers various types of server and web meta information, but also marketing data and knowledge. Furthermore, heterogeneity resolution thereof and Internet- and electronic commerce-specific pre-processing activities are embedded. A generic web log data hypercube is formally defined and schematic designs for analytical and predictive activities are given. From these materialised views, various online analytical web usage data mining techniques are shown, which include marketing expertise as domain knowledge and are specifically designed for electronic commerce purposes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a framework for providing consistent and recoverable agent-based access to heterogeneous mobile databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: evaggelia pitoura , bharat bhargava
",y
"LEFT id: NA
RIGHT id: 802

LEFT text: Various types of computer systems are used behind the scenes in many parts of the telecommunications network to ensure its efficient and trouble-free operation. These systems are large, complex, and expensive real-time computer systems that are mission critical, and contains a database engine as a critical component. These systems share some of common database issues with conventional applications, but they also exhibit rather unique characteristics that present challenging database issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: issues in network management in the next millennium

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael l. brodie , surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 1924

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the semantics of now in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james clifford , curtis dyreson , tom &#225; s isakowitz , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: An Object-Oriented database can utilize the benefits of both the design and implementation of any application. Due to the increased popularity of database systems many new database systems based on varying data model and implementation have entered in the market. Database systems have complex architecture but they are the key factors behind the business transformations. Choosing the best one in any category is an important task based on performance analysis. This chapter deals with the database estimation methodology which integrates the database analysis task and performance analysis task. There are three major techniques for the performance estimation which are analytical modeling, simulation modeling and benchmarking.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 53

LEFT text: Data warehouses store large volumes of data which are used frequently by decision support applications. Such applications involve complex queries. Query performance in such an environment is critical because decision support applications often require interactive query response time. Because data warehouses are updated infrequently, it becomes possible to improve query performance by caching sets retrieved by queries in addition to query execution plans. In this paper we report on the design of an intelligent cache manager for sets retrieved by queries called WATCHMAN, which is particularly well suited for data warehousing environment. Our cache manager employs two novel, complementary algorithms for cache replacement and for cache admission. WATCHMAN aims at minimizing query response time and its cache replacement policy swaps out entire retrieved sets of queries instead of individual pages. The cache replacement and admission algorithms make use of a profit metric, which considers for each retrieved set its average rate of reference, its size, and execution cost of the associated query. We report on a performance evaluation based on the TPC-D and Set Query benchmarks. These experiments show that WATCHMAN achieves a substantial performance improvement in a decision support environment when compared to a traditional LRU replacement algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamat : a dynamic view management system for data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 919

LEFT text: Abstract. Relational database systems have traditionally optimized for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results (which were obtained without using any indices on the participating relations), when compared to NSM: (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM's stall time due to data cache accesses; (b) range selection queries and updates on memory-resident relations execute 17–25% faster; and (c) TPC-H queries involving I/O execute 11–48% faster. Finally, we show that PAX performs well across different memory system designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data page layouts for relational databases on deep memory hierarchies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: anastassia ailamaki , david j. dewitt , mark d. hill
",y
"LEFT id: NA
RIGHT id: 1511

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 2284

LEFT text: Significant advances have been achieved in system, syntactic and structural/schematic interoperability in a distributed network. Yet, meaningful exchange of information among autonomously designed and populated, dynamic, structured and semi-structured Heterogeneous Information Sources (HIS) remains a major challenge. A pure Peer-to-Peer architecture lends itself naturally to this problem as the information sources are totally autonomous and, practically, a-priori integration cannot be assumed. While ontologies may play a role in facilitating integration, they are not the panacea [3] advocated by many researchers. Rather, integration ought to be viewed as an emergent phenomenon constructed incrementally, and its state is dependent on the frequency and the quality of interactions between the peers and their subsequent negotiations and agreements to reach common interpretations within the context of a given task.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: in-context peer-to-peer information filtering on the web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: aris m. ouksel
",y
"LEFT id: NA
RIGHT id: 2239

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 2090

LEFT text: This paper presents a novel strategy for temporal coalescing. Temporal coalescing merges the temporal extents of value-equivalent tuples. A temporal extent is usually coalesced offline and stored since coalescing is an expensive operation. But the temporal extent of a tuple with now, times at different granularities, or incomplete times cannot be determined until query evaluation. This paper presents a strategy to partially coalesce temporal extents by identifying regions that are potentially covered. The covered regions can be used to evaluate temporal predicates and constructors on the coalesced extent. Our strategy uses standard relational database technology. We quantify the cost using the Oracle DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal coalescing with now granularity , and incomplete information

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: curtis e. dyreson
",y
"LEFT id: NA
RIGHT id: 663

LEFT text: Amongst the wide range of parking solutions that can contribute to reduce parking problems or regulate parking activities, e Parking looks at developing and applying an innovative e-business application for parking space optimization. The purpose of this paper is to present the innovative e-business platform that has been deve loped, from a technical point of view, by the University of Zurich. The ideas are coming from a transcross European consortium within the framework of the IST Information Society Technologies of the 5th framework program. E-Parking provides a database-centered Web application solution based on our proposed conceptual model CIA (Channel, Integration, Application) for Web applications. The WAP, WEB and Bluetooth communication channels enable drivers to obtain early information on available parking space, make a reservation, access the reserved place and pay for the service booked. In reaching this goal, the innovative solutions seek to benefit all social segments, to optimize existing parking resources, and to contribute to achieving a more sustainable urban transport, reducing congestion and pollution.t

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 516

LEFT text: This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 605

LEFT text: This article summarizes my recent job search that effectively began in the late fall of 2000 and ended in the early summer of 2001. The opinions I express here are largely based on what I experienced, heard and read from various sources, and should be taken as mere tips or suggestions for Ph.D. students who are soon to graduate and look for a position in a research-oriented academic institution.This is by no means a comprehensive guide to job searching: in limited space, I address only the issues that I deem more relevant or important, in an effort to provide information and insight that I believe is not readily available elsewhere. I do, however, try to provide pointers to (hopefully) complementary information throughout the text wherever appropriate and in Section 13.Figure 1 illustrates the typical timeline for the entire process, from pre-application to final decision, and the documents and activities required at each stage. The rest of the article briefly discusses each of these stages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on the academic interview circuit : an end-to-end discussion

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: u &#287; ur &#199; etintemel
",y
"LEFT id: NA
RIGHT id: 1419

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 534

LEFT text: This second special issue provides a forum for topical issues that demonstrate the usefulness of PLS-SEM by piloting applications of this method in the field of strategic management with strong implications for strategic research and practice. As such, the special issue targets two audiences: academics involved in the fields of strategy and management, and practitioners such as consultants. The six articles in this issue are summarized in the following paragraphs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial : charter and scope

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 596

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information warfare and security

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: h. v. jagadish
",n
"LEFT id: NA
RIGHT id: 1878

LEFT text: We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: open issues in parallel query optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: waqar hasan , daniela florescu , patrick valduriez
",y
"LEFT id: NA
RIGHT id: 1340

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: smooth - a distributed multimedia database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: harald kosch , l &#225; szl &#243; b &#246; sz &#246; rm &#233; nyi , alexander bachlechner , christian hanin , christian hofbauer , margit lang , carmen riedler , roland tusch
",n
"LEFT id: NA
RIGHT id: 1617

LEFT text: The Object Database Management Group (ODMG) is a consortium of object-oriented DBMS vendors that have developed a standard interface for their products, ODMG-93. The standard includes a common architecture and deftition for an object-oriented DBMS, a common object model with an object deftition language, a common object query language, and standardized progr amming language bindings, cumently for C++ and Smalltalk. An object-oriented DBMS (by ODMG’S defiition) provides programming language bindings with direct, transparent persistence for data structures, in contrast to the embedded language bindings used in most DBMSS. The common object model allows data to be shared across programming languages. This model incorporates object Ills, encapsulation, methods, frost-class types, multiple inheritance, relationships, lists, sets, bags, arrays, and many other features.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-oriented extensions in sql3 : a status report

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: krishna g. kulkarni
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 1128

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: constructing efficient decision trees by using optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 771

LEFT text: We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to ""adapt"" the view in response to changes in the view definition.Such techniques are needed for applications where the user can change queries dynamically and see the changes in the results fast. Data archaeology, data visualization, and dynamic queries are examples of such applications.We consider all possible redefinitions of SQL SELECT-FROM-WHERE-GROUPBY, UNION, and EXCEPT views, and show how these views can be adapted using the old materialization for the cases where it is possible to do so. We identify extra information that can be kept with a materialization to facilitate redefinition. Multiple simultaneous changes to a view can be handled without necessarily materializing intermediate results. We identify guidelines for users and database administrators that can be used to facilitate efficient view adaptation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views in oracle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: randall g. bello , karl dias , alan downing , james j. feenan , jr. , james l. finnerty , william d. norcott , harry sun , andrew witkowski , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1577

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: acm multimedia '94 conference workshop on multimedia database management systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bruce berra , kingsley nwosu , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: s [41, 42]. academia [1932, 1926, 2267, 2275]. academic [1908, 2441]. acceleration [1869]. Access [2370, 13, 38, 1530, 1620, 1555, 2106, 1643, 1410, 602, 1672, 1033, 531, 601, 803, 1532, 1531, 949, 2204, 1455, 657, 598, 1263, 1086, 749, 409, 1969, 319, 2119, 450, 872, 1808, 171, 1559, 238, 821, 812, 1209, 1198, 694, 126, 1357, 883, 2139]. accesses [406]. Accessing [1295, 127, 515]. Accommodating [626]. account [1889]. accounting [679]. accrued [2385]. Accurate [276, 855]. achieve [858]. Achievements [1238, 619, 627]. ACID [1688]. ACM [38, 2139, 2216, 14, 2106, 2290, 2291, 42, 2239, 2445, 13, 2472, 2475, 2476, 2477, 2479, 2480, 1580, 1099, 2478, 2130, 2474, 1946, 1697, 2213, 2029, 2143, 2319, 2397, 2262]. ACM-SIGFIDET [38, 14]. Acquisition [288]. Across [1645, 2393, 1936]. ACT [1322]. ACT-NET [1322]. ACTA [586]. action [761]. Actions [1329]. activation [851]. Active [1322, 1239, 970, 833, 1471, 272, 1574, 1407, 1071, 1321, 1236, 1174, 420, 1171, 546, 1914, 1178, 1470, 766, 1098, 521, 1228, 1444, 1126, 1065]. Activities [136, 984, 216, 156, 1965, 587, 67, 748, 820]. activity [1311, 196]. Actual [211]. Ad [1394, 2001, 2056, 3]. ad-hoc [2001]. Ada [249, 458]. ADAM [1897]. adapt [2406]. Adaptable [1807]. adaptation [2113, 2328, 1209]. Adapting [1121, 254]. Adaptive [1730, 1675, 1002, 1642, 1024, 1826, 1112, 1737, 304, 1282, 307, 1809, 567]. ADBIS’2001 [1931]. ADBMS [1321, 1322]. adding [463]. Addison [2043, 66]. Addison-Wesley [66]. Address [1148, 749, 1147, 642]. Addressing [1199]. ADEPT [1588]. Administering [221, 1303]. Administration [1478, 53]. Ado [1317]. ADO.NET [2236]. ADS [1071]. Advanced [1881, 952, 740, 1765, 2405, 1062, 678, 924, 1177]. Advancements [2105]. Advances [1227, 1931]. advantage [1163]. adventure [2310]. adversarial [2337]. Affiliation [2402]. after [1121]. again [1875]. against [471, 1939]. Agenda [1040]. Agent [1386, 1607, 2047, 1588, 1919, 1198, 2065]. Agent-Based [1386, 1607, 1588, 1198].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: With about 8.000 researchers and 40.000 students, RWTH Aachen is the largest technical university in Europe. The science and engineering departments and their industrial collaborators offer a lot of challenges for database research.The chair Informatik V (Information Systems) focuses on the theoretical analysis, prototypical development, and practical evaluation of meta information systems. Meta information systems, also called repositories, document and coordinate the distributed processes of producing, integrating, operating, and evolving database-intensive applications.Our research approaches these problems from a technological and from an application perspective.On the one hand, we pursue theory and system aspects of the integration of deductive and object-oriented technologies. One outcome of this work is a deductive object manager called ConceptBase which has been developed over the past eight years and is currently used by many research groups and industrial teams throughout the world.On the other hand, a wide range of application-driven projects aims at building a sound basis of empirical knowledge about the demands on meta information systems, and about the quality of proposed solutions. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 1131

LEFT text: To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage estimation for multidimensional aggregates in the presence of hierarchies

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton , karthikeyan ramasamy
",y
"LEFT id: NA
RIGHT id: 346

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 307

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view selection and maintenance using multi-query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hoshi mistry , prasan roy , s. sudarshan , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: Currently, the Internet provides access to a very large number and wide variety of information sources (e.g., textual databases, sites containing technical reports, directory listings), and systems to access these sources (e.g., World Wide Web, Gopher, WAIS). The challenge is to provide easy, efficient, robust and secure access to this information and other kinds (e.g., relational and object oriented databases). This aim of this panel is to explore whether there are any new technical problems, relevant to the Database field, that need to be solved in order to realize such global information systems. In particular, we debate whether existing techniques from database systems (e.g., multidatabases and distributed databases) can be applied or straigtitforwardly extended to global information systems. Furthermore, we attempt to establish realistic goals for database technologies in global information systems. Some of the specific issues discussed are the following:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 1715

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (XopY) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ∈{<, ≤, =, ≠, >, ≥). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ≤, =, ≥, >} and {<, ≤, =, ≠, ≥, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from <archive.fiu.edu>.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 863

LEFT text: A central development in the database area concerns tools that allow non expert users to understand and easily extract information from a database. Fourth generation query languages, although non-procedural, are not friendly enough for a casual user who must know both the logical structure of the database and the syntax and semantics of the DBMS query language. Instead, recently proposed visual systems which allow a user to extract information by means of interactive graphical commands, have not yet been able to combine ease of use and high expressive power.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic access : semantic interface for querying databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: naphtali rishe , jun yuan , rukshan athauda , shu-ching chen , xiaoling lu , xiaobin ma , alexander vaschillo , artyom shaposhnikov , dmitry vasilevsky
",n
"LEFT id: NA
RIGHT id: 1689

LEFT text: Microsoft’s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David’s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch’s vision, and, indeed, in Microsoft’s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch’s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital’s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft’s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at nthu and itri

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arbee l. p. chen
",n
"LEFT id: NA
RIGHT id: 904

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1030

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 704

LEFT text: We describe the design and implementation of a new data layout scheme, called multi-dimensional clustering, in DB2 Universal Database Version 8. Many applications, e.g., OLAP and data warehousing, process a table or tables in a database using a multi-dimensional access paradigm. Currently, most database systems can only support organization of a table using a primary clustering index. Secondary indexes are created to access the tables when the primary key index is not applicable. Unfortunately, secondary indexes perform many random I/O accesses against the table for a simple operation such as a range query. Our work in multi-dimensional clustering addresses this important deficiency in database systems. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. We describe novel techniques for maintaining this physical layout efficiently and methods of processing database operations that provide significant performance improvements. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hd-eye : visual clustering of high dimensional data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim , markus wawryniuk
",n
"LEFT id: NA
RIGHT id: 797

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an adaptive hybrid server architecture for client caching odbmss

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaladhar voruganti , m. tamer &#214; zsu , ronald c. unrau
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: DISCOVER operates on relational databases and facilitates information discovery on them by allowing its user to issue keyword queries without any knowledge of the database schema or of SQL. DISCOVER returns qualified joining networks of tuples, that is, sets of tuples that are associated because they join on their primary and foreign keys and collectively contain all the keywords of the query. DISCOVER proceeds in two steps. First the Candidate Network Generator generates all candidate networks of relations, that is, join expressions that generate the joining networks of tuples. Then the Plan Generator builds plans for the efficient evaluation of the set of candidate networks, exploiting the opportunities to reuse common subexpressions of the candidate networks.    We prove that DISCOVER finds without redundancy all relevant candidate networks, whose size can be data bound, by exploiting the structure of the schema. We prove that the selection of the optimal execution plan (way to reuse common subexpressions) is NP-complete. We provide a greedy algorithm and we show that it provides near-optimal plan execution time cost. Our experimentation also provides hints on tuning the greedy algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1093

LEFT text: Many commercial database systems use some form of statistics, typically histograms, to summarize the contents of relations and permit efficient estimation of required quantities. While there has been considerable work done on identifying good histograms for the estimation of query-result sizes, little attention has been paid to the estimation of the data distribution of the result, which is of importance in query optimization. In this paper, we prove that the optimal histogram for estimating the size of the result of a join operator is optimal for estimating its data distribution as well. We also study the effectiveness of these optimal histograms in the context of an important application that requires estimates for the data distribution of a query result: load-balancing for parallel Hybrid hash joins. We derive a cost formula to capture the effect of data skew in both the input and output relations on the load and use the optimal histograms to estimate this cost most accurately. We have developed and implemented a load balancing algorithm using these histograms on a simulator for the Gamma parallel database system. The experiments establish the superiority of this approach compared to earlier ones in handling all kinds and levels of skew while incurring negligible overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimation of query-result distribution and its application in parallel-join load balancing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: viswanath poosala , yannis e. ioannidis
",y
"LEFT id: NA
RIGHT id: 108

LEFT text: Traditionally, databases have stored textual data and have been used to store administrative information. The computers used. and more specifically the storage available, have been neither large enough nor fast enough to allow databases to be used for more technical applications. In recent years these two bottlenecks have started to di sappear and there is an increasing interest in using databases to store non-textual data like sensor measurements or other types of process-related data. In a database a sequence of sensor measurements can be represented as a time series. The database can then be queried to find, for instance, subsequences, extrema points, or the points in time at which the time series had a specific value. To make this search efficient, indexing methods are required. Finding appropriate indexing methods is the focus of this thesis.There are two major problems with existing time series indexing strategies: the size of the index structures and the lack of general indexing strategies that are application independent. These problems have been thoroughly researched and solved in the case of text indexing files. We have examined the extent to which text indexing methods can be used for indexing time series.A method for transforming time series into text sequences has been investigated. An investigation was then made on how text indexing methods can be applied on these text sequences. We have examined two well known text indexing methods: the signature files and the B-tree. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: indexing large metric spaces for similarity search queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1340

LEFT text: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: smooth - a distributed multimedia database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: harald kosch , l &#225; szl &#243; b &#246; sz &#246; rm &#233; nyi , alexander bachlechner , christian hanin , christian hofbauer , margit lang , carmen riedler , roland tusch
",n
"LEFT id: NA
RIGHT id: 860

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: toward learning based web query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yanlei diao , hongjun lu , songting chen , zengping tian
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 477

LEFT text: The Senior Therapist's Grandiosity: Clinical and Ethical Consequences of Merging Multiple Roles"" is the second paper by ROBERT S. PEPPER, C.S.W., Ph.D. to appear in this Journal on this very important topic in the contemporary practice of psychotherapy. He notes that some senior therapists engage in multiple roles with grandiosity and other unresolved narcissistic pathology, doing harm to their patients, while violating professional and ethical codes of conduct.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards knowledge-based digital libraries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ling feng , marfred a. jeusfeld , jeroen hoppenbrouwers
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 822

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on efficiently implementing schemasql on an sql database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , fereidoon sadri , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 1749

LEFT text: We describe the external data manager component of the Lore database system for semistructured data. Lore's external data manager enables dynamic retrieval and integration of data from arbitrary, heterogeneous external sources during query processing. The distinction between Lore-resident and external data is invisible to the user. We introduce a flexible notion of arguments that limits the amount of data fetched from an external source, and we have incorporated optimizations to reduce the number of calls to an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating dynamically-fetched external information into a dbms for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",y
"LEFT id: NA
RIGHT id: 468

LEFT text: In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: iceberg-cube computation with pc clusters

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: raymond t. ng , alan wagner , yu yin
",n
"LEFT id: NA
RIGHT id: 1451

LEFT text: In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast similarity search in the presence of noise , scaling , and translation in time-series databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rakesh agrawal , king-ip lin , harpreet s. sawhney , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: We describe the TIGUKAT objectbase management system, which is under development at the Laboratory for Database Systems Research at the University of Alberta. TIGUKAT has a novel object model, whose identifying characteristics include a purely behavioral semantics and a uniform approach to objects. Everything in the system, including types, classes, collections, behaviors, and functions, as well as meta-information, is a first-class object with well-defined behavior. In this way, the model abstracts everything, including traditional structural notions such as instance variables, method implementation, and schema definition, into a uniform semantics of behaviors on objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: With the advent of XML as a format for data ex-change and semistructured databases, query languagesfor XML and semistructured data have become in-creasingly popular.Many such query languages, like XPath andXQuery, are navigational in the sense that their vari-able binding paradigm requires the programmer tospecify path navigations through the document (ordata item). In contrast, some other languages – suchas UnQL [1] and Xcerpt [2] – are pattern-based: theirvariable binding paradigm is that of mathematical log-ics, i.e. the programmer speciﬁes patterns (or terms)including variables. Arguably, a pattern-based vari-able binding paradigm makes complex queries mucheasier to specify and to read, thus improving theprogramming eﬃciency. Sustaining this ﬁrst claimwith practical examples is one of the objectives of thepresent demonstration.Xcerpt [2] is an experimental pattern-based queryand transformation language for XML and semistruc-tured data. Xcerpt uses patterns both for bindingvariables in query expressions and for reassembling thevariables (bound to data items in query expressions) inso-called construct terms. Arguably, a pattern-baseddocument construction combined with a pattern-basedvariable binding results in a rather intuitive, userfriendly, and programming eﬃcient language. Sustain-ing this second claim is another objective of the presentdemonstration.Xcerpt is experimental in the sense that its purposeis to investigate and test another, non-navigational ap-proach to retrieve data from the Web than that ofthe widespread query languages XPath and XQuery.Nonetheless, Xcerpt has been prototypically imple-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1169

LEFT text:  Several formal models for database access control have been proposed. However, little attention has been paid to temporal issues like authorizations with limited validity or obtained by deductive reasoning with temporal constraints. We present an access control model in which authorizations contain periodic temporal intervals of validity. An authorization is automatically granted in the time intervals specified by a periodic expression and revoked when such intervals expire. Deductive temporal rules with periodicity and order constraints are provided to derive new authorizations based on the presence or absence of other authorizations in specific periods of time. We prove the uniqueness of the set of implicit authorizations derivable at a given instant from the explicit ones, and we propose an algorithm to compute the global set of valid authorizations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial and temporal content-based access to hypervideo databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: haitao jiang , ahmed k. elmagarmid
",n
"LEFT id: NA
RIGHT id: 719

LEFT text: Though the query is posted in key words, the returned results contain exactly the information that the user is querying for, which may not be explicitly specified in the input query. The required information is often not contained in the Web pages whose URLs are returned by a search engine. FACT is capable of navigating in the neighborhood of these pages to find those that really contain the queried segments. The system does not require a prior knowledge about users such as user profiles or preprocessing of Web pages such as wrapper generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rainbow : mapping-driven xquery processing system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: xin zhang , mukesh mulchandani , steffen christ , brian murphy , elke a. rundensteiner
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1345

LEFT text: Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching technologies for web applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 1239

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 18

LEFT text: Because of the Internet we believe that in the long run there will be alternative providers for all of these three resources for any given application. Data providers will bring more and more data and more and more different kinds of data to the net. Likewise, function providers will develop new methods to process and work with the data; e.g., function providers might develop new algorithms to compress data or to produce thumbnails out of large images and try to sell these on the Internet. It is also conceivable, that some people allow other people to use spare cycles of their idle machines in the Internet (as in the Condor system of the University of Wisconsin) or that some companies (cycle providers) even specialize on selling computing time to businesses that occasionally need to carry out very complex operations for which regular hardware is not sufficient.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database patchwork on the internet

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: reinhard braumandl , alfons kemper , donald kossmann
",y
"LEFT id: NA
RIGHT id: 1375

LEFT text: We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: work and information practices in the sciences of biodiversity

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: geoffrey c. bowker
",n
"LEFT id: NA
RIGHT id: 1288

LEFT text: Majority of current Business Intelligence Data (BID) is spread out between incompatible tools like RDBMSs, OLAP engines, and spreadsheets. Bridges between them allow interchange of data and metadata thus providing a small degree of BID sharing, but due to lack of metadata standards, this sharing is limited to speci c installations. This creates an unacceptable situation where an analyst cannot get a complete picture of a business. A solution is needed, the Collaborative Analytical Processing (CAP) solution, which imposes a very tight integration between the tools so (meta) data interchange, change management, scalability and data availability is as good as in RDBMSs, and performance of OLAP queries is as good as in the specialized OLAP engines. Any CAP solution must assist business tools through simpli cation and integration so they can focus on business models, relationship management, fact discovery and presentation than on availability, performance and scalability. This panel will discuss feasibility of such solution and its place in the spectrum between RDBMS and specialized OLAP engines. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: collaborative analytical processing - dream or reality ? ( panel abstract )

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: william o'connell , andrew witkowski , goetz graefe
",y
"LEFT id: NA
RIGHT id: 220

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 135

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: information gathering in the world-wide web : the w3ql query language and the w3qs system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david konopnicki , oded shmueli
",n
"LEFT id: NA
RIGHT id: 1568

LEFT text: Client-server database systems based on a data shipping model can exploit client memory resources by caching copies of data items across transaction boundaries. Caching reduces the need to obtain data from servers or other sites on the network. In order to ensure that such caching does not result in the violation of transaction semantics, a transactional cache consistency maintenance algorithm is required. Many such algorithms have been proposed in the literature and, as all provide the same functionality, performance is a primary concern in choosing among them. In this article we present a taxonomy that describes the design space for transactional cache consistency maintenance algorithms and show how proposed algorithms relate to one another. We then investigate the performance of six of these algorithms, and use these results to examine the tradeoffs inherent in the design choices identified in the taxonomy. The results show that the interactions among dimensions of the design space impact performance in many ways, and that classifications of algorithms as simply “pessimistic” or “optimistic” do not accurately characterize the similarities and differences among the many possible cache consistency algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: transaction chopping : algorithms and performance studies

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: dennis shasha , francois llirbat , eric simon , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",y
"LEFT id: NA
RIGHT id: 117

LEFT text: Global information systems have the potential of providing decision makers with timely spatial information about earth systems. This information will come from diverse sources, including field monitoring, remotely sensed imagery, and environmental models. Of the three the latter has the greatest potential of providing regional and global scale information on the behavior of environmental systems, which may be vital for setting multi-governmental policy and for making decisions that are critical to quality of life. However, environmental models have limited prootocol for quality control and standardization. They tend to have weak or poorly defined semantics and so their output is often difficult to interpret outside a very limited range of applications for which they are designed. This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of environmental models for application to global information systems and decision-making

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: d. scott mackay
",y
"LEFT id: NA
RIGHT id: 1436

LEFT text: There is currently considerable interest in developing multimedia digital libraries. However, it has become clear that existing architectures for management systems do not support the particular requirements of continuous media types. This is particularly the case in the important area of quality of service support. In this correspondence, we discuss quality of service issues within digital libraries and present a reference architecture able to support some quality aspects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a product specification database for visual prototyping

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kazutoshi sumiya , kouichi yasutake , hirohiko tanaka , norio sanada , yoshihiko imai
",n
"LEFT id: NA
RIGHT id: 1718

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive selectivity estimation using query feedback

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chungmin melvin chen , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: We describe the design and implementation of the Glue-Nail deductive database system. Nail is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code are both compiled into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm and supports well-founded models. The Glue compiler's static optimizer uses peephole techniques and data flow analysis to improve code.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 957

LEFT text: The database community has been researching problems in similarity query for time series databases for many years. The techniques developed in the area might shed light on the query by humming problem. In this demo, we treat both the melodies in the music databases and the user humming input as time series. Such an approach allows us to integrate many database indexing techniques into a query by humming system, improving the quality of such system over the traditional (contour) string databases approach. We design special searching techniques that are invariant to shifting, time scaling and local time warping. This makes the system robust and allows more flexible user humming input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the oracle universal server buffer

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: william bridge , ashok joshi , m. keihl , tirthankar lahiri , juan loaiza , n. macnaughton
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: With the advent of XML as a format for data ex-change and semistructured databases, query languagesfor XML and semistructured data have become in-creasingly popular.Many such query languages, like XPath andXQuery, are navigational in the sense that their vari-able binding paradigm requires the programmer tospecify path navigations through the document (ordata item). In contrast, some other languages – suchas UnQL [1] and Xcerpt [2] – are pattern-based: theirvariable binding paradigm is that of mathematical log-ics, i.e. the programmer speciﬁes patterns (or terms)including variables. Arguably, a pattern-based vari-able binding paradigm makes complex queries mucheasier to specify and to read, thus improving theprogramming eﬃciency. Sustaining this ﬁrst claimwith practical examples is one of the objectives of thepresent demonstration.Xcerpt [2] is an experimental pattern-based queryand transformation language for XML and semistruc-tured data. Xcerpt uses patterns both for bindingvariables in query expressions and for reassembling thevariables (bound to data items in query expressions) inso-called construct terms. Arguably, a pattern-baseddocument construction combined with a pattern-basedvariable binding results in a rather intuitive, userfriendly, and programming eﬃcient language. Sustain-ing this second claim is another objective of the presentdemonstration.Xcerpt is experimental in the sense that its purposeis to investigate and test another, non-navigational ap-proach to retrieve data from the Web than that ofthe widespread query languages XPath and XQuery.Nonetheless, Xcerpt has been prototypically imple-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1401

LEFT text: We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query translation across heterogeneous information sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 54

LEFT text: Prior studies have documented how deindustrialization poses a bleak outlook for both individuals and their communities: longterm unemployment, elevated poverty, and the erosion of once vital areas. What can people do to mitigate the effects of declining industries that once employed several generations of workers? More importantly, how can collective action help transform society into realizing diverse interests, rather than just a few, narrowly defined interests? Jeremy Brecher’s Banded Together: Economic Democratization in the Brass Valley shares a much-needed account of how such efforts unfold in Western Connecticut’s Naugatuck Valley, a community known for its brass manufacturing since the 1800s.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: bottom-up computation of sparse and iceberg cube

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kevin beyer , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1715

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1356

LEFT text: Motivation  The field of data warehousing has emerged over the last decades. A data warehouse is developed at a moment in time to support business intelligence for an indefinite period of time. During the life of the data warehouse the world around it evolves, including the systems that are a source for the data warehouse. In order for a data warehouse to remain functioning and guarantee the quality of its data, it needs to be adjusted to the evolving world around it. The concept of Delta Impact Analysis is used by the company BI4U for the activities of analysing the impact of specific changes. This concept is important because it provides insight into how a data warehouse can be adjusted to the evolving world around it. The motivation to perform this research was the fact that a clear definition on the concept of DIA and what it comprehends was lacking.  Goals  The main goals of the research were to examine the topic of DIA in practice, to gather insights from literature and other research, to design and develop a practical model for DIA, to test the DIA model, and to provide recommendations to better support changes in data warehouse source systems. These goals resulted in the following main problem statement: How can a Delta Impact Analysis model be designed that supports the process of analyzing the impact of changes in a data warehouse source system situation?  Approach  The research approach is based on the design science framework by Hevner in combination with action science theory to validate the resulting DIA model from the research. The design science perspective resulted in an approach that is both rigorous, by performing a literature study, and relevant, by applying the research to practice. The research started by investigating the concept of DIA in practice at BI4U, in order to provide more insight into what it comprehends and what was relevant for the focus of the research. Next a thorough literature study was performed. Finally an artifact was proposed, a model for the process of DIA, which was validated in practice with a field study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrency in the data warehouse

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: richard taylor
",n
"LEFT id: NA
RIGHT id: 245

LEFT text: Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. Consequently, I will also briefly introduce the related XML, Java and OMG technologies like SOAP, J2EE and CORBA. One of the most important features of ASs is their ability to integrate the modern application environments with legacy data sources like IMS, CICS, VSAM, etc. They provide a number of connectors for this purpose, typically using asynchronous transactional messaging technologies like MQSeries and JMS. Traditional TPM-style requirements for industrial strength features like scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state. Recently, the ECPerf benchmark has been developed via the Java Community Process to evaluate in a standardized way the cost performance of J2EE-compliant ASs. Several caching technologies have been developed to improve performance of ASs.Soon after this conference is over, the slides of this tutorial will be available on the web at the following URL: http://www.almaden.ibm.com/u/mohan/AppServersTutorial_SIGMOD2002_Slides.pdf

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multiview project : object-oriented view technology and applications

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: e. a. rundensteiner , h. a. kuno , y.-g . ra , v. crestana-taube , m. c. jones , p. j. marron
",n
"LEFT id: NA
RIGHT id: 1459

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 493

LEFT text: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental maintenance of recursive views using relational calculus/sql

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: guozhu dong , jianwen su
",n
"LEFT id: NA
RIGHT id: 1098

LEFT text: We propose a semantic model for client-side caching and replacement in a client-server database system and compare this approach to page caching and tuple caching strategies. Our caching model is based on, and derives its advantages from, three key ideas. First, the client maintains a semantic description of the data in its cache,which allows for a compact specification, as a remainder query, of the tuples needed to answer a query that are not available in the cache. Second, usage information for replacement policies is maintained in an adaptive fashion for semantic regions, which are associated with collections of tuples. This avoids the high overheads of tuple caching and, unlike page caching, is insensitive to bad clustering. Third, maintaining a semantic description of cached data enables the use of sophisticated value functions that incorporate semantic notions of locality, not just LRU or MRU, for cache replacement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic data caching and replacement

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: shaul dar , michael j. franklin , bj &#246; rn &#222; &#243; r j &#243; nsson , divesh srivastava , michael tan
",y
"LEFT id: NA
RIGHT id: 2175

LEFT text: Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically diffcult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT { Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT's outperform previous data structures in a number of applications. Keywords { near neighbor, metric space, approximate queries, data mining, Dirichlet domains, Voronoi regions

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 2064

LEFT text: This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: We give a tutorial introduction to the basic definitions surrounding the idea of constraint databases, and survey and indicate some of the achieved research results on this subject. This paper is not written as a scholarly piece, nor as polished course notes, but rather as something like the transcript of an invited talk I gave at a meeting bringing together researchers from finite model theory, database theory, and computer-aided verification, which was held at Schloss Dagstuhl in October 1999.Very recently the first book on the subject appeared [20]. It covers the state of the art in constraint databases up to, say, mid 1999 [20]. You should see this paper merely as an appetizer for the book. I will also not try to be complete in my bibliographical references. Again, see the book for that.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1831

LEFT text: DISIMA (Distributed Image Database Management System) is a research project under development at the University of Alberta. DISIMA implements a database approach to developing an image database system. Image contents are modeled using objectoriented paradigms while a declarative query language and a corresponding visual query language allow queries over syntactic and semantic features of images. The distributed and interoperable architecture is designed using common facilities as defined in the Object Management Architecture (OMA).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 444

LEFT text: Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space. In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for mining outliers from large data sets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1933

LEFT text: The principal objective of this paper has been to suppress this drawback while conserving the strong correctness of 2LSR executions We propose defining precisely the notion of value dependencies, and managing them so as not to impose the LDP property.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: highly concurrent cache consistency for indices in client-server database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: markos zaharioudakis , michael j. carey
",n
"LEFT id: NA
RIGHT id: 1837

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sr-tree : an index structure for high-dimensional nearest neighbor queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: norio katayama , shin ` ichi satoh
",n
"LEFT id: NA
RIGHT id: 1466

LEFT text: Data warehouses support the analysis of historical data. This often involves aggregation over a period of time. Furthermore, data is typically incorporated in the warehouse in the increasing order of a time attribute, e.g., date of sale or time of a temperature measurement. In this paper we propose a framework to take advantage of this append only nature of updates due to a time attribute. The framework allows us to integrate large amounts of new data into the warehouse and generate historical summaries efficiently. Query and update costs are virtually independent from the extent of the data set in the time dimension, making our framework an attractive aggregation approach for append-only data streams. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: eager aggregation and lazy aggregation

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weipeng p. yan , per - &#197; ke larson
",n
"LEFT id: NA
RIGHT id: 2168

LEFT text: Emerging applications in the field of biotechnology hold great promise for promoting the health and well-being of the global community, especially in developing states. Yet significant concerns have emerged about biotechnology in the transnational sphere, concerns that no doubt will increase in decades to come. The purpose of the article is to assess the strengths and limits of existing international norms and structures designed to address these concerns, and to suggest a means for augmenting current structures to make them more effective. International law develops and regulates transnational behavior in a manner that goes well beyond the development treaty regimes. International law is driven in large part by the self-interest of states, but they also arise from the social interaction of states and non-state actors, and they ultimately must become grounded in national laws and society in order to become effective. This article accordingly emphasizes the need for coordination at different levels of state and non-state behavior as the law develops over time as well as the need for coordination across different treaty regimes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scientific olap for the biotech domain

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nam huyn
",y
"LEFT id: NA
RIGHT id: 1965

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: performance tradeoffs for client-server query processing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. franklin , bj &#246; rn th &#243; r j &#243; nsson , donald kossmann
",n
"LEFT id: NA
RIGHT id: 2175

LEFT text: In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: In this issue of Leaven, we explore the theme of local church ministry by honoring the legacy of Paul and Kay Watson. The following reflections and essays are written by those who bear appreciative witness to the faithful service of this Christian couple. Paul and Kay have dedicated their time, love, and spiritual gifts for the last three decades to the Cole Mill Road congregation in Durham, North Carolina. And through their missionary travels and a host of teaching opportunities, their influence has been felt by those far beyond their home church.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 459

LEFT text: Personalization, advertising, and the sheer volume of online data generate a staggering amount of dynamic web content. In addition to web caching, View Materialization has been shown to accelerate the generation of dynamic web content. View materialization is an attractive solution as it decouples the serving of access requests from the handling of updates. In the context of the Web, selecting which views to materialize must be decided online and needs to consider both performance and data freshness, which we refer to as the Online View Selection problem. In this paper, we define data freshness metrics, provide an adaptive algorithm for the online view selection problem, and present experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the implementation and performance of compressed databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: till westmann , donald kossmann , sven helmer , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 256

LEFT text: Tree patterns form a natural basis to query tree-structured data such as XML and LDAP. To improve the efficiency of tree pattern matching, it is essential to quickly identify and eliminate redundant nodes in the pattern. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. In the absence of ICs, we develop a polynomial-time query minimization algorithm called CIM, whose efficiency stems from two key properties: (i) a node cannot be redundant unless its children are; and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we develop a technique for query minimization based on three fundamental operations: augmentation (an adaptation of the well-known chase procedure), minimization (based on homomorphism techniques), and reduction. We show the surprising result that the algorithm, referred to as ACIM, obtained by first augmenting the tree pattern using ICs, and then applying CIM, always finds the unique minimal equivalent query. While ACIM is polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating ”information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: minimization of tree pattern queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sungran cho , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: A decade ago, the connection between objects and databases was new and was being explored in a number of different ways within our community. Driven by the perception that managing traditional business data was largely a solved problem, projects were investigating ideas such as adding abstract data types to relational databases and building extensible database systems, objectoriented database systems, and toolkits for constructing special-purpose database systems. In addition, work was underway elsewhere in the computer science research community on extending programming languages with database-inspired features such as persistence and transactions. In this paper, we take a look at where our field was a decade ago and where it is now in terms of database support for objects (and vice versa). We look both at research projects and at commercial database products. We share our vision and our biases about the future of objects and databases, and we identify a number of research challenges that remain to be addressed in order to ultimately achieve our vision. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 215

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: The constraint theory developed for traditional structured databases no longer applies to XML data. Thus, many efforts focus on the key constraints for XML. In the paper, motivated by the problem encountered in the evaluation of change detection for XML documents, we present the notion of multi-instance-based key, which can be fundamental to a highly efficient change detection algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",n
"LEFT id: NA
RIGHT id: 1768

LEFT text: We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to ""adapt"" the view in response to changes in the view definition.Such techniques are needed for applications where the user can change queries dynamically and see the changes in the results fast. Data archaeology, data visualization, and dynamic queries are examples of such applications.We consider all possible redefinitions of SQL SELECT-FROM-WHERE-GROUPBY, UNION, and EXCEPT views, and show how these views can be adapted using the old materialization for the cases where it is possible to do so. We identify extra information that can be kept with a materialization to facilitate redefinition. Multiple simultaneous changes to a view can be handled without necessarily materializing intermediate results. We identify guidelines for users and database administrators that can be used to facilitate efficient view adaptation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: materialized views and data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 131

LEFT text: Automated database design systems embody knowledge about the database design process. However, their lack of knowledge about the domains for which databases are being developed significantly limits their usefulness. A methodology for acquiring and using general world knowledge about business for database design has been developed and implemented in a system called the Common Sense Business Reasoner, which acquires facts about application domains and organizes them into a a hierarchical, context-dependent knowledge base. This knowledge is used to make intelligent suggestions to a user about the entities, attributes, and relationships to include in a database design. A distance function approach is employed for integrating specific facts, obtained from individual design sessions, into the knowledge base (learning) and for applying the knowledge to subsequent design problems (reasoning).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: database design with common sense business reasoning and learning

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: veda c. storey , roger h. l. chiang , debabrata dey , robert c. goldstein , shankar sudaresan
",y
"LEFT id: NA
RIGHT id: 1055

LEFT text: MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the role of integrity constraints in database interoperation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark w. w. vermeer , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 1150

LEFT text: This is the seventh bibliography concerning temporal databases. In this bibliography, we collect 331 new temporal databases papers. Most of these papers were published in 1996-1997, some in 1995 and some will appear in 1997 or 1998.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 1706

LEFT text: A decade ago, the connection between objects and databases was new and was being explored in a number of different ways within our community. Driven by the perception that managing traditional business data was largely a solved problem, projects were investigating ideas such as adding abstract data types to relational databases and building extensible database systems, objectoriented database systems, and toolkits for constructing special-purpose database systems. In addition, work was underway elsewhere in the computer science research community on extending programming languages with database-inspired features such as persistence and transactions. In this paper, we take a look at where our field was a decade ago and where it is now in terms of database support for objects (and vice versa). We look both at research projects and at commercial database products. We share our vision and our biases about the future of objects and databases, and we identify a number of research challenges that remain to be addressed in order to ultimately achieve our vision. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: text databases : a survey of text models and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arjan loeffen
",n
"LEFT id: NA
RIGHT id: 44

LEFT text: In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: emc information sharing : direct access to mvs data from unix and nt

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: walt kohler
",y
"LEFT id: NA
RIGHT id: 1996

LEFT text: This paper describes the Dwarf structure and the Dwarf cube construction algorithm. Further optimizations are then introduced for improving clustering and query performance. Experiments with the current implementation include comparisons on detailed measurements with real and synthetic datasets against previously published techniques. The comparisons show that Dwarfs by far out-perform these techniques on all counts: storage space, creation time, query response time, and updates of cubes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guidelines for presentation and comparison of indexing techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: justin zobel , alistair moffat , kotagiri ramamohanarao
",n
"LEFT id: NA
RIGHT id: 805

LEFT text: The goal of this demonstration is to present the main features of (i) Axielle, an XML repository developed by Ardent Software [3] on top of the O2 object-oriented DBMS and (ii) the ActiveView system which has been built by the Verso project at INRIA [1] on top of Axielle. The demonstration is based on a simple electronic commerce application which will be described in Section 2. Electronic commerce is emerging as a major Web-supported application. It involves handling and exchange of data (e.g. product catalogs, yellow pages, etc.) and must provide (i) database functionalities (query language, transactions, concurrency control, distribution and recovery) for the efficient management of large data volumes and hundreds of users as well as (ii) standard data storage and exchange formats (e.g. XML, SGML) for the easy integration of existing software and data. The ActiveView system combined with the Axielle XML repository enables a fast deployment of electronic commerce applications based on a new high-level declarative specification language (AVL), advanced database technology (object-oriented data model, XML query language, notifications), Web standards (HTTP, HTML) and other Internet compliant

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xml repository and active views demonstration

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: serge abiteboul , vincent aguilera , s &#233; bastien ailleret , bernd amann , sophie cluet , brendan hills , fr &#233; d &#233; ric hubert , jean-claude mamou , am &#233; lie marian , laurent mignet , tova milo , cassio souza dos santos , bruno tessier , anne-marie vercoustre
",y
"LEFT id: NA
RIGHT id: 990

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 368

LEFT text: Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: standard for multimedia databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john r. smith
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 811

LEFT text: XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: relational databases for querying xml documents : limitations and opportunities

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 283

LEFT text: Research has investigated mappings among data sources under two perspectives. On one side, there are studies of practical tools for schema mapping generation; these focus on algorithms to generate mappings based on visual specifications provided by users. On the other side, we have theoretical researches about data exchange. These study how to generate a solution - i.e., a target instance - given a set of mappings usually specified as tuple generating dependencies. However, despite the fact that the notion of a core of a data exchange solution has been formally identified as an optimal solution, there are yet no mapping systems that support core computations. In this paper we introduce several new algorithms that contribute to bridge the gap between the practice of mapping generation and the theory of data exchange. We show how, given a mapping scenario, it is possible to generate an executable script that computes core solutions for the corresponding data exchange problem. The algorithms have been implemented and tested using common runtime engines to show that they guarantee very good performances, orders of magnitudes better than those of known algorithms that compute the core as a post-processing step.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: clio : a semi-automatic tool for schema mapping

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mauricio a. hern &#225; ndez , ren &#233; e j. miller , laura m. haas
",y
"LEFT id: NA
RIGHT id: 1185

LEFT text: The paper proposes two extended R -trees that permit the indexing of data regions that grow continuously over time, by also letting the internal bounding regions grow. Internal bounding regions may be triangular as well as rectangular. New heuristics for the algorithms that govern the index structure are provided. As a result, dead space and overlap, now also functions of time, are reduced. Performance studies indicate that the best extended index is typically 3‐5 times faster than the existing R-tree based indices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: Many papers have examined how to efficiently export a materialized view but to our knowledge none have studied how to efficiently import one. To import a view, i.e., to install a stream of updates, a real-time database system must process new updates in a timely fashion to keep the database ""fresh,"" but at the same time must process transactions and ensure they meet their time constraints. In this paper, we discuss the various properties of updates and views (including staleness) that affect this tradeoff. We also examine, through simulation, four algorithms for scheduling transactions and installing updates in a soft real-time database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1949

LEFT text: Although the relational model for databases provides a great range of advantages over other data models, it lacks a comprehensive way to handle incomplete and uncertain data. Uncertainty in data values, however, is pervasive in all real-world environments and has received much attention in the literature. Several methods have been proposed for incorporating uncertain data into relational databases. However, the current approaches have many shortcomings and have not established an acceptable extension of the relational model. In this paper, we propose a consistent extension of the relational model. We present a revised relational structure and extend the relational algebra. The extended algebra is shown to be closed, a consistent extension of the conventional relational algebra, and reducible to the latter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a probabilistic relational model and algebra

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: debabrata dey , sumit sarkar
",y
"LEFT id: NA
RIGHT id: 1866

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 810

LEFT text: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 989

LEFT text: The size of The Boeing Company posts some stringent requirements on data warehouse design and implementation. We summarize four interesting and challenging issues in developing very large scale data warehouses, namely failure recovery, incremental update maintenance, cost model for schema design and query optimization, and metadata definition and management. For each issue, we give the reasons we think it is important but not well-addressed in research literature and commercial products, and our current research to solve it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: issues in developing very large data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: lyman do , pamela drew , wei jin , vish jumani , david van rossum
",y
"LEFT id: NA
RIGHT id: 1455

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1382

LEFT text: This study presents a modified B2B CRM using the Genetic algorithm and Data Mining Techniques to improve decision making. The model classifies consumers into consumers of Repeat and Shop-and-Go. Modified data mining C5.0 and the Genetic algorithm was employed to optimize rules generated by the decision tree algorithm. The findings showed that the proposed model allocates resources effectively to the most profitable customers’ decisions. The output metrics are machine time, calibration graph, and ROC curve. In comparison with the conventional C5.0, k-NN, and Support Vector Machine, the proposed model has greater accuracy of 89.3 percent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design and implementation of a genetic-based algorithm for data mining

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sunil choenni
",y
"LEFT id: NA
RIGHT id: 1939

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 1206

LEFT text: Abstract. Various temporal extensions to the relational model have been proposed. All of these, however, deviate significantly from the original relational model. This paper presents a temporal extension of the relational algebra that is not significantly different from the original relational model, yet is at least as expressive as any of the previous approaches. This algebra employs multidimensional tuple time-stamping to capture the complete temporal behavior of data. The basic relational operations are redefined as consistent extensions of the existing operations in a manner that preserves the basic algebraic equivalences of the snapshot (i.e., conventional static) algebra. A new operation, namely temporal projection, is introduced. The complete update semantics are formally specified and aggregate functions are defined. The algebra is closed, and reduces to the snapshot algebra. It is also shown to be at least as expressive as the calculus-based temporal query language TQuel. In order to assess the algebra, it is evaluated using a set of twenty-six criteria proposed in the literature, and compared to existing temporal relational algebras. The proposed algebra appears to satisfy more criteria than any other existing algebra. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a complete temporal relational algebra

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: debabrata dey , terence m. barron , veda c. storey
",y
"LEFT id: NA
RIGHT id: 1630

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 2209

LEFT text: We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering followed by parallelization. We focus on the parallelization phase and develop algorithms for exploiting pipelined parallelism. We formulate parallelization as scheduling a weighted operator tree to minimize response time. Our model of response time captures the fundamental tradeoff between parallel execution and its communication overhead. We assess the quality of an optimization algorithm by its performance ratio which is the ratio of the response time of the generated schedule to that of the optimal. We develop fast algorithms that produce near-optimal schedules the performance ratio is extremely close to 1 on the average and has a worst case bound of about 2 for many cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization algorithms for exploiting the parallelism-communication tradeoff in pipelined parallelism

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: waqar hasan , rajeev motwani
",y
"LEFT id: NA
RIGHT id: 1675

LEFT text: The advent of new database applications such as engineering design stresses the need for new functie nalities in database systems. It includes the management of multiple representations for database objects, long transactions as well as dynamic data structures. This paper presents the approach used in CADB, a prototype expert database system dedicated to CAD, for the management and control of the consistency of design objects. It concerns both the operations on the object property values and the interactive manipulation of their structure. They involve concepts of the object models as well as the application semantics. They rely therefore on concepts used for representing the design objects and on semantic notions related to expert knowledge in the application domain. Among these are the notions of consistency and of completeness of the objects. These two complementary aspects are detailed and their relationships described. The emphasis is on the heuristic rules that provide a unified knowledge-based approach for their management independent of the particular application being considered. Dynamic inheritance mechanisms are also presented that sup port the manipulations performed on the object structures. It is shown how they help providing expert database facilities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1584

LEFT text: We present a generic solution to a problem which lies at the heart of the unpredictable worst-case performance characteristics of a wide class of multi-dimensional index designs: those which employ a recursive partitioning of the data space. We then show how this solution can produce modified designs with fully predictable and controllable worst-case characteristics. In particular, we show how the recursive partitioning of an n-dimensional dataspace can be represented in such a way that the characteristics of the one-dimensional B-tree are preserved in n dimensions, as far as is topologically possible i.e. a representation guaranteeing logarithmic access and update time, while also guaranteeing a one-third minimum occupancy of both data and index nodes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a general solution of the n-dimensional b-tree problem

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael freeston
",y
"LEFT id: NA
RIGHT id: 1670

LEFT text: The project focuses on the field of Technical Information Systems, where there is a need for tools supporting modeling of complex objects. Designers in this field usually use incremental design or step by step prototyping, because this seems to be best suited for users coping with complexity and uncertainty about their own needs or requirements. The IMPRESS DDT aims at supporting the database design part of this process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the impress ddt : a database design toolbox based on a formal specification language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jan flokstra , maurice van keulen , jacek skowronek
",y
"LEFT id: NA
RIGHT id: 465

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the fourth international conference on flexible query answering systems ( fqas 2000 )

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: patrick bosc , amihai motro , gabriella pasi
",n
"LEFT id: NA
RIGHT id: 599

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 3

LEFT text: We present the information mediator prototype called Kind, recently developed as part of an integrated Neuroscience workbench project at SDSC/UCSD within the NPACI project. The broad goal of the workbench is to serve as an environment where, among other tasks, the Neuroscientist can query a mediator to retrieve information from across a number of information sources, and use the results to perform her own analysis on the data. The Kind mediator is an instance of a novel model-centered mediator architecture that extends current XML-based mediator approaches by incorporating a semantic model of an information source as an integral part of the mediation process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xml-based information mediation with mix

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chaitan baru , amarnath gupta , bertram lud &#228; scher , richard marciano , yannis papakonstantinou , pavel velikhov , vincent chu
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia’s Programming Systems Laboratory (Section 7).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 1223

LEFT text: Abstract. We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By “categorical data,” we mean tables with fields that cannot be naturally ordered by a metric – e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: wavecluster : a wavelet-based clustering approach for spatial data in very large databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: gholamhosein sheikholeslami , surojit chatterjee , aidong zhang
",n
"LEFT id: NA
RIGHT id: 199

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 670

LEFT text: The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: supply chain infrastructures : system integration and information sharing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: michael o. ball , meng ma , louiqa raschid , zhengying zhao
",n
"LEFT id: NA
RIGHT id: 0

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the wasa2 object-oriented workflow management system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: gottfried vossen , mathias weske
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 964

LEFT text: Specifically, we use state-of-the-art concepts from morphology, n;,mely the ‘pattern spectrum’ of a shape, to map each shape to a point in n-dimensional space. FollowingThis text is a guide to the foundations of method engineering, a developing field concerned with the definition of techniques for designing software systems. The approach is based on metamodeling, the construction of a model about a collection of other models. The book applies the metamodeling approach in five case studies, each describing a solution to a problem in a specific domain. Suitable for classroom use, the book is also useful as a reference for practitioners. The book first presents the theoretical basis of metamodeling for method engineering, discussing information modeling, the potential of metamodeling for software systems development, and the introduction of the metamodeling tool ConceptBase. , we organize the n-d points in an R-tree. We show that the L, (= max) norm in the n-d space lower-bounds the actual distance. This guarantees no false dismissals for range queries. In addition, we present a nearest neighbor algorithm that also guarantees no false dismissals.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast high-dimensional data search in incomplete databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: beng chin ooi , cheng hian goh , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1754

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semistructured and structured data in the web : going back and forth

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 1350

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting incremental join queries on ranked inputs

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: apostol natsev , yuan-chi chang , john r. smith , chung-sheng li , jeffrey scott vitter
",n
"LEFT id: NA
RIGHT id: 685

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: time-parameterized queries in spatio-temporal databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1486

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on index selection schemes for nested object hierarchies

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , ming-syan chen , philip s. yu
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 278

LEFT text: • "" … make a computer so imbedded, so fitting, so natural, that we use it without even thinking about it. "" • "" Ubiquitous (pervasive) computing is roughly the opposite of virtual reality. Where virtual reality puts people inside a computer-generated world, ubiquitous computing forces the computer to live out here in the world with people. "" – Mark Weiser, Xerox PARC

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management : lasting impact on wild , wild , web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: reed m. meseck
",n
"LEFT id: NA
RIGHT id: 1825

LEFT text: Materialized views in data warehouses are maintained incrementally, for reasons of efficiency, to present the latest updates to the users. These views are used by many warehouse readers (users) to execute OLAP queries by running several reader sessions and these views are maintained periodically by maintenance transactions. Therefore, there is an inherent problem of maintaining these views while the reader sessions continue to receive consistent data from these views. In this paper, we discuss a method that allows warehouse maintenance transactions to run concurrently with the reader sessions. Concurrency allows the readers to read the data from the views while the maintenance transaction updates these views. In our proposed method we create additional versions of views dynamically that contain only the modified tuples of the views and provide a mechanism to collapse these versions into the views periodically when there are no reader sessions accessing the views. These versions allow the reader sessions to access the old and the new information. The collapsing of the views is done by a low-priority process executing periodically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient view maintenance at data warehouses

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. agrawal , a. el abbadi , a. singh , t. yurek
",n
"LEFT id: NA
RIGHT id: 147

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multidimensional database system rasdaman

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. baumann , a. dehmel , p. furtado , r. ritsch , n. widmann
",y
"LEFT id: NA
RIGHT id: 1640

LEFT text: Abstract Spatial-Query-by-Sketch is the design of a query language for geographic information systems. It allows a user to formulate a spatial query by drawing the desired configuration with a pen on a touch-sensitive computer screen and translates this sketch into a symbolic representation that can be processed against a geographic database. Since the configurations queried usually do not match exactly the sketch, it is necessary to relax the spatial constraints drawn. This paper describes the representation of a sketch and outlines the design of the constraint relaxation methods used during query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 892

LEFT text: For the past few decades, there has been a significant interest in data integration, and lots of work has been introduced in this field. Herein we discussed a way to handle the problem of integrating heterogeneous data model, namely relational and XML. Since the relational model is the most data model used to manage data for years. Similarly, XML is rapidly becoming more and more popular as a standard format for exchanging information. In such way, building a sort of connection bridging these two models is clearly a need. To this point, we aim to define a system to extract data regardless of the nature of their model and make one query enough to retrieve data from different models, which are XML and relational in our case.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: agora : living with xml and relational

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann , florian xhumari , dan olteanu
",y
"LEFT id: NA
RIGHT id: 1937

LEFT text: SHORE (Scalable Heterogeneous Object REpository) is a persistent object system under development at the University of Wisconsin. SHORE represents a merger of object-oriented database and file system technologies. In this paper we give the goals and motivation for SHORE, and describe how SHORE provides features of both technologies. We also describe some novel aspects of the SHORE architecture, including a symmetric peer-to-peer server architecture, server customization through an extensible value-added server facility, and support for scalability on multiprocessor systems. An initial version of SHORE is already operational, and we expect a release of Version 1 in mid-1994.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ptool : a light weight persistent object manager

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: r. l. grossman , d. hanley , x. qin
",n
"LEFT id: NA
RIGHT id: 442

LEFT text: For a number of reasons, even the best query optimizers can very often produce sub-optimal query execution plans, leading to a significant degradation of performance. This is especially true in databases used for complex decision support queries and/or object-relational databases. In this paper, we describe an algorithm that detects sub-optimality of a query execution plan during query execution and attempts to correct the problem. The basic idea is to collect statistics at key points during the execution of a complex query. These statistics are then used to optimize the execution of the query, either by improving the resource allocation for that query, or by changing the execution plan for the remainder of the query. To ensure that this does not significantly slow down the normal execution of a query, the Query Optimizer carefully chooses what statistics to collect, when to collect them, and the circumstances under which to re-optimize the query. We describe an implementation of this algorithm in the Paradise Database System, and we report on performance studies, which indicate that this can result in significant improvements in the performance of complex queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: counting , enumerating , and sampling of execution plans in a cost-based query optimizer

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: florian waas , c &#233; sar galindo-legaria
",n
"LEFT id: NA
RIGHT id: 1534

LEFT text: This paper describes the design and implementation of NAOS, an active rule component in the object-oriented database system 02. The contribution of this work is related to two main aspects. The first concerns the integration of the rule concept within the 02 model, providing a way to structure applications. Rules are part of a schema and do not belong to a class. Program execution and data manipulation, including method calls, can be driven on rules. The second aspect concerns the way NAOS interacts with the kernel of the 02 system. To support a reactive capability the object manager semantics has been extended, thus providing an efficient event detection. Applications produce events and the subscribed event types react to these events. As a result, rules are triggered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 963

LEFT text: We consider the view data lineageproblem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: expiring data in a warehouse

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: hector garcia-molina , wilburt labio , jun yang
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 234

LEFT text: The goal of the Garlic [1] project is to build a multimedia information system capable of integrating data that resides in different database systems as well as in a variety of non-database data servers. This integration must be enabled while maintaining the independence of the data servers, and without creating copies of their data. ""Multimedia"" should be interpreted broadly to mean not only images, video, and audio, but also text and application specific data types (e.g., CAD drawings, medical objects, &hellip;). Since much of this data is naturally modeled by objects, Garlic provides an object-oriented schema to applications, interprets object queries, creates execution plans for sending pieces of queries to the appropriate data servers, and assembles query results for delivery back to the applications. A significant focus of the project is support for ""intelligent"" data servers, i.e., servers that provide media-specific indexing and query capabilities [2]. Database optimization technology is being extended to deal with heterogeneous collections of data servers so that efficient data access plans can be employed for multi-repository queries.A prototype of the Garlic system has been operational since January 1995. Queries are expressed in an SQL-like query language that has been extended to include object-oriented features such as reference-valued attributes and nested sets. In addition to a C++ API, Garlic supports a novel query/browser interface called PESTO. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the garlic project

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: m. tork roth , m. arya , l. haas , m. carey , w. cody , r. fagin , p. schwarz , j. thomas , e. wimmers
",y
"LEFT id: NA
RIGHT id: 1

LEFT text: Since multimedia retrieval is based on similarity calculations of semantics and media-based search, exact matches are not expected. We view querying multimedia database as a combination of IR, image matching, and traditional database query processing and it should be conducted in a way of perpetual query reformulation for honing target results. In this paper we present a hybrid multimedia database system, which employs a hierarchical database statistics structure for both query optimization and reformulation analysis without adding additional query processing cost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a user-centered interface for querying distributed multimedia databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: isabel f. cruz , kimberly m. james
",n
"LEFT id: NA
RIGHT id: 1095

LEFT text: In this paper we study how to build an effective incremental crawler. The crawler selectively and incrementally updates its index and/or local collection of web pages, instead of periodically refreshing the collection in batch mode. The incremental crawler can improve the ``freshness'' of the collection significantly and bring in new pages in a more timely manner. We first present results from an experiment conducted on more than half million web pages over 4 months, to estimate how web pages evolve over time. Based on these experimental results, we compare various design choices for an incremental crawler and discuss their trade-offs. We propose an architecture for the incremental crawler, which combines the best design choices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the changing landscape of the software industry and its implications for india

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: umang gupta
",n
"LEFT id: NA
RIGHT id: 2195

LEFT text: Data extraction from HTML pages is performed by software modules, usually called wrappers. Roughly speaking, a wrapper identifies and extracts relevant pieces of text inside a web page, and reorganizes them in a more structured format. In the literature there is a number of systems to (semi-)automatically generate wrappers for HTML pages. We have recently investigated for original approaches that aims at pushing further the level of automation of the wrapper generation process. Our main intuition is that, in a dataintensive web site, pages can be classified in a small number of classes, such that pages belonging to the same class share a rather tight structure. Based on this observation, we have studied an novel technique, we call the matching technique, that automatically generates a common wrapper by exploiting similarities and differences among pages of the same class. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: roadrunner : towards automatic data extraction from large web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: valter crescenzi , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 1504

LEFT text: This paper proposes a dynamic buffer allocation scheme that allocates to user requests buffers of the minimum size in a partially loaded state as well as in the fully loaded state. The inherent difficulty in determining the buffer size in the dynamic buffer allocation scheme is that the size of the buffer currently being allocated is dependent on the number of and the sizes of the buffers to be allocated in the next service period. We solve this problem by the predict-and-enforce strategy, where we predict the number and the sizes of future buffers based on inertia assumptions and enforce these assumptions at runtime. Any violation of these assumptions is resolved by deferring service to the violating new user request until the assumptions are satisfied. Since the size of the current buffer is dependent on the sizes of the future buffers, the size is represented by a recurrence equation. We provide a solution to this equation, which can be computed at the system initialization time for runtime efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the spiffi scalable video-on-demand system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: craig s. freedman , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative formalism for specifying these kinds of probabilistic information, and we propose algorithms for ordering the information sources. Finally, we discuss a preliminary experimental evaluation of these algorithms on the domain of bibliographic sources available on the WWW.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",y
"LEFT id: NA
RIGHT id: 1065

LEFT text: In this paper, we introduce a new type of integrity constraint, which we call a statistical constraint, and discuss its applicability to enhancing database correctness. Statistical constraints manifest embedded relationships among current attribute values in the database and are characterized by their probabilistic nature. They can be used to detect potential errors not easily detected by the conventional constraints. Methods for extracting statistical constraints from a relation and enforcement of such constraints are described. Preliminary performance evaluation of enforcing statistical constraints on a real life database is also presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sting : a statistical information grid approach to spatial data mining

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wei wang , jiong yang , richard r. muntz
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: The chief editor' s ethics are the requirements put forward by the authors,editors and readers in ethical system based on identifying the scholarly publishing environment values for maintaining the moral attitudes and behavior in academic exchanges.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 1594

LEFT text: The cost functions are parameterized to accommodate various hardware characteristics appropriately. Combining the basic patterns, we can describe the memory access patterns of database operations. The cost functions of database operations can automatically be derived by combining the basic patterns' cost functions accordingly. To validate our approach, we performed experiments using our DBMS prototype Monet. The results presented here confirm the accuracy of our cost models for different operations. Aside from being useful for query optimization, our models provide insight to tune algorithms not only in a main-memory DBMS, but also in a disk-based DBMS with a large main-memory buffer cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: recovery protocols for shared memory database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: lory d. molesky , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1067

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: watchman : a data warehouse intelligent cache manager

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter scheuermann , junho shim , radek vingralek
",n
"LEFT id: NA
RIGHT id: 1084

LEFT text: Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",y
"LEFT id: NA
RIGHT id: 1534

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",y
"LEFT id: NA
RIGHT id: 1754

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semistructured and structured data in the web : going back and forth

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 289

LEFT text: In this paper, we investigate how to scale hierarchical clustering methods (such as OPTICS) to extremely large databases by utilizing data compression methods (such as BIRCH or random sampling). We propose a three step procedure: 1) compress the data into suitable representative objects; 2) apply the hierarchical clustering algorithm only to these objects; 3) recover the clustering structure for the whole data set, based on the result for the compressed data. The key issue in this approach is to design compressed data items such that not only a hierarchical clustering algorithm can be applied, but also that they contain enough information to infer the clustering structure of the original data set in the third step. This is crucial because the results of hierarchical clustering algorithms, when applied naively to a random sample or to the clustering features (CFs) generated by BIRCH, deteriorate rapidly for higher compression rates. This is due to three key problems, which we identify. To solve these problems, we propose an efficient post-processing step and the concept of a Data Bubble as a special kind of compressed data item. Applying OPTICS to these Data Bubbles allows us to recover a very accurate approximation of the clustering structure of a large data set even for very high compression rates. A comprehensive performance and quality evaluation shows that we only trade very little quality of the clustering result for a great increase in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data bubbles : quality preserving performance boosting for hierarchical clustering

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: markus m. breunig , hans-peter kriegel , peer kr &#246; ger , j &#246; rg sander
",y
"LEFT id: NA
RIGHT id: 1044

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the x-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim , hans-peter kriegel
",y
"LEFT id: NA
RIGHT id: 1225

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: The automatic reclamation of storage for unreferenced objects is very important in object databases. Existing language system algorithms for automatic storage reclamation have been shown to be inappropriate. In this paper, we investigate methods to improve the performance of algorithms for automatic for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. Specifically, we investigate the policy that is used to select what partition in the database should be collected. The policies that we propose and investigate are based on the intuition that the values of overwritten pointers provide good hints about  where to find garbage. Using trace-driven simulation, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy and performs close to a near-optimal policy over a wide range of database sizes and object connectivities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1564

LEFT text: We present a framework for designing, in a declarative and flexible way, efficient migration programs and an undergoing implementation of a migration tool called RelOO whose targets are any ODBC compliant system on the relational side and the 02 system on the object side. The framework consists of (i) a declarative language to specify database transformations from relations to objects, but also physical properties on the object database (clustering and sorting) and (ii) an algebrabased program rewriting technique which optimizes the migration processing time while taking into account physical properties and transaction decomposition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: addressing techniques used in database object managers o2 and orion

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: andr &#233; gamache , nadjiba sahraoui
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 946

LEFT text: The paper discusses the issue of views in the Web context. We introduce a set of languages for managing and restructuring data coming from the World Wide Web. We present a specific data model, called the ARANEUS Data Model, inspired to the structures typically present in Web sites. The model allows us to describe the scheme of a Web hypertext, in the spirit of databases. Based on the data model, we develop two languages to support a sophisticate view definition process: the first, called ULIXES, is used to build database views of the Web, which can then be analyzed and integrated using database techniques; the second, called PENELOPE, allows the definition of derived Web hypertexts from relational views. This can be used to generate hypertextual views over the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: to weave the web

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , giansalvatore mecca , paolo merialdo
",y
"LEFT id: NA
RIGHT id: 654

LEFT text: The Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS 2000) was held at the Crowne Plaza San Jose/Silicon Valley in Milpitas, California on June 8-9, 2000. The purpose of this workshop was to bring together leading practitioners, developers and researchers to explore the challenging technical issues and find feasible solutions for advancing the current state of the art in e-commerce and web-based information systems. In particular, the workshop was interested in the infrastructure issues to facilitate e-commerce and Web-based information systems.WECWIS 2000 was successful. There were three invited talks, one industrial panel discussion and six technical sessions. The keynote speech, ""The global trading web: A strategic vision for the Internet economy,"" was delivered by Dr. Jay M. Tenebaum, VP and Chief Scientist, Commerce One, Inc., on June 8 immediately following the opening remarks by the conference chair. The banquet address, ""Business issues in e-commerce,"" was delivered by Mr. Daniel Druker, General Manager, Hyperion e-Business Division. Finally, a lunch address, ""B2C, B2B, N2N, N2M: Why 2 is so instrumental?"" was delivered by Mr. Mstafa A. Syed, VP of Technology, VertialNet, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in databases and information systems : report of 5th east european conference adbis ' 2001

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: albertas caplinskas , johann eder , olegas vasilecas
",n
"LEFT id: NA
RIGHT id: 955

LEFT text: One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure  (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: m-tree : an efficient access method for similarity search in metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo ciaccia , marco patella , pavel zezula
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: We deal with the issue of combining dozens of classifiers into a better one. Our first contribution is the introduction of the notion of communities of classifiers. We build a complete graph with one node per classifier and edges weighted by a measure of similarity between connected classifiers. The resulting community structure is uncovered from this graph using the state-of-the-art Louvain algorithm. Our second contribution is a hierarchical fusion approach driven by these communities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 2

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: world wide database-integrating the web , corba and databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: athman bouguettaya , boualem benatallah , lily hendra , james beard , kevin smith , mourad quzzani
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",y
"LEFT id: NA
RIGHT id: 506

LEFT text: The advent of new database applications such as engineering design stresses the need for new functie nalities in database systems. It includes the management of multiple representations for database objects, long transactions as well as dynamic data structures. This paper presents the approach used in CADB, a prototype expert database system dedicated to CAD, for the management and control of the consistency of design objects. It concerns both the operations on the object property values and the interactive manipulation of their structure. They involve concepts of the object models as well as the application semantics. They rely therefore on concepts used for representing the design objects and on semantic notions related to expert knowledge in the application domain. Among these are the notions of consistency and of completeness of the objects. These two complementary aspects are detailed and their relationships described. The emphasis is on the heuristic rules that provide a unified knowledge-based approach for their management independent of the particular application being considered. Dynamic inheritance mechanisms are also presented that sup port the manipulations performed on the object structures. It is shown how they help providing expert database facilities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a survey of logical models for olap databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: panos vassiliadis , timos sellis
",n
"LEFT id: NA
RIGHT id: 2063

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic sample selection for approximate query processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 1395

LEFT text: One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure  (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: similarity search for adaptive ellipsoid queries using spatial transformation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , ryoji kataoka , shunsuke uemura
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1413

LEFT text: With the growing popularity of the internet and the World Wide Web (Web), there is a fast growing demand for access to database management systems (DBMS) from the Web. We describe here techniques that we invented to bridge the gap between HTML, the standard markup language of the Web, and SQL, the standard query language used to access relational DBMS. We propose a flexible general purpose variable substitution mechanism that provides cross-language variable substitution between HTML input and SQL query strings as well as between SQL result rows and HTML output thus enabling the application developer to use the full capabilities of HTML for creation of query forms and reports, and SQL for queries and updates. The cross-language variable substitution mechanism has been used in the design and implementation of a system called DB2 WWW Connection that enables quick and easy construction of applications that access relational DBMS data from the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2101

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amr el abbadi , gunter schlageter , kyu-young whang
",n
"LEFT id: NA
RIGHT id: 659

LEFT text: Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data mining : practical machine learning tools and techniques with java implementations

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ian h. witten , eibe frank
",y
"LEFT id: NA
RIGHT id: 1031

LEFT text: In this paper we investigate the problem of incremental maintenance of materialized views in data warehouses. We consider views defined by relational algebraic operators and aggregate functions. We show that a materialized view can be maintained without accessing the view itself by materializing and maintaining additional relations. These relations are derived from the intermediate results of the view computation. We first give an algorithm for determining what additional relations need to be materialized in order to maintain a materialized view incrementally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 1497

LEFT text: Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the lotus notes storage system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kenneth moore
",y
"LEFT id: NA
RIGHT id: 220

LEFT text: We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier, real point sets: (a) violate consistently the “uniformity” and “independence” assumptions, (b) can often be described as “fractals”, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called “Correlation Dimension” Dz is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with LI& as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for “biased” range queries (i.e., queries whose centers prefer areas of high point density).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 49

LEFT text: A new access method, called M-tree, is proposed to organize and search large data sets from a generic “metric space”, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O’s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 595

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: Publisher Summary eXtensible Markup Language (XML) is becoming the predominant data exchange format in a variety of application domains (supply-chain, scientific data processing, telecommunication infrastructure, etc.). Not only is an increasing amount of XML data now being processed, but XML is also increasingly being used in business-critical applications. Efficient and reliable storage is an important requirement for these applications. By relying on relational engines for this purpose, XML developers can benefit from a complete set of data management services (including concurrency control, crash recovery, and scalability) and from the highly optimized relational query processors. Strategies that automate the process of generating XML to relational mappings have been proposed in the literature. Due to the flexibility of the XML infrastructure, different XML applications exhibit widely different characteristics (for example, permissive vs. strict schemas, different access patterns).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: The strength of commercial query optimizers like DB2 comes from their ability to select an optimal order by generating all equivalent reorderings of binary operators. However, there are no known methods to generate all equivalent reorderings for a SQL query containing joins, outer joins, and groupby aggregations. Consequently, some of the reorderings with significantly lower cost may be missed. Using hypergraph model and a set of novel identities, we propose a method to reorder a SQL query containing joins, outer joins, and groupby aggregations. While these operators are sufficient to capture the SQL semantics, it is during their reordering that we identify a powerful primitive needed for a dbms. We report our findings of a simple, yet fundamental operator, generalized selection, and demonstrate its power to solve the problem of reordering of SQL queries containing joins, outer joins, and groupby aggregations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1629

LEFT text: This paper describes XDB-IPG, an open and extensible database architecture that supports efficient and flexible integration of heterogeneous and distributed information resources. XDB-IPG provides a novel “schema-less” database approach using a document-centered object-relational XML database mapping. This enables structured, unstructured, and semi-structured information to be integrated without requiring document schemas or translation tables. XDB-IPG utilizes existing international protocol standards of the World Wide Web Consortium Architecture Domain and the Internet Engineering Task Force, primarily HTTP, XML and WebDAV .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the montage extensible datablade architecture

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael ubell
",y
"LEFT id: NA
RIGHT id: 1266

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: progressive evaluation of nested aggregate queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kian-lee tan , cheng hian goh , beng chin ooi
",n
"LEFT id: NA
RIGHT id: 359

LEFT text: Financial mathematicians think they can predict the future by looking at time series of trades and quotes (called ticks) from the past. The main evidence for this hypothesis is that prices fluctuate only by a small amount in a given day and more or less obey the mathematics of a random walk. The hypothesis allows traders to price options and to speculate on stocks. This demonstration presents a query language and a parallel database (50-way parallelism) to support traders who want to analyze every tick, not just end-of-day ticks, using temporal statistical queries such as time-delayed correlations and tick trends. This is the first attempt that we know of to store and analyze hundreds of gigabytes of time series data and to query that data using a declarative time series extension to SQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lots o'ticks : real time high performance time series queries on billions of trades and quotes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: arthur whitney , dennis shasha
",y
"LEFT id: NA
RIGHT id: 1248

LEFT text: We report on the design and implementation of the Privacy Integrated Queries (PINQ) platform for privacy-preserving data analysis. PINQ provides analysts with a programming interface to unscrubbed data through a SQL-like language. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's unconditional structural guarantees require no trust placed in the expertise or diligence of the analysts, substantially broadening the scope for design and deployment of privacy-preserving data analysis, especially by non-experts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: advanced data processing in krisys : modeling concepts , implementation techniques , and client/server issues

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: stefan de &#223; loch , theo h &#228; rder , nelson mattos , bernhard mitschang , joachim thomas
",n
"LEFT id: NA
RIGHT id: 1459

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",y
"LEFT id: NA
RIGHT id: 2073

LEFT text: This paper addresses the problem of finding the K closest pairs between two spatial data sets, where each set is stored in a structure belonging in the R-tree family. Five different algorithms (four recursive and one iterative) are presented for solving this problem. The case of 1 closest pair is treated as a special case. An extensive study, based on experiments performed with synthetic as well as with real point data sets, is presented. A wide range of values for the basic parameters affecting the performance of the algorithms, especially the effect of overlap between the two data sets, is explored.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: spatial queries in dynamic environments

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 2234

LEFT text: As our dependency on hlformation systems grows, the threat of having those disrupted by cyber attacks becomes a very pressing reality. We have witnesses nmltiple occurrences of attacks in the recent past that have seriously disrupted businesses and organizations. And, unfortunately, this trend is only increasing. For some time now, some research groups have been doing research oil data mining techniques that can potentially help in meeting the challenges posed by the attacks. This special issue is an attempt to bring some of these people together and disseminate some of the results among the SIGMOD audience, and perhaps spark the interest of the community for this emerging field. In this issue we have six papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guest editor 's introduction

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 497

LEFT text: We consider the view data lineageproblem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tracing the lineage of view data in a warehousing environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom , janet l. wiener
",y
"LEFT id: NA
RIGHT id: 1534

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 1457

LEFT text: The DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool that, for a given workload, automatically recommends physical design features that are any subset of indexes, materialized query tables (also called materialized views), shared-nothing database partitionings, and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features, a significant advance to existing tools, which support no more than just indexes and materialized views. Building such a tool is challenging, because of not only the large search space introduced by the interactions among features, but also the extensibility needed by the tool to support additional features in the future. We adopt a novel ""hybrid"" approach in the Design Advisor that allows us to take important interdependencies into account as well as to encapsulate design features as separate components to lower the reengineering cost. The Design Advisor also features a built-in module that automatically reduces the given workload, and therefore provides great scalability for the tool. Our experimental results demonstrate that our tool can quickly provide good physical design recommendations that satisfy users' requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: db2 common server : technology , progress , & ; directions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bruce g. lindsay
",y
"LEFT id: NA
RIGHT id: 1032

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",y
"LEFT id: NA
RIGHT id: 2201

LEFT text: Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization by predicate move-around

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , inderpal singh mumick , yehoshua sagiv
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2044

LEFT text: Testing an SQL database system by running large sets of deterministic or stochastic SQL statements is common practice in commercial database development. However, code defects often remain undetected as the query optimizer's choice of an execution plan is not only depending on the query but strongly influenced by a large number of parameters describing the database and the hardware environment. Modifying these parameters in order to steer the optimizer to select other plans is difficult since this means anticipating often complex search strategies implemented in the optimizer. In this paper we devise algorithms for counting, exhaustive generation, and uniform sampling of plans from the complete search space. Our techniques allow extensive validation of both generation of alternatives, and execution algorithms with plans other than the optimized one—if two candidate plans fail to produce the same results, then either the optimizer considered an invalid plan, or the execution code is faulty. When the space of alternatives becomes too large for exhaustive testing, which can occur even with a handful of joins, uniform random sampling provides a mechanism for unbiased testing. The technique is implemented in Microsoft's SQL Server, where it is an integral part of the validation and testing process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: estimating compilation time of a query optimizer

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ihab f. ilyas , jun rao , guy lohman , dengfeng gao , eileen lin
",n
"LEFT id: NA
RIGHT id: 2211

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: including group-by in query optimization

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 1879

LEFT text: The current main memory (DRAM) access speeds lag far behind CPU speeds. Cache memory, made of static RAM, is being used in today’s architectures to bridge this gap. It provides access latencies of 2-4 processor cycles, in contrast to main memory which requires 15-25 cycles. Therefore, the performance of the CPU depends upon how well the cache can be utilized. We show that there are significant benefits in redesigning our traditional query processing algorithms so that they can make better use of the cache. The new algorithms run 8%-200% faster than the traditional ones.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: control strategies for complex relational query processing in shared nothing systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: lionel brunie , harald kosch
",n
"LEFT id: NA
RIGHT id: 579

LEFT text: Curated databases are databases that are populated and updated with a great deal of human effort. Most reference works that one traditionally found on the reference shelves of libraries -- dictionaries, encyclopedias, gazetteers etc. -- are now curated databases. Since it is now easy to publish databases on the web, there has been an explosion in the number of new curated databases used in scientific research. The value of curated databases lies in the organization and the quality of the data they contain. Like the paper reference works they have replaced, they usually represent the efforts of a dedicated group of people to produce a definitive description of some subject area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: declare and sds : early efforts to commercialize deductive database technology

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: werner kie &#223; ling , helmut schmidt , werner strau &#223; , gerhard d &#252; nzinger
",n
"LEFT id: NA
RIGHT id: 2245

LEFT text: This paper examines the characteristics and challenges presented by medical databases and medical information systems. It begins with a survey of medical databases/information systems. This is followed by a list of challenges for database management systems generated by the needs of these systems. It concludes with a look at some systems which address these challenges. In the context of this background information, the database community is asked to consider whether the results of database research are reaching those who are making day-to-day decisions regarding design and implementation of medical information systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: challenges for global information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova
",n
"LEFT id: NA
RIGHT id: 320

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the indian institute of technology , bombay

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 287

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vqbd : exploring semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , thomas baby , jihwang yoo
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1222

LEFT text: In this paper we study how to refresh a local copy of an autonomous data source to maintain the copy up-to-date. As the size of the data grows, it becomes more difficult to maintain the copy \ fresh, “making it crucial to synchronize the copy effectively. We define two freshness metrics, change models of the underlying data, and synchronization policies. We analytically study how effective the various policies are. We also experimentally verify our analysis, based on data collected from 270 web sites for more than 4 months, and we show that our new policy improves the \ freshness” very significantly compared to current policies in use.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: update propagation strategies to improve freshness in lazy master replicated databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: esther pacitti , eric simon
",n
"LEFT id: NA
RIGHT id: 1235

LEFT text: Managing multiple versions of XML documents represents an important problem, because of many applications ranging from traditional ones, such as software configuration control, to new ones, such as link permanence of web documents. Research on managing multiversion XML documents seeks to provide efficient and robust techniques for (i) storing and retrieving, (ii) exchanging, and (iii) querying such documents. In this paper, we first show that traditional version control methods, such as RCS, and SCCS, fall short from satisfying these three requirements, and discuss alternative solutions. First, we enhance RCS with a temporal page clustering policy to achieve objective (i). Then, we discuss a reference-based versioning scheme that achieves both objectives (i) and (ii) and is also effective at supporting simple queries. The topic of supporting complex queries, including temporal ones, meshes with the burgeoning interest of database researchers in XML as a database description language, and in XML query languages. In this context, the XML versioning problems are akin to those of transaction time management for databases of objects and semistructured information. Nevertheless, the need to preserve the natural ordering of XML documents frequently requires different techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient schemes for managing multiversionxml documents

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s.-y . chien , v. j. tsotras , c. zaniolo
",n
"LEFT id: NA
RIGHT id: 1708

LEFT text: Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: evaluation of remote backup algorithms for transaction-processing systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christos a. polyzois , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 1782

LEFT text: A star schema is very popular for modeling data warehouses and data marts. Therefore, it is important that a database system which is used for implementing such a data warehouse or data mart is able to efficiently handle operations on such a schema. In this paper we will describe how one of these operations, the join operation --- probably the most important operation --- is implemented in the IBM Informix Extended Parallel Server (XPS).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient mid-query re-optimization of sub-optimal query execution plans

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 512

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1938

LEFT text: Today's industrial control systems store large amounts of monitored sensor data in order to optimize industrial processes. In the last decades, architects have designed such systems mainly under the assumption that they operate in closed, plant-side IT infrastructures without horizontal scalability. Cloud technologies could be used in this context to save local IT costs and enable higher scalability, but their maturity for industrial applications with high requirements for responsiveness and robustness is not yet well understood. We propose a conceptual architecture as a basis to designing cloud-native monitoring systems. As a first step we benchmarked three open source time-series databases (OpenTSDB, KairosDB and Databus) on cloud infrastructures with up to 36 nodes with workloads from realistic industrial applications. We found that at least KairosDB fulfills our initial hypotheses concerning scalability and reliability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 1442

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coloring away communication in parallel query optimization

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: waqar hasan , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 1863

LEFT text: In this paper we present a tool for enhanced exploration of OLAP data that is adaptive to a user’s prior knowledge of the data. The tool continuously keeps track of the parts of the cube that a user has visited. The information in these scattered visited parts of the cube is pieced together to form a model of the user’s expected values in the unvisited parts. The mathematical foundation for this modeling is provided by the classical Maximum Entropy principle. At any time, the user can query for the most surprising unvisited parts of the cube. The most surprising values are dened as those which if known to the user would bring the new expected values closest to the actual values. This process of updating the user’s context based on visited parts and querying for regions to explore further continues in a loop until the user’s mental model perfectly matches the actual cube.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quasi-cubes : exploiting approximations in multidimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , mark sullivan
",n
"LEFT id: NA
RIGHT id: 2097

LEFT text: In this paper we propose a new framework for dynamic distributed query processing based on so-called HyperQueries which are essentially query evaluation sub-plans “sitting behind” hyperlinks. We illustrate the flexibility of this distributed query processing architecture in the context of B2B electronic market places. Architecting an electronic market place as a data warehouse by integrating all the data fromall participating enterprises in one centralized repository incurs severe problems. Using HyperQueries, application integration is achieved via dynamic distributed query evaluation plans. The electronic market place serves as an intermediary between clients and providers executing their sub-queries referenced via hyperlinks. The hyperlinks are embedded within data objects of the intermediary’s database. Retrieving such a virtual object will automatically initiate the execution of the referenced HyperQuery in order to materialize the entire object. Thus, sensitive data remains under the full control of the data providers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: objectglobe : ubiquitous query processing on the internet

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: r. braumandl , m. keidl , a. kemper , d. kossmann , a. kreutz , s. seltzsam , k. stocker
",n
"LEFT id: NA
RIGHT id: 1542

LEFT text: The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1131

LEFT text: Computing multiple related group-bys and aggregates is one of the core operations of On-Line Analytical Processing (OLAP) applications. Recently, Gray et al. [GBLP95] proposed the “Cube” operator, which computes group-by aggregations over all possible subsets of the specified dimensions. The rapid acceptance of the importance of this operator has led to a variant of the Cube being proposed for the SQL standard. Several efficient algorithms for Relational OLAP (ROLAP) have been developed to compute the Cube. However, to our knowledge there is nothing in the literature on how to compute the Cube for Multidimensional OLAP (MOLAP) systems, which store their data in sparse arrays rather than in tables. In this paper, we present a MOLAP algorithm to compute the Cube, and compare it to a leading ROLAP algorithm. The comparison between the two is interesting, since although they are computing the same function, one is value-based (the ROLAP algorithm) whereas the other is position-based (the MOLAP algorithm). Our tests show that, given appropriate compression techniques, the MOLAP algorithm is significantly faster than the ROLAP algorithm. In fact, the difference is so pronounced that this MOLAP algorithm may be useful for ROLAP systems as well as MOLAP systems, since in many cases, instead of cubing a table directly, it is faster to first convert the table to an array, cube the array, then convert the result back to a table.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage estimation for multidimensional aggregates in the presence of hierarchies

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton , karthikeyan ramasamy
",n
"LEFT id: NA
RIGHT id: 1240

LEFT text: Smartcards are the most secure portable computing device today. They have been used successfully in applications involving money, and proprietary and personal data (such as banking, healthcare, insurance, etc.). As smartcards get more powerful (with 32-bit CPU and more than 1 MB of stable memory in the next versions) and become multi-application, the need for database management arises. However, smartcards have severe hardware limitations (very slow write, very little RAM, constrained stable memory, no autonomy, etc.) which make traditional database technology irrelevant. The major problem is scaling down database techniques so they perform well under these limitations. In this paper, we give an in-depth analysis of this problem and propose a PicoDBMS solution based on highly compact data structures, query execution without RAM, and specific techniques for atomicity and durability. We show the effectiveness of our techniques through performance evaluation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: picodmbs : scaling down database techniques for the smartcard

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christophe bobineau , luc bouganim , philippe pucheral , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1526

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efendi : federated database system of cadlab

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: e. radeke , r. b &#246; ttger , b. burkert , y. engel , g. kachel , s. kolmschlag , d. nolte
",n
"LEFT id: NA
RIGHT id: 1721

LEFT text: We describe an algorithm for estimating the number of page fetches for a partial or complete scan of a B-tree index. The algorithm obtains estimates for the number of page fetches for an index scan when given the number of tuples selected and the number of LRU buffers currently available. The algorithm has an initial phase that is performed exactly once before any estimates are calculated. This initial phase, involving LRU buffer modeling, requires a scan of all the index entries and calculates the number of page fetches for different buffer sizes. An approximate empirical model is obtained from this data. Subsequently, an inexpensive estimation procedure is called by the query optimizer whenever it needs an estimate of the page fetches for the index scan. This procedure utilizes the empirical model obtained in the initial phase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: estimating page fetches for index scans with finite lru buffers

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arun swami , k. bernhard schiefer
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 535

LEFT text: We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:<ul><li>Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 581

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the glue-nail deductive database system : design , implementation , and evaluation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marcia a. derr , shinichi morishita , geoffrey phipps
",n
"LEFT id: NA
RIGHT id: 1350

LEFT text: This paper investigates the problem of incremental joins of multiple ranked data sets when the join condition is a list of arbitrary user-defined predicates on the input tuples. This problem arises in many important applications dealing with ordered inputs and multiple ranked data sets, and requiring the top solutions. We use multimedia applications as the motivating examples but the problem is equally applicable to traditional database applications involving optimal resource allocation, scheduling, decision making, ranking, etc. We propose an algorithm that enables querying of ordered data sets by imposing arbitrary userdefined join predicates. The basic version of the algorithm does not use any random access but a variation can exploit available indexes for efficient random access based on the join predicates. A special case includes the join scenario considered by Fagin [1] for joins based on identical keys, and in that case, our algorithms perform as efficiently as Fagin’s. Our main contribution, however, is the generalization to join scenarios that were previously unsupported, including cases where random access in the algorithm is not possible due to lack of unique keys. In addition, can support multiple join levels, or nested join hierarchies, which are the norm for modeling multimedia data. We also give -approximation versions of both of the above algorithms. Finally, we give strong optimality results for some of the proposed algorithms, and we study their performance empirically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting incremental join queries on ranked inputs

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: apostol natsev , yuan-chi chang , john r. smith , chung-sheng li , jeffrey scott vitter
",y
"LEFT id: NA
RIGHT id: 2264

LEFT text: We enunciate the need for watermarking database relations to deter data piracy, identify the characteristics of relational data that pose unique challenges for watermarking, and delineate desirable properties of a watermarking system for relational data. We then present an effective watermarking technique geared for relational data. This technique ensures that some bit positions of some of the attributes of some of the tuples contain specific values. The specific bit locations and values are algorithmically determined under the control of a secret key known only to the owner of the data. This bit pattern constitutes the watermark. Only if one has access to the secret key can the watermark be detected with high probability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: watermarking relational data : framework , algorithms and analysis

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",y
"LEFT id: NA
RIGHT id: 1600

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 300

LEFT text: This paper is a survey of work and issues on multidimensional search trees. We provide a classification of such methods, we describe the related algorithms, we present performance analysis efforts, and finally outline future research directions. Multi-dimensional search trees and Spatial Access Methods, in general, are designed to handle spatial objects, like points, line segments, polygons, polyhedra etc. The goal is to support spatial queries, such as nearest neighbors queries (find all cities within 10 miles from Washington D.C.), or range queries (find all the lakes on earth, within 30 and 40 degrees of latitude), and so on. The applications are numerous, including traditional database multi-attribute indexing, Geographic Information Systems and spatial database systems, and indexing multimedia databases by content. $‘rom the spatial databases viewpoint we can dist,inguish between two major classes of access methods:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing multidimensional index trees for main memory access

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kihong kim , sang k. cha , keunjoo kwon
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1863

LEFT text: We propose a novel index structure, the A-tree (approximation tree), for similarity searches in high-dimensional data. The basic idea of the A-tree is the introduction of virtual bounding rectangles (VBRs) which contain and approximate MBRs or data objects. VBRs can be represented quite compactly and thus affect the tree configuration both quantitatively and qualitatively. First, since tree nodes can contain a large number of VBR entries, fanout becomes large, which increases search speed. More importantly, we have a free hand in arranging MBRs and VBRs in the tree nodes. Each A-tree node contains an MBR and its children VBRs. Therefore, by fetching an A-tree node, we can obtain information on the exact position of a parent MBR and the approximate position of its children. We have performed experiments using both synthetic and real data sets. For the real data sets, the A-tree outperforms the SR-tree and the VA-file in all dimensionalities up to 64 dimensions, which is the highest dimension in our experiments. Additionally, we propose a cost model for the A-tree. We verify the validity of the cost model for synthetic and real data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quasi-cubes : exploiting approximations in multidimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , mark sullivan
",n
"LEFT id: NA
RIGHT id: 1091

LEFT text: The DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool that, for a given workload, automatically recommends physical design features that are any subset of indexes, materialized query tables (also called materialized views), shared-nothing database partitionings, and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features, a significant advance to existing tools, which support no more than just indexes and materialized views. Building such a tool is challenging, because of not only the large search space introduced by the interactions among features, but also the extensibility needed by the tool to support additional features in the future. We adopt a novel ""hybrid"" approach in the Design Advisor that allows us to take important interdependencies into account as well as to encapsulate design features as separate components to lower the reengineering cost. The Design Advisor also features a built-in module that automatically reduces the given workload, and therefore provides great scalability for the tool. Our experimental results demonstrate that our tool can quickly provide good physical design recommendations that satisfy users' requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: extracting large data sets using db2 parallel edition

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sriram padmanabhan
",n
"LEFT id: NA
RIGHT id: 490

LEFT text: Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: generating dynamic content at database-backed web servers : cgi-bin vs. mod_perl

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alexandros labrinidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 2072

LEFT text: We analyze this algorithm, and prove that is optimal when the maximally specific sentences are ""small"". We also point out its limitations.We then present a new algorithm, the Dualize and Advance algorithm, and prove worst-case complexity bounds that are favorable in the general case. Our results use the concept of hypergraph transversals. Our analysis shows that the a priori algorithm can solve the problem of enumerating the transversals of a hypergraph, improving on previously known results in a special case. On the other hand, using results for the general case of the hypergraph transversal enumeration problem, we can show that the Dualize and Advance algorithm has worst-case running time that is sub-exponential to the output size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: discovering all most specific sentences

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , roni khardon , heikki mannila , sanjeev saluja , hannu toivonen , ram sewak sharma
",y
"LEFT id: NA
RIGHT id: 1693

LEFT text: Analysis of expected and experimental results of various join algorithms show that a combination of the optimal nested block and optimal GRACE hash join algorithms usually provide the greatest cost benefit, unless the relation size is a small multiple of the memory size. Algorithms to quickly determine a buffer allocation producing the minimal cost for each of these algorithms are presented. When the relation size is a small multiple of the amount of main memory available (typically up to three to six times), the hybrid hash join algorithm is preferable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a new join algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dong keun shin , arnold charles meltzer
",n
"LEFT id: NA
RIGHT id: 1669

LEFT text: Testing the reliability of high performance tlansaetion processing systams poses many difficult challenges that are not adequately answered by conventional testing techniques. We discuss a new test paradigm, which is dynamic and exploratory in nature, and discuss its ability to meet these challenges. We describe an implementation of thii paradigm in products that aid in efficiently testing reliable, high performance transaction processing systems at T~adem Computers Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: relaxed transaction processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munindar p. singh , christine tomlinson , darrell woelk
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: Wireless and mobile computing have advanced significantly in the last decade. In particular, we now face the challenge to spontaneously establish wireless self-organizing networks, such as ad hoc, disruption-tolerant, sensor, and wireless mesh networks. These spontaneous self-organizing networks have been the focus of intensive research activity in recent years. Spontaneous networks arise from the cooperation of mobile devices in an ad hoc fashion requiring no previous infrastructure in place. A key point to couple research and real-life applications in this context is to understand how mobility (of devices, users, and applications) impacts practical networking aspects

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Today's E-Commerce systems are a complex assembly of databases, web servers, home grown glue code, and networking services for security and scalability. The trend is towards larger pieces of these coming together in bundled offerings from leading software vendors, and the networking/hardware being offered through service delivery companies. In this paper we examine the bundle by looking in detail at IBM's WebSphere, Commerce Edition, and its deployment at a major customer site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: Multiversion support for XML documents is needed in many critical applications, such as software configuration control, cooperative authoring, web information warehouses, and ”e-permanence” of web documents. In this paper, we introduce efficient and robust techniques for: (i) storing and retrieving; (ii) viewing and exchanging; and (iii) querying multiversion XML documents. We first discuss the limitations of traditional version control methods, such as RCS and SCCS, and then propose novel techniques that overcome their limitations. Initially, we focus on the problem of managing secondary storage efficiently, and introduce an edit-based versioning scheme that enhances RCS with an effective clustering policy based on the concept of page-usefulness. The new scheme drastically improves version retrieval at the expense of a small (linear) space overhead. However, the edit-based approach falls short of achieving objectives (ii) and (iii). Therefore, we introduce and investigate a second scheme, which is reference-based and preserves the structure of the original document. In the reference-based approach, a multiversion document can be represented as yet another XML document, which can be easily exchanged and viewed on the web; furthermore, simple queries are also expressed and supported well under this representation. To achieve objective (i), we extend the page-usefulness clustering technique to the reference-based scheme. After characterizing the asymptotic behavior of the new techniques proposed, the paper presents the results of an experimental study evaluating and comparing their performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 137

LEFT text: DTL's DataSpot is an advanced, programming-free tool that lets Web designers and database developers automatically publish their databases for Web browser access. DataSpot enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation, in a fashion similar to using search engines such as Alta Vista to search text files on the Internet.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dtl 's dataspot : database exploration as easy as browsing the web ...

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shaul dar , gadi entin , shai geva , eran palmon
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2231

LEFT text: The emergence of Big Data has amounted to the complexity of the discussion on data reuse. The benefits of Big Data lie in the possibilities to discover novel trends, patterns and relationships by combining very large amounts of data from different sources. Current personal data protection requirements like data minimization and purpose specification are potentially inimical to Big Data as they limit the size and use of Big Data. Substantial loss of economic and social benefits of Big Data may be the result. In order to avoid this, the reuse of data could be encouraged. Data reuse, when done properly, may be both privacy preserving and economically and socially beneficial. In this paper, we provide a taxonomy of data reuse from both the data controller’s and the data subject’s perspective that may be useful to determine the extent to which data reuse should be allowed and under which conditions. From the data controller’s perspective we distinguish data recycling, data repurposing and data recontextualisation. From the data subject’s perspective, we distinguish data sharing and data portability. It is argued that forms of data reuse that stay close to the awareness and intentions of data subjects should be approached less tight (for instance, by assuming informed consent), whereas forms of data reuse that are ‘at a distance’, i.e., in which awareness and transparency may be lacking and data subject’s rights may prove more difficult to exercise, more restrictions and additional protection should be considered (for instance, by requiring explicit consent).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data integration in the large : the challenge of reuse

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arnon rosenthal , leonard j. seligman
",y
"LEFT id: NA
RIGHT id: 871

LEFT text: In this paper, we report our success in building efficient scalable classifiers in the form of decision tables by exploring capabilities of modern relational database management systems. In addition to high classification accuracy, the unique features of the approach include its high training speed, linear scalability, and simplicity in implementation.  The novel classification approach based on grouping and counting and its implementation on top of RDBMS is described. The results of experiments conducted for performance evaluation and analysis are presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: decision tables : scalable classification exploring rdbms capabilities

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hongjun lu , hongyan liu
",y
"LEFT id: NA
RIGHT id: 1285

LEFT text: Digital libraries are being built upon a firm foundation of prior work as the high-end information systems of the future. A component architecture approach is becoming popular, with well established support for key components like the repository, especially through the Open Archives Initiative. We consider digital objects, metadata, harvesting, indexing, searching, browsing, rights management, linking, and powerful interfaces. Flexible interaction will be possible through a variety of architectures, using buses, agents, and other technologies. The field as a whole is undergoing rapid growth, supported by advances in storage, processing, networking, algorithms, and interaction. There are many initiatives and developments, including those supporting education, and these will certainly be of benefit in Latin America.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: flexible and scalable digital library search

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: henk ernst blok , menzo windhouwer , roelof van zwol , milan petkovic , peter m. g. apers , martin l. kersten , willem jonker
",y
"LEFT id: NA
RIGHT id: 177

LEFT text: A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: high-dimensional index structures database support for next decade 's applications ( tutorial )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 996

LEFT text: The challenge in a database of evolving time series is to provide efficient algorithms and access methods for query processing, taking into consideration the fact that the database changes continuously as new data become available. Traditional access methods that continuously update the data are considered inappropriate, due to significant update costs. In this paper, we use the IDC-Index (Incremental DFT Computation - Index), an efficient technique for similarity query processing in streaming time series. The index is based on a multidimensional access method enhanced with a deferred update policy and an incremental computation of the Discrete Fourier Transform (DFT), which is used as a feature extraction method. We focus both on range and nearest-neighbor queries, since both types are frequently used in modern applications. An important characteristic of the proposed approach is its ability to adapt to the update frequency of the data streams. By using a simple heuristic approach, we manage to keep the update frequency at a specified level to guarantee efficiency. In order to investigate the efficiency of the proposed method, experiments have been performed for range queries and k-nearest-neighbor queries on real-life data sets. The proposed method manages to reduce the number of false alarms examined, achieving high answers vs. candidates ratio. Moreover, the results have shown that the new techniques exhibit consistently better performance in comparison to previously proposed approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a quantitative analysis and performance study for similarity-search methods in high-dimensional spaces

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roger weber , hans-j &#246; rg schek , stephen blott
",y
"LEFT id: NA
RIGHT id: 2113

LEFT text: A prototyped data mining system, DBLearn, has been developed, which efficiently and effectively extracts different kinds of knowledge rules from relational databases. It has the following features: high level learning interfaces, tightly integrated with commercial relational database systems, automatic refinement of concept hierarchies, efficient discovery algorithms and good performance. Substantial extensions of its knowledge discovery power towards knowledge mining in object-oriented, deductive and spatial databases are under research and development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: complex relationships and knowledge discovery support in the infoquilt system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: a. sheth , s. thacker , s. patel
",n
"LEFT id: NA
RIGHT id: 1981

LEFT text: As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional resource scheduling for parallel queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1303

LEFT text: We describe the tools and theory of a comprehensive system for database design, and show how they work together to support multiple conceptual and logical design processes. The Database Design and Evaluation Workbench (DDEW) system uses a rigorous, information-content-preserving approach to schema transformation, but combines it with heuristics, guess work, and user interactions. The main contribution lies in illustrating how theory was adapted to a practical system, and how the consistency and power of a design system can be increased by use of theory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tools and transformations-rigorous and otherwise-for practical database design

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arnon rosenthal , david reiner
",y
"LEFT id: NA
RIGHT id: 480

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 344

LEFT text: One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a thery and practice of intellectual property cost management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce : guest editor 's introduction

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1088

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sampling large databases for association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hannu toivonen
",n
"LEFT id: NA
RIGHT id: 1628

LEFT text: Solid state disks (SSDs) provide much faster random access to data compared to conventional hard disk drives. Therefore, the response time of a database engine could be improved by moving the objects that are frequently accessed in a random fashion to the SSD. Considering the price and limited storage capacity of solid state disks, the database administrator needs to determine which objects (tables, indexes, materialized views, etc.), if placed on the SSD, would most improve the performance of the system. In this paper we propose a tool called ""Object Placement Advisor"" for making a wise decision for the object placement problem. By collecting profile inputs from workload runs, the advisor utility provides a list of objects to be placed on the SSD by applying heuristics like the greedy knapsack technique or dynamic programming. To show that the proposed approach is effective in conventional database management systems, we have conducted experiments on IBM DB2 with queries and schemas based on the TPC-H and TPC-C benchmarks. The results indicate that using a relatively small amount of SSD storage, the response time of the system can be reduced significantly by considering the recommendation of the advisor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-oriented features of db2 client/server

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: hamid pirahesh
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 1401

LEFT text: In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query translation across heterogeneous information sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 394

LEFT text: Internet websites increasingly rely on database management systems. There are several reasons for this trend: As sites grow larger, managing the content becomes impossible without the use of a DBMS to keep track of the nature, origin, authorship, and modification history of each article. As sites become more interactive, tracking and logging user activity and user contributions creates valuable new data, which again is best managed using a DBMS. The emerging paradigm of Customer-Centric e-Business places a premium on engaging users, building a relationship with them across visits, and leveraging their expertise and feed-back. Supporting this paradigm means that we not only have to track what users visit on a site, we also have to enable them to offer opinions and contribute to the content of the website in various ways; naturally, this requires us to use a DBMS. In order to personalize a user's experience, a site must dynamically construct (or at least fine-tune) each page as it is delivered, taking into account information about the user's past activity and the nature of the content on the current page. In other words, personalization is made possible by utilizing the information (about content and user activity) that we already indicated is best managed using a DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: from browsing to interacting : dbms support for responsive websites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 458

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraint databases : a tutorial introduction

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jan van den bussche
",y
"LEFT id: NA
RIGHT id: 372

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 884

LEFT text: TimesTen Performance Software's Front-Tier product is an application-tier data cache that inter-operates with disk-based relational database management systems (RDBMSs) to achieve breakthrough response time and throughput, scalability in transaction load, high availability, and ease of administration and deployment. Front-Tier caches frequently used subsets of the corporate database on multiple servers in the application tier and supports SQL queries and updates to the caches. The caches may or may not be overlapping, are kept synchronized with the corporate database and with each other, and may be dynamically reconfigured to contain different subsets of the corporate database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: high-performance and scalability through application tier,in-memory data management

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: NA
",y
"LEFT id: NA
RIGHT id: 1463

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 787

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the new locking , logging , and recovery architecture of microsoft sql server 7.0

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: david campbell
",y
"LEFT id: NA
RIGHT id: 1873

LEFT text: With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: Semantic Binary Object-oriented Data Model (Sem-ODM) provides an expressive data model (similar to Object-oriented Data Models) with a well-known declarative query facility - SQL (similar to relational databases). Advantages of using Sem-ODM include (i.) friendlier and more intelligent generic user interfaces; (ii.) comprehensive enforcement of integrity constraints; (iii.) greater flexibility; (iv.) substantially shorter application programs; and (v.) easier query facility. SemanticAccess is a set of tools developed to provide a semantic interface to Semantic Binary Object-oriented Databases (Sem-ODB) as well as relational databases. This presentation focuses on the system architecture of SemanticAccess including Semantic Binary Object-oriented Data Model, Semantic SQL query language, Semantic Binary Database and a wrapper developed for relational databases. 1. Purpose

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: In this work, we address the efficient evaluation of XQuery expressions over continuous XML data streams, which is essential for a broad range of applications including monitoring systems and information dissemination systems. While previous work has shown that automata theory is suited for on-the-fly pattern retrieval over XML data streams, we find that automata-based approaches suffer from being not as flexibly optimizable as algebraic query systems. In fact, they enforce a rigid data-driven paradigm of execution. We thus now propose a unified query model to augment automata-style processing with algebra-based query optimization techniques. The proposed model has been successfully applied in the Raindrop stream processing system. Our experimental study confirms considerable performance gains with both established optimization techniques and our novel query rewrite rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 407

LEFT text: The analysis of web usage has mostly focused on sites composed of conventional static pages. However, huge amounts of information available in the web come from databases or other data collections and are presented to the users in the form of dynamically generated pages. The query interfaces of such sites allow the specification of many search criteria. Their generated results support navigation to pages of results combining cross-linked data from many sources. For the analysis of visitor navigation behaviour in such web sites, we propose the web usage miner (WUM), which discovers navigation patterns subject to advanced statistical and structural constraints. Since our objective is the discovery of interesting navigation patterns, we do not focus on accesses to individual pages. Instead, we construct conceptual hierarchies that reflect the query capabilities used in the production of those pages. Our experiments with a real web site that integrates data from multiple databases, the German SchulWeb, demonstrate the appropriateness of WUM in discovering navigation patterns and show how those discoveries can help in assessing and improving the quality of the site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: image mining in iris : integrated retinal information system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wynne hsu , mong li lee , kheng guan goh
",n
"LEFT id: NA
RIGHT id: 2173

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: preference formulas in relational queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jan chomicki
",n
"LEFT id: NA
RIGHT id: 434

LEFT text: With the advent of GIS, multi-media, and warehousing technologies, database systems have started focusing on storage and access of multi-dimensional data such as spatial, OLAP, image, audio, and video attributes. As a step in this direction, Oracle% launched the interMedia product to support spatial and image data, and Materialized Views (MV) to support warehousing applications. Although 2-dimensional spatial data is efficiently indexed using OracleBi Spatial, and highdimensional image data using a combination of bitmap indexing and the Visual Information Retrieval (VIR) product, there is still a need for efficient indexing mechanisms for medium-dimensionality data such as OLAP, and CAD/CAM applications. In this paper, we describe the implementation of a new indextype, called the R-tree, to support medium-dimensionality data (i.e., data whose dimensionality is in the range of 310) using the extensible indexing framework of OracleSi.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing images in oracle8i

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: melliyal annamalai , rajiv chopra , samuel defazio , susan mavris
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 1837

LEFT text: nvited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sr-tree : an index structure for high-dimensional nearest neighbor queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: norio katayama , shin ` ichi satoh
",n
"LEFT id: NA
RIGHT id: 1745

LEFT text: In June 1997, an international workshop on engineering of federated database systems has been held in Barcelona in conjunction with the 9th Conference on Advanced Information Systems Engineering (CAiSE'97). This paper reports on the results of this workshop and summarises the identified open issues for future research in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in federated database systems : report of efdbs '97 workshop

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. conrad , b. eaglestone , w. hasselbring , m. roantree , m. sch &#246; hoff , m. str &#228; ssler , m. vermeer , f. saltor
",y
"LEFT id: NA
RIGHT id: 1511

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1121

LEFT text: This paper showcases some of the newly introduced parallel execution methods in Oracle RDBMS. These methods provide highly scalable and adaptive evaluation for the most commonly used SQL operations - joins, group-by, rollup/cube, grouping sets, and window functions. The novelty of these techniques is their use of multi-stage parallelization models, accommodation of optimizer mistakes, and the runtime parallelization and data distribution decisions. These parallel plans adapt based on the statistics gathered on the real data at query execution time. We realized enormous performance gains from these adaptive parallelization techniques. The paper also discusses our approach to parallelize queries with operations that are inherently serial. We believe all these techniques will make their way into big data analytics and other massively parallel database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation and analysis of a parallel collection query language

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dan suciu
",y
"LEFT id: NA
RIGHT id: 1583

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at university of hagen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes
",n
"LEFT id: NA
RIGHT id: 1066

LEFT text: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance of future database systems : bottlenecks and bonananzas

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: chaitanya k. baru
",y
"LEFT id: NA
RIGHT id: 2202

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1489

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: carnot and infosleuth : database technology and the world wide web

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: d. woelk , b. bohrer , n. jacobs , k. ong , c. tomlinson , c. unnikrishnan
",n
"LEFT id: NA
RIGHT id: 1306

LEFT text: This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast evaluation techniques for complex similarity queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , michael mlivoncic , hans-j &#246; rg schek , roger weber
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1001

LEFT text: Outlier detection is an integral component of statistical modelling and estimation. For high-dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. The cut-off value is obtained from the asymptotic distribution of the distance, which enables us to control the Type I error and deliver robust outlier detection. Simulation studies show that the proposed method behaves well for high-dimensional data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized view selection for multidimensional datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 2024

LEFT text: In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: panel : querying networked databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nick koudas , divesh srivastava
",y
"LEFT id: NA
RIGHT id: 839

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: active storage hierarchy , database systems and applications - socratic exegesis

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: felipe cari &#241; o , william o'connell , john burgess , joel h. saltz
",n
"LEFT id: NA
RIGHT id: 2106

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query mapping : accounting for translation closeness

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: A data mining system, DBMiner, has been developed for interactive mining of multiple-level knowledge in large relational databases. The system implements a wide spectrum of data mining functions, including generalization, characterization, association, classification, and prediction. By incorporating several interesting data mining techniques, including attribute-oriented induction, statistical analysis, progressive deepening for mining multiple-level knowledge, and meta-rule guided mining, the system provides a user-friendly, interactive data mining environment with good performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 104

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1837

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sr-tree : an index structure for high-dimensional nearest neighbor queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: norio katayama , shin ` ichi satoh
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 1287

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an extendible hash for multi-precision similarity querying of image databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shu lin , m. tamer &#214; zsu , vincent oria , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1929

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of data cubes and summary tables in a warehouse

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: inderpal singh mumick , dallan quass , barinderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 1802

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental distance join algorithms for spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 270

LEFT text: General-purpose commercial disk-based database systems, though widely employed in practice, have failed to meet the performance requirements of applications requiring short, predictable response times, and extremely high throughput rates. Main memory is the only technology capable of these characteristics. DataBlitz is a main-memory storage manager product that supports the development of high-performance and fault-resilient applications requiring concurrent access to shared data. In DataBlitz, core algorithms for concurrency, recovery, index management and space management are optimized for the case that data is memory resident.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storhouse metanoia - new applications for database , storage & ; data warehousing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: felipe cari &#241; o , jr. , pekka kostamaa , art kaufmann , john burgess
",n
"LEFT id: NA
RIGHT id: 1127

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what is the data warehousing problem ? ( are materialized views the answer ? )

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ashish gupta , inderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 2155

LEFT text: The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: large databases for remote sensing and gis

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: a. r. dasgupta
",n
"LEFT id: NA
RIGHT id: 828

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for maintaining replica consistency in lazy master replicated databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: esther pacitti , pascale minet , eric simon
",n
"LEFT id: NA
RIGHT id: 761

LEFT text: 1. MOTIVATION AND SUMMARY Traditional Database Management Systems (DBMS) software is built on the concept of persistent data sets, that are stored reliably in stable storage and queried/updated several times throughout their lifetime. For several emerging application domains, however, data arrives and needs to be processed on a continuous ( ) basis, without the benefit of several passes over a static, persistent data image. Such continuous data streams arise naturally, for example, in the network installations of large Telecom and Internet service providers where detailed usage information (Call-Detail-Records (CDRs), SNMP/RMON packet-flow data, etc.) from different parts of the underlying network needs to be continuously collected and analyzed for interesting trends. Other applications that generate rapid, continuous and large volumes of stream data include transactions in retail chains, ATM and credit card operations in banks, financial tickers, Web server log records, etc. In most such applications, the data stream is actually accumulated and archived in the DBMS of a (perhaps, off-site) data warehouse, often making access to the archived data prohibitively expensive. Further, the ability to make decisions and infer interesting patterns on-line (i.e., as the data stream arrives) is crucial for several mission-critical tasks that can have significant dollar value for a large corporation (e.g., telecom fraud detection). As a result, recent years have witnessed an increasing interest in designing data-processing algorithms that work over continuous data streams, i.e., algorithms that provide results to user queries while looking at the relevant data items only once and in a fixed order (determined by the stream-arrival pattern). Two key parameters for query processing over continuous datastreams are (1) the amount of memory made available to the online algorithm, and (2) the per-item processing time required by the query processor. The former constitutes an important constraint on the design of stream processing algorithms, since in a typical streaming environment, only limited memory resources are available to the query-processing algorithms. In these situations, we need algorithms that can summarize the data stream(s) involved in a concise, but reasonably accurate, synopsis that can be stored in the allotted (small) amount of memory and can be used to provide approximate answers to user queries along with some reasonable guarantees on the quality of the approximation. Such approx-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: querying and mining data streams : you only get one look a tutorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 848

LEFT text: There are two key motivations for this work. First, the implementation of object-oriented databases has grown to a significant number. Second, there has been a need for integrated access of information from multiple data sources. The multidatabase system has been proposed as a solution for integrated access of data from multiple distributed, heterogeneous, and autonomous database systems. To present a single database illusion to its users, a multidatabase system maintains a single global database schema, which is the integration of all component database schemas and against which its users will issue queries and updates. Many approaches to schema integration have been proposed in the literature. Most of the previous approaches are concerned with relational databases. In this paper, we propose an approach to the integration of database schemas between object-oriented databases in a multidatabase system environment. The underlying principle of our approach is to facilitate the automation of the schema integration process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",y
"LEFT id: NA
RIGHT id: 1471

LEFT text: Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules. This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides SQL-like operator, named MINE RULE, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 307

LEFT text: Heavily used in both academic and corporate R&D settings, ACM Transactions on Database Systems (TODS) is a key publication for computer scientists working in data abstraction, data modeling, and designing data management systems. Topics include storage and retrieval, transaction management, distributed and federated databases, semantics of data, intelligent databases, and operations and algorithms relating to these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view selection and maintenance using multi-query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hoshi mistry , prasan roy , s. sudarshan , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 899

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in information managment at dublin city university

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mark roantree , alan f. smeaton
",n
"LEFT id: NA
RIGHT id: 1519

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an effective hash-based algorithm for mining association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jong soo park , ming-syan chen , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1896

LEFT text: With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report from the nsf workshop on workflow and process automation in information systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: amit sheth , dimitrios georgakopoulos , stef m. m. joosten , marek rusinkiewicz , walt scacchi , jack wileden , alexander l. wolf
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1867

LEFT text: Abstract Spatial-Query-by-Sketch is the design of a query language for geographic information systems. It allows a user to formulate a spatial query by drawing the desired configuration with a pen on a touch-sensitive computer screen and translates this sketch into a symbolic representation that can be processed against a geographic database. Since the configurations queried usually do not match exactly the sketch, it is necessary to relax the spatial constraints drawn. This paper describes the representation of a sketch and outlines the design of the constraint relaxation methods used during query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: geominer : a system prototype for spatial data mining

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jaiwei han , krzysztof koperski , nebojsa stefanovic
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 544

LEFT text: Environmental Management Information Systems (EMIS) are socio-technological systems used as business applications to gather, process, and provide environmental information, inside companies and in exchange with other actors in industry. They help to identify environmental impacts and support measures avoiding these impacts or reducing them. EMIS provide the necessary information support for decision making in companies. Hence, EMIS can be viewed as certain Information Systems (IS) usually implemented in companies as a part of their Environmental Management Systems (EMS). In order to give a tangible example with practical implications, the developments that EMIS have passed the last years are described along the field of “online communication and sustainability reporting” and illustrated by a case study. This area represents an emerging digital and fully ICT (information and communication technologies) supported approach within EMIS, using current internet technologies and services. It makes clear the array of capabilities of latest EMIS to be exploited for the improvement of advanced environmental and sustainability management, finally to the benefit for companies and their various stakeholders. The case study describes the concept and implementation of a software tool with shopping cart functionality providing sustainability reports a la carte so that stakeholders (i.e. users, readers) can create their own report on the fly, exactly meeting their detailed information needs and preferred media out from a single publishing database. This software tool which represents a module of a comprehensive EMIS is implemented as a web-based ICT application. Its performance goes beyond the leadingedge approach of O2 who provides a personalized reporting feature on its website that could be regarded as best practice and pioneering effort in sustainability online reporting, so far.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the ecobase project : database and web technologies for environmental information systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luc bouganim , maria claudia cavalcanti , fran &#231; oise fabret , maria luiza campos , fran &#231; ois llirbat , marta mattoso , rubens melo , ana maria moura , esther pacitti , fabio porto , margareth simoes , eric simon , asterio tanaka , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 2199

LEFT text: This paper introduces a new data structure, called V-trees, designed to store long sequences of points in 2D space and yet allow efficient access to their fragments. They also optimire access to a sequence of points when the query involves changes to a smaller scale. V-trees operate in much the same way as positional B-Trees do in the context of long fields and they can be viewed as a variant of Rtrees. The design of V-trees was motivated by the problem of storing and retrieving geographic objects that are fairly long, such as river margins or political boundaries, and the fact that geographic queries typically access just fragments of such objects, frequently using a smaller scale.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: v-trees - a storage method for long vector data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maur &#236; cio r. mediano , marco a. casanova , marcelo dreux
",y
"LEFT id: NA
RIGHT id: 1002

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",n
"LEFT id: NA
RIGHT id: 1187

LEFT text: This paper proposes a system for personalization of web portals. A specic implementation is discussed in reference to a web portal containing a news feed service. Techniques are proposed for effective categorization, management, and personalization of news feeds obtained from a live news wire service. The process consists of two steps: first manual input is required to build the domain knowledge which could be site-specific; then the automated component uses this domain knowledge in order to perform the personalization, categorization and presentation. Effective schemes for advertising are proposed, where the targeting is done using both the information about the user and the content of the web page on which the advertising icon appears. Automated techniques for identifying sudden variations in news patterns are described; these may be used for supporting news-alerts. A description of a version of this software for our customer web site is provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an authorization system for digital libraries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: e. ferrari , n.r. adam , v. atluri , e. bertino , u. capuozzo
",n
"LEFT id: NA
RIGHT id: 441

LEFT text: We present a random walk as an eÆcient and accurate approach to approximating cer-tain aggregate queries about web pages. Our method uses a novel random walk to produce an almost uniformly distributed sample of web pages. The walk traverses a dynamically built regular undirected graph. Queries we have es-timated using this method include the cover-age of search engines, the proportion of pages belonging to.com and other domains, and the average size of web pages. Strong experimen-tal evidence suggests that our walk produces accurate results quickly using very limited re-sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximating multi-dimensional aggregate range queries over real attributes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi
",n
"LEFT id: NA
RIGHT id: 317

LEFT text: Heavily used in both academic and corporate R&D settings, ACM Transactions on Database Systems (TODS) is a key publication for computer scientists working in data abstraction, data modeling, and designing data management systems. Topics include storage and retrieval, transaction management, distributed and federated databases, semantics of data, intelligent databases, and operations and algorithms relating to these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries using materialized views : a practical , scalable solution

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jonathan goldstein , per - &#197; ke larson
",y
"LEFT id: NA
RIGHT id: 903

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a multi-paradigm querying approach for a generic multimedia database management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ji-rong wen , qing li , wei-ying ma , hong-jiang zhang
",n
"LEFT id: NA
RIGHT id: 952

LEFT text: To address this problem, we developed Cache Investment - a novel approach for integrating query optimization and data placement that looks beyond the performance of a single query. Cache Investment sometimes intentionally generates a “suboptimal” plan for a particular query in the interest of effecting a better data placement for subsequent queries. Cache Investment can be integrated into a distributed database system without changing the internals of the query optimizer. In this paper, we propose Cache Investment mechanisms and policies and analyze their performance. The analysis uses results from both an implementation on the SHORE storage manager and a detailed simulation model. Our results show that Cache Investment can significantly improve the overall performance of a system and demonstrate the trade-offs among various alternative policies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 43

LEFT text: Disk-based database systems use buffer managers in order to transparently manage data sets larger than main memory. This traditional approach is effective at minimizing the number of I/O operations, but is also the major source of overhead in comparison with in-memory systems. To avoid this overhead, in-memory database systems therefore abandon buffer management altogether, which makes handling data sets larger than main memory very difficult. In this work, we revisit this fundamental dichotomy and design a novel storage manager that is optimized for modern hardware. Our evaluation, which is based on TPC-C and micro benchmarks, shows that our approach has little overhead in comparison with a pure in-memory system when all data resides in main memory. At the same time, like a traditional buffer manager, it is fully transparent and can manage very large data sets effectively. Furthermore, due to low-overhead synchronization, our implementation is also highly scalable on multi-core CPUs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: in-memory data management for consumer transactions the timesten approach

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: corporate timesten team
",y
"LEFT id: NA
RIGHT id: 1604

LEFT text: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia’s Programming Systems Laboratory (Section 7).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data and knowledge base research at hong kong university of science and technology

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: p. drew , b. hamidzadeh , k. karlapalem , a. kean , d. lee , q. li , f. lochovsky , c. d. shum , b. wuthrich
",n
"LEFT id: NA
RIGHT id: 1229

LEFT text: We propose a novel index structure, termed XTrie, that supports the efficient filtering of XML documents based on XPath expressions. Our XTrie index structure offers several novel features that make it especially attractive for large scale publish/subscribe systems. First, XTrie is designed to support effective filtering based on complex XPath expressions (as opposed to simple, single-path specifications). Second, our XTrie structure and algorithms are designed to support both ordered and unordered matching of XML data. Third, by indexing on sequences of element names organized in a trie structure and using a sophisticated matching algorithm, XTrie is able to both reduce the number of unnecessary index probes as well as avoid redundant matchings, thereby providing extremely efficient filtering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents with xpath expressions

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: c.-y . chan , p. felber , m. garofalakis , r. rastogi
",y
"LEFT id: NA
RIGHT id: 1453

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying shapes of histories

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rakesh agrawal , giuseppe psaila , edward l. wimmers , mohamed za &#239; t
",y
"LEFT id: NA
RIGHT id: 1956

LEFT text: The Prospector Multimedia Object Manager prototype is a general-purpose content analysis multimedia server designed for massively parallel processor environments. Prospector defines and manipulates user defined functions which are invoked in parallel to analyze/manipulate the contents of multimedia objects. Several computationally intensive applications of this technology based on large persistent datasets include: fingerprint matching, signature verification, face recognition, and speech recognition/translation [OIS96].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a teradata content-based multimedia object manager for massively parallel architectures

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: w. o'connell , i. t. ieong , d. schrader , c. watson , g. au , a. biliris , s. choo , p. colin , g. linderman , e. panagos , j. wang , t. walter
",n
"LEFT id: NA
RIGHT id: 1666

LEFT text: SHORE (Scalable Heterogeneous Object REpository) is a persistent object system under development at the University of Wisconsin. SHORE represents a merger of object-oriented database and file system technologies. In this paper we give the goals and motivation for SHORE, and describe how SHORE provides features of both technologies. We also describe some novel aspects of the SHORE architecture, including a symmetric peer-to-peer server architecture, server customization through an extensible value-added server facility, and support for scalability on multiprocessor systems. An initial version of SHORE is already operational, and we expect a release of Version 1 in mid-1994.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ptool : a scalable persistent object manager

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. l. grossman , x. qin
",y
"LEFT id: NA
RIGHT id: 7

LEFT text: Microsoft’s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David’s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch’s vision, and, indeed, in Microsoft’s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch’s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital’s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft’s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the jungle database search engine

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael b &#246; hlen , linas bukauskas , curtis dyreson
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1

LEFT text: New types of data processing applications are no longer satisfied with the capabilities offered by the relational data model. One example of this phenomenon is the growing use of the Internet as a source of data. The data on the Internet is inherently non-relational. As a result, demand developed for database management systems natively built on advanced data models. The semantic binary data model (Rishe, 1992), satisfies the criteria for the models required for today’s applications by providing the ability to build rich schemas with arbitrarily flexible relationships between objects. In this paper, we discuss a new design for a semantic database management system which is based on the semantic binary data model. Our challenge was to design and implement a database engine which, while being native to the model, is reasonably efficient on a wide variety of industrial applications, and which surpasses relational systems in performance and flexibility on those applications that require non-relational modelling. Special attention is given to multi-platform support by the semantic database engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a user-centered interface for querying distributed multimedia databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: isabel f. cruz , kimberly m. james
",n
"LEFT id: NA
RIGHT id: 1249

LEFT text: This paper proposes an extension of the multiversion two phase locking protocol, called EMVZPL, which enables update transactions to use versions while guaranteeing the serializability of all transactions. The use of the protocol is restricted to transactions, called write-then-read transactions that consist of two consecutive parts: a write part containing both read and write operations in some arbitrary order, and an abusively called read part, containing read operations or write operations on data items already locked in the write part of the transaction. With EMVZPL, read operations in the read part use versions and read locks acquired in the write part can be released just before entering the read part. We prove the correctness of our protocol, and show that its implementation requires very few changes to classical implementations of MVZPL. After presenting various methods used by application developers to implement integrity checking, we show how EMV2PL can be effectively used to optimize the processing of update transactions that perform integrity checks. Finally, performance studies show the benefits of our protocol compared to a (strict) two phase locking protocol.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: maintenance of implication integrity constraints under updates to constraints

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: naci s. ishakbeyoglu , z. meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1145

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance of externally materialized views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: martin staudt , matthias jarke
",y
"LEFT id: NA
RIGHT id: 1179

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",y
"LEFT id: NA
RIGHT id: 1521

LEFT text: A key aspect of interoperation among data-intensive systems involves the mediation of metadata and ontologies across database boundaries. One way to achieve such mediation between a local database and a remote database is to fold remote metadata into the local metadata, thereby creating a common platform through which information sharing and exchange becomes possible. Schema implantation and semantic evolution, our approach to the metadata folding problem, is a partial database integration scheme in which remote and local (meta)data are integrated in a stepwise manner over time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 539

LEFT text: This chapter describes an efficient method for maintaining materialized views with non-distributive aggregate functions, even in the presence of super aggregates. Incremental view maintenance is an extremely important aspect of the modern database management systems. It enables the fast execution of complex queries without sacrificing the freshness of the data. However, the maintenance of views defined with non-distributive aggregate functions was not sufficiently explored. Incremental refresh has been studied in depth only for a subset of the aggregate functions. Materialized views, or automatic summary tables (ASTs), are increasingly being used to facilitate the analysis of the large amounts of data being collected in relational databases. The use of ASTs can significantly reduce the execution time of a query, often by orders of magnitude, which is particularly significant for databases with sizes in the terabyte to petabyte range, whose queries are designed by business intelligence tools or decision support systems. Such queries tend to be extremely complex, involving a large number of join and grouping operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1284

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data management for pervasive computing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mitch cherniack , michael j. franklin , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 873

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational model that would make it more appropriate for processing queries over XML documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 901

LEFT text: We provide a concise yet complete formal definition of the semantics of XPath 1 and summarize efficient algorithms for processing queries in this language. Our presentation is intended both for the reader who is looking for a short but comprehensive formal account of XPath as well as the software developer in need of material that facilitates the rapid implementation of XPath engines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xpath processing in a nutshell

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: georg gottlob , christoph koch , reinhard pichler
",y
"LEFT id: NA
RIGHT id: 1758

LEFT text: The paper describes the ARANEUS Wel-Base Management System [l, 5, 4, 61, a system developed at Universitb di Roma Tre, which represents a proposal towards the definition of a new kind of data-repository, designed to manage Web data in the database style. We call a WebBase a collection of data of heterogeneous nature, and more specifically: (i) highly structured data, such as the ones typically stored in relational or objectoriented database systems; (G) semistructured data, in the Web style.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 148

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: flowback : providing backward recovery for workflow management systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bartek kiepuszewski , ralf muhlberger , maria e. orlowska
",n
"LEFT id: NA
RIGHT id: 1007

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: public : a decision tree classifier that integrates building and pruning

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: rajeev rastogi , kyuseok shim
",y
"LEFT id: NA
RIGHT id: 971

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: heterogeneous database query optimization in db2 universal datajoiner

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shivakumar venkataraman , tian zhang
",n
"LEFT id: NA
RIGHT id: 2017

LEFT text: Dual Match, recently proposed as a dual approach of FRM, improves performance significantly over FRM by exploiting point filtering effect. However, it has the problem of having a smaller allowable window size---half that of FRM---given the minimum query length. A smaller window increases false alarms due to window size effect. General Match offers advantages of both methods: it can reduce window size effect by using large windows like FRM and, at the same time, can exploit point-filtering effect like Dual Match. General Match divides data sequences into generalized sliding windows (J-sliding windows) and the query sequence into generalized disjoint windows (J-disjoint windows).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: timber : a native system for querying xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: stelios paparizos , shurug al-khalifa , adriane chapman , h. v. jagadish , laks v. s. lakshmanan , andrew nierman , jignesh m. patel , divesh srivastava , nuwee wiwatwattana , yuqing wu , cong yu
",n
"LEFT id: NA
RIGHT id: 1904

LEFT text: In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: www-udk : a web-based environmental meta-information system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer , ralf nicholai , arne koschel , claudia rolker , peter lockemann , andree keitel , rudolf legat , konrad tirm
",n
"LEFT id: NA
RIGHT id: 1951

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining quantitative association rules in large relational tables

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1274

LEFT text: Efficient user-adaptable similarity search more and more increases in its importance for multimedia and spatial database systems. As a general similarity model for multi-dimensional vectors that is adaptable to application requirements and user preferences, we use quadratic form distance functions which have been successfully applied to color histograms in image databases [Fal+ 94]. The components aij of the matrix A denote similarity of the components i and j of the vectors. Beyond the Euclidean distance which produces spherical query ranges, the similarity distance defines a new query type, the ellipsoid query. We present new algorithms to efficiently support ellipsoid query processing for various user-defined similarity matrices on existing precomputed indexes. By adapting techniques for reducing the dimensionality and employing a multi-step query processing architecture, the method is extended to high-dimensional data spaces. In particular, from our algorithm to reduce the similarity matrix, we obtain the greatest lowerbounding similarity function thus guaranteeing no false drops. We implemented our algorithms in C++ and tested them on an image database containing 12,000 color histograms. The experiments demonstrate the flexibility of our method in conjunction with a high selectivity and efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: view management in multimedia databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , eric lemar , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1857

LEFT text: Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information. Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies. Which strategy is appropriate depends very much on the known interdependencies among the events involved. Previous work on probabilistic databases has assumed a fixed and restrictivecombination strategy (e.g., assuming all events are pairwise independent). In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a genericprobabilistic relational algebra that neatly captures various strategiessatisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0. We validate our complexity results with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probview : a flexible probabilistic database system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , nicola leone , robert ross , v. s. subrahmanian
",y
"LEFT id: NA
RIGHT id: 1862

LEFT text: XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1991

LEFT text: Multiversion access methods have been emerged in the literature primarily to support queries on a transaction-time database where records are never physically deleted. For a popular class of efficient methods (including the multiversion Btree), data records and index entries are occasionally duplicated to separate data according to time. In this paper, we present techniques for improving query processing in multiversion access methods. In particular, we address the problem of avoiding duplicates in the response sets. We first discuss traditional approaches that eliminate duplicates using hashing and sorting. Next, we propose two new algorithms for avoiding duplicates without using additional data structures. The one performs queries in a depth-first order starting from a root, whereas the other exploits links between data pages. These methods are discussed in full details and their main properties are identitied. Preliminary performance results confirm the advantages of these methods in comparison to traditional ones according to CPU-time, disk accesses and storage.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query execution techniques for caching expensive methods

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 897

LEFT text: XQuery is a query language for real and virtual XML documents and collections of these documents. Its development began in the second half of 1999. With roughly 3 years of work completed, it’s high time that we provided an initial description of this language, and a sense of where it is in its development cycle. XQuery is being developed within W3C. Every consortium of this type has its own rules and its own ways of getting its work done. W3C provides visibility to the public by making available drafts of the specifications that it has under development at relatively frequent intervals. Mailing lists are established for each specification to allow the public to provide feedback on these drafts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an early look at xquery

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: andrew eisenberg , jim melton
",y
"LEFT id: NA
RIGHT id: 1272

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: malcolm p. atkinson
",n
"LEFT id: NA
RIGHT id: 1216

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo atzeni , alberto o. mendelzon
",n
"LEFT id: NA
RIGHT id: 1442

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coloring away communication in parallel query optimization

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: waqar hasan , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: To truly meet the requirements of multimedia database (MMDB) management, an integrated framework for modeling, managing and retrieving various kinds of media data in a uniform way is necessary. MediaLand is an experimental MMDB platform being developed at Microsoft Research Asia for users with different levels of experiences and expertise to manage and search multimedia repositories easily, efficiently, and cooperatively. Key features of MediaLand include a uniform data model for describing all kinds of media objects and their relationships, and a 4-tier architecture based on this data model. In this paper, a multi-paradigm querying approach of MediaLand is presented, in which multimedia queries are processed based on a seamless integration of various existing search approaches. In doing so, MediaLand also offers the feature of ""media independence"" which is analogous to the notion of ""data independence"" from the classic ANSI SPARC standard. By incorporating a rich set of facilities and techniques, MediaLand lays down a good foundation for addressing further research issues, such as multimedia query rewriting, optimization, and presentation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 462

LEFT text: The duplicate elimination problem of detecting multiple tuples, which describe the same real world entity, is an important data cleaning problem. Previous domain independent solutions to this problem relied on standard textual similarity functions (e.g., edit distance, cosine metric) between multi-attribute tuples. However, such approaches result in large numbers of false positives if we want to identify domain-specific abbreviations and conventions. In this paper, we develop an algorithm for eliminating duplicates in dimensional tables in a data warehouse, which are usually associated with hierarchies. We exploit hierarchies to develop a high quality, scalable duplicate elimination algorithm, and evaluate it on real datasets from an operational data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: knowledge discovery in data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: themistoklis palpanas
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 334

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1050

LEFT text: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: reordering query execution in tertiary memory databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sunita sarawagi , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 1399

LEFT text: Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching strategies for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: khaled yagoub , daniela florescu , val &#233; rie issarny , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1212

LEFT text: Abstract. In this paper we describe the design and implementation of ParSets, a means of exploiting parallelism in the SHORE OODBMS. We used ParSets to parallelize the graph traversal portion of the OO7 OODBMS benchmark, and present speedup and scaleup results from parallel SHORE running these traversals on a cluster of commodity workstations connected by a standard ethernet. For some OO7 traversals, SHORE achieved excellent speedup and scaleup; for other OO7 traversals, only marginal speedup and scaleup occurred. The characteristics of these traversals shed light on when the ParSet approach to parallelism can and cannot be applied to speed up an application. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parallelizing oodbms traversals : a performance evaluation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david j. de witt , jeffrey f. naughton , john c. shafer , shivakumar venkataraman
",y
"LEFT id: NA
RIGHT id: 1802

LEFT text: For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental distance join algorithms for spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1762

LEFT text: We present here a new vertical mining algorithm called VIPER, which is general-purpose, making no special requirements of the underlying database. VIPER stores data in compressed bit-vectors called “snakes” and integrates a number of novel optimizations for efficient snake generation, intersection, counting and storage. We analyze the performance of VIPER for a range of synthetic database workloads. Our experimental results indicate significant performance gains, especially for large databases, over previously proposed vertical and horizontal mining algorithms. In fact, there are even workload regions where VIPER outperforms an optimal, but practically infeasible, horizontal mining algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards on-line analytical mining in large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jiawei han
",n
"LEFT id: NA
RIGHT id: 480

LEFT text: EFIS 2000 was held at Dublin City University in June 2000. The principal aim of this third workshop was to bring together new insights from academic research with industry-driven developments and perspectives in the area of federated information systems. This report describes the observations of the workshop together with the outcome and future research possibilities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 1847

LEFT text: We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits ""horizontal"" aggregation and even aggregation over more general ""blocks"" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: languages for multi-database interoperability

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri
",n
"LEFT id: NA
RIGHT id: 1462

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 1401

LEFT text: As XML seems to become the preferred candidate language for the interchange of data on the Internet, the integration of distributed, heterogeneous, and autonomous XML data sources in a mediation architecture is becoming a critical issue. In this paper, we present a novel and original query rewriting algorithm for the answering of queries to XML disparate sources in the presence of XML keys. The algorithm combines features of the MiniCon (Mini-Con descriptions) and the Styx algorithms (prefix and suffix queries) into an algorithm that returns more rewritings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query translation across heterogeneous information sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 1878

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: open issues in parallel query optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: waqar hasan , daniela florescu , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1112

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient search of multi-dimensional b-trees

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: harry leslie , rohit jain , dave birdsall , hedieh yaghmai
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 989

LEFT text: The duplicate elimination problem of detecting multiple tuples, which describe the same real world entity, is an important data cleaning problem. Previous domain independent solutions to this problem relied on standard textual similarity functions (e.g., edit distance, cosine metric) between multi-attribute tuples. However, such approaches result in large numbers of false positives if we want to identify domain-specific abbreviations and conventions. In this paper, we develop an algorithm for eliminating duplicates in dimensional tables in a data warehouse, which are usually associated with hierarchies. We exploit hierarchies to develop a high quality, scalable duplicate elimination algorithm, and evaluate it on real datasets from an operational data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: issues in developing very large data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: lyman do , pamela drew , wei jin , vish jumani , david van rossum
",n
"LEFT id: NA
RIGHT id: 1979

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: improved histograms for selectivity estimation of range predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: viswanath poosala , peter j. haas , yannis e. ioannidis , eugene j. shekita
",n
"LEFT id: NA
RIGHT id: 1587

LEFT text: In this paper, we explore the execution of pipelined hash joins in a multiprocessor-based database system. To improve the query execution, an innovative approach on query execution tree selection is proposed to exploit segmented right-deep trees, which are bushy trees of right-deep subtrees. We first derive an analytical model for the execution of a pipeline segment, and then, in light of the model, develop heuristic schemes to determine the query execution plan based on a segmented right-deep tree so that the query can be efficiently executed. As shown by our simulation, the proposed approach, without incurring additional overhead on plan execution, possesses more flexibility in query plan generation, and leads to query plans of significantly better performance than those achievable by the previous schemes using right-deep trees.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel evaluation of multi-join queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: annita n. wilschut , jan flokstra , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 2174

LEFT text: This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: path sharing and predicate evaluation for high-performance xml filtering

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: yanlei diao , mehmet altinel , michael j. franklin , hao zhang , peter fischer
",n
"LEFT id: NA
RIGHT id: 932

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: groupwise processing of relational queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1140

LEFT text: 1 Introduction to SQL-92 2 Getting Started with SQL-92 3 Basic Table Creation and Data Manipulation 4 Basic Data Definition Language (DDL) 5 Values, Basic Functions, and Expressions 6 Advanced Value Expressions: CASE, CAST, and Row Value Expressions 7 Predicates 8 Working with Multiple Tables: The Relational Operators 9 Advanced SQL Query Expressions 10 Constraints, Assertions, and Referential Integrity 11 Accessing SQL from the Real World 12 Cursors 13 Privileges, Users, and Security 14 Transaction Management 15 Connections and Remote Database Access 16 DYNAMIC SQL 17 Diagnostics and Error Management 18 Internationalization Aspects of SQL-92 19 Information Schema 20 A Look to the Future A Designing SQL-92 Databases B A Complete SQL-92 Example C The SQL-92 Annexes: Differences, Implementation-Defined and Implementation-Dependent Features, Deprecated Features, and Leveling D Relevant Standards Bodies E Status Codes F The SQL Standardization Process G The Complete SQL-92 Language

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting procedural constructs in sql compilers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: nelson mendon &#231; a mattos
",y
"LEFT id: NA
RIGHT id: 1773

LEFT text: Market research companies predict a huge market for services to be delivered to mobile users. Services include route guidance, point-of-interest search, metering services such as road pricing and parking payment, traffic monitoring, etc. We believe that no single such service will be the killer service, but that suites of integrated services are called for. Such integrated services reuse integrated content obtained from multiple content providers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1699

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a hypertext query language for images

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: li yang
",n
"LEFT id: NA
RIGHT id: 1374

LEFT text: Central to any XML query language is a path language such as XPath which operates on the tree structure of the XML document. We demonstrate in this paper that the tree structure can be effectively compressed and manipulated using techniques derived from symbolic model checking. Specifically, we show first that succinct representations of document tree structures based on sharing subtrees are highly effective. Second, we show that compressed structures can be queried directly and efficiently through a process of manipulating selections of nodes and partial decompression. We study both the theoretical and experimental properties of this technique and provide algorithms for querying our compressed instances using node-selecting path query languages such as XPath.    We believe the ability to store and manipulate large portions of the structure of very large XML documents in main memory is crucial to the development of efficient, scalable native XML databases and query engines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries on compressed bitmaps

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sihem amer-yahia , theodore johnson
",n
"LEFT id: NA
RIGHT id: 1144

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing techniques for multiversion access methods

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1506

LEFT text: A well-known challenge in data warehousing is the efficient incremental maintenance of warehouse data in the presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype data warehousing system supporting incremental maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 2192

LEFT text: On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe GLUE, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology GLUE finds the most similar concept in the other ontology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: composing web services on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brahim medjahed , athman bouguettaya , ahmed k. elmagarmid
",n
"LEFT id: NA
RIGHT id: 1548

LEFT text: Corporations worldwide are finding that understanding and managing rapidly growing, enterprisewide data is critical for making timely decisions and responding to changing business conditions. To manage and use business information competitively, many companies are establishing decision support systems built around a data warehouse of subject-oriented, integrated, historical information. In order to understand why the data warehouse must replace old legacy applications for effective information processing, it is necessary to understand the root causes of the difficulty in getting information in the first place. The first difficulty in getting information from the base of old applications is that those old applications were shaped around business requirements that were relevant as much as twenty-five years ago. These applications that were shaped yesterday do not reflect today’s business. The second reason why older applications are so hard to use as a basis for information is that those applications were shaped around the clerical needs of the corporation. A clerically focused application of necessity does not have the historical foundation required to support a long-term view. Another reason why the clerical perspective of applications does not support management’s need for information is that the clerical community focuses on detailed data. While detailed data is tine for the day-to-day clerical needs of the organization, management needs to see summary data in order to identify trends, challenges and opportunities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data extraction and transformation for the data warehouse

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: case squire
",y
"LEFT id: NA
RIGHT id: 1900

LEFT text: Water has an outstanding importance for the life on earth. From this results the necessity for the monitoring and interpretation of mari ne data. For that, the visual analysis is a suitable and effective tool, whereby special demands arise from the heterogeneity of data (different data type s, different data sources), the quality of data (missing values, incorrect values ), and the large quantity of data. The visualization of marine data is particularly import ant within both their geographic context and their temporal course. First, this paper introduces a classification for the visualization of spatial and ti me related data, which is not only appropriate for marine data. Following special visual ization and interaction techniques for marine data are discussed. Thereby we do not raise the claim, to create new visualization paradigms. Rather we want to show solution concepts and special methods using well known paradigms for a special and complex application area, but also to address limits of the visualization paradigms in these applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: pixel-oriented database visualizations

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 246

LEFT text: DISIMA (Distributed Image Database Management System) is a research project under development at the University of Alberta. DISIMA implements a database approach to developing an image database system. Image contents are modeled using objectoriented paradigms while a declarative query language and a corresponding visual query language allow queries over syntactic and semantic features of images. The distributed and interoperable architecture is designed using common facilities as defined in the Object Management Architecture (OMA).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac , ugur halici , ebru kilic , gokhan ozhan , fatma ozcan , sena nural , cevdet dengi , sema mancuhan , budak arpinar , pinar koksal , cem evrendilek
",n
"LEFT id: NA
RIGHT id: 544

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the ecobase project : database and web technologies for environmental information systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luc bouganim , maria claudia cavalcanti , fran &#231; oise fabret , maria luiza campos , fran &#231; ois llirbat , marta mattoso , rubens melo , ana maria moura , esther pacitti , fabio porto , margareth simoes , eric simon , asterio tanaka , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1502

LEFT text: Recent technological advances have made multimedia on-demand servers feasible. Two challenging tasks in such systems are: a) satisfying the real-time requirement for continuous delivery of objects at specified bandwidths and b) efficiently servicing multiple clients simultaneously. To accomplish these tasks and realize economies of scale associated with servicing a large user population, the multimedia server can require a large disk subsystem. Although a single disk is fairly reliable, a large disk farm can have an unacceptably high probability of disk failure. Further, due to the real-time constraint, the reliability and availability requirements of multimedia systems are very stringent. In this paper we investigate techniques for providing a high degree of reliability and availability, at low disk storage, bandwidth, and memory costs for on-demand multimedia servers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fault tolerant design of multimedia servers

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: steven berson , leana golubchik , richard r. muntz
",y
"LEFT id: NA
RIGHT id: 1889

LEFT text: Water has an outstanding importance for the life on earth. From this results the necessity for the monitoring and interpretation of mari ne data. For that, the visual analysis is a suitable and effective tool, whereby special demands arise from the heterogeneity of data (different data type s, different data sources), the quality of data (missing values, incorrect values ), and the large quantity of data. The visualization of marine data is particularly import ant within both their geographic context and their temporal course. First, this paper introduces a classification for the visualization of spatial and ti me related data, which is not only appropriate for marine data. Following special visual ization and interaction techniques for marine data are discussed. Thereby we do not raise the claim, to create new visualization paradigms. Rather we want to show solution concepts and special methods using well known paradigms for a special and complex application area, but also to address limits of the visualization paradigms in these applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tiziana catarci , isabel f. cruz
",y
"LEFT id: NA
RIGHT id: 548

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: continuous queries over data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shivnath babu , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1016

LEFT text: An electronic dictionary system (EDS) is developed with object-oriented database techniques based on ObjectStore. The EDS is composed of two parts: the Database Building Program (DBP), and the Database Querying Program (DQP). DBP reads in a dictionary encoded in SGML tags, and builds a database composed of a collection of trees which holds dictionary entries, and several lists which contain items of various lexical categories. With text exchangeability introduced by the SGML, DBP is able to accommodate dictionaries of different languages with different structures, after easy modification of a configuration file. The tree model, the Category Lists, and an optimization procedure enables DQP to quickly accomplish complicated queries, including context requirements, via simple SQL-like syntax and straightforward search methods. Results show that compared with relational database, DQP enjoys much higher speed and flexibility.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bulk-loading techniques for object databases and an application to relational data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sophie cluet , claude delobel
",n
"LEFT id: NA
RIGHT id: 1748

LEFT text: Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extracting entity profiles from semistructured information spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: robert a. nado , scott b. huffman
",n
"LEFT id: NA
RIGHT id: 745

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 1130

LEFT text: We propose and evaluate two indexing schemes for improving the efficiency of data retrieval in high-dimensional databases that are incomplete. These schemes are novel in that the search keys may contain missing attribute values. The first is a multi-dimensional index structure, called the Bitstring-augmented R-tree (BR-tree), whereas the second comprises a family of multiple one-dimensional one-attribute (MOSAIC) indexes. Our results show that both schemes can be superior over exhaustive search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",n
"LEFT id: NA
RIGHT id: 212

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: belief reasoning in mls deductive databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: hasan m. jamil
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 295

LEFT text: In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: space-efficient online computation of quantile summaries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: michael greenwald , sanjeev khanna
",n
"LEFT id: NA
RIGHT id: 1958

LEFT text: Businessestoday need to interrelate data stored in diverse systems with differing capabilities, ideally via a single high-level query interface. We present the design of a query optimizer for Garlic [C 95], a middleware system designedto integrate data from a broad range of data sources with very different query capabilities. Garlic’s optimizer extends the rule-based approach of [Loh88] to work in a heterogeneous environment, by defining generic rules for the middleware and using wrapper-provided rules to encapsulate the capabilities of each data source. This approach offers great advantages in terms of plan quality, extensibility to new sources, incremental implementationof rules for new sources, and the ability to express the capabilities of a diverse set of sources. We describe the design and implementationof this optimizer, and illustrate its actions through an example.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries over multimedia repositories

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 698

LEFT text: To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: apex : an adaptive path index for xml data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: chin-wan chung , jun-ki min , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 810

LEFT text: Computing multiple related group-bys and aggregates is one of the core operations of On-Line Analytical Processing (OLAP) applications. Recently, Gray et al. [GBLP95] proposed the “Cube” operator, which computes group-by aggregations over all possible subsets of the specified dimensions. The rapid acceptance of the importance of this operator has led to a variant of the Cube being proposed for the SQL standard. Several efficient algorithms for Relational OLAP (ROLAP) have been developed to compute the Cube. However, to our knowledge there is nothing in the literature on how to compute the Cube for Multidimensional OLAP (MOLAP) systems, which store their data in sparse arrays rather than in tables. In this paper, we present a MOLAP algorithm to compute the Cube, and compare it to a leading ROLAP algorithm. The comparison between the two is interesting, since although they are computing the same function, one is value-based (the ROLAP algorithm) whereas the other is position-based (the MOLAP algorithm). Our tests show that, given appropriate compression techniques, the MOLAP algorithm is significantly faster than the ROLAP algorithm. In fact, the difference is so pronounced that this MOLAP algorithm may be useful for ROLAP systems as well as MOLAP systems, since in many cases, instead of cubing a table directly, it is faster to first convert the table to an array, cube the array, then convert the result back to a table.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1250

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data partitioning and load balancing in parallel disk systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter scheuermann , gerhard weikum , peter zabback
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 414

LEFT text: Data extraction from HTML pages is performed by software modules, usually called wrappers. Roughly speaking, a wrapper identifies and extracts relevant pieces of text inside a web page, and reorganizes them in a more structured format. In the literature there is a number of systems to (semi-)automatically generate wrappers for HTML pages. We have recently investigated for original approaches that aims at pushing further the level of automation of the wrapper generation process. Our main intuition is that, in a dataintensive web site, pages can be classified in a small number of classes, such that pages belonging to the same class share a rather tight structure. Based on this observation, we have studied an novel technique, we call the matching technique, that automatically generates a common wrapper by exploiting similarities and differences among pages of the same class. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: homer : a model-based case tool for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo merialdo , paolo atzeni , marco magnante , giansalvatore mecca , marco pecorone
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1609

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1274

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: view management in multimedia databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , eric lemar , v. s. subrahmanian
",y
"LEFT id: NA
RIGHT id: 1376

LEFT text: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating the ub-tree into a database system kernel

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: frank ramsak , volker markl , robert fenk , martin zirkel , klaus elhardt , rudolf bayer
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: In this column, we review these three books.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: Both databases and knowledge bases are used to represent the relevant parts of an application domain, and to allow convenient access to the stored information. Research in knowledge representation (KR) originally concentrated on expressive formalisms with sophisticated reasoning services, usually under the assumption that the size of the knowledge base (KB) was relatively small. In contrast, database (DB) research was concerned with e ciently storing, retrieving, and sharing large amounts of simple data, but the languages for describing schema informationwere rather simple, and reasoning about the schema played only a minor role.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 462

LEFT text: A data warehouse is a redundant collection of data replicated from several possibly distributed and loosely coupled source databases, organized to answer OLAP queries. Relational views are used both as a specification technique and as an execution plan for the derivation of the warehouse data. In this position paper, we summarize the versatility of relational views and their potential.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: knowledge discovery in data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: themistoklis palpanas
",n
"LEFT id: NA
RIGHT id: 59

LEFT text: Semantic Web Enabled Web Services (SWWS) will transform the web from a static collection of information into a distributed device of computation on the basis of Semantic technology making content within the World Wide Web machine-processable and machine-interpretable. Semantic Web Enabled Web Services will allow the automatic discovery, selection and execution of inter-organization business logic making areas like dynamic supply chain composition a reality. In this paper we introduce the vision of Semantic Web Enabled Web Services, describe requirements for building semantics-driven web services and sketch a first draft of conceptual architecture for implementing semantic web enabled web services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a layered architecture for querying dynamic web content

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: hasan davulcu , juliana freire , michael kifer , i. v. ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1301

LEFT text: Efficient algorithms for incrementally computing nested query expressions do not exist. Nested query expressions are query expressions in which selection/join predicates contain subqueries. In order to respond to this problem, we propose a two-step strategy for incrementaly computing nested query expressions. In step (1), the query expression is transformed into an equivalent unnested flat query expression. In step (2), the flat query expression is incrementally computed. To support step (1), we have developed a very concise algebra-to-algebra transformation algorithm, and we have formally proved its correctness. The flat query expressions resulting from the transformation make intensive use of the relational set-difference operator. To support step (2), we present and analyze an efficient algorithm for incrementally computing set differences based on view pointer caches. When combined with existing incremental algorithms for SPJ queries, our incremental set-difference algorithm can be used to compute the unnested flat query expressions efficiently. It is important to notice that without our incremental set-difference algorithm the existing incremental algorithms for SPJ queries are useless for any query involving the set-difference operator, including queries that are not the result of unnesting nested queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on completeness of historical relational query languages

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: james clifford , albert croker , alexander tuzhilin
",n
"LEFT id: NA
RIGHT id: 590

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1843

LEFT text: Previous research has demonstrated that space can be saved in a MultiMedia DataBase Management System (MMDBMS) by storing some of the data items virtually, meaning they are stored as sequences of editing operations. The existing approaches for performing ContentBased Retrieval (CBR) in an MMDBMS, however, typically assume that the data items are stored as large, binary objects. The result is that an MMDBMS cannot use the existing approaches without losing the space savings gained by storing the data items virtually. In our demo, we present a prototype CBR system for virtual images that avoids this problem by using the semantic information in the editing operations during retrieval.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semcog : an object-based image retrieval system and its visual query interface

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wen-syan li , k. sel &#231; uk candan , kyoji hirata , yoshinori hara
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1656

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: multigranularity locking in multiple job classes transaction processing system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: shan-hoi ng , sheung-lun hung
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 883

LEFT text: In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents for selective dissemination of information

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mehmet altinel , michael j. franklin
",n
"LEFT id: NA
RIGHT id: 974

LEFT text: The buffer manager is integral to the performance, scalability, and reliability of Oracle’s Universal Dam Server, a high performance object-relational database manager that provides robust data-management services for a variety of applications and tools. The rich functionality of the Universal Data Server poses special challenges to the design of the buffer manager. Buffer management algorithms must be scalable and efficient across a broad spectrum of OLTP, decision support, and multimedia workloads which impose very different concurrency, throughput and bandwidth requirements. The need for portability across a wide range of platforms further complicates buffer management; the database server must run efficiently with buffer pool sizes ranging from 50 buffers to several million buffers and on a wide variety of architectures including uniprocessors, shared-disk clusters, messagepassing MPP systems, and shared-memory muhiprocessors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: architecture of oracle parallel server

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roger bamford , d. butler , boris klots , n. macnaughton
",n
"LEFT id: NA
RIGHT id: 1162

LEFT text: Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIRL is much faster than naive inference methods, even for short queries. We also show that inferences made by WHIRL are surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outperforming exact matching with a plausible global domain on a second.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: graphical interaction with heterogeneous databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: t. catarci , g. santucci , j. cardiff
",n
"LEFT id: NA
RIGHT id: 840

LEFT text: This paper reports on the principles underlying the semantic and pedagogic interoperability mechanisms built in the European Knowledge Pool System, developed by the European research project ARIADNE. This system, which is the central feature of ARIADNE, consists in a distributed repository of pedagogical documents (or learning objects) of diverse granularity, origin, content, type, language, etc., which are stored in view of their use (and reuse) in telematics-based training or teaching curricula.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what can hierarchies do for data warehouses ?

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , laks v. s. lakshmanan , divesh srivastava
",y
"LEFT id: NA
RIGHT id: 964

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast high-dimensional data search in incomplete databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: beng chin ooi , cheng hian goh , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1055

LEFT text: This paper describes a model that integrates the execution of triggers with the evaluation of declarative constraints in SQL database systems. This model achieves full compatibility with the 1992 international standard for SQL (SQL92). It preserves the set semantics for declarative constraint evaluation while allowing the execution of powerful procedural triggers. It was implemented in DB2 for common servers and was recently accepted as the model for the emerging SQL standard (SQW).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the role of integrity constraints in database interoperation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark w. w. vermeer , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 619

LEFT text: In this column, we review these three books.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1697

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constructing the next 100 database management systems : like the handyman or like the engineer ?

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: andreas geppert , klaus r. dittrich
",n
"LEFT id: NA
RIGHT id: 990

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1032

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 2027

LEFT text:  In addition it constructs a special node Authors() and connects it to all pages corresponding to ""Author""s. The output graph is called SiteGraph. One way to write this in StruQL is: input DataGraph where Root(x); x ! ! y; y ! l ! z; l in f""Paper"", ""TechReport"", ""Title"", ""Abstract"", ""Author""g create Authors(); Page(y); Page(z) link Page(y) ! l ! Page(z) where x ! ! y1; y1 ! ""Author"" ! z1 link Authors() ! ""Author"" ! Page(z1) output SiteGraph 2 In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph. Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together: input DataGraph where Root

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xquery : a query language for xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: don chamberlin
",n
"LEFT id: NA
RIGHT id: 258

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: communication-efficient distributed mining of association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: assaf schuster , ran wolff
",n
"LEFT id: NA
RIGHT id: 89

LEFT text: We propose a multi-resolution transmission mechanism that allows various organizational units of a web document to be transferred and browsed according to the amount of information captured. We define the notion of information content for each individual organizational unit of a web document as an indication of its captured information. The concept of information content is used as a foundation for defining the notion of relative information content which determines the transmission order of various units. Our mechanism allows a web client to explore the more content-bearing portion of a web document earlier so as to be able to terminate browsing a possibly irrelevant document sooner. This scheme is based on our observation that different organizational units of a document contribute to different amount of information to the document. Such a multi-resolution transmission paradigm is particularly useful in mobile web where the wireless bandwidth is a scarce resource and browsing every document in detail would consume the bandwidth unnecessarily. This is becoming more serious when the size of a web document is getting large, such as technical documents. We then present a prototype of the system in Java and CORBA to illustrate its feasibility.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on mutli-resolution document transmission in mobile web

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stanley m. t. yau , hong va leong , dennis mcleod , antonio si
",y
"LEFT id: NA
RIGHT id: 2241

LEFT text: Computers running database management applications often manage large amounts of data. Typically, the price of the I/O subsystem is a considerable portion of the computing hardware. Fierce price competition demands every possible savings. Lossless data compression methods, when appropriately integrated with the dbms, yield signiflcant savings. Roughly speaking, a slight increase in cpu cycles is more than offset by savings in I/O subsystem. Various design issues arise in the use of data compression in the dbms from the choice of algorithm, statistics collection, hardware versus software based compression, location of the compression function in the overall computer system architecture, unit of compression, update in place, and the application of log’ to compressed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data compression support in databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: balakrishna r. iyer , david wilhite
",y
"LEFT id: NA
RIGHT id: 1232

LEFT text: XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: timber : a native xml database

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: h. v. jagadish , s. al-khalifa , a. chapman , l. v. s. lakshmanan , a. nierman , s. paparizos , j. m. patel , d. srivastava , n. wiwatwattana , y. wu , c. yu
",n
"LEFT id: NA
RIGHT id: 655

LEFT text: The Brazilian Symposium on Database Systems (SBBD) is a traditional conference in Brazil, and is sponsored by the Brazilian Computer Society. SBBD's technical program contemplates the following activities: presentation of peer reviewed full technical papers, invited talks, tutorials (either invited and selected from submissions), discussion panels and presentation of tools.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 18th british national conference on databases ( bncod )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: carole goble , brian read
",n
"LEFT id: NA
RIGHT id: 1883

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: Queries on XML documents typically combine selections on element contents, and, via path expressions, the structural relationships between tagged elements. Structural joins are used to find all pairs of elements satisfying the primitive structural relationships specified in the query, namely, parent-child and ancestor-descendant relationships. Efficient support for structural joins is thus the key to efficient implementations of XML queries. Recently proposed node numbering schemes enable the capturing of the XML document structure using traditional indices (such as B+-trees or R-trees). This paper proposes efficient structural join algorithms in the presence of tag indices. We first concentrate on using B+- trees and show how to expedite a structural join by avoiding collections of elements that do not participate in the join. We then introduce an enhancement (based on sibling pointers) that further improves performance. Such sibling pointers are easily implemented and dynamically maintainable. We also present a structural join algorithm that utilizes R-trees. An extensive experimental comparison shows that the B+-tree structural joins are more robust. Furthermore, they provide drastic improvement gains over the current state of the art.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: We present an information retrieval system that simultaneously allows to search for text and speech documents. The retrieval system accepts vague queries and performs a best-match search to find those documents that are relevant to the query. The output of the retrieval system is a list of ranked documents where the documents on the top of the list satisfy best the user's information need. The relevance of the documents is estimated by means of metadata (document description vectors). The metadata is automatically generated and it is organized such that queries can be processed efficiently. We introduce a controlled indexing vocabulary for both speech and text documents. The size of the new indexing vocabulary is small (1000 features) compared with the sizes of indexing vocabularies of conventional text retrieval (10000 - 100000 features). We show that the retrieval effectiveness based on such a small indexing vocabulary is similar to the retrieval effectiveness of a Boolean retrieval system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",n
"LEFT id: NA
RIGHT id: 2037

LEFT text: In this paper, we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. Our estimation algorithms are probabilistic in nature and rely on a novel, hash-based synopsis data structure, termed ""2-level hash sketch. We demonstrate how our 2-level hash sketch synopses can be used to provide low-error, high-confidence estimates for the cardinality of set expressions (including operators such as set union, intersection, and difference) over continuous update streams, using only small space and small processing time per update. Furthermore, our estimators never require rescanning or resampling of past stream items, regardless of the number of deletions in the stream. We also present lower bounds for the problem, demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. Preliminary experimental results verify the effectiveness of our approach

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing set expressions over continuous update streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sumit ganguly , minos garofalakis , rajeev rastogi
",y
"LEFT id: NA
RIGHT id: 1650

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1904

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: www-udk : a web-based environmental meta-information system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer , ralf nicholai , arne koschel , claudia rolker , peter lockemann , andree keitel , rudolf legat , konrad tirm
",n
"LEFT id: NA
RIGHT id: 1329

LEFT text: Traditional statistical methods deal with corroborating given hypotheses on a given body of data. However, generating the hypothesis itself is a matter of intuition and ingenuity. It is clearly impossible to test all hypotheses on a database with millions of records and hundreds of elds. There have been attempts to bridge this gap through data mining. Association generation is a method of creating such statistical hypotheses for binary data. For quantitative databases the situation is still not good. There are a number of known methods. One is a reduction to binary data by creating intervals and then generating associations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: analyzing quantitative databases : image is everything

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amihood amir , reuven kashi , nathan s. netanyahu
",y
"LEFT id: NA
RIGHT id: 236

LEFT text: Many interesting examples in view maintenance involve semijoin and outerjoin queries. In this paper we develop algebraic change propagation algorithms for the following operators: semijoin, anti-semijoin, left outerjoin, right outerjoin, and full outerjoin.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: algebraic change propagation for semijoin and outerjoin queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: timothy griffin , bharat kumar
",y
"LEFT id: NA
RIGHT id: 1494

LEFT text: Extended transaction models have drawn much interest recently in academia and industry [2]. Such models seek to address the limitations of traditional ACID transactions for supporting multisystem applications that operate in heterogeneous environments. Such applications are increasingly proving to be of strategic importance to a number of businesses and governmental agencies. Different transaction models, however, tend to be closed in that they cannot be easily combined with other such models, thus limiting their applicability to situations which exactly match one of them. We do not propose yet another transaction model. Instead, we have developed a general specification facility that enables the formalization of any transaction model that can be stated in terms of dependencies amongst significant events in different subtransactions. Such significant events include start, commit, and abort. We make no assumptions that these are the only kinds of events. Our approach is viable because most extended transaction models can be naturally formalized in terms of dependencies among different subtransactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: directv and oracle rdb : the challenge of vldb transaction processing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: william l. gettys
",n
"LEFT id: NA
RIGHT id: 303

LEFT text: Many emerging application domains require database systems to support efficient access over highly multidimensional datasets. The current state-of-the-art technique to indexing high dimensional data is to first reduce the dimensionality of the data using Principal Component Analysis and then indexing the reduceddimensionality space using a multidimensional index structure. The above technique, referred to as global dimensionality reduction (GDR), works well when the data set is globally correlated, i.e. most of the variation in the data can be captured by a few dimensions. In practice, datasets are often not globally correlated. In such cases, reducing the data dimensionality using GDR causes significant loss of distance information resulting in a large number of false positives and hence a high query cost. Even when a global correlation does not exist, there may exist subsets of data that are locally correlated. In this paper, we propose a technique called Local Dimensionality Reduction (LDR) that tries to find local correlations in the data and performs dimensionality reduction on the locally correlated clusters of data individually. We develop an index structure that exploits the correlated clusters to efficiently support point, range and k-nearest neighbor queries over high dimensional datasets. Our experiments on synthetic as well as real-life datasets show that our technique (1) reduces the dimensionality of the data with significantly lower loss in distance information compared to GDR and (2) significantly outperforms the GDR, original space indexing and linear scan techniques in terms of the query cost for both synthetic and real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: eamonn keogh , kaushik chakrabarti , michael pazzani , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: . In more deteil, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We develop new evaluation strategies essential to obtaining good performance, including a stack-based TermJoin algorithm for efficiently scoring composite elements. We report results from an extensive experimental evaluation, which show, among other things, that the new TermJoin access method outperforms a direct implementation of the same functionality using standard operators by a large factor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1055

LEFT text: Integrity constraint checking for stratifiable deductive databases has been studied by many authors. However, most of these methods may perform unnecessary checking if the update is irrelevant to the constraints. [Lee94] proposed a set called relevant set which can be incorporated in these works to reduce unnecessary checking. [Lee94] adopts a top-down approach and makes use of constants and evaluable functions in the constraints and deductive rules to reduce the search space. In this paper, we further extend this idea to make use of relational predicates, instead of only constants and evaluable functions in [Lee94]. We first show that this extension is not a trivial one as extra database retrieval cost is incurred. We then present a new method to construct a pre-test which can be incorporated in most existing methods to reduce the average checking costs in terms of database accesses by a significant factor. Our method also differs from other partial checking methods as we can handle multiple updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the role of integrity constraints in database interoperation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark w. w. vermeer , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 722

LEFT text: While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: web caching for database applications with oracle web cache

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jesse anton , lawrence jacobs , xiang liu , jordan parker , zheng zeng , tie zhong
",n
"LEFT id: NA
RIGHT id: 680

LEFT text: Publisher Summary  This chapter presents the first XPath query evaluation algorithm that runs in polynomial time with respect to the size of both the data and of the query. XPath has been proposed by the W3C as a practical language for selecting nodes from XML document trees. XPath is important because of its potential application as an XML query language per se, it being at the core of several other XML-related technologies, such as XSLT, XPointer, and XQuery, and the great and well-deserved interest such technologies receive. Since XPath and related technologies will be tested in ever-growing deployment scenarios, its implementations need to scale well both with respect to the size of the XML data and the growing size and intricacy of the queries (usually referred to as combined complexity).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for minimizing tree pattern queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: prakash ramanan
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: A data mining system, DBMiner, has been developed for interactive mining of multiple-level knowledge in large relational databases. The system implements a wide spectrum of data mining functions, including generalization, characterization, association, classification, and prediction. By incorporating several interesting data mining techniques, including attribute-oriented induction, statistical analysis, progressive deepening for mining multiple-level knowledge, and meta-rule guided mining, the system provides a user-friendly, interactive data mining environment with good performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 775

LEFT text: We propose a novel index structure, the A-tree (approximation tree), for similarity searches in high-dimensional data. The basic idea of the A-tree is the introduction of virtual bounding rectangles (VBRs) which contain and approximate MBRs or data objects. VBRs can be represented quite compactly and thus affect the tree configuration both quantitatively and qualitatively. First, since tree nodes can contain a large number of VBR entries, fanout becomes large, which increases search speed. More importantly, we have a free hand in arranging MBRs and VBRs in the tree nodes. Each A-tree node contains an MBR and its children VBRs. Therefore, by fetching an A-tree node, we can obtain information on the exact position of a parent MBR and the approximate position of its children. We have performed experiments using both synthetic and real data sets. For the real data sets, the A-tree outperforms the SR-tree and the VA-file in all dimensionalities up to 64 dimensions, which is the highest dimension in our experiments. Additionally, we propose a cost model for the A-tree. We verify the validity of the cost model for synthetic and real data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: r-tree based indexing of now-relative bitemporal data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: rasa bliujute , christian s. jensen , simonas saltenis , giedrius slivinskas
",n
"LEFT id: NA
RIGHT id: 2075

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the computation of relational view complements

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jens lechtenb &#246; rger , gottfried vossen
",n
"LEFT id: NA
RIGHT id: 1553

LEFT text: We introduce a logical formalism for the specification of the dynamic behavior of databases. The evolution of databases is characterized by both the dynamic integrity constraints which describe the properties of state transitions and the transactions whose executions lead to state transitions. Our formalism is based on a variant of first-order situational logic in which the states of computations are explicit objects. Integrity constraints and transactions are uniformly specifiable as expressions in our language. We also point out the application of the formalism to the verification and synthesis of transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: turmoil at nasa , and numerous funding announcements

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1138

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying heterogeneous information sources using source descriptions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy , anand rajaraman , joann j. ordille
",n
"LEFT id: NA
RIGHT id: 1604

LEFT text: Database management is one of the main areas of research of the School of Computer Science at The University of Oklahoma (OU). The objective of the database research team at OU (OUDB) is to help solve the many issues and challenges facing the database research community, especially with respect to emerging technology. Currently, many projects are being conducted in the following areas: real-time databases, object-oriented databases, mobile databases, multimedia databases, data mining and data warehouses. These projects have been funded by federal and state agencies as well as private industries such as National Science Foundation, the U.S. Department of Education, Oklahoma State Department of Environmental Quality, and Objectivity, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data and knowledge base research at hong kong university of science and technology

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: p. drew , b. hamidzadeh , k. karlapalem , a. kean , d. lee , q. li , f. lochovsky , c. d. shum , b. wuthrich
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: During the past few years our research efforts have been inspired by two different needs. On one hand, the number of non-expert users accessing databases is growing apace. On the other, information systems will no longer be characterized by a single centralized architecture, but rather by several heterogeneous component systems. In order to address such needs we have designed a new query system with both user-oriented and multidatabase features. The system's main components are an adaptive visual interface, providing the user with different and interchangeable interaction modalities, and a “translation layer”, which creates and offers to the user the illusion of a single homogeneous schema out of several heterogeneous components. Both components are founded on a common ground, i.e. a formally defined and semantically rich data model, the Graph Model, and a minimal set of Graphical Primitives, in terms of which general query operations may be visually expressed. The Graph Model has a visual syntax, so that graphical operations can be applied on its components without unnecessary mappings, and an object-based semantics. The aim of this paper is twofold. We first present an overall view of the system architecture and then give a comprehensive description of the lower part of the system itself. In particular, we show how schemata expressed in different data models can be translated in terms of Graph Model, possibly by exploiting reverse engineering techniques. Moreover, we show how mappings can be established between well-known query languages and the Graphical Primitives. Finally, we describe in detail how queries expressed by using the Graphical Primitives can be translated in terms of relational expressions so to be processed by actual DBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 1345

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching technologies for web applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c. mohan
",y
"LEFT id: NA
RIGHT id: 902

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a graphical query language for mobile information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ya-hui chang
",n
"LEFT id: NA
RIGHT id: 761

LEFT text: This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: querying and mining data streams : you only get one look a tutorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: minos garofalakis , johannes gehrke , rajeev rastogi
",y
"LEFT id: NA
RIGHT id: 1785

LEFT text: Graph databases have aroused a large interest in the last years due to their large scope of potential applications (e.g., social networks, biomedical networks, data stemming from the web). However, much published data suffer from quality problems, and graph data are no exception. In this paper, we investigate the issue of dealing with quality information in graph databases, at querying time. A framework is provided that makes it possible to introduce fuzzy quality preferences into graph pattern queries. This question is answered first from a theoretical point of view and then with an application to the Neo4j database management system by the extension of the cypher query language, for which implementation issues are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1473

LEFT text: MTCache is a prototype midtier database caching solution for SQL server that achieves this transparency. It builds on SQL server's support for materialized views, distributed queries and replication. We describe MTCache and report experimental results on the TPC-W benchmark. The experiments show that a significant part of the query workload can be offloaded to cache servers, resulting in greatly improved scale-out on the read-dominated workloads of the benchmark. Replication overhead was small with an average replication delay of less than two seconds.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient incremental garbage collection for client-server object database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: laurent amsaleg , michael j. franklin , olivier gruber
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",y
"LEFT id: NA
RIGHT id: 1702

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards an infrastructure for temporal databases : report of an invitational arpa/nsf workshop

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: niki pissinou , richard thomas snodgrass , ramez elmasri , inderpal s. mumick , tamer &#214; zsu , barbara pernici , arie segev , babis theodoulidis , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative formalism for specifying these kinds of probabilistic information, and we propose algorithms for ordering the information sources. Finally, we discuss a preliminary experimental evaluation of these algorithms on the domain of bibliographic sources available on the WWW.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",n
"LEFT id: NA
RIGHT id: 1518

LEFT text: This paper presents, QuickStore, a memory-mapped storage system for persistent C++ built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. The paper also presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. These systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. Both systems use the same underlying storage manager and compiler allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementing crash recovery in quickstore : a performance study

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1405

LEFT text: Success of commercial query optimizers and database management systems (object-oriented or relational) depend on accurate cost estimation of various query reordering [BGI]. Estimating predicate selectivity, or the fraction of rows in a database that satisfy a selection predicate, is key to determining the optimal join order. Previous work has concentrated on estimating selectivity for numeric fields [ASW, HaSa, IoP, LNS, SAC, WVT]. With the popularity of textual data being stored in databases, it has become important to estimate selectivity accurately for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the SQL like predicate [Dat]. Techniques used for estimating numeric selectivity are not suited for estimating alphanumeric selectivity.In this paper, we study for the first time the problem of estimating alphanumeric selectivity in the presence of wildcards. Based on the intuition that the model built by a data compressor on an input text encapsulates information about common substrings in the text, we develop a technique based on the suffix tree data structure to estimate alphanumeric selectivity. In a statistics generation pass over the database, we construct a compact suffix tree-based structure from the columns of the database. We then look at three families of methods that utilize this structure to estimate selectivity during query plan costing, when a query with predicates on alphanumeric attributes contains wildcards in the predicate.We evaluate our methods empirically in the context of the TPC-D benchmark. We study our methods experimentally against a variety of query patterns and identify five techniques that hold promise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimating the selectivity of xml path expressions for internet scale applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ashraf aboulnaga , alaa r. alameldeen , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 2024

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: panel : querying networked databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1331

LEFT text: An ∈-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of N. We present a new online algorithm for computing∈-approximate quantile summaries of very large data sequences. The algorithm has a worst-case space requirement of &Ogr;(1÷∈ log(∈N)). This improves upon the previous best result of &Ogr;(1÷∈ log2(∈N)). Moreover, in contrast to earlier deterministic algorithms, our algorithm does not require a priori knowledge of the length of the input sequence. Finally, the actual space bounds obtained on experimental data are significantly better than the worst case guarantees of our algorithm as well as the observed space requirements of earlier algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient progressive skyline computation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kian-lee tan , pin-kwang eng , beng chin ooi
",n
"LEFT id: NA
RIGHT id: 189

LEFT text: The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel's key idea is separating the management of the site's data, the creation and management of the site's structure, and the visual presentation of the site's pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site's structure by applying a “site-definition query” to the underlying data. The result of evaluating this query is a “site graph”, which represents both the site's content and structure. Third, the builder specifies the visual presentation of pages in Strudel's HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel's key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: catching the boat with strudel : experiences with a web-site management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mary fern &#225; ndez , daniela florescu , jaewoo kang , alon levy , dan suciu
",y
"LEFT id: NA
RIGHT id: 2247

LEFT text: In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: investigation of algebraic query optimisation techniques for database programming languages

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 459

LEFT text: Over the last few years, the information technology industry has witnessed revolutions in multiple dimensions. Increasing ubiquitous sources of data have posed two connected challenges to data management solutions -- processing unprecedented volumes of data, and providing ad-hoc real-time analysis in mainstream production data stores without compromising regular transactional workload performance. In parallel, computer hardware systems are scaling out elastically, scaling up in the number of processors and cores, and increasing main memory capacity extensively. The data processing challenges combined with the rapid advancement of hardware systems has necessitated the evolution of a new breed of main-memory databases optimized for mixed OLTAP environments and designed to scale.    The Oracle RDBMS In-memory Option (DBIM) is an industry-first distributed dual format architecture that allows a database object to be stored in columnar format in main memory highly optimized to break performance barriers in analytic query workloads, simultaneously maintaining transactional consistency with the corresponding OLTP optimized row-major format persisted in storage and accessed through database buffer cache. In this paper, we present the distributed, highly-available, and fault-tolerant architecture of the Oracle DBIM that enables the RDBMS to transparently scale out in a database cluster, both in terms of memory capacity and query processing throughput. We believe that the architecture is unique among all mainstream in-memory databases. It allows complete application-transparent, extremely scalable and automated distribution of Oracle RDBMS objects in-memory across a cluster, as well as across multiple NUMA nodes within a single server. It seamlessly provides distribution awareness to the Oracle SQL execution framework through affinitized fault-tolerant parallel execution within and across servers without explicit optimizer plan changes or query rewrites.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the implementation and performance of compressed databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: till westmann , donald kossmann , sven helmer , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 206

LEFT text: We propose a new dynamic method for multidimensional selectivity estimation for range queries that works accurately independent of data distribution. Good estimation of selectivity is important for query optimization and physical database design. Our method employs the multilevel grid file (MLGF) for accurate estimation of multidimensional data distribution. The MLGF is a dynamic, hierarchical, balanced, multidimensional file structure that gracefully adapts to nonuniform and correlated distributions. We show that the MLGF directory naturally represents a multidimensional data distribution. We then extend it for further refinement and present the selectivity estimation method based on the MLGF. Extensive experiments have been performed to test the accuracy of selectivity estimation. The results show that estimation errors are very small independent of distributions, even with correlated and/or highly skewed ones. Finally, we analyze the cause of errors in estimation and investigate the effects of various parameters on the accuracy of estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a comparison of selectivity estimators for range queries on metric attributes

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: bj &#246; rn blohsfeld , dieter korus , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1702

LEFT text: Specifically, an initial glossary of temporal database concepts and a. test suite of temporal queries were distributed before the workshop. Both of these document*s were amended based on the analysis and critique of the workshop. A language design committee was constituted after the workshop to develop a consensus temporal query la,nguage extension to SQL-92; this design also benefited from the discussion at the workshop. This report documents the discussions and consensus reached at the workshop. The report. reflects the conclusions rea.ched at the workshop in June, 1993 and further discussions amongst the group participants through electronic mail.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards an infrastructure for temporal databases : report of an invitational arpa/nsf workshop

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: niki pissinou , richard thomas snodgrass , ramez elmasri , inderpal s. mumick , tamer &#214; zsu , barbara pernici , arie segev , babis theodoulidis , umeshwar dayal
",y
"LEFT id: NA
RIGHT id: 310

LEFT text: We propose an indexing technique for the fast retrieval of objects in 2D images based on similarity between their boundary shapes. Our technique is robust in the presence of noise and supports several important notions of similarity including optimal matches irrespective of variations in orientation and/or position. Our method can also handle size-invariant matches using a normalization technique, although optimality is not guaranteed here. We implemented our method and performed experiments on real (hand-written digits) data. Our experimental results showed the superiority of our method compared to search based on sequential scanning, which is the only obvious competitor. The performance gain of our method increases with any increase in the number or the size of shapes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and tumble similar set retrieval

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: aristides gionis , dimitrios gunopulos , nick koudas
",n
"LEFT id: NA
RIGHT id: 1789

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel mining algorithms for generalized association rules with classification hierarchy

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: takahiko shintani , masaru kitsuregawa
",y
"LEFT id: NA
RIGHT id: 171

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1027

LEFT text: Our GC uses a new synchronization mechanism (mechanism that allows the GC to operate concurrently with ordinary users of the database), called CC-consistent cuts. A GC-consistent cut is a set of virtual copies of database pages. The copies are taken at times such that an object may appear as garbage in the cut only if it is garbage in the system. Our GC examines the copies, instead of the real database, in order to determine which objects are garbage. More sophisticated GCs can execute concurrently with the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrent garbage collection in o2

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marcin skubiszewski , patrick valduriez
",y
"LEFT id: NA
RIGHT id: 881

LEFT text: Abstract. Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide  the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional database allocation for parallel data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: thomas st &#246; hr , holger m &#228; rtens , erhard rahm
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1119

LEFT text: Data abstractions were originally conceived as a specification tool in programming. They also appear to be useful for exploring and explaining the capabilities and shortcomings of the data definition and manipulation facilities of present-day database systems. Moreover they may lead to new approaches to the design of these facilities. In the first section the paper introduces an axiomatic method for specifying data abstractions and, on that basis, gives precise meaning to familiar notions such as data model, data type, and database schema. In a second step the various possibilities for specifying data types within a given data model are examined and illustrated. It is shown that data types prescribe the individual operations that are allowed within a database. Finally, some additions to the method are discussed which permit the formulation of interrelationships between arbitrary operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: schema and database evolution in the o2 object database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari , guy ferran , jo &#235; lle madec
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: Traditionally, databases have stored textual data and have been used to store administrative information. The computers used. and more specifically the storage available, have been neither large enough nor fast enough to allow databases to be used for more technical applications. In recent years these two bottlenecks have started to di sappear and there is an increasing interest in using databases to store non-textual data like sensor measurements or other types of process-related data. In a database a sequence of sensor measurements can be represented as a time series. The database can then be queried to find, for instance, subsequences, extrema points, or the points in time at which the time series had a specific value. To make this search efficient, indexing methods are required. Finding appropriate indexing methods is the focus of this thesis.There are two major problems with existing time series indexing strategies: the size of the index structures and the lack of general indexing strategies that are application independent. These problems have been thoroughly researched and solved in the case of text indexing files. We have examined the extent to which text indexing methods can be used for indexing time series.A method for transforming time series into text sequences has been investigated. An investigation was then made on how text indexing methods can be applied on these text sequences. We have examined two well known text indexing methods: the signature files and the B-tree. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 1432

LEFT text: In spite of the rapid decrease in magnetic disk prices, tertiary storage (i.e., removable media in a robotic storage library) is becoming increasingly popular. The fact that so much data can be stored encourages applications that use ever more massive data sets. Application drivers include multimedia databases, data warehouses, scientific databases, and digital libraries and archives. The database research community has responded with investigations into systems integration, performance modeling, and performance optimization. Tertiary storage systems present special challenges because of their unusual performance characteristics. Access latencies can range into minutes even on unloaded systems, but transfer rates can be very high. Tertiary storage is implemented with a wide array of technologies, each with its own performance quirks. However, little detailed performance information about tertiary storage devices has been published. In this paper we present detailed measurements of several tape drives and robotic storage libraries. The tape drives we measure include the DLT 4000, DLT 7000, Ampex 310, IBM 3590, 4mm DAT, and the Sony DTF drive. This mixture of equipment includes high and low performance drives, serpentine and helical scan drives, and cartridge The detailed measurements of different aspects of tertiary storage system performance provides an understanding of the issues related to integrating tapebased tertiary storage with a DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dali : a high performance main memory storage manager

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. v. jagadish , daniel f. lieuwen , rajeev rastogi , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1323

LEFT text: Data integration is frequently required to obtain the full value of data from multiple sources. In spite of extensive research on tools to assist users, data integration remains hard, particularly for users with limited technical proficiency. To address this barrier, we study how much we can do with no user guidance. Our vision is that the user should merely specify two input datasets to be joined and get a meaningful integrated result. It turns out that our vision can be realized if the system can correctly determine the join key, for example based on domain knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: warlock : a data allocation tool for parallel warehouses

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: thomas st &#246; hr , erhard rahm
",y
"LEFT id: NA
RIGHT id: 54

LEFT text: We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) >= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: bottom-up computation of sparse and iceberg cube

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kevin beyer , raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 1142

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: of objects and databases : a decade of turmoil

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 387

LEFT text: The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The grand vision — a decentralized community of machines pooling their resources to benefit everyone — is compelling for many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship. Existing peer-to-peer (P2P) systems have focused on specific application domains (e.g. music files) or on providing filesystem-like capabilities; these systems ignore the semantics of data. An important question for the database community is how data management can be applied to P2P, and what we can learn from and contribute to the P2P area. We address these questions, identify a number of potential research ideas in the overlap between data management and P2P systems, present some preliminary fundamental results, and describe our initial work in constructing a P2P data management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: self-organizing data sharing communities with sagres

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: zachary ives , alon levy , jayant madhavan , rachel pottinger , stefan saroiu , igor tatarinov , shiori betzler , qiong chen , ewa jaslikowska , jing su , wai tak theodora yeung
",y
"LEFT id: NA
RIGHT id: 2218

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: managing uncertainty in sensor database

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: reynold cheng , sunil prabhakar
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Empirical research based on time series is a data intensive activity that needs a data base management system (DBMS). We investigate the special properties a time series management system (TSMS) should have. We then show that currently available solutions and related research directions are not well suited to handle the existing problems. Therefore, we propose the development of a special purpose TSMS, which will offer particular modeling, retrieval, and computation capabilities. It will be suitable for end users, offer direct manipulation interfaces, and allow data exchange with a variety of data sources, including other databases and application packages. We intend to build such a system on top of an off-the-shelf object-oriented DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",y
"LEFT id: NA
RIGHT id: 1392

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general ""sketch""-based methods for capturing various linear projections and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of wavelet-based histograms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",y
"LEFT id: NA
RIGHT id: 1422

LEFT text: Expert database systems extend the functionality of conventional database systems by providing a facility for creating and automatically executing Condition-Action rules. While Condition-Action rules in database systems are very powerful, they also can be very difficult to program, due to the unstructured and unpredictable nature of rule processing. We provide methods for static analysis of Condition-Action rules; our methods determine whether a given rule set is guaranteed to terminate, and whether rule execution is confluent (has a guaranteed unique final state). Our methods are based on previous methods for analyzing rules in active database systems. We improve considerably on the previous methods by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not. Our improved analysis is based on a ``propagation'''' algorithm, which uses a formal approach based on an extended relational algebra to accurately determine when the action of one rule can affect the condition of another. Our algebraic approach yields methods that are applicable to a broad class of expert database rule languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an algebraic approach to rule analysis in expert database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elena baralis , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1565

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. dogac , c. dengi , e. kilic , g. ozhan , f. ozcan , s. nural , c. evrendilek , u. halici , b. arpinar , p. koksal , n. kesim , s. mancuhan
",n
"LEFT id: NA
RIGHT id: 1197

LEFT text: The processing of spatial joins can be greatly improved by the use of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of nonintersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. This article presents a polygon approximation for spatial join processing which we call four-colors raster signature (4CRS). The performance of a filter using this approximation was evaluated with real world data sets. The results showed that our approach, when compared to other approaches presented in the related literature, reduced the inconclusive answers by a factor of more than two. As a result, the need for retrieving the representation of polygons and carrying out exact geometry tests is reduced by a factor of more than two, as well. A Raster Approximation for the Processing of Spatial Joins

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: searching in metric spaces by spatial approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: gonzalo navarro
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 2083

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: information sharing across private databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , alexandre evfimievski , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 1795

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integrating association rule mining with relational database systems : alternatives and implications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sunita sarawagi , shiby thomas , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 2290

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1722

LEFT text: Spatial joins are one of the most important operations for combining spatial objects of several relations. In this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year's conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidate which is handled by the following two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaining candidates has to be tested against the join predicate. The time required for computing spatial join predicates can essentially be reduced when objects are adequately organized in main memory. In our approach, objects are first decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial joins using seeded trees

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 1150

LEFT text: In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 173

LEFT text: During the past few years our research efforts have been inspired by two different needs. On one hand, the number of non-expert users accessing databases is growing apace. On the other, information systems will no longer be characterized by a single centralized architecture, but rather by several heterogeneous component systems. In order to address such needs we have designed a new query system with both user-oriented and multidatabase features. The system's main components are an adaptive visual interface, providing the user with different and interchangeable interaction modalities, and a “translation layer”, which creates and offers to the user the illusion of a single homogeneous schema out of several heterogeneous components. Both components are founded on a common ground, i.e. a formally defined and semantically rich data model, the Graph Model, and a minimal set of Graphical Primitives, in terms of which general query operations may be visually expressed. The Graph Model has a visual syntax, so that graphical operations can be applied on its components without unnecessary mappings, and an object-based semantics. The aim of this paper is twofold. We first present an overall view of the system architecture and then give a comprehensive description of the lower part of the system itself. In particular, we show how schemata expressed in different data models can be translated in terms of Graph Model, possibly by exploiting reverse engineering techniques. Moreover, we show how mappings can be established between well-known query languages and the Graphical Primitives. Finally, we describe in detail how queries expressed by using the Graphical Primitives can be translated in terms of relational expressions so to be processed by actual DBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql open heterogeneous data access

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: berthold reinwald , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 1763

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: intelligent access to heterogeneous information sources : report on the 4th workshop on knowledge representation meets databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: franz baader , manfred a. jeusfeld , werner nutt
",n
"LEFT id: NA
RIGHT id: 1889

LEFT text: Hello Everyone, I hope you all enjoyed your summer as our thoughts now turn to fall and all the wonders it brings. In this issue, I am responding to several assisted living (AL) nursing concerns we have received regarding advance directives (ADs).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tiziana catarci , isabel f. cruz
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: Concurrency control is essential to the correct functioning of a database due to the need for correct, reproducible results. For this reason, and because concurrency control is a well-formulated problem, there has developed an enormous body of literature studying the performance of concurrency control algorithms. Most of this literature uses either analytic modeling or random number-driven simulation, and explicitly or implicitly makes certain assumptions about the behavior of transactions and the patterns by which they set and unset locks. Because of the difficulty of collecting suitable measurements, there have been only a few studies which use trace-driven simulation, and still less study directed toward the characterization of concurrency control behavior of real workloads. In this paper, we present a study of three database workloads, all taken from IBM DB2 relational database systems running commercial applications in a production environment. This study considers topics such as frequency of locking and unlocking, deadlock and blocking, duration of locks, types of locks, correlations between applications of lock types, two-phase versus non-two-phase locking, when locks are held and released, etc. In each case, we evaluate the behavior of the workload relative to the assumptions commonly made in the research literature and discuss the extent to which those assumptions may or may not lead to erroneous conclusions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",y
"LEFT id: NA
RIGHT id: 1318

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: vxmlr : a visual xml-relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: aoying zhou , hongjun lu , shihui zheng , yuqi liang , long zhang , wenyun ji , zengping tian
",n
"LEFT id: NA
RIGHT id: 274

LEFT text: The focus of our work is to design and build a dynamic data distribution system that is coherence-preserving, i.e. the delivered data must preserve associated coherence requirements (the user-specified bound on tolerable imprecision) and resilient to failures. To this end, we consider a system in which a set of repositories cooperate with each other and the sources, forming a peer-to-peer network. In this system, necessary changes are pushed to the users so that they are automatically informed, about changes of interest. We present techniques 1) to determine when to push an update from one repository to another for coherence maintenance, 2) to construct an efficient dissemination tree for propagating changes from sources to cooperating repositories, and 3) to make the system resilient to failures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dissemination of dynamic data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pavan deolasee , amol katkar , ankur panchbudhe , krithi ramamritham , prashant shenoy
",y
"LEFT id: NA
RIGHT id: 1884

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 914

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at ut arlington

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sharma chakravarthy , alp aslandogan , ramez elmasri , leonidas fegaras , junghwan oh
",n
"LEFT id: NA
RIGHT id: 1445

LEFT text: Active database systems have been a hot research topic for quite some years now. However, while “active functionality” has been claimed for many systems, and notions such as “active objects” or “events” are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of “active database management system” as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bigsur : a system for the management of earth science data

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: paul brown , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 176

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: java and relational databases ( tutorial ) : sqlj

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gray clossman , phil shaw , mark hapner , johannes klein , richard pledereder , brian becker
",n
"LEFT id: NA
RIGHT id: 382

LEFT text: DART '96 was held in conjunction with the Conference of Information and Knowledge Management (CIKM) on Nov 15th in Baltimore. Its goal was to provide a forum for researchers and practitioners involved in integrating concepts and technologies from active and real-time databases to discuss the state of the art and chart a course of action. To this end, nine speakers from academia, industry, and research laboratories were invited to provide a perspective on the theory and practice underlying active real-time databases. In addition, some selected papers were presented briefly to complement the invited speakers' talks. The second half of the workshop was devoted to discussions aimed at identifying the problems that still need to be addressed in the contexts of the diverse target applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report : the first international workshop on active and real-time database systems ( artdb-95 )

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mikael berndtsson , j &#246; rgen hansson
",n
"LEFT id: NA
RIGHT id: 858

LEFT text: Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance issues in incremental warehouse maintenance

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wilburt labio , jun yang , yingwei cui , hector garcia-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 2234

LEFT text: The children of today grow up having information and communication technologies (ICTs) as essential and natural parts of their daily life. As they grow, they are expected to become active and self-directed members in their own local communities and also in the information society at large. Technology creates versatile possibilities for the acquisition and creation of information, for self-expression, and for communication and interaction with other people locally, nationally, and worldwide.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guest editor 's introduction

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1635

LEFT text: Recent work in query optimization has addressed the issue of placing expensive predicates in a query plan. In this paper we explore the predicate placement options considered in the Montage DBMS, presenting a family of algorithms that form successively more complex and effective optimization solutions. Through analysis and performance measurements of Montage SQL queries, we classify queries and highlight the simplest solution that will optimize each class correctly. We demonstrate limitations of previously published algorithms, and discuss the challenges and feasibility of implementing the various algorithms in a commercial-grade system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: practical predicate placement

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 1161

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: An order-dependent query is one whose result (interpreted as a multiset) changes if the order of the input records is changed. In a stock-quotes database, for instance, retrieving all quotes concerning a given stock for a given day does not depend on order, because the collection of quotes does not depend on order. By contrast, finding a stock's five-price moving-average in a trades table gives a result that depends on the order of the table. Query languages based on the relational data model can handle order-dependent queries only through add-ons. SQL:1999, for instance, has a new ""window"" mechanism which can sort data in limited parts of a query. Add-ons make order-dependent queries di_cult to write and to optimize. In this paper we show that order can be a natural property of the underlying data model and algebra. We introduce a new query language and algebra, called AQuery, that supports order from-the-ground-up. New order-related query transformations arise in this setting. We show by experiment that this framework - language plus optimization techniques - brings orders-of-magnitude improvement over SQL:1999 systems on many natural order-dependent queries

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",n
"LEFT id: NA
RIGHT id: 1718

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive selectivity estimation using query feedback

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chungmin melvin chen , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",y
"LEFT id: NA
RIGHT id: 894

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 199

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1444

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: document management as a database problem

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rudolf bayer
",n
"LEFT id: NA
RIGHT id: 1772

LEFT text: The database area has been one of those areas of computerscience which have very directly been driven by applicationrequirements; this is true today in three ways: First, the userswant more application specific support from the database, and theyexpect the DBMS to have more semantic application knowledge.Second, users want database support for new applications which aresometimes far from the traditional database applications andintroduce completely new requirements as well as the need tosmoothly integrate database technology with other advancedtechnologies (e.g. neural nets) in one application. Finally, theembedding of databases into interactive work environments - forinstance, the use of databases in cooperative environments(computer supported cooperative work) - forces the databasecommunity to reconsider some of the traditional beliefs aboutdatabases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 152

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the araneus web-based management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. mecca , p. atzeni , a. masci , g. sindoni , p. merialdo
",n
"LEFT id: NA
RIGHT id: 1840

LEFT text: Translators (wrappers) convert queries over information in the common del (OEM) into requests the source can execute. The data returned by the source is converted back into the common model. Mediators are programs that collect information from one or more sources, process and combine it, and export the resulting information to the end user or an application program. Users or applications can choose to interact either directly with the translators or indirectly via one or more mediators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: template-based wrappers in the tsimmis system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joachim hammer , h &#233; ctor garc &#237; a-molina , svetlozar nestorov , ramana yerneni , marcus breunig , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 315

LEFT text: We address the new issues pose by nested queries. In particular, the answer space begins with a superset of the final answers and is refined as the aggregates from the inner query blocks are refined. For the intermediary answers to be meaningful, they have to be interpreted with the aggregates from the inner queries. We also propose a multi-threaded model in evaluating such queries: each query block is assigned to a thread, and the threads can be evaluated concurrently and independently. The time slice across the threads is nondeterministic in the sense that the user controls the relative rate at which these subqueries are being evaluated. For enumerative nested queries, we propose a priority-based evaluation strategy to present answers that are certainly in the final answer space first, before presenting those whose validity may be affected as the inner query aggregates are refined. We implemented a prototype system using Java and evaluated our system. Results for nested queries with a level and multiple levels of nesting are reported. Our results show the effectiveness of the proposed mechanisms in providing progressive feedback that reduces the initial waiting time of users significantly without sacrificing the quality of the answers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: progressive approximate aggregate queries with a multi-resolution tree structure

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: iosif lazaridis , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1802

LEFT text: A set containment join is a join between set-valued attributes of two relations, whose join condition is specified using the subset (⊆) operator. Set containment joins are deployed in many database applications, even those that do not support set-valued attributes. In this article, we propose two novel partitioning algorithms, called the Adaptive Pick-and-Sweep Join (APSJ) and the Adaptive Divide-and-Conquer Join (ADCJ), which allow computing set containment joins efficiently. We show that APSJ outperforms previously suggested algorithms for many data sets, often by an order of magnitude. We present a detailed analysis of the algorithms and study their performance on real and synthetic data using an implemented testbed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental distance join algorithms for spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1947

LEFT text: Database replication is traditionally seen as a way to increase the availability and performance of distributed databases. Although a large number of protocols providing data consistency and fault-tolerance have been proposed, few of these ideas have ever been used in commercial products due to their complexity and performance implications. Instead, current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication. As an alternative, we propose a suite of replication protocols that addresses the main problems related to database replication. On the one hand, our protocols maintain data consistency and the same transactional semantics found in centralized systems. On the other hand, they provide flexibility and reasonable performance. To do so, our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases. This allows us to eliminate the possibility of deadlocks, reduce the message overhead and increase performance. A detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: solving satisfiability and implication problems in database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sha guo , wei sun , mark a. weiss
",n
"LEFT id: NA
RIGHT id: 2145

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs. Design principles have been proposed for persistent systems. By following these principles, languages that provide persistence as a basic abstraction have been developed. In this paper, the motivation for orthogonal persistence is reviewed along with the above mentioned design principles. The concepts for integrating programming languages and databases through the persistence abstraction, and their benefits, are given. The technology to support persistence, the achievements, and future directions of persistence research are then discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on persistent object systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: NA
",y
"LEFT id: NA
RIGHT id: 476

LEFT text: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1409

LEFT text: Corporations worldwide are finding that understanding and managing rapidly growing, enterprisewide data is critical for making timely decisions and responding to changing business conditions. To manage and use business information competitively, many companies are establishing decision support systems built around a data warehouse of subject-oriented, integrated, historical information. In order to understand why the data warehouse must replace old legacy applications for effective information processing, it is necessary to understand the root causes of the difficulty in getting information in the first place. The first difficulty in getting information from the base of old applications is that those old applications were shaped around business requirements that were relevant as much as twenty-five years ago. These applications that were shaped yesterday do not reflect today’s business. The second reason why older applications are so hard to use as a basis for information is that those applications were shaped around the clerical needs of the corporation. A clerically focused application of necessity does not have the historical foundation required to support a long-term view. Another reason why the clerical perspective of applications does not support management’s need for information is that the clerical community focuses on detailed data. While detailed data is tine for the day-to-day clerical needs of the organization, management needs to see summary data in order to identify trends, challenges and opportunities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom
",n
"LEFT id: NA
RIGHT id: 497

LEFT text: Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing ” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tracing the lineage of view data in a warehousing environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 51

LEFT text: We propose multi-precision similarity matching where the image is divided into a number of subblocks, each with its associated color histogram. We present experimental results showing that the spatial distribution information recorded by multiprecision color histograms helps to make similarity matching more precise. We also show that sub-image queries are much better supported with multi-precision color histograms. To minimize the overhead, we employ a filtering scheme based on the 3-dimensional average color vectors. We provide a formal result proving that filtering with multi-precision color histograms is complete. Finally, we develop a novel extendible hashing structure for indexing the average color vectors. We give experimental results showing that the proposed structure significantly outperforms the SR-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: walrus : a similarity retrieval algorithm for image databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: apostol natsev , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: Expert database systems extend the functionality of conventional database systems by providing a facility for creating and automatically executing Condition-Action rules. While Condition-Action rules in database systems are very powerful, they also can be very difficult to program, due to the unstructured and unpredictable nature of rule processing. We provide methods for static analysis of Condition-Action rules; our methods determine whether a given rule set is guaranteed to terminate, and whether rule execution is confluent (has a guaranteed unique final state). Our methods are based on previous methods for analyzing rules in active database systems. We improve considerably on the previous methods by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not. Our improved analysis is based on a ``propagation'''' algorithm, which uses a formal approach based on an extended relational algebra to accurately determine when the action of one rule can affect the condition of another. Our algebraic approach yields methods that are applicable to a broad class of expert database rule languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 1808

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for implementing hypothetical queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: timothy griffin , richard hull
",y
"LEFT id: NA
RIGHT id: 1599

LEFT text: In recent years there has been an increasing interest in databases of moving objects where the motion and extent of objects are represented as a function of time. The focus of this paper is on the maintenance of continuous K- nearest neighbor (k-NN) queries on moving points when updates are allowed. Updates change the functions describing the motion of the points, causing pending events to change. Events are processed to keep the query result consistent as points move. It is shown that the cost of maintaining a continuous k-NN query result for moving points represented in this way can be significantly reduced with a modest increase in the number of events processed in the presence of updates. This is achieved by introducing a continuous within query to filter the number of objects that must be taken into account when maintaining a continuous k-NN query. This new approach is presented and compared with other recent work. Experimental results are presented showing the utility of this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: nearest neighbor queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: nick roussopoulos , stephen kelley , fr &#233; d &#233; ric vincent
",n
"LEFT id: NA
RIGHT id: 1069

LEFT text: Hyper-programming is a technology only available in persistent systems, since hyper-program source code contains both text and links to persistent objects. A hyper-programming system has already been prototyped in the persistent programming language Napier88. Here we report on the transfer of that technology to an object-oriented platform, Java. The component technologies required for hyperprogramming include linguistic reflection, a persistent store, and a browsing mechanism, all of which have been reported elsewhere. The topics of discussion here are the additional technologies of: the specification of denotable hyper-links in Java; a mechanism for preserving hyper-links over compilation; a hyper-program editor; and the integration of the editor and the browser with the hyper-programming user interface. We describe their design and implementation. In total, these technologies constitute a hyper-programming system in Java.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: hyper-programming in java

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: evangelos zirintsis , graham n. c. kirby , ronald morrison
",y
"LEFT id: NA
RIGHT id: 952

LEFT text: QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 440

LEFT text: Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: making b + - trees cache conscious in main memory

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jun rao , kenneth a. ross
",y
"LEFT id: NA
RIGHT id: 2003

LEFT text: Data warehousing and On-Line Analytical Processing (OLAP) are becoming critical components of decision support as advances in technology are improving the ability to manage and retrieve large volumes of data. Data warehousing refers to \a collection of decision support technologies aimed at enabling the knowledge worker (executive, manager, analyst) to make better and faster decisions"" [1]. OLAP refers to the technique of performing complex analysis over the information stored in a data warehouse. It is often used by management analysts and decision makers in a variety of functional areas such as sales and marketing planning. Typically, OLAP queries look for speci c trends and anomalies in the base information by aggregating, ranging, ltering and grouping data in many di erent ways [8]. E cient query processing is a critical requirement for OLAP because the underlying data warehouse is very large, queries are often quite complex, and decision support applications typically require in-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cmvf : a novel dimension reduction scheme for efficient indexing in a large image database

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jialie shen , anne h. h. ngu , john shepherd , du q. huynh , quan z. sheng
",n
"LEFT id: NA
RIGHT id: 255

LEFT text: A Bayesian network is an appropriate tool to deal with the uncertainty that is typical of real-life applications. Bayesian network arcs represent statistical dependence between different variables. In the data mining field, association and correlation rules can be interpreted as well as expressing statistical dependence relations. K2 is a well-known algorithm which is able to learn Bayesian networks. In this paper we present two extensions of K2 called K2-Lift and K2-X2 that exploit two parameters normally defined in relation to association and correlation rules. The experiments performed show that K2-Lift and K2-X2 improve K2 with respect to both the quality of the learned network and the execution time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jiawei han
",n
"LEFT id: NA
RIGHT id: 847

LEFT text: Cleaning data of errors in structure and content is important for data warehousing and integration. Current solutions for data cleaning involve many iterations of data “auditing” to find errors, and long-running transformations to fix them. Users need to endure long waits, and often write complex transformation scripts. We present Potter’s Wheel, an interactive data cleaning system that tightly integrates transformation and discrepancy detection. Users gradually build transformations to clean the data by adding or undoing transforms on a spreadsheet-like interface; the effect of a transform is shown at once on records visible on screen. These transforms are specified either through simple graphical operations, or by showing the desired effects on example data values. In the background, Potter’s Wheel automatically infers structures for data values in terms of user-defined domains, and accordingly checks for constraint violations. Thus users can gradually build a transformation as discrepancies are found, and clean the data without writing complex programs or enduring long delays.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: online dynamic reordering for interactive data processing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vijayshankar raman , bhaskaran raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 2014

LEFT text: With t,ha exponential proliferation of databases and advances in wide area networking, interest, in worldwide dat,abase interoperability has gained momentum. Scalability and language support for this new environment remain open questSions. We propose a scheme where database nodes are dynamically clustered around current, areas of ibresl. Data sharing is then pursued, with any relationship informat8ion discovered being fed bac,k for reclustering. In order to achieve scalability, t,he proposed architect(ure sub-divides both the rrlntionship and illformntiorl spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : middle-tier database caching for highly scalable e-business architectures

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: christof bornh &#246; vd , mehmet altinel , sailesh krishnamurthy , c. mohan , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1586

LEFT text: A set containment join is a join between set-valued attributes of two relations, whose join condition is specified using the subset (⊆) operator. Set containment joins are deployed in many database applications, even those that do not support set-valued attributes. In this article, we propose two novel partitioning algorithms, called the Adaptive Pick-and-Sweep Join (APSJ) and the Adaptive Divide-and-Conquer Join (ADCJ), which allow computing set containment joins efficiently. We show that APSJ outperforms previously suggested algorithms for many data sets, often by an order of magnitude. We present a detailed analysis of the algorithms and study their performance on real and synthetic data using an implemented testbed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive parallel aggregation algorithms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ambuj shatdal , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 990

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 2245

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use relevant information. In the Ecobase project, we address these problems in the context of several environmental applications in Brazil and Europe. We propose a distributed architecture for environmental information systems (EIS) based on the Le Select middleware developed at INRIA. In this paper, we present this architecture and its capabilities, and discuss the lessons learned and open issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: challenges for global information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova
",n
"LEFT id: NA
RIGHT id: 1138

LEFT text: Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability. This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments. A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator, whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL (C-SQL), in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying heterogeneous information sources using source descriptions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy , anand rajaraman , joann j. ordille
",n
"LEFT id: NA
RIGHT id: 76

LEFT text: We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is “find the libraries that are within 10 miles of schools”. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call “pair-count exponent” (PC). We show that this law also holds for self-spatial-joins (“find schools within 5 miles of other schools”) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy). In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial join selectivity using power laws

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christos faloutsos , bernhard seeger , agma traina , caetano traina , jr.
",y
"LEFT id: NA
RIGHT id: 113

LEFT text: This paper describes PREDATOR, a freely available object-relational database system that has been developed at Cornell University. A major motivation in developing PREDATOR was to create a modern code base that could act as a research vehicle for the database community. Pursuing this goal, this paper briefly describes several features of the system that should make it attractive for database research and education.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the asilomar report on database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phil bernstein , michael brodie , stefano ceri , david dewitt , mike franklin , hector garcia-molina , jim gray , jerry held , joe hellerstein , h. v. jagadish , michael lesk , dave maier , jeff naughton , hamid pirahesh , mike stonebraker , jeff ullman
",n
"LEFT id: NA
RIGHT id: 944

LEFT text: Recently there has been an increasing interest in supporting bulk operations on multidimensional index structures. Bulk loading refers to the process of creating an initial index structure for a presumably very large data set. In this paper, we present a generic algorithm for bulk loading which is applicable to a broad class of index structures. Our approach differs completely from previous ones for the following reasons. First, sorting multidimensional data according to a predefined global ordering is completely avoided. Instead, our approach is based on the standard routines for splitting and merging pages which are already fully implemented in the corresponding index structure. Second, in contrast to inserting records one by one, our approach is based on the idea of inserting multiple records simultaneously. As an example we demonstrate in this paper how to apply our technique to the R-tree family. For R-trees we show that the I/O performance of our generic algorithm meets the lower bound of external sorting. Empirical results demonstrate that performance improvements are also achieved in practice without sacrificing query performance

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a generic approach to bulk loading multidimensional index structures

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger , peter widmayer
",y
"LEFT id: NA
RIGHT id: 1868

LEFT text: The Object Database Management Group (ODMG) is a consortium of object-oriented DBMS vendors that have developed a standard interface for their products, ODMG-93. The standard includes a common architecture and deftition for an object-oriented DBMS, a common object model with an object deftition language, a common object query language, and standardized progr amming language bindings, cumently for C++ and Smalltalk. An object-oriented DBMS (by ODMG’S defiition) provides programming language bindings with direct, transparent persistence for data structures, in contrast to the embedded language bindings used in most DBMSS. The common object model allows data to be shared across programming languages. This model incorporates object Ills, encapsulation, methods, frost-class types, multiple inheritance, relationships, lists, sets, bags, arrays, and many other features.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sentinel : an object-oriented dbms with event-based rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. chakravarthy
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1185

LEFT text: Many emerging application domains require database systems to support efficient access over highly multidimensional datasets. The current state-of-the-art technique to indexing high dimensional data is to first reduce the dimensionality of the data using Principal Component Analysis and then indexing the reduceddimensionality space using a multidimensional index structure. The above technique, referred to as global dimensionality reduction (GDR), works well when the data set is globally correlated, i.e. most of the variation in the data can be captured by a few dimensions. In practice, datasets are often not globally correlated. In such cases, reducing the data dimensionality using GDR causes significant loss of distance information resulting in a large number of false positives and hence a high query cost. Even when a global correlation does not exist, there may exist subsets of data that are locally correlated. In this paper, we propose a technique called Local Dimensionality Reduction (LDR) that tries to find local correlations in the data and performs dimensionality reduction on the locally correlated clusters of data individually. We develop an index structure that exploits the correlated clusters to efficiently support point, range and k-nearest neighbor queries over high dimensional datasets. Our experiments on synthetic as well as real-life datasets show that our technique (1) reduces the dimensionality of the data with significantly lower loss in distance information compared to GDR and (2) significantly outperforms the GDR, original space indexing and linear scan techniques in terms of the query cost for both synthetic and real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 1342

LEFT text: In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing : taming the terabytes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: minos n. garofalakis , phillip b. gibbon
",n
"LEFT id: NA
RIGHT id: 652

LEFT text: When Alex Labrinidis asked me to write this essay, I initially balked. I was loathe to speak for academics worldwide, or even just those in SIGMOD. But I then realized that I could speak from personal experience. So these random musings will be of necessity entirely subjective, highly individualistic, and unrepresentative---attributes that a scholar normally attempts to vigorously avoid in his writing. I'm definitely not a ""typical"" academic (I don't know such an animal), but I can speak with some authority as to what motivates me.As another caveat, I make few comparisons with alternatives such as working in a research lab or as a developer. I won't even attempt to speak for them.The final caveat (distrust all commentaries that start with caveats, but perhaps more so those that don't!) is that my assumed audience comprises students who are considering such a profession. Current academics will find some of my observations trite or may disagree loudly, as academics are oft to do (see below).T

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: why i like working in academia

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 97

LEFT text: This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the oasis multidatabase prototype

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mark roantree , john murphy , wilhelm hasselbring
",n
"LEFT id: NA
RIGHT id: 2253

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a multidatabase system for tracking and retrieval of financial data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munir cochinwala , john bradley
",n
"LEFT id: NA
RIGHT id: 1128

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: constructing efficient decision trees by using optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",y
"LEFT id: NA
RIGHT id: 782

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 620

LEFT text: The Grid is an emerging platform to support on-demand ""virtual organisations"" for coordinated resource sharing and problem solving on a global scale. The application thrust is large-scale scientific endeavour, and the scale and complexity of scientific data presents challenges for databases. The Grid is beginning to exploit technologies developed for Web Services and to realise its potential it also stands to benefit from Semantic Web technologies; conversely, the Grid and its scientific users provide application pull which will benefit the Semantic Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: agents , trust , and information access on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: tim finin , anupam joshi
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Traditional protocols for distributed database management have high message overhead, lock or restrain access to resources during protocol execution, and may become impractical for some scenarios like real-time systems and very large distributed databases. In this paper we present the demarcation protocol; it overcomes these problems through the use of explicit linear arithmetic consistency constraints as the correctness criteria. The method establishes safe limits as “lines drawn in the sand” for updates and gives a way of changing these limits dynamically, enforcing the constraints at all times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 2058

LEFT text: Data streams are a new class of data that is becoming pervasively important in a wide range of applications, ranging from sensor networks, environmental monitoring to finance. In this article, we propose a novel framework for the online diagnosis of evolution of multidimensional streaming data that incorporates Recursive Wavelet Density Estimators into the context of Velocity Density Estimation. In the proposed framework changes in streaming data are characterized by the use of local and global evolution coefficients. In addition, we propose for the analysis of changes in the correlation structure of the data a recursive implementation of the Pearson correlation coefficient using exponential discounting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for diagnosing changes in evolving data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",y
"LEFT id: NA
RIGHT id: 780

LEFT text: DISCOVER operates on relational databases and facilitates information discovery on them by allowing its user to issue keyword queries without any knowledge of the database schema or of SQL. DISCOVER returns qualified joining networks of tuples, that is, sets of tuples that are associated because they join on their primary and foreign keys and collectively contain all the keywords of the query. DISCOVER proceeds in two steps. First the Candidate Network Generator generates all candidate networks of relations, that is, join expressions that generate the joining networks of tuples. Then the Plan Generator builds plans for the efficient evaluation of the set of candidate networks, exploiting the opportunities to reuse common subexpressions of the candidate networks.    We prove that DISCOVER finds without redundancy all relevant candidate networks, whose size can be data bound, by exploiting the structure of the schema. We prove that the selection of the optimal execution plan (way to reuse common subexpressions) is NP-complete. We provide a greedy algorithm and we show that it provides near-optimal plan execution time cost. Our experimentation also provides hints on tuning the greedy algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 786

LEFT text: Data warehousing and On-Line Analytical Processing (OLAP) are becoming critical components of decision support as advances in technology are improving the ability to manage and retrieve large volumes of data. Data warehousing refers to \a collection of decision support technologies aimed at enabling the knowledge worker (executive, manager, analyst) to make better and faster decisions"" [1]. OLAP refers to the technique of performing complex analysis over the information stored in a data warehouse. It is often used by management analysts and decision makers in a variety of functional areas such as sales and marketing planning. Typically, OLAP queries look for speci c trends and anomalies in the base information by aggregating, ranging, ltering and grouping data in many di erent ways [8]. E cient query processing is a critical requirement for OLAP because the underlying data warehouse is very large, queries are often quite complex, and decision support applications typically require in-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: curio : a novel solution for efficient storage and indexing in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: anindya datta , krithi ramamritham , helen m. thomas
",y
"LEFT id: NA
RIGHT id: 1787

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 669

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 709

LEFT text: Abstract. In meta-searchers accessing distributed Web-based information repositories, performance is a major issue. Efficient query processing requires an appropriate caching mechanism. Unfortunately, standard page-based as well as tuple-based caching mechanisms designed for conventional databases are not efficient on the Web, where keyword-based querying is often the only way to retrieve data. In this work, we study the problem of semantic caching of Web queries and develop a caching mechanism for conjunctive Web queries based on signature files. Our algorithms cope with both relations of semantic containment and intersection between a query and the corresponding cache items. We also develop the cache replacement strategy to treat situations when cached items differ in size and contribution when providing partial query answers. We report results of experiments and show how the caching mechanism is realized in the Knowledge Broker system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: roadrunner : automatic data extraction from data-intensive web sites

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: valter crescenzi , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 539

LEFT text: Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 1122

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: calibrating the query optimizer cost model of iro-db , an object-oriented federated database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , fei sha , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 990

LEFT text: More than two decades ago, DB researchers faced up to the question of how to design a data-independent database management system (DBMS), that is, a DBMS which offers an appropriate application programming interface (API) to the user and whose architecture is open for permanent evolution. For this purpose, an architectural model based on successive data abstraction steps of record-oriented data was proposed as kind of a standard and later refined to a five-layer hierarchical DBMS model. We review the basic concepts and implementation techniques of this model and survey the major improvements achieved in the system layers to date. Furthermore, we consider the interplay of the layered model with the transactional ACID properties and again outline the progress obtained. In the course of the last 20 years, this DBMS architecture was challenged by a variety of new requirements and changes as far as processing environments, data types, functional extensions, heterogeneity, autonomy, scalability, etc. are concerned. We identify the cases which can be adjusted by our standard system model and which need major extensions or other types of system models.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",y
"LEFT id: NA
RIGHT id: 2063

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic sample selection for approximate query processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 1219

LEFT text: A wide range of Web applications retrieve desired information from remote XML data sources across the Internet, which is usually costly due to transmission delays for large volumes of data. Therefore we propose to apply the ideas of semantic caching to XML query processing systems [2], in particular the XQuery engine. Semantic caching [3] implies view-based query answering and cache management. While it is well studied in the traditional database context, query containment for XQuery is left unexplored due to its complexity coming with the powerful expressiveness of hierarchy, recursion and result construction. We hence have developed the first solution for XQuery processing using cached views.We exploit the connections between XML and tree automata, and use subtype relations between two regular expression types to tackle the XQuery containment mapping problem. Inspired by XDuce [1], which explores the use of tree-automata-based regular expression types for XML processing, we have designed a containment mapping process to incorporate type inference and subtyping mechanisms provided by XDuce to establish containment mappings between regular-expression-type-based pattern variables of two queries. We have implemented a semantic caching system called XCache (see Figure 1), to realize the proposed containment and rewriting techniques for XQuery.The main modules of XCache include: (1) Query Decomposer. An input query is is decomposed into source-specific subqueries explicitly represented by matching patterns and return structures. (2) Query Pattern Register. By registering a few queries into semantic regions, we warm up XCache at its initialization phase. (3) Query Containment Mapper. The XDuce subtyper is incorporated into the containment mapper for establishing query containment mappings between variables of a new query and each cached query. (4) Query Rewriter. We implement the classical bucket algorithm and further apply heuristics to decide on an ""optimal"" rewriting plan if several valid ones exist. (5) Replacement Manager. We free space for new regions by both complete and partial replacement. (6) Region Coalescer. We apply a coalescing strategy to control the region granularity over time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: semantic caching of web queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: boris chidlovskii , uwe m. borghoff
",n
"LEFT id: NA
RIGHT id: 365

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online query processing : a tutorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter j. haas , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: The field of database systems research and development has been enormously successful over its 30 year history. It has led to a $10 billion industry with an installed base that touches virtually every major company in the world. It would be unthinkable to manage the large volume of valuable information that keeps corporations running without support from commercial database management systems (DBMS).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 621

LEFT text: Current Web services standards enable publishing service descriptions and finding services on a match based on criteria such as method signatures or service category. However, current approaches provide no basis for selecting a good service or for comparing ratings of services. We describe a conceptual model for reputation using which reputation information can be organized and shared and service selection can be facilitated and automated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: conceptual model of web service reputation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: e. michael maximilien , munindar p. singh
",y
"LEFT id: NA
RIGHT id: 99

LEFT text: Abstract. In meta-searchers accessing distributed Web-based information repositories, performance is a major issue. Efficient query processing requires an appropriate caching mechanism. Unfortunately, standard page-based as well as tuple-based caching mechanisms designed for conventional databases are not efficient on the Web, where keyword-based querying is often the only way to retrieve data. In this work, we study the problem of semantic caching of Web queries and develop a caching mechanism for conjunctive Web queries based on signature files. Our algorithms cope with both relations of semantic containment and intersection between a query and the corresponding cache items. We also develop the cache replacement strategy to treat situations when cached items differ in size and contribution when providing partial query answers. We report results of experiments and show how the caching mechanism is realized in the Knowledge Broker system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design principles for data-intensive web sites

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 309

LEFT text: Spatial indexes play a major role in fast access to spatial and location data. Most commercial applications insert new data in bulk: in batches or arrays. In this paper, we propose a novel bulk insertion technique for R-Trees that is fast and does not compromise on the quality of the resulting index. We present our experiences with incorporating the proposed bulk insertion strategies into Oracle 10i. Experiments with real datasets show that our bulk insertion strategy improves performance of insert operations by 50%-90%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: improving index performance through prefetching

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shimin chen , phillip b. gibbons , todd c. mowry
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: This paper describes the design and implementation of NAOS, an active rule component in the object-oriented database system 02. The contribution of this work is related to two main aspects. The first concerns the integration of the rule concept within the 02 model, providing a way to structure applications. Rules are part of a schema and do not belong to a class. Program execution and data manipulation, including method calls, can be driven on rules. The second aspect concerns the way NAOS interacts with the kernel of the 02 system. To support a reactive capability the object manager semantics has been extended, thus providing an efficient event detection. Applications produce events and the subscribed event types react to these events. As a result, rules are triggered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 706

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbxplorer : enabling keyword search over relational databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
