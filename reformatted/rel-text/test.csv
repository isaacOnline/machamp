prompt,label
"LEFT id: NA
RIGHT id: 1225

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 749

LEFT text: Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cluster validity methods : part i

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: maria halkidi , yannis batistakis , michalis vazirgiannis
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 679

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hierarchical subspace sampling : a unified framework for high dimensional data reduction , selectivity estimation and nearest neighbor search

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 991

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database system for real-time event aggregation in telecommunication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , stephen blott , henry f. korth , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 2227

LEFT text: The extensions support new data types such as point, circle, etc., and functions such as confains, interval, text-contains, etc. Let the tables Policies (policy-id, name, address, location, vehicle-type, . . .) and Claims (policy-id, claim-tag, accident-date, accident-location, accident-report, . . .) represent the partial schema containing both SQL’92 and user defined data types (UDTs). Consider a scenario in a targeted marketing application that requires a mailing list of all customers within 5 miles of point L, who have insured a ‘sports utility vehicle’ and were involved in a ‘rear-ended’ accident in the past 3 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic integration in heterogeneous databases using neural networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 590

LEFT text: This paper describes a model that integrates the execution of triggers with the evaluation of declarative constraints in SQL database systems. This model achieves full compatibility with the 1992 international standard for SQL (SQL92). It preserves the set semantics for declarative constraint evaluation while allowing the execution of powerful procedural triggers. It was implemented in DB2 for common servers and was recently accepted as the model for the emerging SQL standard (SQW).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1341

LEFT text: Recently, several query languages have been proposed for querying information sources whose data is not constrained by a schema, or whose schema is unknown. Examples include: LOREL (for querying data combined from several heterogeneous sources), W3QS (for querying the World Wide Web); and UnQL (for querying unstructured data). The natural data model for such languages is that of a rooted, labeled graph. Their main novelty is the ability to express queries which traverse arbitrarily long paths in the graph, typically described by a regular expression. Such queries however may prove difficult to evaluate in the case when the data is distributed on severalsites, with many edges going between sites. A typical case is that of a collection of WWW sites, with links pointing freely from one site to another (even forming cycles). A naive query shipping strategy may force the query to migrate back and forth between the various sites, leading to poor performance (or even non-termination). We present a technique for query decomposition, under which the query is shipped exactly once to every site, computed locally, then the local results are shipped to the client, and assembled here into the final result. This technique is efficient, in that (a) only data which is part of the final result is shipped from the data sites to the client site, and (b) the total work done locally at all sites does not exceed that needed for computing the (unoptimized) query on a centralized version of the database. We also show that the query decomposition technique can be adapted to derive a simple view maintenance method, for two forms of updates which we introduce for the graph data model.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: navigating large-scale semi-structured data in business portals

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: man abrol , neil latarche , uma mahadevan , jianchang mao , rajat mukherjee , prabhakar raghavan , michel tourn , john wang , grace zhang
",y
"LEFT id: NA
RIGHT id: 1074

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 2244

LEFT text: A major problem in today's information-driven world is that sharing heterogeneous, semantically rich data is incredibly difficult. Piazza is a peer data management system that enables sharing heterogeneous data in a distributed and scalable way. Piazza assumes the participants to be interested in sharing data, and willing to define pairwise mappings between their schemas. Then, users formulate queries over their preferred schema, and a query answering system expands recursively any mappings relevant to the query, retrieving data from other peers. In this paper, we provide a brief overview of the Piazza project including our work on developing mapping languages and query reformulation algorithms, assisting the users in defining mappings, indexing, and enforcing access control over shared data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the piazza peer data management project

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: igor tatarinov , zachary ives , jayant madhavan , alon halevy , dan suciu , nilesh dalvi , xin ( luna ) dong , yana kadiyska , gerome miklau , peter mork
",y
"LEFT id: NA
RIGHT id: 873

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",y
"LEFT id: NA
RIGHT id: 1629

LEFT text: The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the montage extensible datablade architecture

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael ubell
",n
"LEFT id: NA
RIGHT id: 819

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: networked data management design points

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: james r. hamilton
",n
"LEFT id: NA
RIGHT id: 807

LEFT text: Parallel database systems have to support the effective parallelization of complex queries in multi-user mode, i.e. in combination with inter-query~mter-transaction parallelism. For this purpose, dynamic scheduling and load balancing strategies’ are necessary that umsider the current system state for dekrminhg the degree of intra-query parallelism and for selecting the processors for executing subqueries. We study these issues for parallel hash joinprocessing and show that the two subproblems should be addressed in au integrated way. Even more importantly, however, is the use of a multimannce load balancing approach that considers all potential bottleneck resources. in particular memory, disk and CPU. We discuss basic performance tradeoffs to consider and evalGate the performauce of several oad balancing strategies by means of a detailed simulation model. Simulation results will be analyzed for multiuser configurations with both homogeneous andheterogeneous (query/OLTP) workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing for parallel association rule mining on heterogenous pc cluster systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: masahisa tamura , masaru kitsuregawa
",n
"LEFT id: NA
RIGHT id: 1647

LEFT text: We present new techniques for efficient garbage collection in a large persistent object store. The store is divided into partitions that are collected independently using information about inter-partition references. This information is maintained on disk so that it can be recovered after a crash. We use new techniques to organize and update this information while avoiding disk accesses. We also present a new global marking scheme to collect cyclic garbage across partitions. Global marking is piggybacked on partitioned collection; the result is an efficient scheme that preserves the localized nature of partitioned collection, yet is able to collect all garbage. We have implemented the part of garbage collection responsible for maintaining information about inter-partition references. We present a performance study to evaluate this work; the results show that our techniques result in substantial savings in the usage of disk and memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partition selection policies in object database garbage collection

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jonathan e. cook , alexander l. wolf , benjamin g. zorn
",n
"LEFT id: NA
RIGHT id: 2129

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1880

LEFT text: We present a scalable distributed data structure called LH*. LH* generalizes Linear Hashing (LH) to distributed RAM and disk files. An LH* file can be created from records with primary keys, or objects with OIDs, provided by any number of distributed and autonomous clients. It does not require a central directory, and grows gracefully, through splits of one bucket at a time, to virtually any number of servers. The number of messages per random insertion is one in general, and three in the worst case, regardless of the file size. The number of messages per key search is two in general, and four in the worst case. The file supports parallel operations, e.g., hash joins and scans. Performing a parallel operation on a file of M buckets costs at most 2M + 1 messages, and between 1 and O(log2 Mrounds of messages. We first describle the basic LH* scheme where a coordinator site manages abucket splits, and splits a bucket every time a collision occurs. We show that the average load factor of an LH* file is 65%–70% regardless of file size, and bucket capacity. We then enhance the scheme with load control, performed at no additional message cost. The average load factor then increases to 80–95%. These values are about that of LH, but the load factor for LH* varies more. We nest define LH* schemes without a coordinator. We show that insert and search costs are the same as for the basic scheme. The splitting cost decreases on the average, but becomes more variable, as cascading splits are needed to prevent file overload. Next, we briefly describe two variants of splitting policy, using parallel splits and presplitting that should enhance performance for high-performance applications. All together, we show that LH* files can efficiently scale to files that are orders of magnitude larger in size than single-site files. LH* files that reside in main memory may also be much faster than single-site disk files. Finally, LH* files can be more efficient than any distributed file with a centralized directory, or a static parallel or distributed hash file.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: lh * - a scalable , distributed data structure

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: witold litwin , marie-anna neimat , donovan a. schneider
",y
"LEFT id: NA
RIGHT id: 1667

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1742

LEFT text: This paper presents an algorithm, called ARIES/CSA (Algorithm for Recovery and Isolation Exploiting Semantics for Client-Server Architectures), for performing recovery correctly in client-server (CS) architectures. In CS, the server manages the disk version of the database. The clients, after obtaining database pages from the server, cache them in their buffer pools. Clients perform their updates on the cached pages and produce log records. The log records are buffered locally in virtual storage and later sent to the single log at the server. ARIES/CSA supports a write-ahead logging (WAL), fine-granularity (e.g., record) locking, partial rollbacks and flexible buffer management policies like steal and no-force. It does not require that the clocks on the clients and the server be synchronized. Checkpointing by the server and the clients allows for flexible and easier recovery.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aries/csa : a method for database recovery in client-server architectures

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: c. mohan , inderpal narang
",y
"LEFT id: NA
RIGHT id: 778

LEFT text: A load, such as a logic network of the TTL type, is connected in parallel across a plurality of direct-current sources designed to maintain a substantially constant operating voltage. Each source includes a control unit which compares the load voltage with a reference level in order to stabilize the output voltage of an associated current generator at that level. If the generator current drops below a certain minimum value, however, a threshold sensor in the control unit raises the reference level up to an amount equaling about twice the maximum divergence possible between the reference levels of different control units, thereby ensuring that all sources contribute simultaneously to the load current.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",y
"LEFT id: NA
RIGHT id: 1046

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 835

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: finding intensional knowledge of distance-based outliers

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1531

LEFT text: We are pleased to announce an excellent technical program for the 6th International Conference on Pervasive Computing and Communications. The program covers a broad cross section of topics in pervasive computing and communications. This year, 160 papers were submitted for consideration to the program committee. As a result, the selection process was highly competitive, and the result is a program of high-quality papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infoharness : a system for search and retrieval of heterogeneous information

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leon shklar , amit sheth , vipul kashyap , satish thatte
",y
"LEFT id: NA
RIGHT id: 48

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 531

LEFT text: Not very long ago, we discussed the creation of a new part of SQL, XML-Related Specifications (SQL/XML), in this column [1]. At the time, we referred to the work that had been done as ""infrastructure"". We are pleased to be able to say that significant progress has been made, and SQL/XML [2] is now going out for the first formal stage of processing, Final Committee Draft (FCD) ballot, in ISO/IEC JTC1.In our previous column, we described the mapping of SQL 〈identifier〉s to XML Names, SQL data types to XML Schema data types, and SQL values to XML values. There have been a few small corrections and enhancements in these areas, but for the most part the descriptions in our previous column are still accurate.Thc new work that we will discuss in this column comes in three parts. The first part provides a mapping from a single table, all tables in a schema, or all tables in a catalog to an XML document. The second of these parts includes the creation of an XML data type in SQL and adds functions that create values of this new type. These functions allow a user to produce XML from existing SQL data. Finally, the ""infrastructure"" work that we described in our previous article included the mapping of SQL's predefined data types to XML Schema data types. This mapping has been extended to include the mapping of domains, distinct types, row types, arrays, and multisets.The FCD ballot that we mentioned began in early April. This will allow the comments contained in the ballot responses to be discussed at the Editing Meeting in September or October of this year. We expect the Editing Meeting to recommend progression to Final Draft International Status (FDIS) ballot, which suggests that an International Standard will be published by the middle of 2003.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql/xml is making good progress

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: andrew eisenberg , jim melton
",y
"LEFT id: NA
RIGHT id: 400

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 704

LEFT text: Outlier detection is an integral component of statistical modelling and estimation. For high-dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. The cut-off value is obtained from the asymptotic distribution of the distance, which enables us to control the Type I error and deliver robust outlier detection. Simulation studies show that the proposed method behaves well for high-dimensional data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hd-eye : visual clustering of high dimensional data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim , markus wawryniuk
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",y
"LEFT id: NA
RIGHT id: 1579

LEFT text: The advent of new database applications such as engineering design stresses the need for new functie nalities in database systems. It includes the management of multiple representations for database objects, long transactions as well as dynamic data structures. This paper presents the approach used in CADB, a prototype expert database system dedicated to CAD, for the management and control of the consistency of design objects. It concerns both the operations on the object property values and the interactive manipulation of their structure. They involve concepts of the object models as well as the application semantics. They rely therefore on concepts used for representing the design objects and on semantic notions related to expert knowledge in the application domain. Among these are the notions of consistency and of completeness of the objects. These two complementary aspects are detailed and their relationships described. The emphasis is on the heuristic rules that provide a unified knowledge-based approach for their management independent of the particular application being considered. Dynamic inheritance mechanisms are also presented that sup port the manipulations performed on the object structures. It is shown how they help providing expert database facilities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography of benchmarks for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1216

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo atzeni , alberto o. mendelzon
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 445

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and cost-effective techniques for browsing and indexing large video databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghwan oh , kien a. hua
",n
"LEFT id: NA
RIGHT id: 942

LEFT text: In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast computation of sparse datacubes

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kenneth a. ross , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1381

LEFT text: The ability to store, access and disseminate large amounts of data has altered the social, educational, and governmental landscape throughout the world. The developments in technology, policy, and the economics of computing have been observed by some governments as providing a means to make government agencies more responsive to each others' as well as citizens' needs and also make government actions transparent and accountable to citizens. This panel brings together technocrats who are at the forefront of …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: social , educational , and governmental change enabled through information technology

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: krithi ramamritham , yeha el atfi , carlo batini , michael eitan , valerie gregg , d. b. phatak
",y
"LEFT id: NA
RIGHT id: 1313

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 2015

LEFT text: Many applications today run in a multi-tier environment with browser-based clients, mid-tier (application) servers and a backend database server. Mid-tier database caching attempts to improve system throughput and scalability by offloading part of the database workload to intermediate database servers that partially replicate data from the backend server. The fact that some queries are offloaded to an intermediate server should be completely transparent to applications one of the key distinctions between caching and replication. MTCache is a prototype mid-tier database caching solution for SQL Server that achieves this transparency. It builds on SQL Server's support for materialized views, distributed queries and replication. This paper describes MTCache and reports experimental results on the TPC-W benchmark. The experiments show that a significant part of the query workload can be offloaded to cache servers, resulting in greatly improved scale-out on the read-dominated workloads of the benchmark. Replication overhead was small with an average replication delay of less than two seconds.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: transparent mid-tier database caching in sql server

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: per - &#197; ke larson , jonathan goldstein , jingren zhou
",y
"LEFT id: NA
RIGHT id: 1114

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 1616

LEFT text: Abstract: Technological advances in and convergence of the World Wide Web with Electronic Data Interchange and standard middleware such as CORBA have given rise to a new computing paradigm based on a loosely coupled service-oriented architecture called Web Services. Direct machine to machine interactions that were hitherto deemed infeasible are now possible due to the rapid technological advances in XML and SOAP technologies. There is, however, an unprecedented hype surrounding this new paradigm. In reality, the Web services paradigm lacks a precise definition. Furthermore, the usecase scenarios for Web services and its pros and cons compared to existing, well-defined middleware technologies such as CORBA are not well understood.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: coss : the common object services specifications

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: bruce e. martin
",y
"LEFT id: NA
RIGHT id: 2126

LEFT text: Welcome to IPDPS 2004 in Santa Fe. This year’s program includes 17 workshops with a total of 306 papers. Many of the workshops have grown steadily in strength and are now operating with parallel sessions or on multiple days. We are pleased to welcome one new workshop this year, in the area of High Performance Grid Computing. As always, we are looking for new workshop proposals for the next IPDPS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues for data communication in mobile ad-hoc network database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leslie d. fife , le gruenwald
",n
"LEFT id: NA
RIGHT id: 397

LEFT text: Today's E-Commerce systems are a complex assembly of databases, web servers, home grown glue code, and networking services for security and scalability. The trend is towards larger pieces of these coming together in bundled offerings from leading software vendors, and the networking/hardware being offered through service delivery companies. In this paper we examine the bundle by looking in detail at IBM's WebSphere, Commerce Edition, and its deployment at a major customer site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: anatomy of a real e-commerce system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: anant jhingran
",y
"LEFT id: NA
RIGHT id: 1666

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner, that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ptool : a scalable persistent object manager

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. l. grossman , x. qin
",n
"LEFT id: NA
RIGHT id: 1031

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 902

LEFT text: XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a graphical query language for mobile information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ya-hui chang
",n
"LEFT id: NA
RIGHT id: 943

LEFT text: As XML seems to become the preferred candidate language for the interchange of data on the Internet, the integration of distributed, heterogeneous, and autonomous XML data sources in a mediation architecture is becoming a critical issue. In this paper, we present a novel and original query rewriting algorithm for the answering of queries to XML disparate sources in the presence of XML keys. The algorithm combines features of the MiniCon (Mini-Con descriptions) and the Styx algorithms (prefix and suffix queries) into an algorithm that returns more rewritings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: merging ranks from heterogeneous internet sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: luis gravano , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: This paper describes the design and implementation of an OODBMS, namely the METU Object-Oriented DBMS (MOOD). MOOD [Dog 94b] is developed on the Exodus Storage Manager (ESM) [ESM 92] and therefore some of the kernel functions like storage management, concurrency control, backup and recovery of data were readily available through ESM. In addition ESM has a client-server architecture and each MOOD process is a client application in ESM. The kernel functions provided by MOOD are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management. SQL statements are interpreted whereas functions (which have been previously compiled with C++) within SQL statements are dynamically linked and executed. A query optimizer is implemented by using the Volcano Query Optimizer Generator. A graphical user interface, namely Mood-View [Arp 93a, Arp 93b], is developed using Motif. MoodView displays both the schema information and the query results graphically. Additionally it is possible to update the database schema and to traverse the references in query results graphically.The system is coded in GNU C++ on Sun Sparc 2 workstations. MOOD has a SQL-like object-oriented query language, namely MOODSQL [Ozk 93b, Dog 94c]. MOOD type system is derived from C++, thus eliminating the impedance mismatch between MOOD and C++. The users can also access the MOOD Kernel from their application programs written in C++. For this purpose MOOD Kernel defines a class named UserRequest that contains a method for the execution of MOODSQL statements. The MOOD source code is available both for anonymous ftp users from ftp.cs.wisc.edu and for the WWW users from the site http://www.srdc.metu.edu.tr along with its related documents.In MOOD, each object is given a unique Object Identifier (OID) at object creation time by the ESM which is the disk start address of the object returned by the ESM. Object encapsulation is considered in two parts, method encapsulation and attribute encapsulation. These encapsulation properties are similar to the public and private declarations of C++.Methods can be defined in C++ by users to manipulate user defined classes and after compilation, they are dynamically linked and executed during the interpretation of SQL statements. This late binding facility is essential since database environments enforce run-time modification of schema and objects. With our approach, the interpretation of functions are avoided thus increasing the efficiency of the system. Dynamic linking primitives are implemented by the use of the shared object facility of SunOS [Sun 90]. Overloading is realized by making use of the signature concept of C++.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1195

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing and optimization in oracle rdb

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: gennady antoshenkov , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 1376

LEFT text: This paper describes a model that integrates the execution of triggers with the evaluation of declarative constraints in SQL database systems. This model achieves full compatibility with the 1992 international standard for SQL (SQL92). It preserves the set semantics for declarative constraint evaluation while allowing the execution of powerful procedural triggers. It was implemented in DB2 for common servers and was recently accepted as the model for the emerging SQL standard (SQW).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating the ub-tree into a database system kernel

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: frank ramsak , volker markl , robert fenk , martin zirkel , klaus elhardt , rudolf bayer
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: We introduce a new model of similarity of time sequences that captures the intuitive notion that two sequences should be considered similar if they have enough non-overlapping time-ordered pairs of subsequences thar are similar. The model allows the amplitude of one of the two sequences to be scaled by any suitable amount and its offset adjusted appropriately. Two subsequences are considered similar if one can be enclosed within an envelope of a specified width drawn around the other. The model also allows non-matching gaps in the matching subsequences. The matching subsequences need not be aligned along the time axis. Given this model of similarity, we present fast search techniques for discovering all similar sequences in a set of sequences. These techniques can also be used to find all (sub)sequences similar to a given sequence. We applied this matching system to the U.S. mutual funds data and discovered interesting matches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",n
"LEFT id: NA
RIGHT id: 118

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in global information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: a. m. ouksel , a. sheth
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 762

LEFT text: The MultiView Project is an ongoing 5-year NFS-ftrnded effort at the University of Michigan to develop and apply object-oriented view technology to address the needs of recently emerging applications such as data warehousing and workflow management systems that require the sharing, virtual restructuring, and caching of data [5]. Through Multi-View, users can dynamically create and modify virtual classes and schemata at any time .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: tutorial : application servers and associated technologies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 1088

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sampling large databases for association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hannu toivonen
",n
"LEFT id: NA
RIGHT id: 1254

LEFT text: This paper gives an overview of the IRO-DB architecture and describes in detail the cost evaluator currently under elaboration for the next version of the distributed query optimizer. The cost model is composed of a set of mathematical formulas with coefficients to estimate the cost of the search operators. The coefficients are deduced from a calibrating objectoriented database composed of linked collections of objects. A tuning application is run on each local site to adjust the cost formulas and fix the coefficients. We report on the tuning of O2 and ObjectStore. We show that the estimation is quite accurate for path traversals with the OO7 benchmark on top of ObjectStore.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 71

LEFT text: In this paper, we propose two update propagation strategies that improve freshness. Both of them use immediate propagation: updates to a primary copy are propagated towards a slave node as soon as they are detected at the master node without waiting for the commitment of the update transaction. Our performance study shows that our strategies can improve data freshness by up to five times compared with the deferred approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: synchronizing a database to improve freshness

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghoo cho , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 696

LEFT text: As Internet traffic continues to grow and web sites become increasingly complex, performance and scalability are major issues for web sites. Web sites are increasingly relying on dynamic content generation applications to provide web site visitors with dynamic, interactive, and personalized experiences. However, dynamic content generation comes at a cost --- each request requires computation as well as communication across multiple components.To address these issues, various dynamic content caching approaches have been proposed. Proxy-based caching approaches store content at various locations outside the site infrastructure and can improve Web site performance by reducing content generation delays, firewall processing delays, and bandwidth requirements. However, existing proxy-based caching approaches either (a) cache at the page level, which does not guarantee that correct pages are served and provides very limited reusability, or (b) cache at the fragment level, which requires the use of pre-defined page layouts. To address these issues, several back end caching approaches have been proposed, including query result caching and fragment level caching. While back end approaches guarantee the correctness of results and offer the advantages of fine-grained caching, they neither address firewall delays nor reduce bandwidth requirements.In this paper, we present an approach and an implementation of a dynamic proxy caching technique which combines the benefits of both proxy-based and back end caching approaches, yet does not suffer from their above-mentioned limitations. Our dynamic proxy caching technique allows granular, proxy-based caching where both the content and layout can be dynamic. Our analysis of the performance of our approach indicates that it is capable of providing significant reductions in bandwidth. We have also deployed our proposed dynamic proxy caching technique at a major financial institution. The results of this implementation indicate that our technique is capable of providing order-of-magnitude reductions in bandwidth and response times in real-world dynamic Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: proxy-based acceleration of dynamically generated content on the world wide web : an approach and implementation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: anindya datta , kaushik dutta , helen thomas , debra vandermeer , suresha , krithi ramamritham
",y
"LEFT id: NA
RIGHT id: 1758

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1797

LEFT text: Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources; (2) introduce the notion of transient-views — materialized views that exist only in the context of execution of a query — that is useful for minimizing the redundancies involved in the execution of these queries; (3) develop a cost-based algorithm that takes a query plan as input and generates an optimal “covering plan”, by minimizing redundancies in the original plan; (4) validate our approach by means of an implementation of the algorithms and a detailed performance study based on TPC-D benchmark queries on a commercial database system; and finally, (5) compare and contrast our approach with work in related areas, in particular, the areas of answering queries using views and optimization using common sub-expressions. Our experiments demonstrate the practicality and usefulness of transient-views in significantly improving the performance of decision support queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based optimization of decision support queries using transient-views

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: subbu n. subramanian , shivakumar venkataraman
",y
"LEFT id: NA
RIGHT id: 2036

LEFT text: We present an approach to database interoperation that exploits the semantic information provided by integrity constraints defined on the component databases. We identify two roles of integrity constraints in database interoperation. First, a set of integrity constraints describing valid states of the integrated view can be derived from the constraints defined on the underlying databases. Moreover, local integrity constraints can be used as a semantic check on the validity of the specification of the integrated view. We illustrate our ideas in the context of an instance-based database interoperation paradigm, where objects rather than classes are the unit of integration. We introduce the notions of objectivity and subjectivity as an indication of whether a constraint is valid beyond the context of a specific database, and demonstrate the impact of these notions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: capturing both types and constraints in data integration

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: michael benedikt , chee-yong chan , wenfei fan , juliana freire , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: ROLEX is a research system for closely coupled XML-relational interoperation [2]. Whereas typical XML-based applications interoperate with existing relational databases via a “shred-and-publish” approach, the ROLEX system seeks to provide direct access to relational data via XML interfaces at the speed of cached XML data. To achieve this, ROLEX is integrated tightly with both the DBMS and the application through a standard interface supported by most XML parsers, the Document Object Model (DOM). Thus, in general, an application need not be modified to be used with ROLEX.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic. Probably because many problems remained unsolved, most research works were only able to address separate topics, without a clear context of an overall application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 2290

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 961

LEFT text: We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits ""horizontal"" aggregation and even aggregation over more general ""blocks"" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: nd-sql : a multi-dimensional language for interoperability and olap

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan
",y
"LEFT id: NA
RIGHT id: 513

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 345

LEFT text: In this article, we first describe the XFilter and YFilter approaches and present results of a detailed performance comparison of structure matching for these algorithms as well as a hybrid approach. The results show that the path sharing employed by YFilter can provide order-of-magnitude performance benefits. We then propose two alternative techniques for extending YFilter's shared structure matching with support for value-based predicates, and compare the performance of these two techniques. The results of this latter study demonstrate some key differences between shared XML filtering and traditional database query processing. Finally, we describe how the YFilter approach is extended to handle more complicated queries containing nested path expressions

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the design and performance evaluation of alternative xml storage strategies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: feng tian , david j. dewitt , jianjun chen , chun zhang
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 231

LEFT text: In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: standards in practice

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: andrew eisenberg , jim melton
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 899

LEFT text: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia’s Programming Systems Laboratory (Section 7).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in information managment at dublin city university

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mark roantree , alan f. smeaton
",n
"LEFT id: NA
RIGHT id: 2054

LEFT text: Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - ""given the current information, it is possible that X will become true at time t"" - instead of no information at all.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpath queries on streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: feng peng , sudarshan s. chawathe
",n
"LEFT id: NA
RIGHT id: 640

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rate-based query optimization for streaming information sources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: stratis d. viglas , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 626

LEFT text: This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: interviso : dealing with the complexity of federated database access

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: marjorie templeton , herbert henley , edward maros , darrel j. van buer
",n
"LEFT id: NA
RIGHT id: 1002

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: Hello Everyone, I hope you all enjoyed your summer as our thoughts now turn to fall and all the wonders it brings. In this issue, I am responding to several assisted living (AL) nursing concerns we have received regarding advance directives (ADs).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 1565

LEFT text: This paper describes GnatDb, which is an embedded database system that provides protection against both accidental and malicious corruption of data. GnatDb is designed to run on a wide range of appliances, some of which have very limited resources. Therefore, its design is heavily driven by the need to reduce resource consumption. GnatDb employs atomic and durable updates to protect the data against accidental corruption. It prevents malicious corruption of the data using standard cryptographic techniques that leverage the underlying log-structured storage model. We show that the total memory consumption of GnatDb, which includes the code footprint, the stack and the heap, does not exceed 11 KB, while its performance on a typical appliance platform remains at an acceptable level.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. dogac , c. dengi , e. kilic , g. ozhan , f. ozcan , s. nural , c. evrendilek , u. halici , b. arpinar , p. koksal , n. kesim , s. mancuhan
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: Decision support applications are growing in popularity as more business data is kept on-line. Such applications typically include complex SQL queries that can test a query optimizer's ability to produce an efficient access plan. Many access plan strategies exploit the physical ordering of data provided by indexes or sorting. Sorting is an expensive operation, however. Therefore, it is imperative that sorting is optimized in some way or avoided all together. Toward that goal, this paper describes novel optimization techniques for pushing down sorts in joins, minimizing the number of sorting columns, and detecting when sorting can be avoided because of predicates, keys, or indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",y
"LEFT id: NA
RIGHT id: 1171

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating symbolic images into a multimedia database system using classification and abstraction approaches

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: aya soffer , hanan samet
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 280

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view and index selection tool for microsoft sql server 2000

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1801

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: simultaneous optimization and evaluation of multiple dimensional queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yihong zhao , prasad m. deshpande , jeffrey f. naughton , amit shukla
",n
"LEFT id: NA
RIGHT id: 1899

LEFT text: The language Java is enjoying a rapid rise in popularity as an application programming language. For many applications an effective provision of database facilities is required. Here we report on a particular approach to providing such facilities, called “orthogonal persistence”. Persistence allows data to have lifetimes that vary from transient to (the best approximation we can achieve to) indefinite. It is orthogonal persistence if the available lifetimes are the same for all kinds of data. We aim to show that the programmer productivity gains and possible performance gains make orthogonal persistence a valuable augmentation of Java.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an orthogonally persistent java

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: m. p. atkinson , l. dayn &#232; s , m. j. jordan , t. printezis , s. spence
",y
"LEFT id: NA
RIGHT id: 1075

LEFT text: We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications. The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1055

LEFT text: VideoAnywhere has developed such a capability in the form of an extensible architecture as well as a specific implementation using the latest in Internet programming (Java, agents, XML, etc.) and applicable standards. It automatically extracts and manages an extensible set of metadata of major types of videos that can be queried using either attribute-based or keyword-based search. It also provides user profiling that can be combined with the query processing for filtering. A user-friendly interface provides management of all system functions and capabilities. VideoAnywhere can also be used as a video search engine for the Web, and a servlet-based version has also been implemented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the role of integrity constraints in database interoperation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark w. w. vermeer , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 455

LEFT text: In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: theory of answering queries using views

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 307

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view selection and maintenance using multi-query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hoshi mistry , prasan roy , s. sudarshan , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1991

LEFT text: Arrays are a common and important class of data. At present, database systems do not provide adequate array support: arrays can neither be easily defined nor conveniently manipulated. Further, array manipulations are not optimized. This paper describes a language called the Array Manipulation Language (AML), for expressing array manipulations, and a collection of optimization techniques for AML expressions.In the AML framework for array manipulation, arbitrary externally-defined functions can be applied to arrays in a structured manner. AML can be adapted to different application domains by choosing appropriate external function definitions. This paper concentrates on arrays occurring in databases of digital images such as satellite or medical images.AML queries can be treated declaratively and subjected to rewrite optimizations. Rewriting minimizes the number of applications of potentially costly external functions required to compute a query result. AML queries can also be optimized for space. Query results are generated a piece at a time by pipelined execution plans, and the amount of memory required by a plan depends on the order in which pieces are generated. An optimizer can consider generating the pieces of the query result in a variety of orders, and can efficiently choose orders that require less space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query execution techniques for caching expensive methods

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 33

LEFT text: Galax is a light-weight, portable, open-source implementation of XQuery 1.0. Started in December 2000 as a small prototype designed to test the XQuery static type system, Galax has now become a solid implementation, aiming at full conformance with the family of XQuery 1.0 specifications. Because of its completeness and open architecture, Galax also turns out to be a very convenient platform for researchers interested in experimenting with XQuery optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementing the spirit of sql-99

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: paul brown
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 128

LEFT text: My purpose here at this conference is to provide a background, first for meetings like this on subjects dealing with computers, and second, for this particular meeting. My qualifications for keynoting are, I think, as good as those of anyone here: as far as this particular meeting is concerned, I am not now directly engaged in working on computers of the type that we are going to discuss; and the organization that I represent, the Bell Telephone Laboratories, is not in the business of making such computers. So I speak as a relatively innocent bystander.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: keynote address

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: robert s. epstein
",n
"LEFT id: NA
RIGHT id: 1063

LEFT text: In this paper, we present an original and complete methodology for supervising relational query processing in shared nothing systems. A new control mechanism is introduced which allows the detection and the correction of optimizer estimation errors and load imbalance. We especially focus on the management of intraprocessor communication and on the overlapping of communication and computation. Performance evaluations on an hypercube and a grid interconnection machine show the efficiency and the robustness of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious algorithms for relational query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ambuj shatdal , chander kant , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 952

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: This is the seventh bibliography concerning temporal databases. In this bibliography, we collect 331 new temporal databases papers. Most of these papers were published in 1996-1997, some in 1995 and some will appear in 1997 or 1998.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 639

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tigukat : a uniform behavioral objectbase management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu , randal peters , duane szafron , boman irani , anna lipka , adriana mu &#241; oz
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 2177

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a requirement-based approach to data modeling and re-engineering

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alice h. muntz , christian t. ramiller
",n
"LEFT id: NA
RIGHT id: 1072

LEFT text: In this paper we introduce generalized projections (G P an extension of duplicateeliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinctand duplicate-preserving projections in a common unified framework. Using G P s we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for materialized view design in data warehousing environment

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jian yang , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1311

LEFT text: The number, size, and user population of bibliographic and full-text document databases are rapidly growing. With a high document arrival rate, it becomes essential for users of such databases to have access to the very latest documents; yet the high document arrival rate also makes it difficult for users to keep themselves updated. It is desirable to allow users to submit profiles, i.e., queries that are constantly evaluated, so that they will be automatically informed of new additions that may be of interest. Such service is traditionally called Selective Dissemination of Information (SDI).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index structures for selective dissemination of information under the boolean model

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , h &#233; ctor garc &#237; a-molina
",y
"LEFT id: NA
RIGHT id: 1597

LEFT text: We define a standard of effectiveness for a database calculus relative to a query language. Effectiveness judges suitability to serve as a processing framework for the query language, and comprises aspects of coverage, manipulability and efficient evaluation. We present the monoid calculus, and argue its effectiveness for object-oriented query languages, exemplified by OQL of ODMG-93. The monoid calculus readily captures such features as multiple collection types, aggregations, arbitrary composition of type constructors and nested query expressions. We also show how to extend the monoid calculus to deal with vectors and arrays in more expressive ways than current query languages do, and illustrate how it can handle identity and updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards an effective calculus for object query languages

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leonidas fegaras , david maier
",y
"LEFT id: NA
RIGHT id: 1771

LEFT text: One of the central tasks in managing, monitoring and mining data streams is that of identifying outliers. There is a long history of study of various outliers in statistics and databases, and a recent focus on mining outliers in data streams. Here, we adopt the notion of ""deviants"" from Jagadish et al. (1999) as outliers. Deviants are based on one of the most fundamental statistical concept of standard deviation (or variance). Formally, deviants are defined based on a representation sparsity metric, i.e., deviants are values whose removal from the dataset leads to an improved compressed representation of the remaining items. Thus, deviants are not global maxima/minima, but rather these are appropriate local aberrations. Deviants are known to be of great mining value in time series databases. We present first-known algorithms for identifying deviants on massive data streams. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: This paper presents an algorithm, called ARIES/CSA (Algorithm for Recovery and Isolation Exploiting Semantics for Client-Server Architectures), for performing recovery correctly in client-server (CS) architectures. In CS, the server manages the disk version of the database. The clients, after obtaining database pages from the server, cache them in their buffer pools. Clients perform their updates on the cached pages and produce log records. The log records are buffered locally in virtual storage and later sent to the single log at the server. ARIES/CSA supports a write-ahead logging (WAL), fine-granularity (e.g., record) locking, partial rollbacks and flexible buffer management policies like steal and no-force. It does not require   that the clocks on the clients and the server be synchronized. Checkpointing by the server and the clients allows for flexible and easier recovery.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 2006

LEFT text: We have been developing a mobile passenger guide system for public transports. Passengers can make their travel plans and purchase necessary electronic tickets using mobile terminals via Internet. During the travel, the mobile terminal, which also works as an electronic ticket, compares the stored travel plan with the passenger's actual activities and offers appropriate guide messages. To execute this task, the mobile terminal collects various kinds of information about the travel fields (routes, fares, area maps, station maps, operation schedule, timetables, facilities of stations and vehicles etc.) using multi-channel data communications. The mobile terminal contains a personal database for the passenger by selecting and integrating necessary data according to the user's situation and characteristics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of electronic tickets and personal guide system for public transport using mobile terminals

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: koichi goto , yahiko kambayashi
",y
"LEFT id: NA
RIGHT id: 1353

LEFT text: Many sources on the Internet and elsewhere rank the objects in query results according to how well these objects match the original query. For example, a real-estate agent might rank the available houses according to how well they match the user's preferred location and price. In this environment, ``meta-brokers'' usually query multiple autonomous, heterogeneous sources that might use varying result-ranking strategies. A crucial problem that a meta-broker then faces is extracting from the underlying sources the top objects for a user query according to the meta-broker's ranking function. This problem is challenging because these top objects might not be ranked high by the sources where they appear. In this paper we discuss strategies for solving this ``meta-ranking'' problem. In particular, we present a condition that a source must satisfy so that a meta-broker can extract the top objects for a query from the source without examining its entire contents. Not only is this condition necessary but it is also sufficient, and we show an efficient algorithm to extract the top objects from sources that satisfy the given condition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 340

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distributed query evaluation on semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 382

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report : the first international workshop on active and real-time database systems ( artdb-95 )

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mikael berndtsson , j &#246; rgen hansson
",y
"LEFT id: NA
RIGHT id: 689

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1825

LEFT text: A data warehouse materializes views derived from data that may not reside at the warehouse. Maintaining these views effciently in response to base updates is diffcult, since it may involve querying external sources where the base data reside. This paper considers the problem of view self-maintenance, where the views are maintained without using all the base data. Without full use of the base data, however, maintaining a view unambiguously is not always possible. Thus, the two critical questions that must be addressed are to determine, in a given situation, whether a view is maintainable, and how to maintain it. W e provide algorithms that answer these questions for a general class of views, and for an important subclass, generate SQL queries that test whether a view is self-maintainable and update the view if it is. We improve significantly on previous work by solving the view self-maintenance problem in the presence of multiple views, with optional access to a subset of the base data, and under arbitrary mixes of insertions and deletions. We provide better insight into the problem by showing that view self-maintainability can be reduced to the problem of deciding query containment. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient view maintenance at data warehouses

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. agrawal , a. el abbadi , a. singh , t. yurek
",n
"LEFT id: NA
RIGHT id: 2000

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: winmagic : subquery elimination using window aggregation

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: calisto zuzarte , hamid pirahesh , wenbin ma , qi cheng , linqi liu , kwai wong
",n
"LEFT id: NA
RIGHT id: 219

LEFT text: Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of spatial join algorithms for processing multiple inputs

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: nikos mamoulis , dimitris papadias
",y
"LEFT id: NA
RIGHT id: 2290

LEFT text: Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",y
"LEFT id: NA
RIGHT id: 185

LEFT text: Client-server object-oriented database management systems differ significantly from traditional centralized systems in terms of their architecture and the applications they target. In this paper, we present the client-server architecture of the EOS storage manager and we describe the concurrency control and recovery mechanisms it employs. EOS offers a semi-optimistic locking scheme based on the multi-granularity two-version two-phase locking protocol. Under this scheme, multiple concurrent readers are allowed to access a data item while it is being updated by a single writer. Recovery is based on write-ahead redo-only logging.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient transparent application recovery in client-server information systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 869

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the challenge of process data warehousing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: matthias jarke , thomas list , j &#246; rg k &#246; ller
",y
"LEFT id: NA
RIGHT id: 1503

LEFT text: In a video-on-demand server, resource reservation is required to guarantee continuous delivery. Hence any given storage device (or a striping group treated as a single logical device) can serve only up to a fixed number of client access streams. Each storage device is also limited by the number of video files it can store. For the reasons of availability, incremental growth, and heterogeneity, there may be multiple storage devices in a video server environment. Hence, one or more copies of a particular video may be placed on different storage devices. Since the access rates to different videos are not uniform, there may be load imbalance among the devices. In this paper, we propose a dynamic placement policy (called the Bandwidth to Space Ratio (BSR) Policy) that creates and/or deletes replica of a video, and mixes hot and cold videos so as to make the best use of bandwidth and space of a storage device. The proposed policy is evaluated using a simulation study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an online video placement policy based on bandwidth to space ratio ( bsr )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: asit dan , dinkar sitaram
",y
"LEFT id: NA
RIGHT id: 1327

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: warehousing workflow data : challenges and opportunities

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: angela bonifati , fabio casati , umeshwar dayal , ming-chien shan
",n
"LEFT id: NA
RIGHT id: 1773

LEFT text: Data Warehousing embraces technology and industrial practice to systematically integrate data from multiple distributed data sources and to use that data in annotated and aggregated form to support business decision-making and enterprise management. Although many database techniques have been revisited or newly developed in the context of data warehouses, such as view maintenance and OLAP, little attention has been paid to the design, management and high quality service of the management of a given enterprise. Little attention is also paid to aspects that are intrinsic to the functionality and usage of data warehouses, either in the back-stage (like data cleansing or data extraction and loading) or in the front-end (like similar and uncertain queries, or what-if analysis). The DMDW workshop is intended as a forum to fill this gap.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1254

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 580

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to deductive database languages and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kotagiri ramamohanarao , james harland
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: We present a framework for designing, in a declarative and flexible way, efficient migration programs and an undergoing implementation of a migration tool called RelOO whose targets are any ODBC compliant system on the relational side and the 02 system on the object side. The framework consists of (i) a declarative language to specify database transformations from relations to objects, but also physical properties on the object database (clustering and sorting) and (ii) an algebrabased program rewriting technique which optimizes the migration processing time while taking into account physical properties and transaction decomposition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 997

LEFT text: Scheduling query execution plans is an important component of query optimization in parallel database systems. The problem is particularly complex in a shared-nothing execution environment, where each system node represents a collection of time-shareable resources (e.g., CPU(s), disk(s), etc.) and communicates with other nodes only by message-passing. Significant research effort has concentrated on only a subset of the various forms of intra-query parallelism so that scheduling and synchronization is simplified. In addition, most previous work has focused its attention on one-dimensional models of parallel query scheduling, effectively ignoring the potential benefits of resource sharing. In this paper, we develop an approach that is more general in both directions, capturing all forms of intra-query parallelism and exploiting sharing of multi-dimensional resource nodes among concurrent plan operators. This allows scheduling a set of independent query tasks (i.e., operator pipelines) to be seen as an instance of the multi-dimensional bin-design problem. Using a novel quantification of coarse grain parallelism, we present a list scheduling heuristic algorithm that is provably near-optimal in the class of coarse grain parallel executions (with a worst-case performance ratio that depends on the number of resources per node and the granularity parameter). We then extend this algorithm to handle the operator precedence constraints in a bushy query plan by splitting the execution of the plan into synchronized phases. Preliminary performance results confirm the effectiveness of our scheduling algorithm compared both to previous approaches and the optimal solution. Finally, we present a technique that allows us to relax the coarse granularity restriction and obtain a list scheduling method that is provably near-optimal in the space of all possible parallel schedules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: resource scheduling for composite multimedia objects

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis , banu &#214; zden
",n
"LEFT id: NA
RIGHT id: 936

LEFT text: Maintaining data consistency is known to be hard. Recent approaches have relied on integrity constraints to deal with the problem - correct and complete constraints naturally work towards data consistency. State-of-the-art data cleaning frameworks have used the formalism known as denial constraint (DC) to handle a wide range of real-world constraints. Each DC expresses a relationship between predicates that indicate which combinations of attribute values are inconsistent. The design of DCs, however, must keep pace with the complexity of data and applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using versions in update transactions : application to integrity checking

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fran &#231; ois llirbat , eric simon , dimitri tombroff
",n
"LEFT id: NA
RIGHT id: 1136

LEFT text: A brief overview of Nonstop SQL product will be followed by a description of what is different in ServerWare SQL. The current Nonstop SQL optimizer uses a traditional bottom-up dynamic programming optimizer. This is the same type of optimization algorithm used in System R and many commercial products. The optimizer in the new product is a top-down branch and bound ruledriven cost based optimizer similar to work done on the Volcano optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the query optimizer in tandem 's new serverware sql product

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: pedro celis
",y
"LEFT id: NA
RIGHT id: 344

LEFT text: E-commerce is not a static field, but is constantly evolving to discover new and more effective ways of supporting businesses. Data management is an integral part of this effort, This special issue aims to report on some of the recent developments and identify some research directions in this area. Initially, e-commeree involved the use of ED] and intranets. Today we see the dominance of XML. Almost all recent elecn'onic commerce standards are based on X1VD... As a consequence, the amount of XML data being stored is large, and it is increasing. This naturally leads to the question of how to store and query the XML documents. The paper by Tian, DeWitt, Chen and Zhang describes the design and performance evaluation of alternative XM]., storage strategies. The results of this performance study provide valuable hints on how to store the XM1., files depending on the application. Personalization in e-commerce is about building customer loyalty by understanding and thus addressing the needs of each individual. E-commerce systems need customers' profiles to provide better services, 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce : guest editor 's introduction

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 778

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: binding propagation in disjunctive databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sergio greco
",n
"LEFT id: NA
RIGHT id: 1868

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sentinel : an object-oriented dbms with event-based rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. chakravarthy
",n
"LEFT id: NA
RIGHT id: 640

LEFT text: Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rate-based query optimization for streaming information sources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: stratis d. viglas , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 986

LEFT text: We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation.We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we suggest heuristics that can be used to efficiently determine a useful set of additional views to materialize.Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 1505

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient maintenance of materialized mediated views

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1461

LEFT text: Multimedia applications demand specific support from database management systems due to the characteristics of multimedia data and their interactive usage. This includes integrated support for high-volume and time-dependent (continuous) data types like audio and video. One critical issue is to provide handling of continuous data streams including buffer management as needed for multimedia presentations. Buffer management strategies for continuous data have to consider specific requirements like providing for continuity of presentations, for immediate continuation of presentations after frequent user interactions by appropriate buffer resource consumption. Existing buffer management strategies do not sufficiently support the handling of continuous data streams in highly interactive multimedia presentations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: l/mrp : a buffer management strategy for interactive continuous data flows in a multimedia dbms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: frank moser , achim kraiss , wolfgang klas
",y
"LEFT id: NA
RIGHT id: 1650

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 611

LEFT text: The emergence and growing popularity of Internet-based electronic market-places, in their various forms, has raised the challenge to explore genericity in market design. In this paper we present a domain-specific software architecture that delineates the abstract components of a generic market and specifies control and data-flow constraints between them, and a framework that allows convenient pluggability of components that implement specific market policies. The framework was realized in the GEM system. GEM provides infrastructure services that allow market designers to focus solely on market-issues. In addition, it allows dynamic (re)configuration of components. This functionality can be used to change market-policies as the environment or market trends change, adding another level of flexibility to market designers and administrators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a conceptual architecture for semantic web enabled web services

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: christoph bussler , dieter fensel , alexander maedche
",n
"LEFT id: NA
RIGHT id: 1231

LEFT text: An overview of selected papers related to bone published in 2017 is provided.PurposeThis paper accompanies a lecture at the 2018 Belgian Bone Club annual Clinical Update Symposium held in Brussels on January 20th, discussing the best papers (in the opinion of the author) published in the previous year.MethodsA PubMed search using the keyword “bone” and articles published in 2017.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: foreword by the vldb '98 pc chairmen

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: o. shmueli , j. widom
",y
"LEFT id: NA
RIGHT id: 2259

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 340

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distributed query evaluation on semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dan suciu
",y
"LEFT id: NA
RIGHT id: 1427

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 1676

LEFT text: Abstract. This paper describes the overall design and architecture of the Timber XML database system currently being implemented at the University of Michigan. The system is based upon a bulk algebra for manipulating trees, and natively stores XML. New access methods have been developed to evaluate queries in the XML context, and new cost estimation and query optimization techniques have also been developed. We present performance numbers to support some of our design decisions. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as a deductive database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 1370

LEFT text: Conduct of scientific and engineering research is becoming critically dependent on effective management of scientific and engineering data and technical information. The rapid advances in scientific instrumentation, computer and communication technologies enable the scientists to collect, generate, process, and share unprecedented volumes of data. For example, the Earth Observing System Data and Information System (EOSDIS) has the task to manage the data from NASA’s Earth science research satellites and field measurement programs, and other data essential for the interpretation of these measurements in support of global change research. Apart from being able to handle a stream of 1 terabyte of data daily by the year 2000, EOSDIS will also need to provide transparent access to heterogeneous data held in the archives of several US government agencies, organizations and countries. A single graphical user interface employing the Global Change Master Directory needs to help users locate data sets of interest among massive and diverse data sets, or find the appropriate data analysis tools, regardless of their location. Another major international effort in the area of human genome research faces some similar, as well as unique issues due to the complexity of the genome data, special querying requirements and much more heterogeneous collections of data. Scientific databases can be viewed as critical repositories of knowledge, both existing and yet to be dis-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: panel : future directions of database research - the vldb broadening strategy , part 1

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-j &#246; rg schek
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 2086

LEFT text: We have implemented a compressor (XMilI) and decompressor (XDemill) for XML data, to be used in data exchange and archiving, which can be downloaded from http://www.research.att.com/sw/tools/xmill. XMill compresses about twice as good as gzip, at about the same speed. It does not need a DTD in order to compress, and preserves the input XML file faithfully, including element order, attributes order, PI 's, comments, the DTD, etc. A novelty in XMill is that it allows users to combine existing compressors in order to compress heterogeneous XML data: by default it uses zlib , a library function implementing gz ip ' s functionality, and includes some standard compression techniques for simple data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpress : a queriable compression for xml data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun-ki min , myung-jae park , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1171

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating symbolic images into a multimedia database system using classification and abstraction approaches

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: aya soffer , hanan samet
",n
"LEFT id: NA
RIGHT id: 135

LEFT text: Translators (wrappers) convert queries over information in the common del (OEM) into requests the source can execute. The data returned by the source is converted back into the common model. Mediators are programs that collect information from one or more sources, process and combine it, and export the resulting information to the end user or an application program. Users or applications can choose to interact either directly with the translators or indirectly via one or more mediators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: information gathering in the world-wide web : the w3ql query language and the w3qs system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david konopnicki , oded shmueli
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: Despite decades of research on AQP (approximate query processing), our understanding of sample-based joins has remained limited and, to some extent, even superficial. The common belief in the community is that joining random samples is futile. This belief is largely based on an early result showing that the join of two uniform samples is not an independent sample of the original join, and that it leads to quadratically fewer output tuples. Unfortunately, this early result has little applicability to the key questions practitioners face. For example, the success metric is often the final approximation's accuracy, rather than output cardinality. Moreover, there are many non-uniform sampling strategies that one can employ. Is sampling for joins still futile in all of these settings? If not, what is the best sampling strategy in each case? To the best of our knowledge, there is no formal study answering these questions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management. Even though the idea of randomized linear projections (a.k.a., sketches) was known for some time in the domain of functional analysis (dating back to the famous Johnson-Lindenstrauss Lemma), Alon, Matias, and Szegedy were the first to exploit sketches for small-space data-stream computation, through the use of limited-independence random variates that can be constructed in small space and time. Of course, in addition to small-space sketching, the AMS paper also makes a number of other fundamental contributions in data streaming, including practical approximation algorithms for other frequency moments (e.g., the number of distinct values in a stream), as well as several inapproximability results (i.e., lower bounds) based on beautiful communication-complexity arguments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 295

LEFT text: An ∈-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of N. We present a new online algorithm for computing∈-approximate quantile summaries of very large data sequences. The algorithm has a worst-case space requirement of &Ogr;(1÷∈ log(∈N)). This improves upon the previous best result of &Ogr;(1÷∈ log2(∈N)). Moreover, in contrast to earlier deterministic algorithms, our algorithm does not require a priori knowledge of the length of the input sequence. Finally, the actual space bounds obtained on experimental data are significantly better than the worst case guarantees of our algorithm as well as the observed space requirements of earlier algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: space-efficient online computation of quantile summaries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: michael greenwald , sanjeev khanna
",y
"LEFT id: NA
RIGHT id: 963

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: expiring data in a warehouse

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: hector garcia-molina , wilburt labio , jun yang
",n
"LEFT id: NA
RIGHT id: 1387

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: hypothetical queries in an olap environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: andrey balmin , thanos papadimitriou , yannis papakonstantinou
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 998

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 1012

LEFT text: We propose a new notion of surprising temporal patterns in market. basket data, and algorithms to find such pat,terns. This is distinct, from finding frequent pat-terns as addressed in the common mining literature. We argue that. once the analyst. is already familiar with prevalent patterns in t,he data, the greatest, increment,al benefit. is likely t,o be from changes in the relationship between item frequencies

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining surprising patterns using temporal description length

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: soumen chakrabarti , sunita sarawagi , byron dom
",y
"LEFT id: NA
RIGHT id: 1759

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on dart ' 96 : databases : active and real-time ( concepts meet practice )

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: krithi ramamritham , nandit soparkar
",n
"LEFT id: NA
RIGHT id: 601

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of data distribution for selectivity estimation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kyu young whang , sang wook kim , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 724

LEFT text: For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: tpc-ds , taking decision support benchmarking to the next level

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: meikel poess , bryan smith , lubor kollar , paul larson
",n
"LEFT id: NA
RIGHT id: 1392

LEFT text: In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of wavelet-based histograms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 347

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 298

LEFT text: Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stholes : a multidimensional workload-aware histogram

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri , luis gravano
",y
"LEFT id: NA
RIGHT id: 1350

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting incremental join queries on ranked inputs

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: apostol natsev , yuan-chi chang , john r. smith , chung-sheng li , jeffrey scott vitter
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1832

LEFT text: Our GC uses a new synchronization mechanism (mechanism that allows the GC to operate concurrently with ordinary users of the database), called CC-consistent cuts. A GC-consistent cut is a set of virtual copies of database pages. The copies are taken at times such that an object may appear as garbage in the cut only if it is garbage in the system. Our GC examines the copies, instead of the real database, in order to determine which objects are garbage. More sophisticated GCs can execute concurrently with the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partitioned garbage collection of a large object store

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: umesh maheshwari , barbara liskov
",n
"LEFT id: NA
RIGHT id: 796

LEFT text: We study indexing techniques for main memory, including hash indexes, binary search trees, T-trees, B+-trees, interpolation search, and binary search on arrays. In a decision-support context, our primary concerns are the lookup time, and the space occupied by the index structure. Our goal is to provide faster lookup times than binary search by paying attention to reference locality and cache behavior, without using substantial extra space. We propose a new indexing technique called Cache-Sensitive Search Trees (CSS-trees).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious indexing for decision-support in main memory

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jun rao , kenneth a. ross
",y
"LEFT id: NA
RIGHT id: 1233

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",y
"LEFT id: NA
RIGHT id: 1112

LEFT text: Data in relational databases is frequently stored and retrieved using B-Trees. In &cis,ion isugprt applications the key of the B-Tree frequently involves the concatenation of several fields of the relationdl’ table. During retrieval, it is desirable to be able to access a small subset of the table based’ on partial key information, where some fields of the key may either not be present, involve ranges, or lists ‘of values. It is also advantageous to altow. this type, of access-with gen&il expressions involving any combination of disjuncts on key columns. This paper &scribes a method whereby BTrees can be eficiently used to retrieve small subsets, thus avoiding large scans of potentially huge tables. Another benefit is the ability of this method to reduce the need for additional secondary indexes, thus saving space, maintenance cost, and random accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient search of multi-dimensional b-trees

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: harry leslie , rohit jain , dave birdsall , hedieh yaghmai
",y
"LEFT id: NA
RIGHT id: 2125

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1707

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: databases for gis

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: claudia bauzer medeiros , fatima pires
",y
"LEFT id: NA
RIGHT id: 1159

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1565

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. dogac , c. dengi , e. kilic , g. ozhan , f. ozcan , s. nural , c. evrendilek , u. halici , b. arpinar , p. koksal , n. kesim , s. mancuhan
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 400

LEFT text: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: disima : a distributed and interoperable image database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: vincent oria , m. tamer &#214; zsu , paul j. iglinski , shu lin , bin yao
",n
"LEFT id: NA
RIGHT id: 20

LEFT text: The theme of the paper is to promote research on asynchronous transactions. We discuss our experience of executing synchronous transactions on a large distributed production system in The Boeing Company. Due to the poor performance of synchronous transactions in our environment, it motivated the exploration of asynchronous transactions as an alternate solution. This paper presents the requirements and benefits/limitations of asynchronous transactions. Open issues related to large scale deployments of asynchronous transactions are also discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the need for distributed asynchronous transactions

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: lyman do , prabhu ram , pamela drew
",y
"LEFT id: NA
RIGHT id: 1799

LEFT text: Knowledge based applications require linguistic, terminological and ontological resources. These applications are used to fulfill a set of tasks such as semantic indexing, knowledge extraction from text, information retrieval, etc. Using these resources and combining them for the same application is a tedious task with different levels of complexity. This requires their representation in a common language, extracting the required knowledge and designing effective large scale storage structures offering operators for resources management. For instance, ontology repositories were created to address these issues by collecting heterogeneous ontologies. They generally offer a more effective indexing of these resources than general search engines by generating alignments and annotations to ensure their interoperability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1947

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: solving satisfiability and implication problems in database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sha guo , wei sun , mark a. weiss
",n
"LEFT id: NA
RIGHT id: 1426

LEFT text: A new access method, called M-tree, is proposed to organize and search large data sets from a generic “metric space”, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O’s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: near neighbor search in large metric spaces

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sergey brin
",n
"LEFT id: NA
RIGHT id: 2105

LEFT text: Skyline queries ask for a set of interesting points from a potentially large set of data points. If we are traveling, for instance, a restaurant might be interesting if there is no other restaurant which is nearer, cheaper, and has better food. Skyline queries retrieve all such interesting restaurants so that the user can choose the most promising one. In this paper, we present a new online algorithm that computes the Skyline. Unlike most existing algorithms that compute the Skyline in a batch, this algorithm returns the first results immediately, produces more and more results continuously, and allows the user to give preferences during the running time of the algorithm so that the user can control what kind of results are produced next (e.g., rather cheap or rather near restaurants).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: minicon : a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rachel pottinger , alon halevy
",n
"LEFT id: NA
RIGHT id: 2253

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a multidatabase system for tracking and retrieval of financial data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munir cochinwala , john bradley
",n
"LEFT id: NA
RIGHT id: 188

LEFT text: Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a multi-similarity algebra

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: s. adali , p. bonatti , m. l. sapino , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1488

LEFT text: Database systems have enjoyed a tremendous market because they have served many applications really well -- transaction processing in the beginning, and then decision support. Today, with over 200% cumulative growth rate in certain segments of E-Commerce, it is clear that this new class of applications will be a strong driver for databases to grow, commercially, as well as from a Research perspective. This paper outlines some of the issues that I have learnt in dealing with E-Commerce applications that may well be the focus of some of the research in database systems over the course of next few years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: versant replication : supporting fault-tolerant object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: yuh-ming shyy , h. stephen au-yeung , c. p. chou
",n
"LEFT id: NA
RIGHT id: 53

LEFT text: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamat : a dynamic view management system for data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1443

LEFT text: Many decision support systems, which utilize association rules for discovering interesting patterns, require the discovery of association rules that vary over time. Such rules describe complicated temporal patterns such as events that occur on the “first working day of every month.” In this paper, we study the problem of discovering how association rules vary over time. In particular, we introduce the idea of using a calendar algebra to describe complicated temporal phenomena of interest to the user. We then present algorithms for discovering calendric association rules, which are association rules that follow the patterns set forth in the user supplied calendar expressions. We devise various optimizations that speed up the discovery of calendric association rules. We show, through an extensive series of experiments, that these optimization techniques provide performance benefits ranging from to over a less sophisticated algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of multiple-level association rules from large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu
",n
"LEFT id: NA
RIGHT id: 104

LEFT text: Complex queries containing outer joins are, for the most part, executed by commercial DBMS products in an ""as written"" manner. Only a very few reorderings of the operations are considered and the benefits of considering comprehensive reordering schemes are not exploited. This is largely due to the fact there are no readily usable results for reordering such operations for relations with duplicates and/or outer join predicates that are other than ""simple."" Most previous approaches have ignored duplicates and complex predicates; the very few that have considered these aspects have suggested approaches that lead to a possibly exponential number of, and redundant intermediate joins. Since traditional query graph models are inadequate for modeling outer join queries with complex predicates, we present the needed hypergraph abstraction and algorithms for reordering such queries with joins and outer joins. As a result, the query optimizer can explore a significantly larger space of execution plans, and choose one with a low cost. Further, these algorithms are easily incorporated into well known and widely used enumeration methods such as dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 236

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: algebraic change propagation for semijoin and outerjoin queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: timothy griffin , bharat kumar
",n
"LEFT id: NA
RIGHT id: 1250

LEFT text: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data partitioning and load balancing in parallel disk systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter scheuermann , gerhard weikum , peter zabback
",n
"LEFT id: NA
RIGHT id: 1074

LEFT text: A new access method, called M-tree, is proposed to organize and search large data sets from a generic “metric space”, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O’s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1496

LEFT text: Current client-server object database management systems employ either a page server or an object server architecture. Both of these architectures have their respective strengths, but they also have key drawbacks for important system and workload configurations. We propose a new hybrid server architecture which combines the best features of both page server and object server architectures while avoiding their problems. The new architecture incorporates new or adapted versions of data transfer, recovery, and cache consistency algorithms; in this paper we focus only on the data transfer and recovery issues. The data transfer mechanism allows the hybrid server to dynamically behave as both page and object server. The performance comparison of the hybrid server with object and page servers indicates that the performance of the hybrid server is more robust than the others.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: upsizing form file server to client server architectures

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the access team microsoft
",n
"LEFT id: NA
RIGHT id: 120

LEFT text: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic service matchmaking among agents in open information environments

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: katia sycara , matthias klusch , seth widoff , jianguo lu
",n
"LEFT id: NA
RIGHT id: 2055

LEFT text: In this paper we propose an approach that enables mobile clients to determine the validity of previous queries based on their current locations. In order to make this possible, the server returns in addition to the query result, a validity region around the client's location within which the result remains the same. We focus on two of the most common spatial query types, namely nearest neighbor and window queries, define the validity region in each case and propose the corresponding query processing algorithms. In addition, we provide analytical models for estimating the expected size of the validity region. Our techniques can significantly reduce the number of queries issued to the server, while introducing minimal computational and network overhead compared to traditional spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: location-based spatial queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun zhang , manli zhu , dimitris papadias , yufei tao , dik lun lee
",y
"LEFT id: NA
RIGHT id: 1388

LEFT text: This paper provides an introduction to the major research directions in biodiversity informatics. The biodiversity enterprise is a vast and complex information domain. I describe the need to build infrastructure for this domain, major research thrusts needed to improve its work practices, and areas of research that could contribute to the advancement of the field. I emphasize that the science of biodiversity is fundamentally an information science, worthy of special attention from the computer and information science communities because of its distinctive attributes of scale and socio-technical complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: biodiversity informatics infrastructure : an information commons for the biodiversity community

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: gladys a. cotter , barbara t. bauldock
",n
"LEFT id: NA
RIGHT id: 1382

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design and implementation of a genetic-based algorithm for data mining

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sunil choenni
",n
"LEFT id: NA
RIGHT id: 1333

LEFT text: Mainstream database management systems are designed for general use. Various compromises have been done to satisfy the most common users and the largest markets. One application which has been mostly ignored, is the network equipment made for the telco operators. The equipment used in the telco industry has requirements differing from traditional database applications with respect to availability and real-time performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: online scaling in a highly available database

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: svein erik bratsberg , rune humborstad
",n
"LEFT id: NA
RIGHT id: 8

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: adept : an agent-based approach to business process management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: n. r. jennings , t. j. norman , p. faratin
",y
"LEFT id: NA
RIGHT id: 1513

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: applying update streams in a soft real-time database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: b. adelberg , h. garcia-molina , b. kao
",n
"LEFT id: NA
RIGHT id: 733

LEFT text: Today's Internet based businesses need a level of interoperability which will allow trading partners to seamlessly and dynamically come together and do business without ad hoc and proprietary integrations. Such a level of interoperability involves being able to find potential business partners, discovering their services and business processes, and conducting business ""on the fly"". This process of dynamic interoperation is only possible through standard B2B frameworks. Indeed a number of B2B electronic commerce standard frameworks have emerged recently. Although most of these standards are overlapping and competing, each with its own strenghts and weeknesses, a closer investigation reveals that they can be used in a manner to complement one another.In this paper we describe such an implementation where an ebXML infrastructure is developed by exploiting the Universal Description, Discovery and Integration (UDDI) registries and RosettaNet Partner Interface Processes (PIPs).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an ebxml infrastructure implementation through uddi registries and rosettanet pips

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac , yusuf tambag , pinar pembecioglu , sait pektas , gokce laleci , gokhan kurt , serkan toprak , yildiray kabak
",y
"LEFT id: NA
RIGHT id: 1758

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 420

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a data model and data structures for moving objects databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: luca forlizzi , ralf hartmut g &#252; ting , enrico nardelli , markus schneider
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 2211

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: including group-by in query optimization

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1960

LEFT text: In this paper, we present an efficient method to do online reorganization of sparsely-populated B+-trees. It reorganizes the leaves first, compacting in short operations groups of leaves with the same parent. After compacting, optionally, the new leaves may swap locations or be moved into empty pages so that they are in key order on the disk. After the leaves are reorganized, the method shrinks the tree by making a copy of the upper part of the tree while leaving the leaves in place. A new concurrency method is introduced so that only a minimum number of pages are locked during reorganization. During leaf reorganization, Forward Recovery is used to save all work already done while maintaining consistency after system crashes. A heuristic algorithm is developed to reduce the number of swaps needed during leaf reorganization, so that better concurrency and easier recovery can be achieved. A detailed description of switching from the old B+-tree to the new B+-tree is described for the first time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization of sparsely-populated b + - trees

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: chendong zou , betty salzberg
",y
"LEFT id: NA
RIGHT id: 986

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 154

LEFT text: We describe DataSplash, a direct manipulation system for creating semantic zoom visualizations of tabular (relational) data. DataSplash makes contributions in three areas that are key to the construction of such visualizations. First, DataSplash helps users graphically specify the visual appearance of groups of objects. Second, the system helps users visually program the way the appearance of groups of objects changes as users browse the visualization. Third, DataSplash allows users to create groups of graphical links between canvases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: datasplash

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chris olston , allison woodruff , alexander aiken , michael chu , vuk ercegovac , mark lin , mybrid spalding , michael stonebraker
",y
"LEFT id: NA
RIGHT id: 1130

LEFT text: After revealing the strong performance shortcomings of the state-of-the-art algorithm for k-nearest neighbor search [Korn et al. 1996], we present a novel multi-step algorithm which is guaranteed to produce the minimum number of candidates. Experimental evaluations demonstrate the significant performance gain over the previous solution, and we observed average improvement factors of up to 120 for the number of candidates and up to 48 for the total runtime.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",n
"LEFT id: NA
RIGHT id: 85

LEFT text: With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: engineering federated information systems : report of eefis '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. conrad , w. hasselbring , u. hohenstein , r.-d . kutsche , m. roantree , g. saake , f. saltor
",n
"LEFT id: NA
RIGHT id: 1235

LEFT text: Information Dissemination applications are gaining increasing popularity due to dramatic improvements in communications bandwidth and ubiquity. The sheer volume of data available necessitates the use of selective approaches to dissemination in order to avoid overwhelming users with unnecessaryinformation. Existing mechanisms for selective dissemination typically rely on simple keyword matching or “bag of words” information retrieval techniques. The advent of XML as a standard for information exchangeand the development of query languages for XML data enables the development of more sophisticated filtering mechanisms that take structure information into account. We have developed several index organizations and search algorithms for performing efficient filtering of XML documents for large-scale information dissemination systems. In this paper we describe these techniques and examine their performance across a range of document, workload, and scale scenarios.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient schemes for managing multiversionxml documents

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s.-y . chien , v. j. tsotras , c. zaniolo
",n
"LEFT id: NA
RIGHT id: 850

LEFT text: We propose a novel index structure, the A-tree (approximation tree), for similarity searches in high-dimensional data. The basic idea of the A-tree is the introduction of virtual bounding rectangles (VBRs) which contain and approximate MBRs or data objects. VBRs can be represented quite compactly and thus affect the tree configuration both quantitatively and qualitatively. First, since tree nodes can contain a large number of VBR entries, fanout becomes large, which increases search speed. More importantly, we have a free hand in arranging MBRs and VBRs in the tree nodes. Each A-tree node contains an MBR and its children VBRs. Therefore, by fetching an A-tree node, we can obtain information on the exact position of a parent MBR and the approximate position of its children. We have performed experiments using both synthetic and real data sets. For the real data sets, the A-tree outperforms the SR-tree and the VA-file in all dimensionalities up to 64 dimensions, which is the highest dimension in our experiments. Additionally, we propose a cost model for the A-tree. We verify the validity of the cost model for synthetic and real data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the a-tree : an index structure for high-dimensional spaces using relative approximation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 150

LEFT text: The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only way to achieve this integration is by building specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information mediators that extract, query, and integrate data from web sources. The resulting system, called Ariadne, makes it feasible to rapidly build information mediators that access existing web sources

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ariadne : a system for constructing mediators for internet sources

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jos &#233; luis ambite , naveen ashish , greg barish , craig a. knoblock , steven minton , pragnesh j. modi , ion muslea , andrew philpot , sheila tejada
",y
"LEFT id: NA
RIGHT id: 722

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: web caching for database applications with oracle web cache

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jesse anton , lawrence jacobs , xiang liu , jordan parker , zheng zeng , tie zhong
",n
"LEFT id: NA
RIGHT id: 793

LEFT text: We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating top-k selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: Archiving is important for scientific data, where it is necessary to record all past versions of a database in order to verify findings based upon a specific version. Much scientific data is held in a hierachical format and has a key structure that provides a canonical identification for each element of the hierarchy. In this article, we exploit these properties to develop an archiving technique that is both efficient in its use of space and preserves the continuity of elements through versions of the database, something that is not provided by traditional minimum-edit-distance diff approaches. The approach also uses timestamps. All versions of the data are merged into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. By identifying the semantic continuity of elements and merging them into one data structure, our technique is capable of providing meaningful change descriptions, the archive allows us to easily answer certain temporal queries such as retrieval of any specific version from the archive and finding the history of an element.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 590

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 865

LEFT text: The MOMIS project (Mediator envirOnment for Multiple Information Sources) developed in the past years allows the integration of data from structured and semi-structured data sources. SI-Designer (Source Integrator Designer) is a designer support tool implemented within the MOMIS project for semi-automatic integration of heterogeneous sources schemata. It is a java application where all modules involved are available as CORBA Object and interact using established IDL interfaces. The goal of this demonstration is to present a new tool: SI-Web (Source Integrator on Web), it offers the same features of SI-Designer but it has got the great advantage of being usable on Internet through a web browser.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a case-based approach to information integration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: maurizio panti , luca spalazzi , alberto giretti
",n
"LEFT id: NA
RIGHT id: 97

LEFT text: The OASIS Prototype is under development at Dublin City University in Ireland. We describe a multi-database architecture which uses the ODMG model as a canonical model and describe an extention for construction of virtual schemas within the multidatabase system. The OMG model is used to provide a standard distribution layer for data from local databases. This takes the form of CORBA objects representing export schemas from separate data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the oasis multidatabase prototype

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mark roantree , john murphy , wilhelm hasselbring
",y
"LEFT id: NA
RIGHT id: 998

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 779

LEFT text: In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: atomicity versus anonymity : distributed transactions for electronic commerce

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: j. d. tygar
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: The idea of building data warehouses as central data collections made available for decision support applications in a company is widely accepted. The concrete design and management of a data warehouse from a technical as well as from an organizational point of view, however, turns out to be far from trivial but requires sophisticated and time consuming efforts. The DMDW workshop was held at the CAiSE’99 conference in Heidelberg on June 14-15, 1999. It had the intention to bring together practitioners and researchers to discuss the design and management of data warehouses. The various presentations gave a broad view on the data warehouse life cycle covering aspects relevant at design time, at build time and at run time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",n
"LEFT id: NA
RIGHT id: 30

LEFT text: After a system crash, databases recover to the last committed transaction, but applications usually either crash or cannot continue. The Phoenix purpose is to enable application state to persist across system crashes, transparent to the application program. This simplifies application programming, reduces operational costs, masks failures from users, and increases application availability, which is critical in many scenarios, e.g., e-commerce. Within the Phoenix project, we have explored how to provide application recovery efficiently and transparently via redo logging. This paper describes the conceptual framework for the Phoenix project, and the software infrastructure that we are building.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: phoenix : making applications robust

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: roger barga , david b. lomet
",n
"LEFT id: NA
RIGHT id: 196

LEFT text: We develop algorithms for extracting detailed information about query plans through narrow optimizer interfaces, and we perform the characterization using database statistics from a published run of the TPC-H benchmark and a wide range of storage parameters.We show that, when data structures such as tables, indexes, and sorted runs reside on different storage devices, the optimizer can derive significant benefits from having accurate and timely information regarding the cost of accessing storage devices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in the presence of limited access patterns

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniela florescu , alon levy , ioana manolescu , dan suciu
",n
"LEFT id: NA
RIGHT id: 504

LEFT text: In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users' demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query's results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: practical lessons in supporting large-scale computational science

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ron musick , terence critchlow
",y
"LEFT id: NA
RIGHT id: 1317

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 1998

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 1768

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: materialized views and data warehouses

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1126

LEFT text: Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source's performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants mechanism, which shows how semantic information about data sources may be used to discover cached query results of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: object fusion in mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , serge abiteboul , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 466

LEFT text: We are pleased to announce an excellent technical program for the 6th International Conference on Pervasive Computing and Communications. The program covers a broad cross section of topics in pervasive computing and communications. This year, 160 papers were submitted for consideration to the program committee. As a result, the selection process was highly competitive, and the result is a program of high-quality papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xml and information retrieval : a sigir 2000 workshop

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: david carmel , yoelle maarek , aya soffer
",n
"LEFT id: NA
RIGHT id: 1335

LEFT text: Abstract. Inter-object references are one of the key concepts of object-relational and object-oriented database systems. In this work, we investigate alternative techniques to implement inter-object references and make the best use of them in query processing, i.e., in evaluating functional joins. We will give a comprehensive overview and performance evaluation of all known techniques for simple (single-valued) as well as multi-valued functional joins. Furthermore, we will describe special order-preserving\/ functional-join techniques that are particularly attractive for decision support queries that require ordered results. While most of the presentation of this paper is focused on object-relational and object-oriented database systems, some of the results can also be applied to plain relational databases because index nested-loop joins\/ along key/foreign-key relationships, as they are frequently found in relational databases, are just one particular way to execute a functional join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: functional properties of information filtering

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rie sawai , masahiko tsukamoto , yin-huei loh , tsutomu terada , shojiro nishio
",n
"LEFT id: NA
RIGHT id: 1053

LEFT text: Inheritance hierarchies have become more and more complex according to an enlargement of object-oriented technology. One of the main problems is the effective searching in such hierarchies. More sophisticated algorithms are needed to searching in the data. In this article we present a novel approach to efficient searching in large inheritance hierarchies. The updatable approach employs the multi-dimensional data structures to indexing inheritance hierarchies and effective searching in the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pesto : an integrated query/browser for object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , laura m. haas , vivekananda maganty , john h. williams
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 268

LEFT text: The Mentor-lite prototype has been developed within the research project “Architecture, Configuration, and Administration of Large Workflow Management Systems” funded by the German Science Foundation (DFG). It has evolved from its predecessor Mentor [1], but aims at a simpler architecture. The main goal of Mentor-lite has been to build a light-weight, extensible, and tailorable workflow management system (WFMS) with small footprint and easy-to-use administration capabilities. Our approach is to provide only kernel functionality inside the workflow engine, and consider system components like history management and worklist management as extensions on top of the kernel. The key point to retain the light-weight nature is that these extensions are implemented as workflows themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 881

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional database allocation for parallel data warehouses

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: thomas st &#246; hr , holger m &#228; rtens , erhard rahm
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 1001

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized view selection for multidimensional datasets

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: amit shukla , prasad deshpande , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 969

LEFT text: The paper discusses the LHAM concepts, including concurrency control and recovery, our full-fledged LHAM implementation, and experimental performance results based on this implementation. A detailed comparison with the TSB-tree, both analytically and based on experiments with real implementations, shows that LHAM is highly superior in terms of insert performance, while query performance is in almost all cases at least as good as for the TSB-tree; in many cases it is much better.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design , implementation , and performance of the lham log-structured history data access method

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter muth , patrick e. o'neil , achim pick , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1244

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the persistent cache : improving oid indexing in temporal object-oriented database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kjetil n &#248; rv &#229; g
",n
"LEFT id: NA
RIGHT id: 869

LEFT text: We consider the view data lineageproblem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the challenge of process data warehousing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: matthias jarke , thomas list , j &#246; rg k &#246; ller
",n
"LEFT id: NA
RIGHT id: 797

LEFT text: Current client-server object database management systems employ either a page server or an object server architecture. Both of these architectures have their respective strengths, but they also have key drawbacks for important system and workload configurations. We propose a new hybrid server architecture which combines the best features of both page server and object server architectures while avoiding their problems. The new architecture incorporates new or adapted versions of data transfer, recovery, and cache consistency algorithms; in this paper we focus only on the data transfer and recovery issues. The data transfer mechanism allows the hybrid server to dynamically behave as both page and object server. The performance comparison of the hybrid server with object and page servers indicates that the performance of the hybrid server is more robust than the others.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an adaptive hybrid server architecture for client caching odbmss

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaladhar voruganti , m. tamer &#214; zsu , ronald c. unrau
",y
"LEFT id: NA
RIGHT id: 80

LEFT text: The random data perturbation (RDP) method of preserving the privacy of individual records in a statistical database is discussed. In particular, it is shown that if confidential attributes are allowed as query-defining variables, severe biases may result in responses to queries. It is also shown that even if query definition through confidential variables is not allowed, biases can still occur in responses to queries such as those involving proportions or counts. In either case, serious distortions may occur in user statistical analyses. A modified version of RDP is presented, in the form of a query adjustment procedure and specialized perturbation structure which will produce unbiased results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: security of random data perturbation methods

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: krishnamurty muralidhar , rathindra sarathy
",n
"LEFT id: NA
RIGHT id: 481

LEFT text: As XML is emerging as the data format of the internet era, there is an substantial increase of the amount of data in XML format. To better describe such XML data structures and constraints, several XML schema languages have been proposed. In this paper, we present a comparative analysis of six noteworthy XML schema languages. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: comparative analysis of five xml query languages

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: angela bonifati , stefano ceri
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1676

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as a deductive database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: Phrase matching is a common IR technique to search text and identify relevant documents in a document collection. Phrase matching in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for phrase matching in XML that permits dynamic specification of both the phrase to be matched and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 405

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a goal-driven auto-configuration tool for the distributed workflow management system mentorlite

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael gillmann , jeanine weissenfels , german shegalov , wolfgang wonner , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1067

LEFT text: Data warehouses store large volumes of data which are used frequently by decision support applications. Such applications involve complex queries. Query performance in such an environment is critical because decision support applications often require interactive query response time. Because data warehouses are updated infrequently, it becomes possible to improve query performance by caching sets retrieved by queries in addition to query execution plans. In this paper we report on the design of an intelligent cache manager for sets retrieved by queries called WATCHMAN, which is particularly well suited for data warehousing environment. Our cache manager employs two novel, complementary algorithms for cache replacement and for cache admission. WATCHMAN aims at minimizing query response time and its cache replacement policy swaps out entire retrieved sets of queries instead of individual pages. The cache replacement and admission algorithms make use of a profit metric, which considers for each retrieved set its average rate of reference, its size, and execution cost of the associated query. We report on a performance evaluation based on the TPC-D and Set Query benchmarks. These experiments show that WATCHMAN achieves a substantial performance improvement in a decision support environment when compared to a traditional LRU replacement algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: watchman : a data warehouse intelligent cache manager

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter scheuermann , junho shim , radek vingralek
",y
"LEFT id: NA
RIGHT id: 677

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general ""sketch""-based methods for capturing various linear projections and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic multidimensional histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nitin thaper , sudipto guha , piotr indyk , nick koudas
",n
"LEFT id: NA
RIGHT id: 1572

LEFT text: We present a structured, iterative methodology for user-centered design and evaluation of VE user interaction. We recommend performing (1) user task analysis followed by (2) expert guidelines-based evaluation, (3) formative user-centered evaluation, and finally (4) comparative evaluation. In this article we first give the motivation and background for our methodology, then we describe each technique in some detail. We applied these techniques to a real-world battlefield visualization VE. Finally, we evaluate why this approach provides a cost-effective strategy for assessing and iteratively improving user interaction in VEs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and user testing of a multi-paradigm query interface to an object-oriented database

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: dac khoa doan , norman w. paton , alistair kilgour
",n
"LEFT id: NA
RIGHT id: 81

LEFT text: The traditional approach to relational database design is based on the logical organization of data into a number of related normalized tables. One assumption is that the nature and structure of the data is known at the design stage. In the case of designing a relational database to store historical dental epidemiological data from individual clinical surveys, the structure of the data is not known until the data is presented for inclusion into the database. This paper addresses the issues concerned with the theoretical design of a clinical dynamic database capable of adapting the internal table structure to accommodate clinical survey data, and presents a prototype database application capable of processing, displaying, and querying the dental data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: improving database design through the analysis of relationships

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: debabrata dey , veda c. storey , terence m. barron
",n
"LEFT id: NA
RIGHT id: 432

LEFT text: The DataLinks technology developed at IBM Almaden Research Center and now available in DB2 UDB 5.2 introduces a new data type called DATALINK for a database to reference and manage files stored external to the database. An external file is put under a database control by “linking” the file to the database. Control to a file can also be removed by “unlinking” it. The technology provides transactional semantics with respect to linking or unlinking the file when DATALINK value is stored or updated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dlfm : a transactional resource manager

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hui-i hsiao , inderpal narang
",y
"LEFT id: NA
RIGHT id: 1473

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten. Finally, it exploits schema information, if available, to reduce costs. We have implemented the TCRC algorithm and present results of a performance study of the implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient incremental garbage collection for client-server object database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: laurent amsaleg , michael j. franklin , olivier gruber
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 1757

LEFT text: Multilevel relations, based on the current multilevel secure (MLS) relational data models, can present a user with information that is difficult to interpret and may display an inconsistent outlook about the views of other users. Such ambiguity is due to the lack of a comprehensive method for asserting and interpreting beliefs about lower level information. In this paper we identify different beliefs that can be held by higher level users about lower level information, and we introduce the new concept of a mirage tuple. We present a mechanism for asserting beliefs about all accessible tuples, including lower level tuples. This mechanism provides every user of an MLS database with an unambiguous interpretation of all viewable information and presents a consistent account of the views at all levels below the user's level.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: asserting beliefs in mls relational models

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nenad a. jukic , susan v. vrbsky
",y
"LEFT id: NA
RIGHT id: 251

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ins and outs ( and everything in between ) of data warehousing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: phil fernandez , donovan schneider
",n
"LEFT id: NA
RIGHT id: 109

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: specification and implementation of exceptions in workflow management systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 1220

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: effective timestamping in databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kristian torp , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: First-order formulas allow natural descriptions of queries and rules. Van Gelder's alternating fixpoint semantics extends the well-founded semantics of normal logic programs to general logic programs with arbitrary first-order formulas in rule bodies. However, an implementation of general logic programs through the standard translation into normal logic programs does not preserve the alternating fixpoint semantics. This paper presents a direct method for goal-oriented query evaluation of general logic programs. Every general logic program is first transformed into a normal form where the body of each rule is either an existential conjunction of literals or a universal disjunction of literals. Techniques of memoing and loop checking are incorporated so that termination and polynomial-time data complexity are guaranteed for deductive databases (or function-free programs). Results of the soundness and search space completeness are established.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1940

LEFT text: In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: modularization techniques for active rules design

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: elena baralis , stefano ceri , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",n
"LEFT id: NA
RIGHT id: 1620

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dec data distributor : for data replication and data warehousing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel j. dietterich
",n
"LEFT id: NA
RIGHT id: 2079

LEFT text: The querying and analysis of data streams has been a topic of much recent interest, motivated by applications from the fields of networking, web usage analysis, sensor instrumentation, telecommunications, and others. Many of these applications involve monitoring answers to continuous queries over data streams produced at physically distributed locations, and most previous approaches require streams to be transmitted to a single location for centralized processing. Unfortunately, the continual transmission of a large number of rapid data streams to a central location can be impractical or expensive. We study a useful class of queries that continuously report the k largest values obtained from distributed data streams (""top-k monitoring queries""), which are of particular interest because they can be used to reduce the overhead incurred while running other types of monitoring queries. We show that transmitting entire data streams is unnecessary to support these queries and present an alternative approach that reduces communication significantly. In our approach, arithmetic constraints are maintained at remote stream sources to ensure that the most recently provided top-k answer remains valid to within a user-specified error tolerance. Distributed communication is only necessary on occasion, when constraints are violated, and we show empirically through extensive simulation on real-world data that our approach reduces overall communication cost by an order of magnitude compared with alternatives that o er the same error guarantees.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distributed top-k monitoring

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , chris olston
",y
"LEFT id: NA
RIGHT id: 1657

LEFT text: In this article, we present an extended relational algebra with universally or existentially quantified classes as attribute values. The proposed extension can greatly enhance the expressive power of relational systems, and significantly reduce the size of a database, at small additional computational cost. We also show how the proposed extensions can be built on top of a standard relational database system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a close look at the ifo data model

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: magdy s. hanna
",n
"LEFT id: NA
RIGHT id: 544

LEFT text: A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the ecobase project : database and web technologies for environmental information systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luc bouganim , maria claudia cavalcanti , fran &#231; oise fabret , maria luiza campos , fran &#231; ois llirbat , marta mattoso , rubens melo , ana maria moura , esther pacitti , fabio porto , margareth simoes , eric simon , asterio tanaka , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1491

LEFT text: In the rnid-1980s. Chrts Dare’s “12 rules” for distributed database systems included replication. Repi ication makes transparent the problems of remote access de]dys and the management of data redundancy. The commercial market for distributed database features has been slowly building over the years. beginning with simple remote access gateways. Today. replication appears to dehver on the 1980s ideal, with a robust a-wrrchrcmuus infrasrntctnre. Current commercial tmhnology though. continues to fall shotl of that ideal.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: things every update replication customer should know ( abstract )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rob goldring
",y
"LEFT id: NA
RIGHT id: 144

LEFT text: The CONTROL project at U.C. Berkeley has developed technologies to provide online behavior for data-intensive applications. Using new query processing algorithms, these technologies continuously improve estimates and confidence statistics. In addition, they react to user feedback, thereby giving the user control over the behavior of long-running operations. This demonstration displays the modifications to a database system and the resulting impact on aggregation queries, data visualization, and GUI widgets. We then compare this interactive behavior to batch-processing alternatives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: control : continuous output and navigation technology with refinement on-line

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: ron avnur , joseph m. hellerstein , bruce lo , chris olston , bhaskaran raman , vijayshankar raman , tali roth , kirk wylie
",y
"LEFT id: NA
RIGHT id: 70

LEFT text: We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: answering complex sql queries using automatic summary tables

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: markos zaharioudakis , roberta cochrane , george lapis , hamid pirahesh , monica urata
",y
"LEFT id: NA
RIGHT id: 1473

LEFT text: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient incremental garbage collection for client-server object database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: laurent amsaleg , michael j. franklin , olivier gruber
",y
"LEFT id: NA
RIGHT id: 1213

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: priority assignment in real-time active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rajendran m. sivasankaran , john a. stankovic , don towsley , bhaskar purimetla , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 970

LEFT text: Data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. We provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. Several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. Techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. Potential data mining applications and some research issues will also be discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scalable techniques for mining causal structures

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: craig silverstein , sergey brin , rajeev motwani , jeffrey d. ullman
",n
"LEFT id: NA
RIGHT id: 2193

LEFT text: Multiversion support for XML documents is needed in many critical applications, such as software configuration control, cooperative authoring, web information warehouses, and ”e-permanence” of web documents. In this paper, we introduce efficient and robust techniques for: (i) storing and retrieving; (ii) viewing and exchanging; and (iii) querying multiversion XML documents. We first discuss the limitations of traditional version control methods, such as RCS and SCCS, and then propose novel techniques that overcome their limitations. Initially, we focus on the problem of managing secondary storage efficiently, and introduce an edit-based versioning scheme that enhances RCS with an effective clustering policy based on the concept of page-usefulness. The new scheme drastically improves version retrieval at the expense of a small (linear) space overhead. However, the edit-based approach falls short of achieving objectives (ii) and (iii). Therefore, we introduce and investigate a second scheme, which is reference-based and preserves the structure of the original document. In the reference-based approach, a multiversion document can be represented as yet another XML document, which can be easily exchanged and viewed on the web; furthermore, simple queries are also expressed and supported well under this representation. To achieve objective (i), we extend the page-usefulness clustering technique to the reference-based scheme. After characterizing the asymptotic behavior of the new techniques proposed, the paper presents the results of an experimental study evaluating and comparing their performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient management of multiversion documents by object referencing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shu-yao chien , vassilis j. tsotras , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 53

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamat : a dynamic view management system for data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 291

LEFT text: We present a two-phase Web Query Optimizer (WQO). In a pre-optimization phase, the WQO selects one or more WSIs for a pre-plan; a pre-plan represents a space of query evaluation plans (plans) based on this choice of WSIs. The WQO uses cost-based heuristics to evaluate the choice of WSI assignment in the pre-plan and to choose a good pre-plan. The WQO uses the pre-plan to drive the extended relational optimizer to obtain the best plan for a pre-plan. A prototype of the WQO has been developed. We compare the effectiveness of the WQO, i.e., its ability to efficiently search a large space of plans and obtain a low cost plan, in comparison to a traditional optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of xml middle-ware queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mary fernandez , atsuyuki morishima , dan suciu
",n
"LEFT id: NA
RIGHT id: 642

LEFT text: Due to organizational or operational constraints, the diverse data sources that an enterprise uses do not generally lend themselves to being fully replicated or completely consolidated under a single database, hence the increased demand for data interchange and for federated access to distributed sources. IBM has ongoing work in information integration technology that enables integrated, real-time access to traditional and emerging data sources, transforms information to meet the needs of business analysts, and manages data placement for performance, currency, and availability leading to fast, constant, and easy access for customer e-business solutions. IBM's Information Integration infrastructure today supports SQL—a mature, powerful query language—plus a number of SQL extensions in support of XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient integration and aggregation of historical information

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mirek riedewald , divyakant agrawal , amr el abbadi
",n
"LEFT id: NA
RIGHT id: 1209

LEFT text: In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: algebraic query optimisation for database programming languages

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 545

LEFT text: Like any other data, biological data is a very vast one. Due to emergence of system biology it is necessary to develop various platforms and techniques to analyze and organize the biological data in meaning full manner for which it to be mined and processed carefully. As the complexity associated with biological data is high ,it has to be studied considering various criteria’s and also it is mandatory to study all available databases and then has to undergo several processing mining techniques to finally put in a format which is easy to assess and produce the information of interest. There are various techniques and method for mining biological data. Here we will put forth all possible techniques and operations involved in data mining and will compare them in order to find the advantages and disadvantages of different methods

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data analysis and mining in the life sciences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 1373

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximating aggregate queries about web pages via random walks

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ziv bar-yossef , alexander berg , steve chien , jittat fakcharoenphol , dror weitz
",n
"LEFT id: NA
RIGHT id: 776

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: wavecluster : a multi-resolution clustering approach for very large spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gholamhosein sheikholeslami , surojit chatterjee , aidong zhang
",n
"LEFT id: NA
RIGHT id: 1826

LEFT text: Active database systems are now in widespread use. The use of triggers in these systems, however, is difficult because of the complex interaction between triggers, transactions, and application programs. Repeated calculations of rules may incur costly redundant computations in rule conditions and actions. In this paper, we focus on active relational database systems supporting SQL triggers. In this context, we provide a powerful and complete solution to eliminate redundant computations of SQL triggers when they are costly. We define a model to describe programs, rules and their interactions. We provide algorithms to extract invariant subqueries from trigger's condition and action. We define heuristics to memorize the most “profitable” invariants. Finally, we develop a rewriting technique that enables to generate and execute the optimized code of SQL triggers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: eliminating costly redundant computations from sql trigger executions

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fran &#231; ois llirbat , fran &#231; oise fabret , eric simon
",y
"LEFT id: NA
RIGHT id: 689

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1698

LEFT text: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research perspectives for time series management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 810

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1981

LEFT text: Real-time production systems and other dynamic environments often generate tremendous (potentially infinite) amount of stream data; the volume of data is too huge to be stored on disks or scanned multiple times. Can we perform on-line, multi-dimensional analysis and data mining of such data to alert people about dramatic changes of situations and to initiate timely, high-quality responses? This is a challenging task.    In this paper, we investigate methods for on-line, multi-dimensional regression analysis of time-series stream data, with the following contributions: (1) our analysis shows that only a small number of compressed regression measures instead of the complete stream of data need to be registered for multi-dimensional linear regression analysis, (2) to facilitate on-line stream data analysis, a partially materialized data cube model, with regression as measure, and a tilt time frame as its time dimension, is proposed to minimize the amount of data to be retained in memory or stored on disks, and (3) an exception-guided drilling approach is developed for on-line, multi-dimensional exception-based regression analysis. Based on this design, algorithms are proposed for efficient analysis of time-series data streams. Our performance study compares the proposed algorithms and identifies the most memory- and time- efficient one for multi-dimensional stream data analysis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional resource scheduling for parallel queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1125

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1756

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database systems-breaking out of the box

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: avi silberschatz , stan zdonik
",n
"LEFT id: NA
RIGHT id: 1614

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 2007

LEFT text: Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional clustering : a new data layout scheme in db2

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sriram padmanabhan , bishwaranjan bhattacharjee , tim malkemus , leslie cranston , matthew huras
",n
"LEFT id: NA
RIGHT id: 473

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: re-designing distance functions and distance-based applications for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 2035

LEFT text: XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (à la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exchanging intensional xml data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: tova milo , serge abiteboul , bernd amann , omar benjelloun , fred dang ngoc
",y
"LEFT id: NA
RIGHT id: 1032

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 478

LEFT text: Digital libraries bring about the integration, management, and communication of gigabytes of multimedia data in a distributed environment. Digital library systems currently envision users as being static when they access information. But it is expected in the near future that tens of millions of users will have access to a digital library through wireless access. Providing digital library services to users whose location is constantly changing, whose network connections are through a wireless medium, and whose computing power is low necessitates modifications to existing digital library systems. In this paper, we identify the issues that arise when users are mobile, classify queries that are specific to mobile users and introduce an architecture that supports flexible and transparent access to digital libraries for mobile users. The main features of the architecture include a layered data representation, support of adaptability, dual broadcast and on demand querying, caching, and mobile-specific user interfaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: quality of service in multimedia digital libraries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: elisa bertino , ahmed k. elmagarmid , mohand-sa &#239; d hacid
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1171

LEFT text: This paper investigates the use of sequences of system calls for classifying intrusions and faults induced by privileged processes in Unix. Classification is an essential capability for responding to an anomaly (attack or fault), since it gives the ability to associate appropriate responses to each anomaly type. Previous work using the well known dataset from the University of New Mexico (UNM) has demonstrated the usefulness of monitoring sequences of system calls for detecting anomalies induced by processes corresponding to several Unix Programs, such as sendmail, lpr, ftp, etc. Specifically, previous work has shown that the Anomaly Count of a running process, i.e., the number of sequences spawned by the process which are not found in the corresponding dictionary of normal activity for the Program, is a valuable feature for anomaly detection. To achieve Classification, in this paper we introduce the concept of Anomaly Dictionaries, which are the sets of anomalous sequences for each type of anomaly. It is verified that Anomaly Dictionaries for the UNM's sendmail Program have very little overlap, and can be effectively used for Anomaly Classification. The sequences in the Anomalous Dictionary enable a description of Self for the Anomalies, analogous to the definition of Self for Privileged Programs given by the Normal Dictionaries. The dependence of Classification Accuracy with sequence length is also discussed. As a side result, it is also shown that a hybrid scheme, combining the proposed classification strategy with the original Anomaly Counts can lead to a substantial improvement in the overall detection rates for the sendmail dataset. The methodology proposed is rather general, and can be applied to any situation where sequences of symbols provide an effective characterization of a phenomenon.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating symbolic images into a multimedia database system using classification and abstraction approaches

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: aya soffer , hanan samet
",n
"LEFT id: NA
RIGHT id: 2033

LEFT text: To ensure high data quality, data warehouses must validate and cleanse incoming data tuples from external sources. In many situations, clean tuples must match acceptable tuples in reference tables. For example, product name and description fields in a sales record from a distributor must match the pre-recorded name and description fields in a product reference relation.A significant challenge in such a scenario is to implement an efficient and accurate fuzzy match operation that can effectively clean an incoming tuple if it fails to match exactly with any tuple in the reference relation. In this paper, we propose a new similarity function which overcomes limitations of commonly used similarity functions, and develop an efficient fuzzy match algorithm. We demonstrate the effectiveness of our techniques by evaluating them on real datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: robust and efficient fuzzy match for online data cleaning

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kris ganjam , venkatesh ganti , rajeev motwani
",y
"LEFT id: NA
RIGHT id: 450

LEFT text: The second international workshop on semantic Web technologies for health data management aimed at putting together an interdisciplinary audience that is interested in the fields of semantic web, data management and health informatics to discuss the challenges in health-care data management and to propose new solutions for the next generation data-driven health-care systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerge. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the vldb workshop on technologies for e-services ( tes )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: fabio casati , umesh dayal , ming-chien shan
",n
"LEFT id: NA
RIGHT id: 1620

LEFT text: We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications. The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dec data distributor : for data replication and data warehousing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel j. dietterich
",n
"LEFT id: NA
RIGHT id: 337

LEFT text: We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits ""horizontal"" aggregation and even aggregation over more general ""blocks"" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: schemasql : an extension to sql for multidatabase interoperability

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , fereidoon sadri , subbu n. subramanian
",y
"LEFT id: NA
RIGHT id: 1769

LEFT text: This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applications of java programming language to database management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bradley f. burton , victor w. marek
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 878

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: rethinking database system architecture : towards a self-tuning risc-style database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1718

LEFT text: A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive selectivity estimation using query feedback

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chungmin melvin chen , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1330

LEFT text: Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tavant system architecture for sell-side channel management

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: srinivasa narayanan , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 1114

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 2154

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on fqas 2002 : fifth international conference on flexible query answering systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: amihai motro , troels andreasen
",n
"LEFT id: NA
RIGHT id: 866

LEFT text: In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rachel pottinger , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1560

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",n
"LEFT id: NA
RIGHT id: 27

LEFT text: In this article we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time, DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently or can be bundled within a multiquery expression. In the latter case, we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: tam : a system for dynamic transactional activity management

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tong zhou , ling liu , calton pu
",n
"LEFT id: NA
RIGHT id: 773

LEFT text: We propose a generic approach to parallelization, called TOPAZ. Different forms of parallelism are exploited to obtain maximum speedup combined with lowest resource consumption. The necessary abstractions w.r.t. operator characteristics and system architecture are provided by rules that are used by a cost-based, top-down search engine. A multi-phase pruning based on a global analysis of the plan efficiently guides the search process, thus considerably reducing complexity and achieving optimization performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: topaz : a cost-based , rule-driven , multi-phase parallelizer

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: clara nippl , bernhard mitschang
",y
"LEFT id: NA
RIGHT id: 200

LEFT text: In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: boat-optimistic decision tree construction

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: johannes gehrke , venkatesh ganti , raghu ramakrishnan , wei-yin loh
",n
"LEFT id: NA
RIGHT id: 314

LEFT text: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: modeling high-dimensional index structures using sampling

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian a. lang , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 2053

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an optimal and progressive algorithm for skyline queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: dimitris papadias , yufei tao , greg fu , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 0

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the wasa2 object-oriented workflow management system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: gottfried vossen , mathias weske
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 78

LEFT text: Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: closest pair queries in spatial databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: antonio corral , yannis manolopoulos , yannis theodoridis , michael vassilakopoulos
",n
"LEFT id: NA
RIGHT id: 786

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: curio : a novel solution for efficient storage and indexing in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: anindya datta , krithi ramamritham , helen m. thomas
",n
"LEFT id: NA
RIGHT id: 1250

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data partitioning and load balancing in parallel disk systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter scheuermann , gerhard weikum , peter zabback
",n
"LEFT id: NA
RIGHT id: 530

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1011

LEFT text: The cost of a query plan depends on many parameters, such as predicate selectivities and available memory, whose values may not be known at optimization time. Parametric query optimization (PQO) optimizes a query into a number of candidate plans, each optimal for some region of the parameter space. We propose a heuristic solution for the PQO problem for the case when the cost functions may be nonlinear in the given parameters. This solution is minimally intrusive in the sense that an existing query optimizer can be used with minor modifications. We have implemented the heuristic and the results of the tests on the TPCD benchmark indicate that the heuristic is very effective. The minimal intrusiveness, generality in terms of cost functions and number of parameters and good performance (up to 4 parameters) indicate that our solution is of significant practical importance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: design and analysis of parametric query optimization algorithms

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sumit ganguly
",n
"LEFT id: NA
RIGHT id: 842

LEFT text: Many current database systems use histograms to approximate the frequency distribution of values in the attributes of relations and based on them estimate query result sizes and access plan costs. In choosing among the various histograms, one has to balance between two conflicting goals: optimality, so that generated estimates have the least error, and practicality, so that histograms can be constructed and maintained efficiently. In this paper, we present both theoretical and experimental results on several issues related to this trade-off. Our overall conclusion is that the most effective approach is to focus on the class of histograms that accurately maintain the frequencies of a few attribute values and assume the uniform distribution for the rest, and choose for each relation the histogram in that class that is optimal for a self-join query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: combining histograms and parametric curve fitting for feedback-driven query result-size estimation

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arnd christian k &#246; nig , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 798

LEFT text: In the past decade, advances in the speed of commodity CPUs have far out-paced advances in memory latency. Main-memory access is therefore increasingly a performance bottleneck for many computer applications, including database systems. In this article, we use a simple scan test to show the severe impact of this bottleneck. The insights gained are translated into guidelines for database architecture, in terms of both data structures and algorithms. We discuss how vertically fragmented data structures optimize cache performance on sequential data access. We then focus on equi-join, typically a random-access operation, and introduce radix algorithms for partitioned hash-join. The performance of these algorithms is quantified using a detailed analytical model that incorporates memory access cost. Experiments that validate this model were performed on the Monet database system. We obtained exact statistics on events such as TLB misses and L1 and L2 cache misses by using hardware performance counters found in modern CPUs. Using our cost model, we show how the carefully tuned memory access pattern of our radix algorithms makes them perform well, which is confirmed by experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database architecture optimized for the new bottleneck : memory access

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: peter a. boncz , stefan manegold , martin l. kersten
",n
"LEFT id: NA
RIGHT id: 647

LEFT text: Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - ""given the current information, it is possible that X will become true at time t"" - instead of no information at all.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: continuously adaptive continuous queries over streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , mehul shah , joseph m. hellerstein , vijayshankar raman
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: The land use categories of the Global Land Cover 2000 (GLC2000; Global Land Cover 2000 database, 2003, European Commission, Joint Research Centre; resolution 1 km) are then calibrated with the Swiss dataset in order to derive a Europe-wide birch distribution dataset and aggregated onto the 7 km COSMO-ART grid. This procedure thus assumes that a certain GLC2000 land use category has the same birch density wherever it may occur in Europe. In order to reduce the strict application of this crucial assumption, the birch density distribution as obtained from the previous steps is weighted using the mean Seasonal Pollen Index (SPI; yearly sums of daily pollen concentrations). For future improvement, region-specific birch densities for the GLC2000 categories could be integrated into the mapping procedure.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1423

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: declustering databases on heterogeneous disk systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ling tony chen , doron rotem , sidhar seshadri
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",y
"LEFT id: NA
RIGHT id: 1601

LEFT text: Many database applications need accountability and trace-ability that necessitate retaining previous database states. For a transaction-time database supporting this, the choice of times used to timestamp database records, to establish when records are or were current, needs to be consistent with a committed transaction serialization order. Previous solutions have chosen timestamps at commit time, selecting a time that agrees with commit order. However, SQL standard databases can require an earlier choice because a statement within a transaction may request “current time.” Managing timestamps chosen before a serialization order is established is the challenging problem we solve here. By building on two-phase locking concurrency control, we can delay a transaction’s choice of a timestamp, reducing the chance that transactions may need to be aborted in order keep timestamps consistent with a serialization order. Also, while timestamps stored with records in a transaction-time database make it possible to directly identify write-write and write-read conflicts, handling read-write conflicts requires more. Our simple auxiliary structure conservatively detects read-write conflicts, and hence provides transaction timestamps that are consistent with a serialization order.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 349

LEFT text: Putting electronic business on a sound foundation --- model theoretically as well as technologically --- has to be seen as a central challenge for research as well as for commercial development. This paper concentrates on the discovery and the negotiation phase of concluding an agreement based on a contract. We present a methodology how to come seamlessly from a many-to-many relationship in the discovery phase to a one-to-one relationship in the contract negotiation phase. Making the content of the contracts persistent is achieved by reconstructing contract templates by means of mereologic (logic of the whole-part relation). Possibly nested sub-structures of the contract template are taken as a basis for negotiation in a dialogical way. For the negotiation itself the contract templates are extended by implications (logical) and sequences (topical).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: contracting in the days of ebusiness

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: w. h &#252; mmer , w. lehner , h. wedekind
",y
"LEFT id: NA
RIGHT id: 1594

LEFT text: Disk-based database systems benefit from concurrency among transactions - usually with marginal overhead. For main-memory database systems, however, locking overhead can have a serious impact on performance. This paper proposes SP, a serial protocol for the execution of transactions in main-memory systems, and evaluates its performance against that of strict two-phase locking. The novelty of SP lies in the use of timestamps and mutexes to allow one transaction to begin before its predecessors' commit records have been written to disk, while also ensuring that no committed transactions read uncommitted data. We demonstrate seven-fold and two-fold increases in maximum throughput for read-and update-intensive workloads, respectively. At fixed loads, we demonstrate ten-fold and two-fold improvements in response time for the same transaction mixes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: recovery protocols for shared memory database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: lory d. molesky , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1327

LEFT text: Data in a warehouse typically has multiple dimensions of interest, such as location, time, and product. It is well-recognized that these dimensions have hierarchies deened on them, such as \store-city-state-region"" for location. The standard way to model such data is with a star/snowwake schema. However, current approaches do not give a rst-class status to dimensions. Consequently, a substantial class of interesting queries involving dimension hierarchies and their interaction with the fact tables are quite verbose to write, hard to read, and diicult to optimize. We propose the SQL(H) model and a natural extension to the SQL query language, that gives a rst-class status to dimensions, and we pin down its semantics. Our model permits structural and schematic heterogeneity in dimension hierarchies, situations often arising in practice that cannot be modeled satisfactorily using the star/snowwake approach. We show using examples that sophisticated queries involving dimension hierarchies and their interplay with aggregation can be expressed concisely in SQL(H). By comparison, expressing such queries in SQL would involve a union of numerous complex sequences of joins. Finally, we develop an eecient implementation strategy for computing SQL queries, based on an algorithm for hierarchical joins, and the use of dimension indexes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: warehousing workflow data : challenges and opportunities

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: angela bonifati , fabio casati , umeshwar dayal , ming-chien shan
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2031

LEFT text: A goal of the Biomedical Informatics Research Network (birn) project is to develop a multi-institution information management system for Neurosciences to gain a deeper understanding of several neurological disorders. Each institution specializes in a different subdiscipline and produces a database of its experimental or computationally derived data; a mediator module performs semantic integration over the databases to enable neuroscientists to perform analyses that could not be done from any single institution’s data. The overall system architecture of the birn system is that of a wrapper-mediator system. The information sources are various relational sources including Oracle 9i having userdefined packages, Oracle 8i with the Spatial Data Cartridge, and databases made available over the web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: birn-m : a semantic mediator for solving real-world neuroscience problems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: amarnath gupta , bertram lud &#228; scher , maryann e. martone
",y
"LEFT id: NA
RIGHT id: 206

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a comparison of selectivity estimators for range queries on metric attributes

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: bj &#246; rn blohsfeld , dieter korus , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1132

LEFT text: Association Rule Mining algorithms operate on a data matrix (e.g., customers products) to derive association rules [2, 23]. We propose a new paradigm, namely, Ratio Rules, which are quanti able in that we can measure the \goodness"" of a set of discovered rules. We propose to use the \guessing error"" as a measure of the \goodness"", that is, the rootmean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can \guess"" the amount spent on, say, butter. Thus, we can perform a variety of important tasks such as forecasting, answering \what-if"" scenarios, detecting outliers, and visualizing the data. Moreover, we show how to compute Ratio Rules in a single pass over the dataset with small memory requirements (a few small matrices), in contrast to traditional association rule mining methods that require multiple passes and/or large memory. ExperWork performed while at the University of Maryland. This research was partially funded by the Institute for Systems Research (ISR), and by the National Science Foundation under Grants No. EEC-94-02384, IRI-9205273 and IRI-9625428. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 24th VLDB Conference New York, USA, 1998 iments on several real datasets (e.g., basketball and baseball statistics, biological data) demonstrate that the proposed method consistently achieves a \guessing error"" of up to 5 times less than the straightforward competitor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sprint : a scalable parallel classifier for data mining

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john c. shafer , rakesh agrawal , manish mehta
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: Testing the reliability of high performance tlansaetion processing systams poses many difficult challenges that are not adequately answered by conventional testing techniques. We discuss a new test paradigm, which is dynamic and exploratory in nature, and discuss its ability to meet these challenges. We describe an implementation of thii paradigm in products that aid in efficiently testing reliable, high performance transaction processing systems at T~adem Computers Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 1021

LEFT text: We describe a scheme to fragment and distribute centralized databases. ’ The problem is motivated by trends towards down-sizing and reorganization, reflecting actual, often distributed responsibilities within companies. A major practical requirement is that existing application code must be left unchanged. We present SQL extensions to specify ownership and data replication information declaratively. From this, a compiler generates triggers and view definitions that implement the distributed scheme, on top of a collection of local databases. Our strategy has been applied successfully at Telenor the Norwegian telephone comp.any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: selectivity estimation in extensible databases - a neural network approach

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: m. seetha lakshmi , shaoyu zhou
",n
"LEFT id: NA
RIGHT id: 0

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the wasa2 object-oriented workflow management system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: gottfried vossen , mathias weske
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: A radar transmitter apparatus comprising a radar transmitter equipped with a modulator arranged in an oil-filled housing, the modulator being held in spaced relationship with respect to the inner walls of the housing in order to form an intermediate space for the convection flow of the oil. The housing is substantially trough or vat-shaped and covered by a trough or vat-shaped cover member. In the internal chamber or space between the cover member and the modulator, which internal space is wetted by the oil, there is arranged, on the one hand, a magnetron attached at the cover member and, on the other hand, a thyratron which is mounted directly below an opening at the cover member. This opening is closable by means of oil sealed throughpassage means.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 51

LEFT text: In modern database applications the similarity or dissimilarity of complex objects is examined by performing distance-based queries (DBQs) on data of high dimensionality. The R-tree and its variations are commonly cited multidimensional access methods that can beused for answering such queries. Although the related algorithms work well for low-dimensional data spaces, their performance degrades as the number of dimensions increases (dimensionality curse). In order to obtain acceptable response time in high-dimensional data spaces, algorithms that obtain approximate solutions can be used. Approximation techniques, like N-consider (based on the tree structure), α-allowance and e-approximate (based on distance), or Time-consider (based on time) can be applied in branch-and-bound algorithms for DBQs inorder to control the trade-off between cost and accuracy of the result. In this paper, we improve previous approximate DBQ algorithms by applying a combination of the approximation techniques in the same query algorithm (hybrid approximation scheme). We investigate the performance of these improvements for one of the most representative DBQs (the K-closest pairs query, K-CPQ) in high-dimensional data spaces, as well as the influence of the algorithmic parameters on the control of the trade-off between the response time and the accuracy of the result. The outcome of the experimental evaluation, using synthetic and real datasets, is the derivation of the outperforming DBQ approximate algorithm for large high-dimensional point datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: walrus : a similarity retrieval algorithm for image databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: apostol natsev , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1415

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: lopix : a system for xml data integration and manipulation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: wolfgang may
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1587

LEFT text: In this paper, we explore an approach of interleaving a bushy execution tree with hash filters to improve the execution of multi-join queries. Similar to semi-joins in distributed query processing, hash filters can be applied to eliminate non-matching tuples from joining relations before the execution of a join, thus reducing the join cost. Note that hash filters built in different execution stages of a bushy tree can have different costs and effects. The effect of hash filters is evaluated first. Then, an efficient scheme to determine an effective sequence of hash filters for a bushy execution tree is developed, where hash filters are built and applied based on the join sequence specified in the bushy tree so that not only is the reduction effect optimized but also the cost associated is minimized. Various schemes using hash filters are implemented and evaluated via simulation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel evaluation of multi-join queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: annita n. wilschut , jan flokstra , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 2269

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementing lazy database updates for an object database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1466

LEFT text: Recent demands for querying big data have revealed various shortcomings of traditional database systems. This, in turn, has led to the emergency of a new kind of query mode, approximate query.Online aggregation is a sample-based technology for approximate querying. It becomes quite indispensable in the era of information explosion today. Online aggregation continuously gives an approximate result with some error estimation (usually confidence interval) until all data are processed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: eager aggregation and lazy aggregation

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weipeng p. yan , per - &#197; ke larson
",n
"LEFT id: NA
RIGHT id: 1643

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast subsequence matching in time-series databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christos faloutsos , m. ranganathan , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 1088

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sampling large databases for association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hannu toivonen
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 1789

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel mining algorithms for generalized association rules with classification hierarchy

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: takahiko shintani , masaru kitsuregawa
",n
"LEFT id: NA
RIGHT id: 539

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general ""sketch""-based methods for capturing various linear projections and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1107

LEFT text: Semi-structured documents (e.g. journal art,icles, electronic mail, television programs, mail order catalogs, . ..) a.re often not explicitly typed; the only available t,ype information is the implicit structure. An explicit t,ype, however, is needed in order to a.pply objectoriented technology, like type-specific methods. In this paper, we present a.n experimental vector space cla.ssifier for determining the type of semi-structured documents. Our goal was to design a. high-performa.nce classifier in t,erms of accuracy (recall and precision), speed, and extensibility.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: type classification of semi-structured documents

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: markus tresch , neal palmer , allen luniewski
",y
"LEFT id: NA
RIGHT id: 442

LEFT text: Testing an SQL database system by running large sets of deterministic or stochastic SQL statements is common practice in commercial database development. However, code defects often remain undetected as the query optimizer's choice of an execution plan is not only depending on the query but strongly influenced by a large number of parameters describing the database and the hardware environment. Modifying these parameters in order to steer the optimizer to select other plans is difficult since this means anticipating often complex search strategies implemented in the optimizer. In this paper we devise algorithms for counting, exhaustive generation, and uniform sampling of plans from the complete search space. Our techniques allow extensive validation of both generation of alternatives, and execution algorithms with plans other than the optimized one—if two candidate plans fail to produce the same results, then either the optimizer considered an invalid plan, or the execution code is faulty. When the space of alternatives becomes too large for exhaustive testing, which can occur even with a handful of joins, uniform random sampling provides a mechanism for unbiased testing. The technique is implemented in Microsoft's SQL Server, where it is an integral part of the validation and testing process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: counting , enumerating , and sampling of execution plans in a cost-based query optimizer

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: florian waas , c &#233; sar galindo-legaria
",y
"LEFT id: NA
RIGHT id: 1072

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for materialized view design in data warehousing environment

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jian yang , kamalakar karlapalem , qing li
",y
"LEFT id: NA
RIGHT id: 1074

LEFT text: Efficient user-adaptable similarity search more and more increases in its importance for multimedia and spatial database systems. As a general similarity model for multi-dimensional vectors that is adaptable to application requirements and user preferences, we use quadratic form distance functions which have been successfully applied to color histograms in image databases [Fal+ 94]. The components aij of the matrix A denote similarity of the components i and j of the vectors. Beyond the Euclidean distance which produces spherical query ranges, the similarity distance defines a new query type, the ellipsoid query. We present new algorithms to efficiently support ellipsoid query processing for various user-defined similarity matrices on existing precomputed indexes. By adapting techniques for reducing the dimensionality and employing a multi-step query processing architecture, the method is extended to high-dimensional data spaces. In particular, from our algorithm to reduce the similarity matrix, we obtain the greatest lowerbounding similarity function thus guaranteeing no false drops. We implemented our algorithms in C++ and tested them on an image database containing 12,000 color histograms. The experiments demonstrate the flexibility of our method in conjunction with a high selectivity and efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",y
"LEFT id: NA
RIGHT id: 448

LEFT text: We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: using quantitative information for efficient association rule generation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: b. p &#244; ssas , m. carvalho , r. resende , w. meita , jr.
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 138

LEFT text: The article proposes a scalable protocol for replication management in large-scale replicated systems. The protocol organizes sites and data replicas into a tree-structured, hierarchical cluster architecture. The basic idea of the protocol is to accomplish the complex task of updating replicated data with a very large number of replicas by a set of related but independently committed transactions. Each transaction is responsible for updating replicas in exactly one cluster and invoking additional transactions for member clusters. Primary copies (one from each cluster) are updated by a cross-cluster transaction. Then each cluster is independently updated by a separate transaction. This decoupled update propagation process results in possible multiple views of replicated data in a cluster. Compared to other replicated data management protocols, the proposed protocol has several unique advantages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: multiview access protocols for large-scale replication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: xiangning liu , abdelsalam helal , weimin du
",y
"LEFT id: NA
RIGHT id: 1566

LEFT text: Recent research activities in the area of Temporal Databases have revealed some problems related to the definition of time. In this paper we discuss the problem arising from the definition of valid time and the assumptions about valid time, which exist in current Temporal Database approaches. For this problem we propose a solution, while we identify some consistency problems that may appear in Temporal Databases, and which require further investigation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on the issue of valid time ( s ) in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stavros kokkotos , efstathios v. ioannidis , themis panayiotopoulos , constantine d. spyropoulos
",y
"LEFT id: NA
RIGHT id: 323

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 1278

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: declarative specification of web sites with s

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mary fern &#225; ndez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 2093

LEFT text: In this paper, we investigate algorithms for generic schema matching, outside of any particular data model or application. We first present a taxonomy for past solutions, showing that a rich range of techniques is available. We then propose a new algorithm, Cupid, that discovers mappings between schema elements based on their names, data types, constraints, and schema structure, using a broader set of techniques than past approaches. Some of our innovations are the integrated use of linguistic and structural matching, context-dependent matching of shared types, and a bias toward leaf structure where much of the schema content resides. After describing our algorithm, we present experimental results that compare Cupid to two other schema matching systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on schema matching with opaque column names and data values

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jaewoo kang , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 914

LEFT text: National Tsing Hua University (NTttU) was founded in 1911 and is located in a suburb of the city of Hsinehu, Taiwan, about 50 miles southwest of Taipei, the capital city. Its Computer Science Department was established in 1977, and currently has 23 faculty members

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at ut arlington

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sharma chakravarthy , alp aslandogan , ramez elmasri , leonidas fegaras , junghwan oh
",n
"LEFT id: NA
RIGHT id: 1892

LEFT text: Since it quotes extensively from writings of my own, I feel obliged to respond to the article “Domains, Relations and Religious Wars,” by R. Camps (SIGMOD Record 25, No. 3, September 1996). In that article, Camps is clearly suggesting (among other things) that my definition of the term “domain” has changed over the years. I agree, it has! But Camps goes on to say: “… considering that [Date's book An Introduction to Database Systems] was the bible [Camps' italics] where most university graduates all over the world learnt, I believe that Date can be held partly responsible for the lack of implementation of domains [in today's SQL DBMSs].”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a response to r. camps ' article domains , relations and religious wars

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: c. j. date
",y
"LEFT id: NA
RIGHT id: 1455

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1041

LEFT text: To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 2156

LEFT text: Vendors of commercial database management system face many challenges in incorporating into their products innovative technologies developed in academia. Pragmatic considerat,ions and operational requirements can limit the viability of applying promising research. Technology leadership in commercial products is often the result of taking unconventional approaches rather than following “conventional wisdom”, as illustrated with several examples of technologies in Oracle8 and its predecessors. The challenge shared by researchers and practitioners alike: “making what we do matter.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: innovation in database management : computer science vs. engineering

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kenneth r. jacobs
",y
"LEFT id: NA
RIGHT id: 1772

LEFT text: This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. that could describe a wide variety of sequence data, and a query algebra that could be used to represent queries over sequences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",y
"LEFT id: NA
RIGHT id: 291

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of xml middle-ware queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mary fernandez , atsuyuki morishima , dan suciu
",n
"LEFT id: NA
RIGHT id: 1994

LEFT text: Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for supporting data integration using the materialized and virtual approaches

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: richard hull , gang zhou
",y
"LEFT id: NA
RIGHT id: 803

LEFT text: On-Line Analytical Processing (OLAP) based on a dimensional view of data is being used increasingly for the purpose of analyzing very large amounts of data. To improve query performance, modern OLAP systems use a technique known as practical pre-aggregation, where select combinations of aggregate queries are materialized and re-used to compute other aggregates; full preaggregation, where all combinations of aggregates are materialized, is infeasible. However, this reuse of aggregates is contingent on the dimension hierarchies and the relationships between facts and dimensions satisfying stringent constraints, which severely limits the scope of practical preaggregation. This paper significantly extends the scope of practical pre-aggregation to cover a much wider range of realistic situations. Specifically, algorithms are given that transform “irregular” dimension hierarchies and fact-dimension relationships, which often occur in real-world OLAP applications, into well-behaved structures that, when used by existing OLAP systems, enable practical pre-aggregation. The algorithms have low computational complexity and may be applied incrementally to reduce the cost of updating OLAP structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: extending practical pre-aggregation in on-line analytical processing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: torben bach pedersen , christian s. jensen , curtis e. dyreson
",y
"LEFT id: NA
RIGHT id: 1601

LEFT text: The web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology, and content are evolving. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The web's users are highly diverse and can access the it from a variety of devices and interfaces, at different places and times, and for varying purposes. Thus, new techniques are being developed for personalising the presentation and content of web-based information depending on how it is being accessed and on the individual user's requirements and preferences. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1384

LEFT text: A range query applies an aggregation operation over all selected cells of an OLAP data cube where the selection is specified by providing ranges of values for numeric dimensions. We present fast algorithms for range queries for two types of aggregation operations: SUM and MAX. These two operations cover techniques required for most popular aggregation operations, such as those supported by SQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: temporal queries in olap

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alberto o. mendelzon , alejandro a. vaisman
",n
"LEFT id: NA
RIGHT id: 595

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 737

LEFT text: In the last few years, several works in the literature have addressed the problem of data extraction from Web pages. The importance of this problem derives from the fact that, once extracted, the d...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a brief survey of web data extraction tools

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alberto h. f. laender , berthier a. ribeiro-neto , altigran s. da silva , juliana s. teixeira
",y
"LEFT id: NA
RIGHT id: 78

LEFT text: Conventional spatial queries are usually meaningless in dynamic environments since their results may be invalidated as soon as the query or data objects move. In this paper we formulate two novel query types, time parameterized and continuous queries, applicable in such environments. A time-parameterized query retrieves the actual result at the time when the query is issued, the expiry time of the result given the current motion of the query and database objects, and the change that causes the expiration.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: closest pair queries in spatial databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: antonio corral , yannis manolopoulos , yannis theodoridis , michael vassilakopoulos
",n
"LEFT id: NA
RIGHT id: 2040

LEFT text: We consider the problem of evaluating large numbers of XPath filters, each with many predicates, on a stream of XML documents. The solution we propose is to lazily construct a single deterministic pushdown automata, called the XPush Machine from the given XPath fllters. We describe a number of optimization techniques to make the lazy XPush machine more efficient, both in terms of space and time. The combination of these optimizations results in high, sustained throughput. For example, if the total number of atomic predicates in the filters is up to 200000, then the throughput is at least 0.5 MB/sec: it increases to 4.5 MB/sec when each fllter contains a single predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream processing of xpath queries with predicates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ashish kumar gupta , dan suciu
",y
"LEFT id: NA
RIGHT id: 1178

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 793

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating top-k selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1101

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: further improvements on integrity constraint checking for stratifiable deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sin yeung lee , tok wang ling
",n
"LEFT id: NA
RIGHT id: 383

LEFT text: Knowledge based applications require linguistic, terminological and ontological resources. These applications are used to fulfill a set of tasks such as semantic indexing, knowledge extraction from text, information retrieval, etc. Using these resources and combining them for the same application is a tedious task with different levels of complexity. This requires their representation in a common language, extracting the required knowledge and designing effective large scale storage structures offering operators for resources management. For instance, ontology repositories were created to address these issues by collecting heterogeneous ontologies. They generally offer a more effective indexing of these resources than general search engines by generating alignments and annotations to ensure their interoperability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating contents and structure in text retrieval

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ricardo baeza-yates , gonzalo navarro
",n
"LEFT id: NA
RIGHT id: 2198

LEFT text: Anomaly detection is an important challenge for tasks such as fault diagnosis and intrusion detection in energy constrained wireless sensor networks. A key problem is how to minimise the communication overhead in the network while performing in-network computation when detecting anomalies. Our approach to this problem is based on a formulation that uses distributed, one-class quarter-sphere support vector machines to identify anomalous measurements in the data. We demonstrate using sensor data from the Great Duck Island Project that our distributed approach is energy efficient in terms of communication overhead while achieving comparable accuracy to a centralised scheme.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: distributed deviation detection in sensor networks

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: themistoklis palpanas , dimitris papadopoulos , vana kalogeraki , dimitrios gunopulos
",y
"LEFT id: NA
RIGHT id: 1706

LEFT text: The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: text databases : a survey of text models and systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arjan loeffen
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 460

LEFT text: Database replication is traditionally seen as a way to increase the availability and performance of distributed databases. Although a large number of protocols providing data consistency and fault-tolerance have been proposed, few of these ideas have ever been used in commercial products due to their complexity and performance implications. Instead, current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication. As an alternative, we propose a suite of replication protocols that addresses the main problems related to database replication. On the one hand, our protocols maintain data consistency and the same transactional semantics found in centralized systems. On the other hand, they provide flexibility and reasonable performance. To do so, our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases. This allows us to eliminate the possibility of deadlocks, reduce the message overhead and increase performance. A detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a new approach to developing and implementing eager database replication protocols

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina kemme , gustavo alonso
",y
"LEFT id: NA
RIGHT id: 2250

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: As teachers, if we believe content knowledge matters the most for successful instruction, we may not understand that getting to know students and discovering their strengths as learners are equally important. Teachers are instructional islands with a lot of content to share but perhaps unconnected to the learners that make up the classroom. If as teachers, we describe students by saying, “she is a math wizard,” “she is a science ace,” or “he is a sponge for historical facts,” we can communicate a lot about students with minimal language. These metaphors help us make comparisons that evoke multiple layers of meaning, and yet thinking metaphorically is also an aspect of everyday life. Cognitive scientists Lakoff and Johnson (2008) argued that our conceptualizations of the world around us are metaphorical and provided examples of metaphors such as “time is money” and suggested that the way we construe argument is conceived in metaphors of war when we “attack a position,” for example, to support a philosophical claim. From an educational philosophy perspective, Greene contended learning is a landscape (1973) and teachers are philosophers working to help learners resist the forces that limit and oppress them (1988) to attain freedom to think for themselves. These theorists recognized the epistemological power of metaphor and challenged us to see its educational potential. Comparisons through metaphoric thinking afford different perspectives and open imaginative possibilities, challenging us to see familiar relationships in new ways. Metaphors can push us to think about teacher education differently as well and move beyond familiar views of clinical experiences, teacher interns, teacher preparation programs, and who we are as educators to see these concepts more complexly. The pedagogical innovation study and six empirical studies in this issue evoke metaphors of exploration, dialogue, windows, building, partnering, and innovating to inform and complicate our understanding teacher education. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1836

LEFT text: In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L 1 norm) is consistently more preferable than the Euclidean distance metric (L 2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distance-based indexing for high-dimensional metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: Database systems offer efficient and reliable technology to query structured data. However, because of the explosion of the World Wide Web [11], an increasing amount of information is stored in repositories organized according to less rigid structures, usually as hypertextual documents, and data access is based on browsing and information retrieval techniques. Since browsing and search engines present important limitations [8], several query languages [19, 20, 23] for the Web have been recently proposed. These approaches are mainly based on a loose notion of structure, and tend to see the Web as a huge collection of unstructured objects, organized as a graph. Clearly, traditional database techniques are of little use in this field, and new techniques need to be developed. In this paper, we present the approach to the management of Web data as attacked in the ArtANEUS project carried out by the database group at Universith di l=toma Tre. Our approach is based on a generalization of the notion of view to the Web framework. In fact, in traditional databases, views represent an essential tool for restructuring and integrating da ta to be presented to the user. Since the Web is becoming a major computing platform and a uniform interface for sharing data, we believe that also in this field a sophisticate view mechanism is needed, with novel features due to the semi-structured nature of the Web. First, in this context, restructuring and presenting da ta under different perspectives requires the generation of derived Web hypertexts, in order to re-organize and re-use portions of the Web. To do this, da ta from existing Web sites must be extracted, and then queried and integrated in order to build new hypertexts, i.e., hypertextual views over the original sites; these manipulations can be better attained in a more structured framework, in which traditional database technology can be leveraged to analyze and correlate information. Therefore, there seem to be different view levels in this framework: (i) at the first level, da ta are extracted from the sites of interest and given a database structure, which represents a first structured view over the original semi-structured data; (ii) then, further database views can be built by means of reorganizations and integrations based on traditional database techniques; (iii) finally, a derived hypertext can be generated offering an alternative or integrated hypertextual view over the original sites. In the process, data go from a loosely structured organizat ion-the Web pages-to a very structured onethe database--and then again to Web structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 2117

LEFT text: The continuous growth in scale and diversity of computer networks and network components has made network management one of the most challenging issues facing network administrators. It has become impossible to carry out network management functions without the support of automated tools and applications. In this chapter, the major network management issues, including network management requirements, functions, techniques, security, some wellknown network management protocols and tools, will be discussed. Location management for the wireless cellular networks will also be briefly described. Finally, policy-based network management, which is a promising direction for the next generation of network management, will be briefly described.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: issues in data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lukasz golab , m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 1209

LEFT text: The Java programming language [1,3] from its inception has been publicized as a web programming language. Many programmers have developed simple applications such as games, clocks, news tickers and stock tickers in order to create informative, innovative web sites. However, it is important to note that the Java programming language possesses much more capability. The language components and constructs originally designed to enhance the functionality of Java as a web-based programming language can be utilized in a broader extent. Java provides a developer with the tools allowing for the creation of innovative network, database, and Graphical User Interface (GUI) applications. In fact, Java and its associated technologies such as JDBC API [11,5], JDBC drivers [2,12], threading [10], and AWT provide the programmer with the much-needed assistance for the development of platform-independent database-independent interfaces. Thus, it is possible to build a graphical database interface capable of connecting and querying distributed databases [13,14]. Here are components that are important for building the database interface we have in mind.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: algebraic query optimisation for database programming languages

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 771

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views in oracle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: randall g. bello , karl dias , alan downing , james j. feenan , jr. , james l. finnerty , william d. norcott , harry sun , andrew witkowski , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 824

LEFT text: Data warehousing and On-Line Analytical Processing (OLAP) are becoming critical components of decision support as advances in technology are improving the ability to manage and retrieve large volumes of data. Data warehousing refers to \a collection of decision support technologies aimed at enabling the knowledge worker (executive, manager, analyst) to make better and faster decisions"" [1]. OLAP refers to the technique of performing complex analysis over the information stored in a data warehouse. It is often used by management analysts and decision makers in a variety of functional areas such as sales and marketing planning. Typically, OLAP queries look for speci c trends and anomalies in the base information by aggregating, ranging, ltering and grouping data in many di erent ways [8]. E cient query processing is a critical requirement for OLAP because the underlying data warehouse is very large, queries are often quite complex, and decision support applications typically require in-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a novel index supporting high volume data warehouse insertion

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chris jermaine , anindya datta , edward omiecinski
",n
"LEFT id: NA
RIGHT id: 1867

LEFT text: Better knowledge of natural enemy communities and their relative contribution to biological control is needed to design ecology-based pest management in agro-ecosystems. Here, we investigated the arthropod communities of pearl millet-based agro-ecosystems in sub-Saharan Africa (Senegal), with a focus on natural enemies of the millet head miner (MHM), Heliocheilus albipunctella (de Joannis).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: geominer : a system prototype for spatial data mining

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jaiwei han , krzysztof koperski , nebojsa stefanovic
",n
"LEFT id: NA
RIGHT id: 562

LEFT text: The database area has been one of those areas of computerscience which have very directly been driven by applicationrequirements; this is true today in three ways: First, the userswant more application specific support from the database, and theyexpect the DBMS to have more semantic application knowledge.Second, users want database support for new applications which aresometimes far from the traditional database applications andintroduce completely new requirements as well as the need tosmoothly integrate database technology with other advancedtechnologies (e.g. neural nets) in one application. Finally, theembedding of databases into interactive work environments - forinstance, the use of databases in cooperative environments(computer supported cooperative work) - forces the databasecommunity to reconsider some of the traditional beliefs aboutdatabases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of illinois at urbana-champaign

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: m. winslett , k. chang , a. doan , j. han , c. zhai , y. zhou
",n
"LEFT id: NA
RIGHT id: 271

LEFT text: A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: reconciling schemas of disparate data sources : a machine-learning approach

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anhai doan , pedro domingos , alon y. halevy
",y
"LEFT id: NA
RIGHT id: 1500

LEFT text: We present a structured, iterative methodology for user-centered design and evaluation of VE user interaction. We recommend performing (1) user task analysis followed by (2) expert guidelines-based evaluation, (3) formative user-centered evaluation, and finally (4) comparative evaluation. In this article we first give the motivation and background for our methodology, then we describe each technique in some detail. We applied these techniques to a real-world battlefield visualization VE. Finally, we evaluate why this approach provides a cost-effective strategy for assessing and iteratively improving user interaction in VEs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a database interface for file update

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: serge abiteboul , sophie cluet , tova milo
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1459

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 608

LEFT text: The Semantic Web is a vision the idea of having data on the Web defined and linked in such a way that it can be used by machines not just for display purposes but for automation, integration and reuse of data across various applications. Technically, however, there is a widespread misconception that the Semantic Web is primarily a rehash of existing AI and database work focused on encoding knowledge representation formalisms in markup languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler, and Moran seek to dispel this notion by presenting the broad dimensions of this emerging Semantic Web and the multi-disciplinary technological underpinnings like machine learning, information retrieval, service-oriented architectures, and grid computing, thus combining the informational and computational aspects needed to realize the full potential of the Semantic Web vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on semantic web and data management

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: robert meersman , amit sheth
",n
"LEFT id: NA
RIGHT id: 2003

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cmvf : a novel dimension reduction scheme for efficient indexing in a large image database

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jialie shen , anne h. h. ngu , john shepherd , du q. huynh , quan z. sheng
",n
"LEFT id: NA
RIGHT id: 947

LEFT text: In this paper, we present the first incremental clustering algorithm. Our algorithm is based on the clustering algorithm DBSCAN which is applicable to any database containing data from a metric space, e.g., to a spatial database or to a WWW-log database. Due to the density-based nature of DBSCAN, the insertion or deletion of an object affects the current clustering only in the neighborhood of this object. Thus, efficient algorithms can be given for incremental insertions and deletions to an existing clustering. Based on the formal definition of clusters, it can be proven that the incremental algorithm yields the same result as DBSCAN. A performance evaluation of IncrementalDBSCAN on a spatial database as well as on a WWW-log database is presented, demonstrating the efficiency of the proposed algorithm. IncrementalDBSCAN yields significant speed-up factors over DBSCAN even for large numbers of daily updates in a data warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental organization for data recording and warehousing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: h. v. jagadish , p. p. s. narayan , s. seshadri , s. sudarshan , rama kanneganti
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 2089

LEFT text: Object-oriented and object-relational DBMS support set valued attributes, which are a natural and concise way to model complex information. However, there has been limited research to-date on the evaluation of query operators that apply on sets. In this paper we study the join of two relations on their set-valued attributes. Various join types are considered, namely the set containment, set equality, and set overlap joins. We show that the inverted file, a powerful index for selection queries, can also facilitate the efficient evaluation of most join predicates. We propose join algorithms that utilize inverted files and compare them with signature-based methods for several set-comparison predicates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient processing of joins on set-valued attributes

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: nikos mamoulis
",y
"LEFT id: NA
RIGHT id: 221

LEFT text: This paper is a survey of work and issues on multidimensional search trees. We provide a classification of such methods, we describe the related algorithms, we present performance analysis efforts, and finally outline future research directions. Multi-dimensional search trees and Spatial Access Methods, in general, are designed to handle spatial objects, like points, line segments, polygons, polyhedra etc. The goal is to support spatial queries, such as nearest neighbors queries (find all cities within 10 miles from Washington D.C.), or range queries (find all the lakes on earth, within 30 and 40 degrees of latitude), and so on. The applications are numerous, including traditional database multi-attribute indexing, Geographic Information Systems and spatial database systems, and indexing multimedia databases by content. $‘rom the spatial databases viewpoint we can dist,inguish between two major classes of access methods:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient concurrency control in multidimensional access methods

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 1769

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applications of java programming language to database management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bradley f. burton , victor w. marek
",n
"LEFT id: NA
RIGHT id: 1938

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 277

LEFT text: Availability requirements for database systems are more stringent than ever before with the widespread use of databases as the foundation for ebusiness. This paper highlights Fast-Start™ Fault Recovery, an important availability feature in Oracle, designed to expedite recovery from unplanned outages. Fast-Start allows the administrator to configure a running system to impose predictable bounds on the time required for crash recovery. For instance, fast-start allows fine-grained control over the duration of the roll-forward phase of crash recovery by adaptively varying the rate of checkpointing with minimal impact on online performance. Persistent transaction locking in Oracle allows normal online processing to be resumed while the rollback phase of recovery is still in progress, and fast-start allows quick and transparent rollback of changes made by uncommitted transactions prior to a crash.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast-start : quick fault recovery in oracle

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tirthankar lahiri , amit ganesh , ron weiss , ashok joshi
",y
"LEFT id: NA
RIGHT id: 1522

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 1972

LEFT text: We introduce a new algorithm to compute the spatial join of two or more spatial data sets, when indexes are not available on them. Size Separation Spatial Join (S<3J<) imposes a hierarchical decomposition of the data space and, in contrast with previous approaches, requires no replication of entities from the input data sets. Thus its execution time depends only on the sizes of the joined data sets. We describe S<3J< and present an analytical evaluation of its I/O and processor requirements comparing them with those of previously proposed algorithms for the same problem. We show that S<3J< has relatively simple cost estimation formulas that can be exploited by a query optimizer. S<3J< can be efficiently implemented using software already present in many relational systems. In addition, we introduce Dynamic Spatial Bitmaps< (DSB), a new technique that enables S<3J< to dynamically or statically exploit bitmap query processing techniques. Finally, we present experimental results for a prototype implementation of S<3J< involving real and synthetic data sets for a variety of data distributions. Our experimental results are consistent with our analytical observations and demonstrate the performance benefits of S<3J< over alternative approaches that have been proposed recently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partition based spatial-merge join

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jignesh m. patel , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: In this issue of Leaven, we explore the theme of local church ministry by honoring the legacy of Paul and Kay Watson. The following reflections and essays are written by those who bear appreciative witness to the faithful service of this Christian couple. Paul and Kay have dedicated their time, love, and spiritual gifts for the last three decades to the Cole Mill Road congregation in Durham, North Carolina. And through their missionary travels and a host of teaching opportunities, their influence has been felt by those far beyond their home church.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 276

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting constraint-like data characterizations in query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: parke godfrey , jarek gryz , calisto zuzarte
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1954

LEFT text: In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql query optimization : reordering for a general class of queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 1870

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",y
"LEFT id: NA
RIGHT id: 1063

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache conscious algorithms for relational query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ambuj shatdal , chander kant , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 848

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: This paper examines a technique for dynamically inserting and removing drop operators into query plans as required by the current load. We examine two types of drops: the first drops a fraction of the tuples in a randomized fashion, and the second drops tuples based on the importance of their content. We address the problems of determining when load shedding is needed, where in the query plan to insert drops, and how much of the load should be shed at that point in the plan. We describe efficient solutions and present experimental evidence that they can bring the system back into the useful operating range with minimal degradation in answer quality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 765

LEFT text: Multi-dimensional data is being generated at an ever increasing rate in practically all modern applications. The development of techniques and tools to extract useful information out of such data is one of critical challenges to be tackled in the 21st century. Visualization is one popular technique for achieving effective data exploration by exploiting the visual perception abilities of domain experts. Visualization involves the graphical presentation of data and information for the purposes of communicating results, verifying hypotheses, and qualitative exploration. In this demonstration, we present our solution to particular challenges we have been tackling in this area in the context of our XMDVtool project, a multi-year effort funded by NSF. These include multivariate data visualization to facilitate outlier and pattern discovery via a variety of displays, visual interaction tools, scalability of these visualization techniques to large data sets, interaction with commercial database technology and, more recently, extensions to handle data of very high-dimensionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xmdvtool : visual interactive data exploration and trend discovery of high-dimensional data sets

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: elke a. rundensteiner , matthew o. ward , jing yang , punit r. doshi
",y
"LEFT id: NA
RIGHT id: 1772

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database and information system research group at the university of ulm

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter dadam , wolfgang klas
",n
"LEFT id: NA
RIGHT id: 1125

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 679

LEFT text: In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size. This is a highly desirable property for any data reduction system since the problem itself is motivated by the large size of data sets. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. Furthermore, the subspace sampling technique is able to reveal important local subspace characteristics of high dimensional data which can be harnessed for effective solutions to problems such as selectivity estimation and approximate nearest neighbor search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hierarchical subspace sampling : a unified framework for high dimensional data reduction , selectivity estimation and nearest neighbor search

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",y
"LEFT id: NA
RIGHT id: 783

LEFT text: Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aqua : a fast decision support systems using approximate query answers

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 280

LEFT text: Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view and index selection tool for microsoft sql server 2000

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1831

LEFT text: We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to find the N tuples that satisfy the query the best but not necessarily completely in an efficient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The first step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four different techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare different methods and very promising results are obtained using the method that requires no cooperation from local databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 213

LEFT text: Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a multimedia presentation algebra

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. adali , m. l. sapino , v. s. subrahmanian
",y
"LEFT id: NA
RIGHT id: 1507

LEFT text: A well-known challenge in data warehousing is the efficient incremental maintenance of warehouse data in the presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype data warehousing system supporting incremental maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: view maintenance in a warehousing environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: yue zhuge , h &#233; ctor garc &#237; a-molina , joachim hammer , jennifer widom
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 1002

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",y
"LEFT id: NA
RIGHT id: 178

LEFT text: As we embark on the information age the use of electronic information is spreading through all sectors of society, both nationally and internationally. As a result, commercial organizations, educational institutions and government agencies are finding it essential to be linked by world wide networks, and commercial Internet usage is growing at an accelerating pace.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: electronic commerce : tutorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nabil r. adam , yelena yesha
",y
"LEFT id: NA
RIGHT id: 1032

LEFT text: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 527

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross , theodore johnson , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: A multimedia database is a controlled collection of multimedia data items such as text, images, graphic objects, video and audio. A multimedia database management system (DBMS) provides support for the creation, storage, access, querying and control of a multimedia database. The requirements of a multimedia DBMS are: multimedia data modeling; multimedia object storage; multimedia indexing, retrieval and browsing; and multimedia query support. This paper discusses a general framework for multimedia database systems and describes the requirements and architecture for these systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 997

LEFT text: An important requirement for multimedia presentations is the ability to compose new multimedia objects from the existing ones using temporal relationships. When compositions of continuous media objects are specified dynamically, the task of displaying these objects poses new challenges. These challenges are addressed in this paper. We show that in the case of a single composite object retrieval, a prefetching technique, simple sliding, provides an approach to reduce latency and buffering requirements. We extend this prefetching technique to the problem of retrieving multiple composite objects simultaneously. This new technique is termed buffered sliding. We consider several variants of the buffered sliding algorithm. A simulationbased study is used to compare their usage pattern of available memory and in determining their relative merits in reducing latency and increasing disk bandwidth utilization. *Research supported in part by the National Science Foundation under grants IRI-9203389, IRI-9258362 (NY1 award), and CDA-9216321, and a Hewlett-Packard unrestricted cash/equipment gift. Permission to copy without fee all OT part of this material is granted provided that the copies are not made OT distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, OT to republish, Tequires a fee and/or special permission from the Endowment. Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 An important requirement for multimedia information systems is the ability to compose new multimedia objects from the existing multimedia objects [LG91]. Temporal primitives (e.g., before, after, overlaps [All83]) p rovide one of the most powerful and natural ways of authoring composition. Such composition is necessary in the domain of electronic publishing, computer music, news editing and many other applications. In this paper, we investigate how a multimedia storage system can display a composite object. We focus on composite objects that are authored dynamically. To illustrate an example environment, consider a TVnews editor preparing to present new footage on unrest in Bosnia. He requires background material to provide the audience with a context. He considers playing a sequence of clips one after another from different footage taken at different times to author a thirty second presentation. He may decide to accompany a footage with appropriate music in parts (i.e., music overlaps video). He may conclude his presentatiod with split windows that concurrently display short clips that leave us with the images of diverse scenes in Bosnia. During editing of such a presentation, he would try several possible composition, possibly picking different sets of clips or music from the repository. Surely, the editor would like to display his composition during the authoring process to evaluate his choice.’ Thus, the process of editing a news story consisted of specifying composite objects using temporal relationships and then displaying those. Note that displaying atomic objects of highbandwidth continuous media objects, such as video (requiring no composition) is a challenging task in itself. Video clips require a continuous bandwidth for their display. For example, the bandwidth re-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: resource scheduling for composite multimedia objects

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis , banu &#214; zden
",y
"LEFT id: NA
RIGHT id: 34

LEFT text: Sensors are often employed to monitor continuously changing entities like locations of moving objects and temperature. The sensor readings are reported to a centralized database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), the database may not be able to keep track of the actual values of the entities, and use the old values instead. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. In this paper, we present a frame-work that represents uncertainty of sensor data. Depending on the amount of uncertainty information given to the application, different levels of imprecision are presented in a query answer. We examine the situations when answer imprecision can be represented qualitatively and quantitatively. We propose a new kind of probabilistic queries called Probabilistic Threshold Query, which requires answers to have probabilities larger than a certain threshold value. We also study techniques for evaluating queries under different details of uncertainty, and investigate the tradeoff between data uncertainty, answer accuracy and computation costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: managing web data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1990

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of round-trips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose the use of the context in which an object is loaded as a predictor of future accesses, where a context can be a stored collection of relationships, a query result, or a complex object. When an object O's state is loaded, similar state for other objects in O's context is prefetched. We present a design for maintaining context and for using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe several variations of the optimization: selectively applying the technique based on application and database characteristics, using application-supplied performance hints, using concurrent database queries to support asynchronous prefetch, prefetching across relationship paths, and delayed prefetch to save database round-trips.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based optimization for magic : algebra and implementation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , joseph m. hellerstein , hamid pirahesh , t. y. cliff leung , raghu ramakrishnan , divesh srivastava , peter j. stuckey , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1325

LEFT text: This paper describes the new architecture for supporting the Teradata commercial VLDB on several new operating environments. We start with an overview of the Teradata database software architecture, specifically the Parallel Database Extensions (PDE) that serve as a layer between the NCR Unix OS and the database software. We then describe the challenges of implementing an Open PDE for several new platforms, (Microsoft Windows 2000, Linux, HP-UX, and Microsoft Windows XP).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: operating system extensions for the teradata parallel vldb

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john catozzi , sorana rabinovici
",y
"LEFT id: NA
RIGHT id: 221

LEFT text: Data in relational databases is frequently stored and retrieved using B-Trees. In &cis,ion isugprt applications the key of the B-Tree frequently involves the concatenation of several fields of the relationdl’ table. During retrieval, it is desirable to be able to access a small subset of the table based’ on partial key information, where some fields of the key may either not be present, involve ranges, or lists ‘of values. It is also advantageous to altow. this type, of access-with gen&il expressions involving any combination of disjuncts on key columns. This paper &scribes a method whereby BTrees can be eficiently used to retrieve small subsets, thus avoiding large scans of potentially huge tables. Another benefit is the ability of this method to reduce the need for additional secondary indexes, thus saving space, maintenance cost, and random accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient concurrency control in multidimensional access methods

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: E-commerce is not a static field, but is constantly evolving to discover new and more effective ways of supporting businesses. Data management is an integral part of this effort, This special issue aims to report on some of the recent developments and identify some research directions in this area. Initially, e-commeree involved the use of ED] and intranets. Today we see the dominance of XML. Almost all recent elecn'onic commerce standards are based on X1VD... As a consequence, the amount of XML data being stored is large, and it is increasing. This naturally leads to the question of how to store and query the XML documents. The paper by Tian, DeWitt, Chen and Zhang describes the design and performance evaluation of alternative XM]., storage strategies. The results of this performance study provide valuable hints on how to store the XM1., files depending on the application. Personalization in e-commerce is about building customer loyalty by understanding and thus addressing the needs of each individual. E-commerce systems need customers' profiles to provide better services, 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",y
"LEFT id: NA
RIGHT id: 253

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hyperstorm-administering structured documents using object-oriented database technology

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , karl aberer
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 2293

LEFT text: Object-oriented database (OODB) users bring with them large quantities of legacy data (megabytes and even gigabIn addition, scientific OODB users continually generate new data. All this data must be loaded into the OODB. Every relational database system has a load utility , but most OODBs do not. The process of loading data into an OODB is complicated by inter-object references, or relationships, in the data. These relationships are expressed in the OODB as object identifiers, which are not known at the time the load data is generated; they may contain cycles; and there may be implicit system-maintained inverse relationships that must also be stored. W e introduce seven algorithms for loading data into an OODB that examine different techniques for dealing with circular and inverse relationships. W e present a performance study based on both an analytic model and an implementation of all seven algorithms on top of the Shore object repository . Our study demonstrates that it is ortant to choose a load algorithm carefully; in some cases the best algorithm achieved an improvement of one to two orders of magnitude over the naive algorithm

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bulk loading into an oodb : a performance study

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: janet l. wiener , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 2240

LEFT text: XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 2290

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 53

LEFT text: Zusammenfassung Obwohl der Einsatz von Data-Warehouse-Systemen zur gängigen Praxis moderner IT-Landschaften gehört, haben methodische Untersuchungen zum qualitätsorientierten Schemaentwurf erst in jüngerer Zeit begonnen. Ausgehend von einem Schemaentwurfsprozess für Data-Warehouse-Systeme werden aktuelle Entwicklungen zur Qualitätssicherung und Data-Warehouse-spezifische Qualitätskriterien wie Summierbarkeit sowie deren Verallgemeinerung zu mehrdimensionalen Normalformen und Selbstwartbarkeit vorgestellt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamat : a dynamic view management system for data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1204

LEFT text: The relational model of data incorporates fundamental assertions for entity integrity and referential integrity. Recently, these so-called relational invariants were more precisely specified by the new SQL2 standard. Accordingly, they have to be guaranteed by a relational DBMS to its users and, therefore, all issues of semantics and implementation became very important. The specification of referential integrity embodies quite a number of complications including the MATCH clause and a collection of referential actions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: access path support for referential integrity in sql2

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: theo h &#228; rder , joachim reinert
",y
"LEFT id: NA
RIGHT id: 720

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbcache : database caching for web application servers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mehmet altinel , qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , bruce g. lindsay , honguk woo , larry brown
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in POSTGRES.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 2176

LEFT text: It is a cliche that the Internet has revolutionized many aspects of life in the past decade. Scientific publishing is but one of the many enterprises that have been impacted by the connectivity and high bandwidth afforded by the World Wide Web. Most scientific journals now have a web presence. That said, it is still remarkable the degree to which ACM in general and TODS in particular have embraced the unique capabilities of the web to aid in the propagation of knowledge. Here I summarize the disparate and broad ways in which TODS utilizes the web, in all phases of publishing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: edgar f. codd : a tribute and personal memoir

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: c. j. date
",y
"LEFT id: NA
RIGHT id: 1950

LEFT text: This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: heraclitus : elevating deltas to be first-class citizens in a database programming language

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: shahram ghandeharizadeh , richard hull , dean jacobs
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 2171

LEFT text: In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: efficient dynamic mining of constrained frequent sets

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , carson kai-sang leung , raymond t. ng
",n
"LEFT id: NA
RIGHT id: 1988

LEFT text: Traditional protocols for distributed database management have high message overhead, lock or restrain access to resources during protocol execution, and may become impractical for some scenarios like real-time systems and very large distributed databases. In this paper we present the demarcation protocol; it overcomes these problems through the use of explicit linear arithmetic consistency constraints as the correctness criteria. The method establishes safe limits as “lines drawn in the sand” for updates and gives a way of changing these limits dynamically, enforcing the constraints at all times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintaining database consistency in presence of value dependencies in multidatabase systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: claire morpain , mich &#233; le cart , jean ferri &#233; , jean-fran &#231; ois pons
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: Information integration provides a competitive advantage to businesses and is fundamental to on demand computing. It is strategic area of investment by software companies today whose goal is to provide a unified view of the data regardless of differences in data format, data location and access interfaces, dynamically manage data placement to match availability, currency and performance requirements, and provide autonomic features that reduce the burden on IT staffs for managing complex data architectures. This paper describes the motivation for integrating information for on demand computing, explains its requirements, and illustrates its value through usage scenarios. As shown in the paper, there is still a tremendous amount of research, engineering, and development work needed to make the full information integration vision a reality and it is expected that software companies will continue to heavily invest in aggressively pursing the information integration vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1691

LEFT text: In June 1997, an international workshop on engineering of federated database systems has been held in Barcelona in conjunction with the 9th Conference on Advanced Information Systems Engineering (CAiSE'97). This paper reports on the results of this workshop and summarises the identified open issues for future research in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in active database systems : report from the closing panel at ride-ads '94

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jennifer widom
",n
"LEFT id: NA
RIGHT id: 1330

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tavant system architecture for sell-side channel management

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: srinivasa narayanan , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 1820

LEFT text: This paper introduces techniques for reducing data dissemination costs of query subscriptions. The reduction is achieved by merging queries with overlapping, but not necessarily equal, answers. The paper formalizes the query-merging problem and introduces a general cost model for it. We prove that the problem is NP-hard and propose exhaustive algorithms and three heuristic algorithms: the Pair Merging Algorithm, the Directed Search Algorithm and the Clustering Algorithm. We develop a simulator for evaluating the different heuristics and show that the performance of our heuristics is close to optimal.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: balancing push and pull for data broadcast

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: swarup acharya , michael franklin , stanley zdonik
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: Data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. We provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. Several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. Techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. Potential data mining applications and some research issues will also be discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 46

LEFT text: Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: datablitz storage manager : main-memory database performance for critical applications

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: j. baulier , p. bohannon , s. gogate , c. gupta , s. haldar
",n
"LEFT id: NA
RIGHT id: 1694

LEFT text: The emergence of Big Data has amounted to the complexity of the discussion on data reuse. The benefits of Big Data lie in the possibilities to discover novel trends, patterns and relationships by combining very large amounts of data from different sources. Current personal data protection requirements like data minimization and purpose specification are potentially inimical to Big Data as they limit the size and use of Big Data. Substantial loss of economic and social benefits of Big Data may be the result. In order to avoid this, the reuse of data could be encouraged. Data reuse, when done properly, may be both privacy preserving and economically and socially beneficial. In this paper, we provide a taxonomy of data reuse from both the data controller’s and the data subject’s perspective that may be useful to determine the extent to which data reuse should be allowed and under which conditions. From the data controller’s perspective we distinguish data recycling, data repurposing and data recontextualisation. From the data subject’s perspective, we distinguish data sharing and data portability. It is argued that forms of data reuse that stay close to the awareness and intentions of data subjects should be approached less tight (for instance, by assuming informed consent), whereas forms of data reuse that are ‘at a distance’, i.e., in which awareness and transparency may be lacking and data subject’s rights may prove more difficult to exercise, more restrictions and additional protection should be considered (for instance, by requiring explicit consent).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data modelling in the large

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin bertram
",n
"LEFT id: NA
RIGHT id: 534

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial : charter and scope

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1185

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 247

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sonar : system for optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 2226

LEFT text: Sensor networks are typically unattended because of their deployment in hazardous, hostile or remote environments. This makes the problem of conserving energy at individual sensor nodes challenging. S-MAC and PAMAS are two MAC protocols which periodically put nodes (selected at random) to sleep in order to achieve energy savings. Unlike these protocols, we propose an approach in which node duty cycles (i.e sleep and wake schedules) are based on their criticality. A distributed algorithm is used to find sets of winners and losers, who are then assigned appropriate slots in our TDMA based MAC protocol. We introduce the concept of of energy-criticality of a sensor node as a function of energies and traffic rates. Our protocol makes more critical nodes sleep longer, thereby balancing the energy consumption. Simulation results show that the performance of the protocol with increase in traffic load is better than existing protocols with increase in traffic load is better than existing protocols, thereby illustrating the energy balancing nature of the approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: power efficient data gathering and aggregation in wireless sensor networks

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: h &#252; seyin &#214; zg &#252; r tan , ibrahim k &#246; rpeo &#487; lu
",n
"LEFT id: NA
RIGHT id: 1077

LEFT text: The yellow pages service of GTE SuperPages enables Web users to flexibly search through liitings of 11 million businesses in over 17000 categories. To achieve the flexibility desired it uses an Information Retrieval (IR) engine to search through complex listing objects. The objects themselves are stored in an object database. The use of the IR engine enables us to create an index that spans all the components of a complex object.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: gte superpages : using ir techniques for searching complex objects

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: steven d. whitehead , himanshu sinha , michael murphy
",y
"LEFT id: NA
RIGHT id: 1313

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 377

LEFT text: Client-server database systems based on a data shipping model can exploit client memory resources by caching copies of data items across transaction boundaries. Caching reduces the need to obtain data from servers or other sites on the network. In order to ensure that such caching does not result in the violation of transaction semantics, a transactional cache consistency maintenance algorithm is required. Many such algorithms have been proposed in the literature and, as all provide the same functionality, performance is a primary concern in choosing among them. In this article we present a taxonomy that describes the design space for transactional cache consistency maintenance algorithms and show how proposed algorithms relate to one another. We then investigate the performance of six of these algorithms, and use these results to examine the tradeoffs inherent in the design choices identified in the taxonomy. The results show that the interactions among dimensions of the design space impact performance in many ways, and that classifications of algorithms as simply “pessimistic” or “optimistic” do not accurately characterize the similarities and differences among the many possible cache consistency algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: enhancing external consistency in real-time transactions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kwei-jay lin , ching-shan peng
",n
"LEFT id: NA
RIGHT id: 303

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: eamonn keogh , kaushik chakrabarti , michael pazzani , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 1142

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: of objects and databases : a decade of turmoil

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1434

LEFT text: In this paper, we first discuss the current practices and trends in component-based electronic commerce based on the International Workshop on Component-based Electronic Commerce. Then, we investigate a number of research issues and future directions in component-based development for electronic commerce.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scientific databases - state of the art and future directions

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria zemankova , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: A platform called AnMol for supporting analytical applications over structural data of large biomolecules is described. The term ""biomolecular structure"" has various connotations and different representations. AnMol reduces these representations into graph structures. Each of these graphs are then stored as one or more vectors in a database. Vectors encapsulate structural features of these graphs. Structural queries like similarity and substructure are transformed into spatial constructs like distance and containment within regions. Query results are based on inexact matches. A refinement mechanism is supported for increasing accuracy of the results. Design and implementation issues of AnMol including schema structure and performance results are discussed in this paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1818

LEFT text: Computing multiple related group-bys and aggregates is one of the core operations of On-Line Analytical Processing (OLAP) applications. Recently, Gray et al. [GBLP95] proposed the “Cube” operator, which computes group-by aggregations over all possible subsets of the specified dimensions. The rapid acceptance of the importance of this operator has led to a variant of the Cube being proposed for the SQL standard. Several efficient algorithms for Relational OLAP (ROLAP) have been developed to compute the Cube. However, to our knowledge there is nothing in the literature on how to compute the Cube for Multidimensional OLAP (MOLAP) systems, which store their data in sparse arrays rather than in tables. In this paper, we present a MOLAP algorithm to compute the Cube, and compare it to a leading ROLAP algorithm. The comparison between the two is interesting, since although they are computing the same function, one is value-based (the ROLAP algorithm) whereas the other is position-based (the MOLAP algorithm). Our tests show that, given appropriate compression techniques, the MOLAP algorithm is significantly faster than the ROLAP algorithm. In fact, the difference is so pronounced that this MOLAP algorithm may be useful for ROLAP systems as well as MOLAP systems, since in many cases, instead of cubing a table directly, it is faster to first convert the table to an array, cube the array, then convert the result back to a table.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an array-based algorithm for simultaneous multidimensional aggregates

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yihong zhao , prasad m. deshpande , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 1646

LEFT text: For reasons of simplicity and communication efficiency, a number of existing object-oriented database management systems are based on page server architectures; data pages are their minimum unit of transfer and client caching. Despite their efficiency, page servers are often criticized as being too restrictive when it comes to concurrency, as existing systems use pages as the minimum locking unit as well. In this paper we show how to support object-level locking in a page server context. Several approaches are described, including an adaptive granularity approach that uses page-level locking for most pages but switches to object-level locking when finer-grained sharing is demanded. We study the performance of these approaches, comparing them to both a pure page server and a pure object server. For the range of workloads that we have examined, our results indicate that a page server is clearly preferable to an object server. Moreover, the adaptive page server is shown to provide very good performance, generally outperforming the pure page server, the pure object server, and the other alternatives as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fine-grained sharing in a page server oodbms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey , michael j. franklin , markos zaharioudakis
",y
"LEFT id: NA
RIGHT id: 1186

LEFT text: LeSelect is a mediator system which allows scientists to publish their resources (data and programs) so they can be transparently accessed. The scientists can typically issue queries which access distributed published data and involve the execution of expensive functions (corresponding to programs). Furthermore, the queries can involve large objects, such as images (e.g. archived meteorological satellite data). In this context, the costs of transmitting large objects and invoking expensive functions are the dominant factors of execution time. In this paper, we first propose three query execution techniques which minimize these costs by taking full advantage of the distributed architecture of mediator systems like LeSelect. Then we devise parallel processing strategies for queries including expensive functions. Based on experimentation, we show that it is hard to predict the optimal execution order when dealing with several functions. We propose a new hybrid parallel technique to solve this problem and give some experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 2257

LEFT text: Hash-based scalable distributed data structures (SDDSs), like LH* and DDH, for networks of interconnected computers (multicomputers) were shown to open new perspectives for file management. We propose a family of ordered SDDSs, called RP*, providing for ordered and dynamic files on multicomputers, and thus for more efficient processing of range queries and of ordered traversals of files. The basic algorithm termed RP*N, builds the file with the same key space partitioning as a B-tree, but avoids indexes through the use of multicast. The algorithms, RP*C and RP*S enhance throughput for faster networks, adding the indexes on clients, or on clients and servers, while either decreasing or avoiding multicast. RP* files are shown highly efficient with access performance exceeding traditional files by an order of magnitude or two, and, for non-range queries, very close to LH*.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: rp * : a family of order preserving scalable distributed data structures

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: witold litwin , marie-anne neimat , donovan a. schneider
",y
"LEFT id: NA
RIGHT id: 1413

LEFT text: This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1342

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing : taming the terabytes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: minos n. garofalakis , phillip b. gibbon
",y
"LEFT id: NA
RIGHT id: 570

LEFT text: Author-χ is a Java-based system for access control to XML documents. Author-χ implements a discretionary access control model specifically tailored to the characteristics of XML documents. In particular, our system allows (i) a set-oriented and single-oriented document protection, by supporting authorizations both at document type and document level; (ii) a differentiated protection of document/document type contents by supporting multi-granularity protection objects and positive/negative authorizations; (iii) a controlled propagation of authorizations among protection objects, by enforcing multiple propagation options.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on database theory and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 489

LEFT text: This paper abstracts from the underlying platforms and instead considers the requirements to CRM solutions for the various communication channels, in order to devise a uniform and corporate-wide data architecture for an omni-channel customer view to maximize the business clients' value in customer retention and customer centric analytics. Especially, online customer segmentation integrating channel usage and preferences is presented as a very promising means for constructing a self-energising information loop which will lead to highly improved customer service along the whole customer journey.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: evolution and change in data management - issues and directions

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: john f. roddick , lina al-jadir , leopoldo bertossi , marlon dumas , florida estrella , heidi gregersen , kathleen hornsby , jens lufter , federica mandreoli , tomi m &#228; nnist &#246; , enric mayol , lex wedemeijer
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",n
"LEFT id: NA
RIGHT id: 689

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1454

LEFT text:  This paper studies workfile disk management for concurrent mergesorts ina multiprocessor database system. Specifically, we examine the impacts of workfile disk allocation and data striping on the average mergesort response time. Concurrent mergesorts in a multiprocessor system can creat severe I/O interference in which a large number of sequential write requests are continuously issued to the same workfile disk and block other read requests for a long period of time. We examine through detailed simulations a logical partitioning approach to workfile disk management and evaluate the effectiveness of datastriping.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a performance study of workfile disk management for concurrent mergesorts in a multiprocessor database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu , jen-yao chung , james z. teng
",y
"LEFT id: NA
RIGHT id: 205

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional selectivity estimation using compressed histogram information

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ju-hong lee , deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1114

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",y
"LEFT id: NA
RIGHT id: 511

LEFT text: Schema evolution is a problem that is faced by long-lived data. When a schema changes, existing persistent data can become inaccessible unless the database system provides mechanisms to access data created with previous versions of the schema. Most existing systems that support schema evolution focus on changes local to individual types within the schema, thereby limiting the changes that the database maintainer can perform. We have developed a model of type changes involving multiple types. The model describes both type changes and their impact on data by defining derivation rules to initialize new data based on the existing data. The derivation rules can describe local and nonlocal changes to types to capture the intent of a large class of type change operations. We have built a system   called Tess (Type Evolution Software System) that uses this model to recognize type changes by comparing schemas and then produces a transformer that can update data in a database to correspond to a newer version of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a model for compound type changes encountered in schema evolution

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: barbara staudt lerner
",y
"LEFT id: NA
RIGHT id: 1320

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: Model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 1959

LEFT text: Continuous media servers that provide support for the storage and retrieval of continuous media data (e.g., video, audio) at guaranteed rates are becoming increasingly important. Such servers, typically, rely on several disks to service a large number of clients, and are thus highly susceptible to disk failures. We have developed two fault-tolerant approaches that rely on admission control in order to meet rate guarantees for continuous media requests. The schemes enable data to be retrieved from disks at the required rate even if a certain disk were to fail. For both approaches, we present data placement strategies and admission control algorithms. We also present design techniques for maximizing the number of clients that can be supported by a continuous media server. Finally, through extensive simulations, we demonstrate the effectiveness of our schemes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fault-tolerant architectures for continuous media servers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: banu &#214; zden , rajeev rastogi , prashant shenoy , avi silberschatz
",y
"LEFT id: NA
RIGHT id: 664

LEFT text: • "" … make a computer so imbedded, so fitting, so natural, that we use it without even thinking about it. "" • "" Ubiquitous (pervasive) computing is roughly the opposite of virtual reality. Where virtual reality puts people inside a computer-generated world, ubiquitous computing forces the computer to live out here in the world with people. "" – Mark Weiser, Xerox PARC

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: business data management for business-to-business electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: christoph quix , mareike schoop , manfred jeusfeld
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 2040

LEFT text: Abstract. In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score>8] might be “perfectly” translated as [rating>0.8] at some site, but can only be approximated as [grade=A] at another. Unlike other work, our general framework adopts a customizable “closeness” metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units. We show that, under practical assumptions, our algorithm generates the best approximate translations with respect to the closeness metric of choice. We also present a case study to show how our technique may be applied in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream processing of xpath queries with predicates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ashish kumar gupta , dan suciu
",n
"LEFT id: NA
RIGHT id: 687

LEFT text: Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is 𝒩𝒫-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: statistical synopses for graph-structured xml databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: neoklis polyzotis , minos garofalakis
",y
"LEFT id: NA
RIGHT id: 585

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: 1. Document Acquisition There is a broad spectrum of techniques how to acquire documents in such a way, that they are in computerreadable form and can be stored in a document base. This spectrum ranges from fully automatic at low cost via semiautomatic using tools like scanners and optical character recognition (OCR) to manual acquisition according to elaborate rules and regulations. The purpose of high quality document acquisition is to capture the structure and the semantic content of a document not as far as possible but as far as affordable. Presently the state of the art of semiautomatic acquisition of paper documents is scanning followed by OCR. This yields a facsimile image and the text content, but no structure and no real semantics. In many cases the text produced by OCR is low quality and must be corrected to be useful for effective retrieval. Various projects are under way to capture structure and semantics automatically [1,12], but they have not reached sufficient maturity to be used in production environments like libraries or businesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 640

LEFT text: We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an implemented system that provides uniform access to a heterogeneous collection of more than 100 information sources, many of them on the WWW. IM tackles the above problems by providing a mechanism to describe declaratively the contents and query capabilities of available information sources. There is a clean separation between the declarative source description and the actual details of interacting with an information source. We describe algorithms that use the source descriptions to prune effciently the set of information sources for a given query and practical algorithms to generate executable query plans. The query plans we generate can inolve querying several information sources and combining their answers. We also present experimental studies that indicate that the architecture and algorithms used in the Information Manifold scale up well to several hundred information sources

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rate-based query optimization for streaming information sources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: stratis d. viglas , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 249

LEFT text: Visualizations embody design choices about data access, data transformation, visual representation, and interaction. To interpret a static visualization, a person must identify the correspondences between the visual representation and the underlying data. These correspondences become moving targets when a visualization is dynamic. Dynamics may be introduced in a visualization at any point in the analysis and visualization process. For example, the data itself may be streaming, shifting subsets may be selected, visual representations may be animated, and interaction may modify presentation. In this paper, we focus on the impact of dynamic data. We present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations. Visualization techniques are organized into categories at various levels of abstraction. The salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices. Examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability (and thus utility) of visualizations. The taxonomy presented provides a reference point for further exploration of dynamic data visualization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: Data warehouses collect large quantities of data from distributed sources into a single repository. A typical load to create or maintain a warehouse processes GBs of data, takes hours or even days to execute, and involves many complex and user-defined transformations of the data (e.g., find duplicates, resolve data inconsistencies, and add unique keys). If the load fails, a possible approach is to “redo” the entire load. A better approach is to resume the incomplete load from where it was interrupted. Unfortunately, traditional algorithms for resuming the load either impose unacceptable overhead during normal operation, or rely on the specifics of transformations. We develop a resumption algorithm called DR that imposes no overhead and relies only on the high-level properties of the transformations. We show that DR can lead to a ten-fold reduction in resumption time by performing experiments using commercial software.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 668

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX has to be updated every time an object is updated. This has previously been shown to be a potential bottleneck, and in this paper, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 365

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online query processing : a tutorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter j. haas , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 1933

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: highly concurrent cache consistency for indices in client-server database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: markos zaharioudakis , michael j. carey
",n
"LEFT id: NA
RIGHT id: 334

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1855

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization at the crossroads

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 232

LEFT text: HYPERQUERY is a hypertext query language for object-oriented pictorial database systems. First, we discuss object calculus based on term rewriting. Then, example queries are used to illustrate language facilities. This query language has been designed with a flavor similar to QBE as the highly nonprocedural and conversational language for object-oriented pictorial database management system OISDBS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: safe query languages for constraint databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter z. revesz
",n
"LEFT id: NA
RIGHT id: 118

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in global information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: a. m. ouksel , a. sheth
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1299

LEFT text: Flexible methods supporting the data interchange between autonomous information systems are important for today's increasingly heterogeneous enterprise IT infrastructures. Updates, insertions, and deletions of data objects in autonomous information systems often have to trigger data changes in other autonomous systems, even if the distributed systems are not integrated into a global schema. We suggest a solution to this problem based on the propagation and transformation of data using several XML technologies. Our prototype manages dependencies between the schemas of distributed data sources and allows to define and process arbitrary actions on changed data by manipulating all dependent data sources. The prototype comprises a propagation engine that interprets scripts based on a workflow specification language, a data dependency specification tool, a system administration tool, and a repository that stores all relevant information for these tools.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: using semantic values to facilitate interoperability among heterogeneous information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore , michael siegel , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 1272

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: malcolm p. atkinson
",n
"LEFT id: NA
RIGHT id: 488

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mlpq/gis constraint database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang
",n
"LEFT id: NA
RIGHT id: 639

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tigukat : a uniform behavioral objectbase management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu , randal peters , duane szafron , boman irani , anna lipka , adriana mu &#241; oz
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 770

LEFT text: It is today widely accepted that “Business Rules Independence” is required for information systems to better and more rapidly adjust to changes in the business environment, This paper’ attempts to articulate how logic based database systems provide adequate technology for better “Business Rules Independence”. These systems do so by going beyond “Data Independence” and by providing “Knowledge Independence”. This paper benefits from the experience gained in developing and marketing the VALIDITY deductive and object-oriented database system during the last few years. Current applications and specific data management techniques to be used are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: from data independence to knowledge independence : an on-going story

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: laurent vieille
",y
"LEFT id: NA
RIGHT id: 2136

LEFT text: An efficient management of multiversion data with branched evolution is crucial for many applications. It requires database designers aware of tradeoffs among index structures and policies. This paper defines a framework and an analysis method for understanding the behavior of different indexing policies. Given data and query characteristics the analysis allows determining the most suitable index structure. The analysis is validated by an experimental study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: author index

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: NA
",y
"LEFT id: NA
RIGHT id: 1227

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: clustering categorical data : an approach based on dynamical systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: david gibson , jon kleinberg , prabhakar raghavan
",n
"LEFT id: NA
RIGHT id: 2074

LEFT text: The education industry has a very poor record of productivity gains. In this brief article, I outline some of the ways the teaching of a college course in database systems could be made more efficient, and staff time used more productively. These ideas carry over to other programming-oriented courses, and many of them apply to any academic subject whatsoever. After proposing a number of things that could be done, I concentrate here on a system under development, called OTC (On-line Testing Center), and on its methodology of ""root questions.""

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: improving the efficiency of database-system teaching

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jeffrey d. ullman
",y
"LEFT id: NA
RIGHT id: 1473

LEFT text: The automatic reclamation of storage for unreferenced objects is very important in object databases. Existing language system algorithms for automatic storage reclamation have been shown to be inappropriate. In this paper, we investigate methods to improve the performance of algorithms for automatic for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. Specifically, we investigate the policy that is used to select what partition in the database should be collected. The policies that we propose and investigate are based on the intuition that the values of overwritten pointers provide good hints about  where to find garbage. Using trace-driven simulation, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy and performs close to a near-optimal policy over a wide range of database sizes and object connectivities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient incremental garbage collection for client-server object database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: laurent amsaleg , michael j. franklin , olivier gruber
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",y
"LEFT id: NA
RIGHT id: 1594

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: recovery protocols for shared memory database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: lory d. molesky , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 677

LEFT text: In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic multidimensional histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nitin thaper , sudipto guha , piotr indyk , nick koudas
",y
"LEFT id: NA
RIGHT id: 1542

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing multimedia databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: christos faloutsos
",n
"LEFT id: NA
RIGHT id: 2021

LEFT text: Relational queries on continuous streams of data are the subject of many recent database research projects. In 1998 a small group of people started a similar project with the goal to transform our product, NonStop SQL/MX, into an active RDBMS. This project tried to integrate functionality of transactional queuing systems with relational tables and with SQL, using simple extensions to the SQL syntax and guaranteeing clearly defined query and transactional semantics. The result is the first commercially available RDBMS that incorporates streams. All data flowing through the system is contained in relational tables and is protected by ACID transactions. Insert and update operations on any NonStop SQL table can be considered publishing of data and can therefore be transparent to the (legacy) applications performing them. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: telegraphcq : continuous dataflow processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sirish chandrasekaran , owen cooper , amol deshpande , michael j. franklin , joseph m. hellerstein , wei hong , sailesh krishnamurthy , samuel r. madden , fred reiss , mehul a. shah
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: CoDecide is an experimental user interface toolkit that offers an extension to spreadsheet concepts specifically geared towards support for cooperative analysis of the kinds of multi-dimensional data encountered in data warehousing. It is distinguished from previous proposals by direct support for drill-down/roll-up analysis without redesign of an interface; more importantly, CoDecide can link multiple views on a data cube for synchronous or asynchronoous cooperation by multiple analysts, through a conceptual model visualizing the problem dimensions on so-called tapes. Tapes generalize the ideas of ranging and pivoting in current data warehouses for the multi-perspective and multi-user case. CoDecide allows the rapid composition of multi-matrix interfaces and their linkage to underlying data sources. A LAN version of CoDecide has been used in a number of design decision support applications. A WWW version representing externally materialized views on databases is currently under development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 2115

LEFT text: Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a survey of approaches to automatic schema matching

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: erhard rahm , philip a. bernstein
",y
"LEFT id: NA
RIGHT id: 1283

LEFT text: This paper introduces techniques for reducing data dissemination costs of query subscriptions. The reduction is achieved by merging queries with overlapping, but not necessarily equal, answers. The paper formalizes the query-merging problem and introduces a general cost model for it. We prove that the problem is NP-hard and propose exhaustive algorithms and three heuristic algorithms: the Pair Merging Algorithm, the Directed Search Algorithm and the Clustering Algorithm. We develop a simulator for evaluating the different heuristics and show that the performance of our heuristics is close to optimal.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data staging for on-demand broadcast

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: demet aksoy , michael j. franklin , stanley b. zdonik
",y
"LEFT id: NA
RIGHT id: 407

LEFT text: This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: image mining in iris : integrated retinal information system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wynne hsu , mong li lee , kheng guan goh
",n
"LEFT id: NA
RIGHT id: 1109

LEFT text: Government and industry are investing substantial resources in new technologies for accessing heterogeneous information sources, including text-based corpora, structured data, imagery, geo-spatial data, audio, video, and more. Managers of programs that fund relevant research face a difficult problem: they are required to justify investment in certain technologies and approaches versus alternate ones. These program managers recognize a need for good evaluation criteria, but there is little consensus on which criteria to use

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: metrics for accessing heterogeneous data : is there any hope ? ( panel )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leonard j. seligman , nicholas j. belkin , erich j. neuhold , michael stonebraker , gio wiederhold
",y
"LEFT id: NA
RIGHT id: 2152

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",n
"LEFT id: NA
RIGHT id: 1715

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 2235

LEFT text: We study the effectiveness of probabilistic selection of join-query evaluation plans, without reliance on tree transformation rules. Instead, each candidate plan is chosen uniformly at random from the space of valid evaluation orders. This leads to a transformation-free strategy where a sequence of random plans is generated and the plans are compared on their estimated costs. The success of this strategy depends on the ratio of ``good'' evaluation plans in the space of alternatives, the efficient generation of random candidates, and an accurate estimation of their cost. To avoid a biased exploration of the space, we solved the open problem of efficiently generating random, uniformly-distributed evaluation orders, for queries with acyclic graphs. This benefits any optimization or sampling scheme in which a random choice of (initial) query plans is required. A direct comparison with iterative improvement and simulated annealing, using a proven cost-evaluator, shows that our transformation-free strategy converges faster and yields solutions of comparable cost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast , randomized join-order selection - why use transformations ?

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: c &#233; sar a. galindo-legaria , arjan pellenkoft , martin l. kersten
",y
"LEFT id: NA
RIGHT id: 1236

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: The Summary Schemas Model (SSM) is proposed as an extension to multidatabase systems to aid in semantic identification. The SSM uses a global data structure to abstract the information available in a multidatabase system. This abstracted form allows users to use their own terms (imprecise queries) when accessing data rather than being forced to use system-specified terms. The system uses the global data structure to match the user's terms to the semantically closest available system terms. A simulation of the SSM is presented to compare imprecise-query processing with corresponding query-processing costs in a standard multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 347

LEFT text: In many application fields, such as production lines or stock analysis, it is substantial to create and process high amounts of data at high rates. Such continuous data flows with unknown size and end are also called data streams. The processing and analysis of data streams are a challenge for common data management systems as they have to operate and deliver results in real time. Data Stream Management Systems (DSMS), as an advancement of database management systems, have been implemented to deal with these issues. DSMS have to adapt to the notion of data streams on various levels, such as query languages, processing or optimization. In this chapter we give an overview of the basics of data streams, architecture principles of DSMS and the used query languages. Furthermore, we specifically detail data quality aspects in DSMS as these play an important role for various applications based on data streams. Finally, the chapter also includes a list of research and commercial DSMS and their key properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1272

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: malcolm p. atkinson
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1222

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: update propagation strategies to improve freshness in lazy master replicated databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: esther pacitti , eric simon
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: The abfity to optimize queries is considered one of the key enabling technologies for relational databases and is an active area in research and development. There continues to be work in discovering new transformations for SQL. The problem of building extensible query optimizers continues to draw a lot of attention, not only in academic research but also in commercial development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 1900

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: pixel-oriented database visualizations

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 528

LEFT text: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the ""top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: top-k selection queries over relational databases : mapping strategies and performance evaluation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri , luis gravano
",y
"LEFT id: NA
RIGHT id: 270

LEFT text: Metadata has been identified as a key success factor in data warehouse projects. It captures all kinds of information necessary to analyse, design, build, use, and interpret the data warehouse contents. In order to spread the use of metadata, enable the interoperability between repositories, and tool integration within data warehousing architectures, a standard for metadata representation and exchange is needed. This paper considers two standards and compares them according to specific areas of interest within data warehousing. Despite their incontestable similarities, there are significant differences between the two standards which would make their unification difficult.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storhouse metanoia - new applications for database , storage & ; data warehousing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: felipe cari &#241; o , jr. , pekka kostamaa , art kaufmann , john burgess
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 2109

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 152

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the araneus web-based management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. mecca , p. atzeni , a. masci , g. sindoni , p. merialdo
",n
"LEFT id: NA
RIGHT id: 1752

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: management of semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1828

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: association rules over interval data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. miller , y. yang
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: This paper gives an overview of the IRO-DB architecture and describes in detail the cost evaluator currently under elaboration for the next version of the distributed query optimizer. The cost model is composed of a set of mathematical formulas with coefficients to estimate the cost of the search operators. The coefficients are deduced from a calibrating objectoriented database composed of linked collections of objects. A tuning application is run on each local site to adjust the cost formulas and fix the coefficients. We report on the tuning of O2 and ObjectStore. We show that the estimation is quite accurate for path traversals with the OO7 benchmark on top of ObjectStore.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: In this paper, we introduce a formalism for expressing and reasoning about order properties: ordering and grouping constraints that hold of physical representations of relations. In so doing, we can reason about how the relation is ordered or grouped, both in terms of primary and secondary orders. After formally defining order properties, we introduce a plan refinement algorithm that infers order properties for intermediate and final query results on the basis of those known to hold of query inputs, and then exploits these inferences to avoid unnecessary sorting and grouping.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1986

LEFT text: An important problem faced by many database management systems is the ""online object placement problem""--the problem of choosing a disk page to hold a newly allocated object. In the absence of clustering criteria, the goal is to maximize storage utilization. For main-memory based systems, simple heuristics exist that provide reasonable space utilization in the worst case and excellent utilization in typical cases. However, the storage management problem for databases includes significant additional challenges, such as minimizing I/O traffic, coping with crash recovery, and gracefully integrating space management with locking and logging.We survey several object placement algorithms, including techniques that can be found in commercial and research database systems. We then present a new object placement algorithm that we have designed for use in Shore, an object-oriented database system under development at the University of Wisconsin--Madison. Finally, we present results from a series of experiments involving actual Shore implementations of some of these algorithms. Our results show that while current object placement algorithms have serious performance deficiencies, including excessive CPU or main memory overhead, I/O traffic, or poor disk utilization, our new algorithm consistently excellent performance in all of these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards effective and efficient free space management

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark l. mcauliffe , michael j. carey , marvin h. solomon
",y
"LEFT id: NA
RIGHT id: 927

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",y
"LEFT id: NA
RIGHT id: 2197

LEFT text: Encrypted databases, a popular approach to protecting data from compromised database management systems (DBMS's), use abstract threat models that capture neither realistic databases, nor realistic attack scenarios. In particular, the ""snapshot attacker"" model used to support the security claims for many encrypted databases does not reflect the information about past queries available in any snapshot attack on an actual DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xxl - a library approach to supporting efficient implementations of advanced database queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jochen van den bercken , bj &#246; rn blohsfeld , jens-peter dittrich , j &#252; rgen kr &#228; mer , tobias sch &#228; fer , martin schneider , bernhard seeger
",y
"LEFT id: NA
RIGHT id: 984

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the discovery of interesting patterns in association rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , sameer mahajan , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 1082

LEFT text: Given the present cost of memories and the very large storage and bandwidth requirements of large-scale multimedia databases, hierarchical storage servers (which consist of RAM, disk storage, and robot-based tertiary libraries) are becoming increasingly popular. However, related research is scarce and employs tertiary storage for storage augmentation purposes only. This work, exploiting the ever-increasing performance offered by (par. titularly) modern tape library products, aims to utilize tertiary storage in order to augment the system’s performance. We consider the issue of elevating continuous data from its permanent place in tertiary for display purposes. Our primary goals are to save on the secondary storage bandwidth that traditional techniques require for the display of continuous objects, while requiring no additional . RAM buffer space. To this end we develop algorithms for sharing the responsibility for * Research supported by the European Community under the ESPRIT Long Term Research Project HERMES no. 9141. t Research supported by the Canadian government under NSERC grant number 0155218, and by the 1996-97 Going Global STEP program. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on-demand data elevation in hierarchical multimedia storage servers

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: peter triantafillou , thomas papadakis
",y
"LEFT id: NA
RIGHT id: 147

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multidimensional database system rasdaman

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. baumann , a. dehmel , p. furtado , r. ritsch , n. widmann
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1151

LEFT text: This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: practical issues with commercial use of federated databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jim kleewein
",y
"LEFT id: NA
RIGHT id: 2254

LEFT text: I describe the format of the new version of an introductory database course that I taught at the University ofWashington inWinter, 2003. The key idea underlying the course is to expose the students to some of the challenges that arise when working with and integrating data from multiple database systems and applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: learning about data integration challenges from day one

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: alon y. halevy
",y
"LEFT id: NA
RIGHT id: 913

LEFT text: Integrated access to heterogeneous information is an increasingly important topic as more and more sources that developed independently from each other become accessible over networks. The structure of the information and the abilities of the sources to answer queries may vary widely. Therefore, systems are needed that are able to use knowledge about the contents and the capabilities of sources to break down a global query into portions that can be processed locally and to reassemble the answers. Problems of this kind arise when one …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 1263

LEFT text: Although many query tree optimization strategies have been proposed in the literature, there still is a lack of a formal and complete representation of all possible permutations of query operations (i.e., execution plans) in a uniform manner. A graph-theoretic approach presented in the paper provides a sound mathematical basis for representing a query and searching for an execution plan. In this graph model, a node represents an operation and a directed edge between two nodes indicates the older of executing these two operations in an execution plan. Each node is associated with a weight and so is an edge. The weight is an expression containing optimization required parameters, such as relation size, tuple size, join selectivity factors. All possible execution plans are representable in this graph and each spanning tree of the graph becomes an execution plan. It is a general model which can be used in the optimizer of a DBMS for internal query representation. On the basis of this model, we devise an algorithm that finds a near optimal execution plan using only polynomial time. The algorithm is compared with a few other popular optimization methods. Experiments show that the proposed algorithm is superior to the others under most circumstances.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a graph-theoretic model for optimizing queries involving methods

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chiang lee , chi-sheng shih , yaw-huei chen
",y
"LEFT id: NA
RIGHT id: 60

LEFT text: The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automatic discovery of language models for text databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jamie callan , margaret connell , aiqun du
",y
"LEFT id: NA
RIGHT id: 1604

LEFT text: The National Technical University of Athens (NTUA) is the leading Technical University in Greece. The Computer Science Division of the Electrical and Computer Engineering Department covers several fields of practical, theoretical and technical computer science and is involved in several research projects supported by the EEC, the government and industrial companies. The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media. The KDBS Laboratory employs one full-time research engineer and several graduate students. Its infrastructure includes a LAN with several DECstation 5000/200 and 5000/240 workstations, an HP Multimedia Workstation, several PCs and software for database and multimedia applications. The basic research interests of our Laboratory include: Spatial Database Systems, Multimedia Database Systems and Active Database Systems. Apart from the above database areas, interests of the KDBS Laboratory span several areas of Information Systems, such as Software Engineering Databases, Transactional Systems, Image Databases, Conceptual Modeling, Information System Development, Temporal Databases, Advanced Query Processing and Optimization Techniques.  The group's efforts on Spatial Database Systems, include the study of new data structures, storage techniques, retrieval mechanisms and user interfaces for large geographic data bases. In particular, we look at specialized, spatial data structures (R-Trees and their variations) which allow for the direct access of the data based on their spatial properties, and not some sort of encoded representation of the objects' coordinates. We study implementation and optimization techniques of spatial data structures and develop models that make performance estimation. Finally, we are investigating techniques for the efficient representation of relationships and reasoning in space. The activities on Multimedia Database Systems, include the study of advanced data models, storage techniques, retrieval mechanisms and user interfaces for large multimedia data bases. The data models under study include the object-oriented model and the relational model with appropriate extensions to support multimedia data. We are also investigating content-based search techniques for image data bases. In a different direction, we are studying issues involved in the development of multimedia front-ends for conventional, relational data base systems. In the area of Active Database Systems, we are developing new mechanisms for implementing triggers in relational databases. Among the issues involved, we address the problem of efficiently finding qualifying rules against updates in large sets of triggers. This problem is especially critical in database system implementations of triggers, where large amounts of data may have to be searched in order to find out if a particular trigger may qualify to run or not.  Continuing work that started at the Foundation for Research and Technology (FORTH), Institute of Computer Science, the group is investigating reuse-oriented approaches to information systems application development. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data and knowledge base research at hong kong university of science and technology

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: p. drew , b. hamidzadeh , k. karlapalem , a. kean , d. lee , q. li , f. lochovsky , c. d. shum , b. wuthrich
",y
"LEFT id: NA
RIGHT id: 1041

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",y
"LEFT id: NA
RIGHT id: 1511

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1185

LEFT text: Real-world entities are inherently spatially and temporally referenced, and database applications increasingly exploit databases that record the past, present, and anticipatedfu tu locations of entities, e.g., the residences ofcuEERRx7 obtained by the geo-coding of addresses. Indices that efficiently suient quient on the spatio-temporal extents ofsuE entities are needed. However, past indexing research has progressed in largely separate spatial and temporal streams. Adding time dimensions to spatial indices, as if time were a spatial dimension, neither suther7 nor exploits the special properties of time. On the other hand, temporal indices are generally not amenable to extension with spatial dimensions. This paper proposes the first efficient and versatile index for a general class of spatio-temporal data: the discretely changing spatial aspect of an object may be a point or may have an extent; both transaction time and valid time are su;wkP-7y and a generalized notion of thecu;kx: time, now, is accommodated for both temporal dimensions. The index is based on the R # -tree and provides means of prioritizing space versu time, which enables it to adapt to spatially and temporally restrictivequ@;-P7 Performance experiments are reported that evalu-; pertinent aspects of the index.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 612

LEFT text: In the mobile wireless computing environment of the future a large number of users equipped with low powered palm-top machines will query databases over the wireless communication channels. Palmtop based units will often be disconnected for prolonged periods of time due to the battery power saving measures; palmtops will also frequencly relocate between different cells and connect to different data servers at different times. Caching of frequently accessed data items will be an important technique that will reduce contention on the narrow bandwidth wireless channel. However, cache invalidation strategies will be severely affected by the disconnection and mobility of the clients. The server may no longer know which clients are currently residing under its cell and which of them are   currently on. We propose a taxonomy of different cache invalidation strategies and study the impact of client's disconnection times on their performance. We determine that for the units which are often disconnected (sleepers) the best cache invalidation strategy is based on signatures previously used for efficient file comparison. On the other hand, for units which are connected most of the time (workaholics), the best cache invalidation strategy is based on the periodic broadcast of changed data items.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: sleepers and workaholics : caching strategies in mobile environments ( extended version )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel barbar &#225; , tomasz imieli &#324; ski
",y
"LEFT id: NA
RIGHT id: 689

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1326

LEFT text: The rapid growth of structured data on the Web has created a high demand for making this content more reusable and consumable. Companies are competing not only on gathering structured content and making it public, but also on encouraging people to reuse and profit from this content. Many companies have made their content publicly accessible not only through APIs but also started to widely adopt web metadata standards such as XML, RDF, RDFa, and microformats. This trend of structured data on the Web (Data Web) is shifting the focus of Web technologies towards new paradigms of structured-data retrieval. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query engines for web-accessible xml data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonidas fegaras , ramez elmasri
",y
"LEFT id: NA
RIGHT id: 2027

LEFT text: XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xquery : a query language for xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: don chamberlin
",y
"LEFT id: NA
RIGHT id: 1145

LEFT text: This chapter describes an efficient method for maintaining materialized views with non-distributive aggregate functions, even in the presence of super aggregates. Incremental view maintenance is an extremely important aspect of the modern database management systems. It enables the fast execution of complex queries without sacrificing the freshness of the data. However, the maintenance of views defined with non-distributive aggregate functions was not sufficiently explored. Incremental refresh has been studied in depth only for a subset of the aggregate functions. Materialized views, or automatic summary tables (ASTs), are increasingly being used to facilitate the analysis of the large amounts of data being collected in relational databases. The use of ASTs can significantly reduce the execution time of a query, often by orders of magnitude, which is particularly significant for databases with sizes in the terabyte to petabyte range, whose queries are designed by business intelligence tools or decision support systems. Such queries tend to be extremely complex, involving a large number of join and grouping operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance of externally materialized views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: martin staudt , matthias jarke
",n
"LEFT id: NA
RIGHT id: 1938

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: This paper describes the design and implementation of NAOS, an active rule component in the object-oriented database system 02. The contribution of this work is related to two main aspects. The first concerns the integration of the rule concept within the 02 model, providing a way to structure applications. Rules are part of a schema and do not belong to a class. Program execution and data manipulation, including method calls, can be driven on rules. The second aspect concerns the way NAOS interacts with the kernel of the 02 system. To support a reactive capability the object manager semantics has been extended, thus providing an efficient event detection. Applications produce events and the subscribed event types react to these events. As a result, rules are triggered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 159

LEFT text:  Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: free parallel data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bin li , dennis shasha
",y
"LEFT id: NA
RIGHT id: 1186

LEFT text: We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1665

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a language based multidatabase system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eva k &#252; hn , thomas tschernko , konrad schwarz
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: Relational Database Management Systems (RDBMS) have been very successful at managing structured data with well-defined schemas. Despite this, relational systems are generally not the first choice for management of data where schemas are not pre-defined or must be flexible in the face of variations and changes. Instead, No-SQL database systems supporting JSON are often selected to provide persistence to such applications. JSON is a light-weight and flexible semi-structured data format supporting constructs common in most programming languages. In this paper, we analyze the way in which requirements differ between management of relational data and management of JSON data. We present three architectural principles that facilitate a schema-less development style within an RDBMS so that RDBMS users can store, query, and index JSON data without requiring schemas. We show how these three principles can be applied to industry-leading RDBMS platforms, such as the Oracle RDBMS Server, with relatively little effort. Consequently, an RDBMS can unify the management of both relational data and JSON data in one platform and use SQL with an embedded JSON path language as a single declarative language to query both relational data and JSON data. This SQL/JSON approach offers significant benefits to application developers as they can use one product to manage both relational data and semi-structured flexible schema data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1369

LEFT text: XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational model that would make it more appropriate for processing queries over XML documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xperanto : middleware for publishing object-relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael j. carey , jerry kiernan , jayavel shanmugasundaram , eugene j. shekita , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: We study sliding window multi-join processing in continuous queries over data streams. Several algorithms are reported for performing continuous, incremental joins, under the assumption that all the sliding windows fit in main memory. The algorithms include multiway incremental nested loop joins (NLJs) and multi-way incremental hash joins. We also propose join ordering heuristics to minimize the processing cost per unit time. We test a possible implementation of these algorithms and show that, as expected, hash joins are faster than NLJs for performing equi-joins, and that the overall processing cost is influenced by the strategies used to remove expired tuples from the sliding windows.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 1710

LEFT text: In this issue we briefly touch on the continuing turmoil over NSF, ARPA, and HPCC, and the brighter news regarding the US National Information Infrastructure plan. We then describe funding opportunities from NSF, ARPA, the National Security Agency, the National Center for Automated Information Research, the Air Force, and NASA.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: progress on hpcc and nii

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett
",y
"LEFT id: NA
RIGHT id: 370

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimizing object queries using an effective calculus

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , david maier
",n
"LEFT id: NA
RIGHT id: 361

LEFT text: In this paper we introduce generalized projections (G P an extension of duplicateeliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinctand duplicate-preserving projections in a common unified framework. Using G P s we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dyda : data warehouse maintenance in fully concurrent environments

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jun chen , xin zhang , songting chen , andreas koeller , elke a. rundensteiner
",n
"LEFT id: NA
RIGHT id: 1390

LEFT text: In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: icicles : self-tuning samples for approximate query answering

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: venkatesh ganti , mong-li lee , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1016

LEFT text: The problem of analyzing and classifying conceptual schemas is becomig increasingly important due to the availability of a large number of schemas related to existing applications. The purposes of schema analysis and classification activities can be different: to extract information on intensional properties of legacy systems in order to restructure or migrate to new architectures; to build libraries of reference conceptual components to be used in building new applications in a given domain; and to identify information flows and possible replication of data in an organization. This article proposes a set of techniques for schema analysis and classification to be used separately or in combination. The techniques allow the analyst to derive significant properties from schemas, with human intervention limited as far as possible. In particular, techniques for associating descriptors with schemas, for abstracting reference conceptual schemas based on schema clustering, and for determining schema similarity are presented. A methodology for systematic schema analysis is illustrated, with the purpose of identifying and abstracting into reference components the similar and potentially reusable parts of a set of schemas. Experiences deriving from the application of the proposed techniques and methodology on a large set of Entity-Relationship conceptual schemas of information systems in the Italian Public Administration domain are described

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bulk-loading techniques for object databases and an application to relational data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sophie cluet , claude delobel
",n
"LEFT id: NA
RIGHT id: 1370

LEFT text: Conduct of scientific and engineering research is becoming critically dependent on effective management of scientific and engineering data and technical information. The rapid advances in scientific instrumentation, computer and communication technologies enable the scientists to collect, generate, process, and share unprecedented volumes of data. For example, the Earth Observing System Data and Information System (EOSDIS) has the task to manage the data from NASA’s Earth science research satellites and field measurement programs, and other data essential for the interpretation of these measurements in support of global change research. Apart from being able to handle a stream of 1 terabyte of data daily by the year 2000, EOSDIS will also need to provide transparent access to heterogeneous data held in the archives of several US government agencies, organizations and countries. A single graphical user interface employing the Global Change Master Directory needs to help users locate data sets of interest among massive and diverse data sets, or find the appropriate data analysis tools, regardless of their location. Another major international effort in the area of human genome research faces some similar, as well as unique issues due to the complexity of the genome data, special querying requirements and much more heterogeneous collections of data. Scientific databases can be viewed as critical repositories of knowledge, both existing and yet to be dis-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: panel : future directions of database research - the vldb broadening strategy , part 1

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-j &#246; rg schek
",y
"LEFT id: NA
RIGHT id: 368

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: standard for multimedia databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john r. smith
",n
"LEFT id: NA
RIGHT id: 332

LEFT text: In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: answering queries with useful bindings

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chen li , edward chang
",y
"LEFT id: NA
RIGHT id: 195

LEFT text: There has been much research on various aspects of Approximate Query Processing (AQP), such as different sampling strategies, error estimation mechanisms, and various types of data synopses. However, many subtle challenges arise when building an actual AQP engine that can be deployed and used by real world applications. These subtleties are often ignored (or at least not elaborated) by the theoretical literature and academic prototypes alike. For the first time to the best of our knowledge, in this article, we focus on these subtle challenges that one must address when designing an AQP system. Our intention for this article is to serve as a handbook listing critical design choices that database practitioners must be aware of when building or using an AQP system, not to prescribe a specific solution to each challenge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: join synopses for approximate query answering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 2219

LEFT text: Should digital libraries be based on image or text display? Which will serve users better? Experience and experiments show that users can employ either, and that there are technical advantages to each format. Often, material in both formats can be used together, and the long-run trend is probably towards Ascii material, even if reached by a circuitous path via images and OCR during a transition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: experiments on access to digital libraries : how can images and text be used together

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael lesk
",y
"LEFT id: NA
RIGHT id: 207

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 878

LEFT text: Database technology is one of the cornerstones for the new millennium’s IT landscape. However, database systems as a unit of code packaging and deployment are at a crossroad: commercial systems have been adding features for a long time and have now reached complexity that makes them a difficult choice, in terms of their ""gain/pain ratio"", as a central platform for value-added information services such as ERP or e-commerce. It is critical that database systems be easy to manage, predictable in their performance characteristics, and ultimately self-tuning. For this elusive goal, RISC-style simplification of server functionality and interfaces is absolutely crucial. We suggest a radical architectural departure in which database technology is packaged into much smaller RISC-style data managers with lean, specialized APIs, and with built-in self-assessment and auto-tuning capabilities 1. The Need for a New Departure Database technology has an extremely successful track record as a backbone of information technology (IT) throughout the last three decades. High-level declarative query languages like SQL and atomic transactions are key assets in the cost-effective development and maintenance of information systems. Furthermore, database technology continues to play a major role in the trends of our modern cyberspace society with applications ranging from webbased applications/services, and digital libraries to information mining on business as well as scientific data. Thus, database technology has impressively proven its benefits and seems to remain crucially relevant in the new millennium as well. Success is a lousy teacher (to paraphrase Bill Gates), and therefore we should not conclude that the database system, as the unit of engineering, deploying, and operating packaged database technology, is in good shape. A closer look at some important application areas and major trends in the software industry strongly indicates that database systems have an overly low “gain/pain ratio”. First, with the dramatic drop of hardware and software prices, the expenses due to human administration and tuning staff dominate the cost of ownership for a database system. The complexity and cost of these feed-and-care tasks is likely to prohibit database systems from further playing their traditionally prominent role in the future IT infrastructure. Next, database technology is more likely to be adopted in unbundled and dispersed form within higher-level application services. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: rethinking database system architecture : towards a self-tuning risc-style database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gerhard weikum
",y
"LEFT id: NA
RIGHT id: 1719

LEFT text: This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 1107

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: type classification of semi-structured documents

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: markus tresch , neal palmer , allen luniewski
",n
"LEFT id: NA
RIGHT id: 1093

LEFT text: Parallel disk systems provide opportunities for exploiting I/O parallelism in two possible ways, namely via inter-request and intra-request parallelism. In this paper, we discuss the main issues in performance tuning of such systems, namely striping and load balancing, and show their relationship to response time and throughput. We outline the main components of an intelligent, self-reliant file system that aims to optimize striping by taking into account the requirements of the applications, and performs load balancing by judicious file allocation and dynamic redistributions of the data when access patterns change. Our system uses simple but effective heuristics that incur only little overhead. We present performance experiments based on synthetic workloads and real-life traces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimation of query-result distribution and its application in parallel-join load balancing

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: viswanath poosala , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1685

LEFT text: The TSQL2 language, which was created to Temporal Databases, has a limited concept of indeterminate information [14]. So, both formalisms and temporal query languages present partial solutions to representing and manipulating indeterminate temporal objects. The main objective of this paper is to show a formalism that gives definitions about representation and manipulation of indeterminate temporal objects in indeterminate temporal database (ITDB). More precisely, we herein introduce LITO (Logic of Indeterminate Temporal Objects), a formalism that allows for retrieval query, logical query, insertion, logical deletion, refinement and update of indeterminate temporal objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: announcement-the temporal query language tsql2 final language definition

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard thomas snodgrass , ilsoo ahn , gad ariav , don batory , james clifford , curtis e. dyreson , ramez elmasri , fabio grandi , christian s. jensen , wolfgang k &#228; fer , nick kline , krishna kulkarni , t. y. cliff leung , nikos lorentzos , john f. roddick , arie segev , michael d. soo , suryanarayana m. sripada
",y
"LEFT id: NA
RIGHT id: 1969

LEFT text: Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated with a database system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language for multidimensional arrays : design , implementation , and optimization techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: leonid libkin , rona machlin , limsoon wong
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 2013

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: peerdb : peering into personal databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , kian-lee tan , aoying zhou , chin hong goh , yingguang li , chu yee liau , bo ling , wee siong ng , yanfeng shu , xiaoyu wang , ming zhang
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 861

LEFT text: As we approach the next century, the software industry landscape is undergoing massive technology and business changes. The client/server revolution has barely reached its half-life and it is already being eclipsed by the Internet revolution. Software development is moving away from the direction of being labour-intensive. Customers are buying more pre-packaged software solutions or software components that can easily be assembled on site by in-house personnel or systems integrators. Except for a handful of players like Microsoft, Oracle and Computer Associates, very few leading software companies of the seventies and eighties have survived into the nineties. A whole new generation of software companies have emerged that are focussed on selling advanced’ software components based on industry standards. For Indian software companies with superior technology development skills, the Internet will open up opportunities to build products that have never been built before and to enter global markets on a scale that was never attempted before.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the evolution of the web and implications for an incremental crawler

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghoo cho , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 426

LEFT text: Today the problem of semantic interoperability in information search on the Internet is solved mostly by means of centralization, both at a system and at a logical level. This approach has been successful to a certain extent. Peer-to-peer systems as a new brand of system architectures indicate that the principle of decentralization might offer new solutions to many problems that scale well to very large numbers of users.In this paper we outline how the peer-to-peer system architectures can be applied to tackle the problem of semantic interoperability in the large, driven in a bottom-up manner by the participating peers. Such a system can readily be used to study semantic interoperability as a global scale phenomenon taking place in a social network of information sharing peers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for expressing and combining preferences

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rakesh agrawal , edward l. wimmers
",n
"LEFT id: NA
RIGHT id: 1443

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of multiple-level association rules from large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu
",n
"LEFT id: NA
RIGHT id: 457

LEFT text: XML is becoming the most relevant new standard for data representation and exchange on the WWW. Novel languages for extracting and restructuring the XML content have been proposed, some in the tradition of database query languages (i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query language has yet been decided, but the discussion is ongoing within the World Wide Web Consortium and within many academic institutions and Internet-related major companies. We present a comparison of five, representative query languages for XML, highlighting their common features and differences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: comparative analysis of six xml schema languages

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dongwon lee , wesley w. chu
",n
"LEFT id: NA
RIGHT id: 1868

LEFT text: An Object-Oriented database can utilize the benefits of both the design and implementation of any application. Due to the increased popularity of database systems many new database systems based on varying data model and implementation have entered in the market. Database systems have complex architecture but they are the key factors behind the business transformations. Choosing the best one in any category is an important task based on performance analysis. This chapter deals with the database estimation methodology which integrates the database analysis task and performance analysis task. There are three major techniques for the performance estimation which are analytical modeling, simulation modeling and benchmarking.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sentinel : an object-oriented dbms with event-based rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. chakravarthy
",y
"LEFT id: NA
RIGHT id: 779

LEFT text: Electronic commerce is emerging as a major Web-supported application. In this paper we argue that database technology can, and should, provide the back-bone for a wide range of such applications. More precisely, we present here the ActiveViews system, which, relaying on an extensive use of database features in-cluding views, active rules (triggers), and enhanced mechanisms for notification, access control and logging/tracing of users activities, provides the needed basis for electronic commerce. Based on the emerging XML standards (DOM, query languages for XML, etc.), the system offers a novel declarative view specification language, describing the relevant data and activities of all actors (e.g. vendors and clients) participat-ing in electronic commerce activities . Then, acting as an application generator, the system generates an actual, possibly customized, Web application that allows users to perform the given set of controlled activities and to work interactively on the specified data in a standard distributed environment. Although closely related to workflow management systems, a major difference here is the importance we give to data. While workflow systems give declarative means for specifying the operations flow, the data involved is typically described in a very abstract manner, often disconnected from the description of the flow itself.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: atomicity versus anonymity : distributed transactions for electronic commerce

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: j. d. tygar
",n
"LEFT id: NA
RIGHT id: 250

LEFT text: This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sence that it changes the replication scheme of the object i.e., the set of processors at which the object inreplicated) as changes occur in the read-write patern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of ta distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorith.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: adaptive , fine-grained sharing in a client-server oodbms : a callback-based approach

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: markos zaharioudakis , michael j. carey , michael j. franklin
",y
"LEFT id: NA
RIGHT id: 2078

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",y
"LEFT id: NA
RIGHT id: 1150

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 2175

LEFT text: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1571

LEFT text: This paper presents a set of aggregation algorithms on very large compressed data warehouses for multidimensional OLAP. These algorithms operate directly on compressed datasets without the need to first decompress them. They are applicable to data warehouses that are compressed using variety of data compression methods. The algorithms have different performance behavior as a function of dataset parameters, sizes of outputs and main memory availability. The analysis and experimental results show that the algorithms have better performance than the traditional aggregation algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast algorithms for universal quantification in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: goetz graefe , richard l. cole
",n
"LEFT id: NA
RIGHT id: 774

LEFT text: In this paper, we consider the filter step of the spatial join problem, for the case where neither of the inputs are indexed. We present a new algorithm, Scalable Sweeping-Based Spatial Join (SSSJ), that achieves both efficiency on real-life data and robustness against highly skewed and worst-case data sets. The algorithm combines a method with theoretically optimal bounds on I/O transfers based on the recently proposed distribution-sweeping technique with a highly optimized implementation of internal-memory plane-sweeping. We present experimental results based on an efficient implementation of the SSSJ algorithm, and compare it to the state-ofthe-art Partition-Based Spatial-Merge (PBSM) algorithm of Patel and DeWitt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scalable sweeping-based spatial join

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: lars arge , octavian procopiuc , sridhar ramaswamy , torsten suel , jeffrey scott vitter
",y
"LEFT id: NA
RIGHT id: 1885

LEFT text: Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1053

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pesto : an integrated query/browser for object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , laura m. haas , vivekananda maganty , john h. williams
",n
"LEFT id: NA
RIGHT id: 1929

LEFT text: A well-known challenge in data warehousing is the efficient incremental maintenance of warehouse data in the presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype data warehousing system supporting incremental maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of data cubes and summary tables in a warehouse

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: inderpal singh mumick , dallan quass , barinderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 1170

LEFT text: In this paper, we study how to find such optimal schedules. In particular, we consider two optimization criteria: (i) one based on maximizing the number of piggybacked clips, and (ii) the other based on maximizing the impact on buffer space. We show that the optimal schedule under the first criterion is equivalent to a maximum matching in a suitably defined bipartite graph, and that under the second criterion, the optimal schedule is equivalent to a maximum matching in a suitably defined weighted bipartite graph.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimal clip ordering for multi-clip queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , paul shum
",y
"LEFT id: NA
RIGHT id: 343

LEFT text: Expectations for change are ubiquitous in education. Teachers who complete education programs and enter classrooms are encouraged to embrace change as new professionals. Instructional change is expected as a result of professional development when teachers examine their instructional practices and reflect on student learning. Two scholars whose work is foundational in teacher education, Lortie (2002) and Britzman (2003) challenged researchers and educators to consider how teachers and teacher education changes and resists change. Lortie (2002), in his sociological study of the profession, speculated on changes and resistance in teaching and argued that the two would “interact contrapuntally” (p. 219). In other words, these two forces exist in tension so that sustained change would often be thwarted. In a similar vein, Britzman’s (2003) critical analysis of learning to teach examined discourses and discursive structures of teacher preparation and noted contradictions and struggles in the lived experiences of teacher candidates. Near the conclusion of her analysis, she wrote, we should “attend to the possible and acknowledge the uncertainty of our educational lives” (p. 241). Possibilities and uncertainties continue to inform the work of educating teacher candidates and supporting inservice teachers as agents of innovation in schools. To be sure, schools and teaching have changed in terms of student demographics, accountability, standardization, and testing since these two studies were published. And yet, the articles in this issue suggest that exploring sources, motivation, and possible outcomes of change, as well as the nature of resistance, remain paramount. In this issue, one pedagogical innovations article and four empirical studies articles explore change and consider how resistance informs teacher education. In an argument for including data literacy in teacher preparation, Reeves describes six hours of classroom-based training with four cohorts of preservice elementary teachers that he claims will help them more deeply understand how data literacy can inform pedagogy. Reeves’ study is grounded in current research indicating that there is an increased expectation for teachers to use data to make instructional decisions. He contends that teacher preparation programs should include instruction about data literacy. This change could help preservice teachers enter classrooms with detailed knowledge about how data can productively inform their instruction. Reeves recognizes that such a change will require thoughtful consideration about when such instruction should be part of curricula; that these shifts will demand new resources; and, resistance to adding another facet to teacher education programs is possible. Reinhardt investigates how six cooperating teachers understand their roles within contexts of clinical experiences using Vygotskian theory. Her qualitative study focused on four female and two male teacher mentors in a diverse school district and examined how the cooperating teachers understand their role in a clinical experience as contrasted with the reality of teachers’ everyday work. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 607

LEFT text: We highlight some of the pleasures of coding with Java, and some of the pains of coding around Java in order to obtain good performance in a data-intensive server. For those issues that were painful, we present concrete suggestions for evolving Java's interfaces to better suit serious software systems development. We believe these experiences can provide insight for other designers to avoid pitfalls we encountered and to decide if Java is a suitable platform for their system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: java support for data-intensive systems : experiences building the telegraph dataflow system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mehul a. shah , michael j. franklin , samuel madden , joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 1992

LEFT text: This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: Conceptual data modeling for complex applications, such as multimedia and spatiotemporal applications, often results in large, complicated and difficult-to-comprehend diagrams. One reason for this is that these diagrams frequently involve repetition of autonomous, semantically meaningful parts that capture similar situations and characteristics. By recognizing such parts and treating them as units, it is possible to simplify the diagrams, as well as the conceptual modeling process. We propose to capture autonomous and semantically meaningful excerpts of diagrams that occur frequently as modeling patterns. Specifically, the paper concerns modeling patterns for conceptual design of spatiotemporal databases. Based on requirements drawn from real applications, it presents a set of modeling patterns that capture spatial, temporal, and spatiotemporal aspects. To facilitate the conceptual design process, these patterns are abbreviated by corresponding spatial, temporal, and spatiotemporal pattern abstractions, termed components. The result is more elegant and less-detailed diagrams that are easier to comprehend, but yet semantically rich. The Entity-Relationship model serves as the context for this study. An extensive example from a real cadastral application illustrates the benefits of using a component-based conceptual model.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1841

LEFT text: MDM is a tool that enables the users to define schemes of different data models and to perform translations of schemes from one model to another. These functionalities can be at the basis of a customizable and integrated CASE environment supporting the analysis and design of information systems. MDM has two main components: the Model Manager and the Schema Manager. The Model Manager supports a specialized user, the model engineer, in the definition of a variety of models, on the basis of a limited set of metaconstructs covering almost all known conceptual models. The Schema Manager allows designers to create and modify schemes over the defined models, and to generate at each time a translation of a scheme into any of the data models currently available. Translations between models are automatically derived, at definition time, by combining a predefined set of elementary transformations, which implement the standard translations between simple combinations of constructs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mdm : a multiple-data model tool for the management of heterogeneous database schemes

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , riccardo torlone
",y
"LEFT id: NA
RIGHT id: 1185

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: spatial indexing of high-dimensional data based on relative approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: Due to organizational or operational constraints, the diverse data sources that an enterprise uses do not generally lend themselves to being fully replicated or completely consolidated under a single database, hence the increased demand for data interchange and for federated access to distributed sources. IBM has ongoing work in information integration technology that enables integrated, real-time access to traditional and emerging data sources, transforms information to meet the needs of business analysts, and manages data placement for performance, currency, and availability leading to fast, constant, and easy access for customer e-business solutions. IBM's Information Integration infrastructure today supports SQL—a mature, powerful query language—plus a number of SQL extensions in support of XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1289

LEFT text: Semantic Web Enabled Web Services (SWWS) will transform the web from a static collection of information into a distributed device of computation on the basis of Semantic technology making content within the World Wide Web machine-processable and machine-interpretable. Semantic Web Enabled Web Services will allow the automatic discovery, selection and execution of inter-organization business logic making areas like dynamic supply chain composition a reality. In this paper we introduce the vision of Semantic Web Enabled Web Services, describe requirements for building semantics-driven web services and sketch a first draft of conceptual architecture for implementing semantic web enabled web services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a data warehousing architecture for enabling service provisioning process

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yannis kotidis
",n
"LEFT id: NA
RIGHT id: 369

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: temporal statement modifiers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 2121

LEFT text: In the last decade the database research community gave valuable results in modelling and retrieving spatial objects, in a temporal framework, e.g., [8] [9]. It is recognized that the representation and the management of GIS play a central part in data manipulation and querying. In fact, the incorporation of time in them may lead to consider new types of information for modelling those highly time dependent spatial data (e.g., temperatures, land use coverage, epidemic information related to a given area, etc.) and moving objects (e.g., entities that change the relative spatial position). The trend of temporal data modelling in GIS is moving from time-stamping layer (the snapshot models, [2]), attributes (space-time composites, [5]), and spatial objects to time-stamping events or processes that are mainly based on the concepts of time sequences [8].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: modelling temporal thematic map contents

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: alberto d'onofrio , elaheh pourabbas
",y
"LEFT id: NA
RIGHT id: 1220

LEFT text: We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:<ul><li>Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: effective timestamping in databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kristian torp , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1407

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: transaction timestamping in ( temporal ) databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian s. jensen , david b. lomet
",n
"LEFT id: NA
RIGHT id: 1861

LEFT text: A methodology of reengineering existing extended Entity-Relationship(EER) model to Object Modeling Technique(OMT) model is described. A set of translation rules from EER model to a generic Object-Oriented(OO) model of OMT methodology is devised. Such reengineering practices not only can provide us with significant insight to the ""interoperability"" between the OO and the traditional semantic modelling techniques, but also can lead us to the development of a practical design methodology for object-oriented databases(OODB).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an extended entity-relationship model for geographic applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thanasis hadzilacos , nectaria tryfona
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 232

LEFT text: The optimization capabilities of RDBMSs make them attractive for executing data transformations. However, despite the fact that many useful data transformations can be expressed as relational queries, an important class of data transformations that produce several output tuples for a single input tuple cannot be expressed in that way. To overcome this limitation, we propose to extend Relational Algebra with a new operator named data mapper. In this paper, we formalize the data mapper operator and investigate some of its properties. We then propose a set of algebraic rewriting rules that enable the logical optimization of expressions with mappers and prove their correctness. Finally, we experimentally study the proposed optimizations and identify the key factors that influence the optimization gains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: safe query languages for constraint databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter z. revesz
",n
"LEFT id: NA
RIGHT id: 1808

LEFT text: Analysts and decision-makers use what-if analysis to assess the e®ects of hypotheti- cal scenarios. What-if analysis is currently supported by spreadsheets and ad-hoc O L AP tools. Unfortunately, the former lack seam- less integration with the data and the lat- ter lack °exibility and performance appropri- ate for O L AP applications. To tackle these problems we developed the Sesamesystem, which models an hypothetical scenario as a list of hypothetical modications on the ware- house views and fact data. We provide formal scenario syntax and semantics, which extend view update semantics for accomodating the special requirements of O L AP. We focus on query algebra operators suitable for perform- ing spreadsheet-style computations. Then we present Sesame's optimizer and its corner- stone substitution and rewriting mechanisms. Substitution enables lazy evaluation of the hy- pothetical updates. The substitution module delivers orders-of-magnitude optimizations in cooperation withtherewriterthatusesknowl- edge of arithmetic, relational, ¯nancial and other operators. Finally we discuss the chal- lenges that the size of the scenario specica- tionsandthe arbitrarynatureof theoperators pose to the rewriter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for implementing hypothetical queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: timothy griffin , richard hull
",n
"LEFT id: NA
RIGHT id: 1956

LEFT text: The Teradata Multimedia Object Manager is a general-purpose content analysis multimedia server designed for symmetric multiprocessing and massively parallel processing environments. The Multimedia Object Manager defines and manipulates user-defined functions (UDFs), which are invoked in parallel to analyze or manipulate the contents of multimedia objects. Several computationally intensive applications of this technology, which use large persistent datasets, include fingerprint matching, signature verification, face recognition, and speech recognition/translation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a teradata content-based multimedia object manager for massively parallel architectures

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: w. o'connell , i. t. ieong , d. schrader , c. watson , g. au , a. biliris , s. choo , p. colin , g. linderman , e. panagos , j. wang , t. walter
",y
"LEFT id: NA
RIGHT id: 1106

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: processing object-oriented queries with invertible late bound functions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: staffan flodin , tore risch
",n
"LEFT id: NA
RIGHT id: 329

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: querying atsql databases with temporal logic

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jan chomicki , david toman , michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 910

LEFT text: This report describes opportunities for the DB/IS community to contribute to the advancement of the Semantic Web and the challenges or new research topics presented by the vision of the Semantic Web to the database and information systems (DB/IS) researchers. It is based on the NSF-OntoWeb Invitational Workshop on DB/IS Research for Semantic Web and Enterprises that was held during April 3-5, 2002 at the Amicalola Falls State Park in the northern Georgia mountains. Most of the workshop participants were

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: amicalola report : database and information systems research challenges and opportunities in semantic web and enterprises

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: amit sheth , robert meersman
",y
"LEFT id: NA
RIGHT id: 2019

LEFT text: Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 739

LEFT text: After a system crash, databases recover to the last committed transaction, but applications usually either crash or cannot continue. The Phoenix purpose is to enable application state to persist across system crashes, transparent to the application program. This simplifies application programming, reduces operational costs, masks failures from users, and increases application availability, which is critical in many scenarios, e.g., e-commerce. Within the Phoenix project, we have explored how to provide application recovery efficiently and transparently via redo logging. This paper describes the conceptual framework for the Phoenix project, and the software infrastructure that we are building.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: phoenix project : fault-tolerant applications

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: roger barga , david lomet
",y
"LEFT id: NA
RIGHT id: 1650

LEFT text: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 2173

LEFT text: The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making.We propose here a simple, logical framework for formulating preferences as preference formulas. The framework does not impose any restrictions on the preference relations, and allows arbitrary operation and predicate signatures in preference formulas. It also makes the composition of preference relations straightforward. We propose a simple, natural embedding of preference formulas into relational algebra (and SQL) through a single  winnow operator parameterized by a preference formula. The embedding makes possible the formulation of complex preference queries, for example, involving aggregation, by piggybacking on existing SQL constructs. It also leads in a natural way to the definition of further, preference-related concepts like ranking. Finally, we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators. The preconditions on the applicability of the laws are captured by logical formulas. The laws provide a formal foundation for the algebraic optimization of preference queries. We demonstrate the usefulness of our approach through numerous examples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: preference formulas in relational queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jan chomicki
",y
"LEFT id: NA
RIGHT id: 1107

LEFT text: A radar transmitter apparatus comprising a radar transmitter equipped with a modulator arranged in an oil-filled housing, the modulator being held in spaced relationship with respect to the inner walls of the housing in order to form an intermediate space for the convection flow of the oil. The housing is substantially trough or vat-shaped and covered by a trough or vat-shaped cover member. In the internal chamber or space between the cover member and the modulator, which internal space is wetted by the oil, there is arranged, on the one hand, a magnetron attached at the cover member and, on the other hand, a thyratron which is mounted directly below an opening at the cover member. This opening is closable by means of oil sealed throughpassage means.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: type classification of semi-structured documents

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: markus tresch , neal palmer , allen luniewski
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 499

LEFT text: The physical performance of the Oracle RAC hardware architecture directly affects Oracle's output performance. In this paper, we perform performance tests on three Oracle RAC hardware architectures, and the test data is simulated using real teaching management system data on campus. The simulation results show that the two new oracle RAC hardware architectures are respectively occupied by the TPM and IOPS indicators, and the overall is stronger than the traditional external storage architecture. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop on performance and architecture of web servers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: krishna kant , prasant mohapatra
",n
"LEFT id: NA
RIGHT id: 2268

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: aurora : a new model and architecture for data stream management

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: daniel j. abadi , don carney , ugur &#199; etintemel , mitch cherniack , christian convey , sangdon lee , michael stonebraker , nesime tatbul , stan zdonik
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 1679

LEFT text: We demonstrate a visual based XML-Relational database system where XML data is managed by commercial RDBMS. A query interface enables users to form path expression based queries against stored data visually. Statistics about data and a special path directory are used to rewrite path expression based queries into efficient SQL statements involving less number of joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dblearn : a system prototype for knowledge discovery in relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu , yue huang , yandong cai , nick cercone
",n
"LEFT id: NA
RIGHT id: 206

LEFT text: A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a comparison of selectivity estimators for range queries on metric attributes

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: bj &#246; rn blohsfeld , dieter korus , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1722

LEFT text: We examine how to apply the hash-join paradigm to spatial joins, and define a new framework for spatial hash-joins. Our spatial partition functions have two components: a set of bucket extents and an assignment function, which may map a data item into multiple buckets. Furthermore, the partition functions for the two input datasets may be different.We have designed and tested a spatial hash-join method based on this framework. The partition function for the inner dataset is initialized by sampling the dataset, and evolves as data are inserted. The partition function for the outer dataset is immutable, but may replicate a data item from the outer dataset into multiple buckets. The method mirrors relational hash-joins in other aspects. Our method needs no pre-computed indices. It is therefore applicable to a wide range of spatial joins.Our experiments show that our method outperforms current spatial join algorithms based on tree matching by a wide margin.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial joins using seeded trees

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 332

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: answering queries with useful bindings

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chen li , edward chang
",n
"LEFT id: NA
RIGHT id: 228

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the microsoft database research group

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1889

LEFT text: Regular expressions with capture variables, also known as ""regex formulas,'' extract relations of spans (interval positions) from text. These relations can be further manipulated via the relational Algebra as studied in the context of ""document spanners,"" Fagin et al.'s formal framework for information extraction. We investigate the complexity of querying text by Conjunctive Queries (CQs) and Unions of CQs (UCQs) on top of regex formulas. Such queries have been investigated in prior work on document spanners, but little is known about the (combined) complexity of their evaluation. We show that the lower bounds (NP-completeness and W[1]-hardness) from the relational world also hold in our setting; in particular, hardness hits already single-character text. Yet, the upper bounds from the relational world do not carry over. Unlike the relational world, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source of hardness is that it may be intractable to instantiate the relation defined by a regex formula, simply because it has an exponential number of tuples. Yet, we are able to establish general upper bounds. In particular, UCQs can be evaluated with polynomial delay, provided that every CQ has a bounded number of atoms (while unions and projection can be arbitrary). Furthermore, UCQ evaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the parameter is the size of the UCQ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tiziana catarci , isabel f. cruz
",n
"LEFT id: NA
RIGHT id: 966

LEFT text: Object-Relational database systems allow users to define new user-defined types and functions. This presents new optimizer and run-time challenges to the database system on shared-nothing architectures. In this paper, we describe a new strategy we are exploring for the NCR Teradata Multimedia Database System; our focus is directing research for real applications we are seeing. In doing so, we will briefly describe optimizer challenges particularly related to predicate use of large multimedia objects, such as video/audio clips, images, and text documents. The motivation for this work is based on database tuning [SD961 for diverse queries related to multimedia objects. Most notably, expensive and/or high variant user defined functions [He198]. Our approach is referred to as plan-per-tuple. The

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: plan-per-tuple optimization solution - parallel execution of expensive user-defined functions

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: felipe cari &#241; o , william o'connell
",y
"LEFT id: NA
RIGHT id: 104

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: We demonstrate a visual based XML-Relational database system where XML data is managed by commercial RDBMS. A query interface enables users to form path expression based queries against stored data visually. Statistics about data and a special path directory are used to rewrite path expression based queries into efficient SQL statements involving less number of joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1632

LEFT text: The bypass technique, which was formerly restricted to selections only [KMPS94], is extended to join operations. Analogous to the selection case, the join operator may generate two output streams-the join result and its complement-whose subsequent operator sequence is optimized individually. By extending the bypass technique to joins, several problems have to be solved. (1) An algorithm for exhaustive generation of the search space for bypass plans has to be developed. (2) The search space for bypass plans is quite large. Hence, partial exploration strategies still resulting in sufficiently efficient plans have to be developed. (3) Since the complement of a join can be very large, those cases where the complement can be restricted to the complement of the semijoin have to be detected. We attack all three problems. Especially, we present an algorithm generating the optimal bypass plan and one algorithm producing near optimal plans exploring the search space only partially. As soon as disjunctions occur, bypassing results in savings. Since the join operator is often more expensive than the selection, the savings for bypassing joins are even higher than those for selections only. We give a quantitative assessment of these savings on the basis of some example queries. Further, we evaluate the performance of the two bypass plan generating algorithms. ‘This work was supported by the German Research Council under contract DFG Ke401/6-2. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing disjunctive queries with expensive predicates

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: a. kemper , g. moerkotte , k. peithner , m. steinbrunn
",n
"LEFT id: NA
RIGHT id: 2015

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: transparent mid-tier database caching in sql server

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: per - &#197; ke larson , jonathan goldstein , jingren zhou
",n
"LEFT id: NA
RIGHT id: 268

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",y
"LEFT id: NA
RIGHT id: 1771

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",n
"LEFT id: NA
RIGHT id: 1722

LEFT text: Two new algorithms, “Jive join” and “Slam join,” are proposed for computing the join of two relations using a join index. The algorithms are duals: Jive join range-partitions input relation tuple ids and then processes each partition, while Slam join forms ordered runs of input relation tuple ids and then merges the results. Both algorithms make a single sequential pass through each input relation, in addition to one pass through the join index and two passes through a temporary file, whose size is half that of the join index. Both algorithms require only that the number of blocks in main memory is of the order of the square root of the number of blocks in the smaller relation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial joins using seeded trees

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1669

LEFT text: In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: relaxed transaction processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munindar p. singh , christine tomlinson , darrell woelk
",n
"LEFT id: NA
RIGHT id: 127

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 1499

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-oriented , rapid application development in a pc database environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate fox development team microsoft
",n
"LEFT id: NA
RIGHT id: 1167

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency control in hierarchical multidatabase systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sharad mehrotra , henry f. korth , avi silberschatz
",n
"LEFT id: NA
RIGHT id: 1781

LEFT text: Phrase matching is a common IR technique to search text and identify relevant documents in a document collection. Phrase matching in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for phrase matching in XML that permits dynamic specification of both the phrase to be matched and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: interaction of query evaluation and buffer management for information retrieval

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bj &#246; rn t. j &#243; nsson , michael j. franklin , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1587

LEFT text: As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, τXQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a τXQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach: selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel evaluation of multi-join queries

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: annita n. wilschut , jan flokstra , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 428

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards self-tuning data placement in parallel database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal
",n
"LEFT id: NA
RIGHT id: 2195

LEFT text: The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: roadrunner : towards automatic data extraction from large web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: valter crescenzi , giansalvatore mecca , paolo merialdo
",y
"LEFT id: NA
RIGHT id: 367

LEFT text: To resolve the syntax, structure and semantic heterogeneity for sharing information resources, the representative technologies are XML and Metadata. XML is used to represent the syntax and structure of information resources but the various XML schema definitions that have been developed by independent organizations without any standards or guidelines, make it difficult to share the semantic meaning of XML encoded information resources. In this paper, we propose a mechanism, named MSDL that represents the exact meaning of XML tags by describing the structural and semantic differences with standard metadata in metadata registries. MSDL overcomes the limitations of other approaches with respect to exactness, flexibility and standardization, and provides an environment for business partners using different metadata to share their XML encoded information resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic b2b integration

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christoph bussler
",n
"LEFT id: NA
RIGHT id: 1019

LEFT text: Abstract. We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By “categorical data,” we mean tables with fields that cannot be naturally ordered by a metric – e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: clustering categorical data : an approach based on dynamical systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david gibson , jon m. kleinberg , prabhakar raghavan
",y
"LEFT id: NA
RIGHT id: 1609

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1593

LEFT text: ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a critique of ansi sql isolation levels

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: hal berenson , phil bernstein , jim gray , jim melton , elizabeth o'neil , patrick o'neil
",y
"LEFT id: NA
RIGHT id: 569

LEFT text: Traditional protocols for distributed database management have high message overhead, lock or restrain access to resources during protocol execution, and may become impractical for some scenarios like real-time systems and very large distributed databases. In this paper we present the demarcation protocol; it overcomes these problems through the use of explicit linear arithmetic consistency constraints as the correctness criteria. The method establishes safe limits as “lines drawn in the sand” for updates and gives a way of changing these limits dynamically, enforcing the constraints at all times.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 418

LEFT text: Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated with a database system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the onion technique : indexing for linear optimization queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yuan-chi chang , lawrence bergman , vittorio castelli , chung-sheng li , ming-ling lo , john r. smith
",n
"LEFT id: NA
RIGHT id: 66

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization in object databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mohana k. lakhamraju , rajeev rastogi , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 791

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation of two semantic query optimization techniques in db2 universal database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: qi cheng , jarek gryz , fred koo , t. y. cliff leung , linqi liu , xiaoyan qian , k. bernhard schiefer
",n
"LEFT id: NA
RIGHT id: 714

LEFT text: We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: acdn : a content delivery network for applications

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: pradnya karbhari , michael rabinovich , zhen xiao , fred douglis
",n
"LEFT id: NA
RIGHT id: 1460

LEFT text: The popularity of the World-Wide Web (WWW) has made it a prime vehicle for disseminating information. The relevance of database concepts to the problems of managing and querying this information has led to a signi cant body of recent research addressing these problems. Even though the underlying challenge is the one that has been traditionally addressed by the database community { how to manage large volumes of data { the novel context of the WWW forces us to signi cantly extend previous techniques. The primary goal of this survey is to classify the di erent tasks to which database concepts have been applied, and to emphasize the technical innovations that were required to do so.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: w3qs : a query system for the world-wide web

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: david konopnicki , oded shmueli
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (à la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 2241

LEFT text: We describe the conceptual model of SORAC, a data modeling system developed at the University of Rhode Island. SORAC supports both semantic objects and relationships, and provides a tool for modeling databases needed for complex design domains. SORAC's set of built-in semantic relationships permits the schema designer to specify enforcement rules that maintain constraints on the object and relationship types. SORAC then automatically generates C++ code to maintain the specified enforcement rules, producing a schema that is compatible with Ontos. This facilitates the task of the schema designer, who no longer has to ensure that all methods on object classes correctly maintain necessary constraints. In addition, explicit specification of enforcement rules permits automated analysis of enforcement propagations. We compare the interpretations of relationships within the semantic and object-oriented models as an introduction to the mixed model that SORAC supports. Next, the set of built-in SORAC relationship types is presented in terms of the enforcement rules permitted on each relationship type. We then use the modeling requirements of an architectural design support system, called ArchObjects, to demonstrate the capabilities of SORAC. The implementation of the current SORAC prototype is also briefly discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data compression support in databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: balakrishna r. iyer , david wilhite
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 1833

LEFT text: In this paper, we consider the filter step of the spatial join problem, for the case where neither of the inputs are indexed. We present a new algorithm, Scalable Sweeping-Based Spatial Join (SSSJ), that achieves both efficiency on real-life data and robustness against highly skewed and worst-case data sets. The algorithm combines a method with theoretically optimal bounds on I/O transfers based on the recently proposed distribution-sweeping technique with a highly optimized implementation of internal-memory plane-sweeping. We present experimental results based on an efficient implementation of the SSSJ algorithm, and compare it to the state-ofthe-art Partition-Based Spatial-Merge (PBSM) algorithm of Patel and DeWitt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: size separation spatial join

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nick koudas , kenneth c. sevcik
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",y
"LEFT id: NA
RIGHT id: 1689

LEFT text: At the University of Illinois at Urbana-Champaign, the computer engineering curriculum is offered by the large, research-oriented Department of Electrical and Computer Engineering. The curriculum features a strong foundation in electrical engineering, an appropriate balance of computer hardware and software topics, a sequence of increasingly sophisticated design experiences, and a wide variety of electives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at nthu and itri

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arbee l. p. chen
",n
"LEFT id: NA
RIGHT id: 1019

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: clustering categorical data : an approach based on dynamical systems

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david gibson , jon m. kleinberg , prabhakar raghavan
",n
"LEFT id: NA
RIGHT id: 1074

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient user-adaptable similarity search in large multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 636

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: fibonacci : a programming language for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: antonio albano , giorgio ghelli , renzo orsini
",n
"LEFT id: NA
RIGHT id: 1938

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 496

LEFT text: Database design commonly assumes, explicitly or implicitly, that instances must belong to classes. This can be termed the assumption of inherent classification. We argue that the extent and complexity of problems in schema integration, schema evolution, and interoperability are, to a large degree, consequences of inherent classification. Furthermore, we make the case that the assumption of inherent classification violates philosophical and cognitive guidelines on classification and is, therefore, inappropriate in view of the role of data modeling in representing knowledge about application domains. As an alternative, we propose a layered approach to modeling in which information about instances is separated from any particular classification. Two data modeling layers are proposed: (1) an instance model consisting of an instance base (i.e., information about instances and properties) and operations to populate, use, and maintain it; and (2) a class model consisting of a class base (i.e., information about classes defined in terms of properties) and operations to populate, use, and maintain it. The two-layered model provides class independence. This is analogous to the arguments of data independence offered by the relational model in comparison to hierarchical and network models.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: emancipating instances from the tyranny of classes in information modeling

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jeffrey parsons , yair wand
",y
"LEFT id: NA
RIGHT id: 1430

LEFT text: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: composite events for active databases : semantics , contexts and detection

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: sharma chakravarthy , v. krishnaprasad , eman anwar , s.-k . kim
",n
"LEFT id: NA
RIGHT id: 1156

LEFT text: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing of relational and eXtensible Markup Language (XML) data that would otherwise remain (at least partially) invisible to the Web. Relational databases store large amounts of data that are queried using structured query languages. A user needs to know the underlying schema and the query language in order to make meaningful ad hoc queries on the data. This is a substantial barrier for casual users, such as users of Web-based information systems. HTML forms can be provided for predefined queries. A university Website may provide a form interface to search for faculty and students. Searching for departments would require yet another form, as would search for courses offered. However, creating an interface for each such task is laborious, and is also confusing to users since they must first expend effort finding which form to use. search can provide a very simple and easy-to-use mechanism for casual users to get information from databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1456

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a data transformation system for biological data sources

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: peter buneman , susan b. davidson , kyle hart , g. christian overton , limsoon wong
",n
"LEFT id: NA
RIGHT id: 1982

LEFT text: Database systems must become more open to retain their relevance as a technology of choice and necessity. Openness implies not only databases exporting their data, but also exporting their services. This is as true in classical application areas as in non-classical (GIS, multimedia, design, etc).This paper addresses the problem of exporting storage-management services of indexing, replication and basic query processing. We describe an abstract-object storage model which provides the basic mechanism, 'likeness', through which these services are applied uniformly to internally-stored, internally-defined data, and to externally-stored, externally-defined data. Managing external data requires the coupling of external operations to the database system. We discuss the interfaces and protocols required of these to achieve correct resource management and admit efficient realisation. Throughout, we demonstrate our solutions in the area of semi-structured file management; in our case, geospatial metadata files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an open abstract-object storage system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stephen blott , lukas relly , hans-j &#246; rg schek
",y
"LEFT id: NA
RIGHT id: 1026

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 2105

LEFT text: We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: minicon : a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rachel pottinger , alon halevy
",n
"LEFT id: NA
RIGHT id: 307

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view selection and maintenance using multi-query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hoshi mistry , prasan roy , s. sudarshan , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1507

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: view maintenance in a warehousing environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: yue zhuge , h &#233; ctor garc &#237; a-molina , joachim hammer , jennifer widom
",y
"LEFT id: NA
RIGHT id: 759

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 2289

LEFT text: Four different pointer swizzling techniques allowing object replacement are investigated and compared with the performance of an object manager employing no pointer swizzling. The extensive qualitative and quantitative evaluation—only part of which could be presented in this article—demonstrate that there is noone superior pointer swizzling strategy forall application profiles. Therefore, an adaptable object base run-time system is devised that employs the full range of pointer swizzling strategies, depending on the application profile characteristics that are determined by, for example, monitoring in combination with sampling, user specifications, and/or program analysis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dual-buffering strategies in object bases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 1940

LEFT text: This article gives methods for statically analyzing sets of active database rules to determine if the rules are (1) guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: modularization techniques for active rules design

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: elena baralis , stefano ceri , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 1183

LEFT text: Information integration provides a competitive advantage to businesses and is fundamental to on demand computing. It is strategic area of investment by software companies today whose goal is to provide a unified view of the data regardless of differences in data format, data location and access interfaces, dynamically manage data placement to match availability, currency and performance requirements, and provide autonomic features that reduce the burden on IT staffs for managing complex data architectures. This paper describes the motivation for integrating information for on demand computing, explains its requirements, and illustrates its value through usage scenarios. As shown in the paper, there is still a tremendous amount of research, engineering, and development work needed to make the full information integration vision a reality and it is expected that software companies will continue to heavily invest in aggressively pursing the information integration vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 1047

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering queries with aggregation using views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: divesh srivastava , shaul dar , h. v. jagadish , alon y. levy
",y
"LEFT id: NA
RIGHT id: 1648

LEFT text: The Object Database Management Group (ODMG) is a consortium of object-oriented DBMS vendors that have developed a standard interface for their products, ODMG-93. The standard includes a common architecture and deftition for an object-oriented DBMS, a common object model with an object deftition language, a common object query language, and standardized progr amming language bindings, cumently for C++ and Smalltalk. An object-oriented DBMS (by ODMG’S defiition) provides programming language bindings with direct, transparent persistence for data structures, in contrast to the embedded language bindings used in most DBMSS. The common object model allows data to be shared across programming languages. This model incorporates object Ills, encapsulation, methods, frost-class types, multiple inheritance, relationships, lists, sets, bags, arrays, and many other features.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu object-oriented dbms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: asuman dogac , budak arpinar , cem evrendilek , cetin ozkan , ilker altintas , ilker durusoy , mehmet altinel , tansel okay , yuksel saygin
",n
"LEFT id: NA
RIGHT id: 2202

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1869

LEFT text: In order to cope with the dynamic scenario of fast changing business requirements enterprises have embraced web technologies to manage their business processes. However, the ability to integrate business processes like procurement, customer relationship management, finance, human resources and manufacturing in a typical supply chain on the web is a challenging task. Today’s virtual enterprises need to integrate different workflows within and across enterprises efficiently so as to provide seamless services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mentor workbench for enterprise-wide workflow management

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dirk wodtke , jeanine weissenfels , gerhard weikum , angelika kotz dittrich , peter muth
",n
"LEFT id: NA
RIGHT id: 498

LEFT text: The ISDO '00 workshop on ""Infrastructures for Dynamic Business-to-Business Service Outsourcing"" [1] was held as a preconference workshop of the 12th Conference on Advanced Information Systems Engineering (CAiSE *00) in Stockholm, Sweden, on June 5 and 6, 2000. C. Bussler (Net-fish Technologies), M. Bichler (Vienna University of Economics and Business Administration), and Y. Hoffner and H. Ludwig (IBM Zurich Research Laboratory) organised the workshop and chaired the program committee.The objective of the workshop was to provide a platform to discuss models and technologies for service outsourcing, with emphasis on the integration of the dynamic establishment, setup, and enactment of service relationships that connect the business processes of service provider and consumer businesses, thereby establishing virtual enterprises.Nowadays, many production companies integrate their procurement processes using online marketplaces and network-based supply chain management systems. However, this is not the case for the service industry. Whereas many service organisations have already automated their internal business process management (e.g. using workflow management systems or enterprise resource planning (ERP) systems), service marketplaces still remain an uncommon phenomenon. The reason for this is that the integration of service sales, service enactment, and customer interaction with the service process still appears to be highly difficult. This is particularly the case where complex services, such as insurance and complex logistics, involve considerable customer interaction.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on isdo ' 00 : the caise * 00 workshop on infrastructures for dynamic business-to-business service outsourcing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: heiko ludwig , paul grefen
",y
"LEFT id: NA
RIGHT id: 579

LEFT text: Active database systems support mechanisms that enable them to respond automatically to events that are taking place either inside or outside the database system itself. Considerable effort has been directed towards improving understanding of such systems in recent years, and many different proposals have been made and applications suggested. This high level of activity has not yielded a single agreed-upon standard approach to the integration of active functionality with conventional database systems, but has led to improved understanding of active behavior description languages, execution models, and architectures. This survey presents the fundamental characteristics of active database systems, describes a collection of representative systems within a common framework, considers  the consequences for implementations of certain design decisions, and discusses tools for developing active applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: declare and sds : early efforts to commercialize deductive database technology

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: werner kie &#223; ling , helmut schmidt , werner strau &#223; , gerhard d &#252; nzinger
",y
"LEFT id: NA
RIGHT id: 1160

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data placement in shared-nothing parallel database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 269

LEFT text: This paper describes issues and solutions related to the creation of a product information database in the enterprise, and using this database as a foundation for deploying an electronic catalog. Today, product information is typically managed in document composition systems and communicated on paper. In the new wired world, these processes are undertaking fundamental changes to cope with the time to market pressure and the need for accurate, complete, and structured presentation of product information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enabling dynamic content caching for database-driven web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , wen-syan li , qiong luo , wang-pin hsiung , divyakant agrawal
",n
"LEFT id: NA
RIGHT id: 276

LEFT text: Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting constraint-like data characterizations in query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: parke godfrey , jarek gryz , calisto zuzarte
",y
"LEFT id: NA
RIGHT id: 2281

LEFT text: The tutorial surveys state-of-the-art methods for storing and retrieving multimedia data from large databases. Records (= documents) may consist of formatted fields, text, images, voice, animation etc. .4 sample query that we would like to support is ‘in a collection of 2-d color images, find images that are similar to a sunset photograph’. Indexing for images and other media is a new, active area of research; the tutorial will present recent approaches and prototype systems, for 2-d and 3-d medical image databases, 2-d color image databases, and l-d time series databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: indexing multiple sets

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christoph kilger , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 1763

LEFT text: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: intelligent access to heterogeneous information sources : report on the 4th workshop on knowledge representation meets databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: franz baader , manfred a. jeusfeld , werner nutt
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 752

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the design of a retrieval technique for high-dimensional data on tertiary storage

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ratko orlandic , jack lukaszuk , craig swietlik
",n
"LEFT id: NA
RIGHT id: 375

LEFT text: The enhanced pay-per-view (EPPV) model for providing continuous-media-on-demand (CMOD) services associates with each continuous media clip a display frequency that depends on the clip’s popularity. The aim is to increase the number of clients that can be serviced concurrently beyond the capacity limitations of available resources, while guaranteeing a constraint on the response time. This is achieved by sharing periodic continuous media streams among multiple clients. In this paper, we provide a comprehensive study of the resource scheduling problems associated with supporting EPPV for continuous media clips with (possibly) different display rates, frequencies, and lengths. Our main objective is to maximize the amount of disk bandwidth that is effectively scheduled under the given data layout and storage constraints. This formulation gives rise to -hard combinatorial optimization problems that fall within the realm of hard real-time scheduling theory. Given the intractability of the problems, we propose novel heuristic solutions with polynomial-time complexity. Preliminary results from an experimental evaluation of the proposed schemes are also presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: real-time database - similarity and resource scheduling

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tei-wei kuo , aloysius k. mok
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, we extent our earlier work on structural XSKETCH synopses and we propose an (augmented) XSKETCH synopsis model that exploits localized stability and value-distribution summaries (e.g., histograms) to accurately capture the complex correlation patterns that can exist between and across path structure and element values in the data graph. We develop a systematic XSKETCH estimation framework for complex path expressions with value predicates and we propose an efficient heuristic algorithm based on greedy forward selection for building an effective XSKETCH for a given amount of space (which is, in general, an NP-hard optimization problem). Implementation results with both synthetic and real-life data sets verify the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 2063

LEFT text: The processing of spatial joins can be greatly improved by the use of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of nonintersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. This article presents a polygon approximation for spatial join processing which we call four-colors raster signature (4CRS). The performance of a filter using this approximation was evaluated with real world data sets. The results showed that our approach, when compared to other approaches presented in the related literature, reduced the inconclusive answers by a factor of more than two. As a result, the need for retrieving the representation of polygons and carrying out exact geometry tests is reduced by a factor of more than two, as well. A Raster Approximation for the Processing of Spatial Joins

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic sample selection for approximate query processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: The Eighth International Workshop on Knowledge Representation Meets Databases (KRDB) was held at the Ponti cia Universit a Urbaniana, in Rome, right after VLDB 2001. KRDB was initiated in 1994 to provide an opportunity for researchers and practitioners from the two areas to exchange ideas and results. This year's focus was on Modeling, Querying andManaging Semistructured Data. The one day program included ten research papers, one invited talk, and a panel. Eight of the accepted papers addressed various topics related to representation of information and reasoning in XML, one was on data integration and one on transaction processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 320

LEFT text: At the University of Illinois at Urbana-Champaign, the computer engineering curriculum is offered by the large, research-oriented Department of Electrical and Computer Engineering. The curriculum features a strong foundation in electrical engineering, an appropriate balance of computer hardware and software topics, a sequence of increasingly sophisticated design experiences, and a wide variety of electives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the indian institute of technology , bombay

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2201

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization by predicate move-around

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , inderpal singh mumick , yehoshua sagiv
",n
"LEFT id: NA
RIGHT id: 1615

LEFT text: There is currently considerable interest in developing multimedia digital libraries. However, it has become clear that existing architectures for management systems do not support the particular requirements of continuous media types. This is particularly the case in the important area of quality of service support. In this correspondence, we discuss quality of service issues within digital libraries and present a reference architecture able to support some quality aspects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: digital library services in mobile computing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bharat bhargava , melliyal annamalai , evaggelia pitoura
",n
"LEFT id: NA
RIGHT id: 780

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1738

LEFT text: Mobility demands the systems be adaptive. One approach is to make adaptation transparent to applications, allowing them to remain unchanged. An alternative approach views adaptation as a collaborative partnership between applications and the system. This paper is a status report on our research on both fronts. We report on our considerable experience with application-transparent adaptation in the Coda File System. We also describe our ongoing work on application-aware adaptation in Odyssey.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data replication for mobile computers

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: yixiu huang , prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1114

LEFT text: Active databases and real-time databases have been important areas of research in the recent past. It has been recognized that many benefits can be gained by integrating active and real-time database technologies. However, there has not been much work done in the area of transaction processing in active real-time databases. This paper deals with an important aspect of transaction processing in active real-time databases, namely the problem of assigning priorities to transactions. In these systems, time-constrained transactions trigger other transactions during their execution. We present three policies for assigning priorities to parent, immediate and deferred transactions executing on a multiprocessor system and then evaluate the policies through simulation. The policies use different amounts of semantic information about transactions to assign the priorities. The simulator has been validated against the results of earlier published studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: promises and realities of active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: eric simon , angelika kotz dittrich
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 2129

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2210

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on the design and management of data warehouses ( dmdw ' 03 )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: hans j. lenz , panos vassiliadis , manfred jeusfeld , martin staudt
",n
"LEFT id: NA
RIGHT id: 1100

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing in hierarchical parallel database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: luc bouganim , daniela florescu , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 1992

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language and optimization techniques for unstructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter buneman , susan davidson , gerd hillebrand , dan suciu
",n
"LEFT id: NA
RIGHT id: 1215

LEFT text: In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an experimental object-based sharing system for networked databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: doug fang , shahram ghandeharizadeh , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 573

LEFT text: Managing multiple versions of XML documents represents an important problem, because of many applications ranging from traditional ones, such as software configuration control, to new ones, such as link permanence of web documents. Research on managing multiversion XML documents seeks to provide efficient and robust techniques for (i) storing and retrieving, (ii) exchanging, and (iii) querying such documents. In this paper, we first show that traditional version control methods, such as RCS, and SCCS, fall short from satisfying these three requirements, and discuss alternative solutions. First, we enhance RCS with a temporal page clustering policy to achieve objective (i). Then, we discuss a reference-based versioning scheme that achieves both objectives (i) and (ii) and is also effective at supporting simple queries. The topic of supporting complex queries, including temporal ones, meshes with the burgeoning interest of database researchers in XML as a database description language, and in XML query languages. In this context, the XML versioning problems are akin to those of transaction time management for databases of objects and semistructured information. Nevertheless, the need to preserve the natural ordering of XML documents frequently requires different techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xml document versioning

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shu yao chien , vassilis j. tsotras , carlo zaniolo
",y
"LEFT id: NA
RIGHT id: 1351

LEFT text: In this paper we present a tool for enhanced exploration of OLAP data that is adaptive to a user’s prior knowledge of the data. The tool continuously keeps track of the parts of the cube that a user has visited. The information in these scattered visited parts of the cube is pieced together to form a model of the user’s expected values in the unvisited parts. The mathematical foundation for this modeling is provided by the classical Maximum Entropy principle. At any time, the user can query for the most surprising unvisited parts of the cube. The most surprising values are dened as those which if known to the user would bring the new expected values closest to the actual values. This process of updating the user’s context based on visited parts and querying for regions to explore further continues in a loop until the user’s mental model perfectly matches the actual cube.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: intelligent rollups in multidimensional olap data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: gayatri sathe , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: This paper presents an algorithm, called ARIES/CSA (Algorithm for Recovery and Isolation Exploiting Semantics for Client-Server Architectures), for performing recovery correctly in client-server (CS) architectures. In CS, the server manages the disk version of the database. The clients, after obtaining database pages from the server, cache them in their buffer pools. Clients perform their updates on the cached pages and produce log records. The log records are buffered locally in virtual storage and later sent to the single log at the server. ARIES/CSA supports a write-ahead logging (WAL), fine-granularity (e.g., record) locking, partial rollbacks and flexible buffer management policies like steal and no-force. It does not require   that the clocks on the clients and the server be synchronized. Checkpointing by the server and the clients allows for flexible and easier recovery.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 287

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vqbd : exploring semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , thomas baby , jihwang yoo
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 329

LEFT text: In system monitoring, one is often interested in checking properties of aggregated data. Current policy monitoring approaches are limited in the kinds of aggregations they handle. To rectify this, we extend an expressive language, metric first-order temporal logic, with aggregation operators. Our extension is inspired by the aggregation operators common in database query languages like SQL. We provide a monitoring algorithm for this enriched policy specification language. We show that, in comparison to related data processing approaches, our language is better suited for expressing policies, and our monitoring algorithm has competitive performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: querying atsql databases with temporal logic

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jan chomicki , david toman , michael h. b &#246; hlen
",y
"LEFT id: NA
RIGHT id: 238

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 5th international workshop on knowledge representation meets databases ( krdb ' 98 )

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alex borgida , vinay k. chaudhri , martin staudt
",n
"LEFT id: NA
RIGHT id: 1220

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: effective timestamping in databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kristian torp , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 127

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 133

LEFT text: We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: ensuring consistency in multidatabases by preserving two-level serializability

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sharad mehrotra , rajeev rastogi , henry f. korth , abraham silberschatz
",y
"LEFT id: NA
RIGHT id: 342

LEFT text: The latest, and the subject of this review, is a set of course notes published by the Mineralogical Association of Canada, compiled to accompany a short course on PGE exploration held at the recent PGE Symposium in Oulu, Finland.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 228

LEFT text: This paper describes PREDATOR, a freely available object-relational database system that has been developed at Cornell University. A major motivation in developing PREDATOR was to create a modern code base that could act as a research vehicle for the database community. Pursuing this goal, this paper briefly describes several features of the system that should make it attractive for database research and education.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the microsoft database research group

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 281

LEFT text: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not suÆcient to associate \any"" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the prototype of the dare system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tiziana catarci , giuseppe santucci
",y
"LEFT id: NA
RIGHT id: 914

LEFT text: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at ut arlington

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sharma chakravarthy , alp aslandogan , ramez elmasri , leonidas fegaras , junghwan oh
",n
"LEFT id: NA
RIGHT id: 1254

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic. Probably because many problems remained unsolved, most research works were only able to address separate topics, without a clear context of an overall application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 2196

LEFT text: Temporal, spatial and spatiotemporal queries are inherently multidimensional, combining predicates on explicit attributes with predicates on time dimension(s) and spatial dimension(s). Much confusion has prevailed in the literature on access methods because no consistent notation exists for referring to such queries. As a contribution towards eliminating this problem, we propose a new and simple notation for spatiotemporal queries. The notation aims to address the selection-based spatiotemporal queries commonly studied in the literature of access methods. The notation is extensible and can be applied to more general multidimensional, selection-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: analysis of predictive spatio-temporal queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: yufei tao , jimeng sun , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 940

LEFT text: As XML seems to become the preferred candidate language for the interchange of data on the Internet, the integration of distributed, heterogeneous, and autonomous XML data sources in a mediation architecture is becoming a critical issue. In this paper, we present a novel and original query rewriting algorithm for the answering of queries to XML disparate sources in the presence of XML keys. The algorithm combines features of the MiniCon (Mini-Con descriptions) and the Styx algorithms (prefix and suffix queries) into an algorithm that returns more rewritings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries across diverse data sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laura m. haas , donald kossmann , edward l. wimmers , jun yang
",n
"LEFT id: NA
RIGHT id: 1837

LEFT text: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sr-tree : an index structure for high-dimensional nearest neighbor queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: norio katayama , shin ` ichi satoh
",y
"LEFT id: NA
RIGHT id: 823

LEFT text: The subject of this paper is the creation of knowledge bases by enumerating and organizing all web occurrences of certain subgraphs. We focus on subgraphs that are signatures of web phenomena such as tightly-focused topic communities, webrings, taxonomy trees, keiretsus, etc. For instance, the signature of a webring is a central page with bidirectional links to a number of other pages. We develop novel algorithms for such enumeration problems. A key technical contribution is the development of a model for the evolution of the web graph, based on experimental observations derived from a snapshot of the web. We argue that our algorithms run efficiently in this model, and use the model to explain some statistical phenomena on the web that emerged during our experiments. Finally, we describe the design and implementation of Campfire, a knowledge base of over one hundred thousand web communities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: extracting large-scale knowledge bases from the web

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ravi kumar , prabhakar raghavan , sridhar rajagopalan , andrew tomkins
",y
"LEFT id: NA
RIGHT id: 2013

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB[2] that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases. Unlike personal web sites which are usually hosted together in a central web server, personal databases are stored in the person’s own PC. In addition, it is increasingly common for people to keep their data in common personal DBMS like MySQL, and MSAccess. Therefore, a PeerDB node allows an user to index and publish his/her personal database for other peers to query. PeerDB builds on and extends BestPeer [1] for DBMS applications. Briefly, BestPeer is a generic P2P system designed to serve as a platform to develop P2P applications easily and efficiently. It has the following features: (1) It employs mobile agents; (2) It shares data at a finer granularity as well as computational power; (3) It can dynamically reconfigure the BestPeer network so that a node is always directly connected to peers that provide the best service; (4) It employs a set of location independent global name lookup (LIGLO) servers to uniquely recognize nodes whose IP addresses may change as a result. In the PeerDB network, a set of PeerDB nodes communicate or share resources with each other. Each node comprises four components that are loosely integrated: (a) a data management system (we used MySQL in our implementation) that facilitates storage, manipulation and retrieval of the data at the node, and the associated local and export dictionaries that reflect the meta-data (schema and keywords); (b) a database agent system called DBAgent that provides the environment for mobile agents to operate on; (c) a cache manager for managing remote meta-data and data in secondary storage; and (d) a user-friendly user interface. PeerDB has several distinguishing features. First, it allows users to query data without knowing the schema of data in other nodes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: peerdb : peering into personal databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , kian-lee tan , aoying zhou , chin hong goh , yingguang li , chu yee liau , bo ling , wee siong ng , yanfeng shu , xiaoyu wang , ming zhang
",y
"LEFT id: NA
RIGHT id: 1

LEFT text: This paper reports on experience obtained during the design, implementation and use of a multi-paradigm query interface to an object-oriented database. The specific system which has been developed allows equivalent data retrieval tasks to be expressed using textual, form-based and graph-based notations, and supports automatic translation of queries between these three paradigms. The motivation behind the development of such an interface is presented, as is the software architecture which supports the multi-paradigm functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a user-centered interface for querying distributed multimedia databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: isabel f. cruz , kimberly m. james
",n
"LEFT id: NA
RIGHT id: 1800

LEFT text: Caching has been proposed (and implemented) by OLAP systems in order to reduce response times for multidimensional queries. Previous work on such caching has considered table level caching and query level caching. Table level caching is more suitable for static schemes. On the other hand, query level caching can be used in dynamic schemes, but is too coarse for “large” query results. Query level caching has the further drawback for small query results in that it is only effective when a new query is subsumed by a previously cached query. In this paper, we propose caching small regions of the multidimensional space called “chunks”. Chunk-based caching allows fine granularity caching, and allows queries to partially reuse the results of previous queries with which they overlap. To facilitate the computation of chunks required by a query but missing from the cache, we propose a new organization for relational tables, which we call a “chunked file.” Our experiments show that for workloads that exhibit query locality, chunked caching combined with the chunked file organization performs better than query level caching. An unexpected benefit of the chunked file organization is that, due to its multidimensional clustering properties, it can significantly improve the performance of queries that “miss” the cache entirely as compared to traditional file organizations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: caching multidimensional queries using chunks

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: prasad m. deshpande , karthikeyan ramasamy , amit shukla , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 1205

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 635

LEFT text: Various models and languages for describing and manipulating hierarchically structured data have been proposed. Algebraic, calculus-based, and logic-programming oriented languages have all been considered. This article presents a general model for complex values (i.e., values with hierarchical structures), and languages for it based on the three paradigms. The algebraic language generalizes those presented in the literature; it is shown to be related to the functional, style of programming advocated by Backus (1978). The notion of domain independence (from relational databases) is defined, and syntactic restrictions (referred to as safety conditions) on calculus queries are formulated to guarantee domain independence. The main results are: The domain-independent calculus, the safe calculus, the algebra, and the logic-programming oriented language have equivalent expressive power. In particular, recursive queries, such as the transitive closure, can be expressed in each of the languages. For this result, the algebra needs the powerset operation. A more restricted version of safety is presented, such that the restricted safe calculus is equivalent to the algebra without the powerset. The results are extended to the case where arbitrary functions and predicates are used in the languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the power of languages for the manipulation of complex values

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: serge abiteboul , catriel beeri
",y
"LEFT id: NA
RIGHT id: 1233

LEFT text: In order to cope with the dynamic scenario of fast changing business requirements enterprises have embraced web technologies to manage their business processes. However, the ability to integrate business processes like procurement, customer relationship management, finance, human resources and manufacturing in a typical supply chain on the web is a challenging task. Today’s virtual enterprises need to integrate different workflows within and across enterprises efficiently so as to provide seamless services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 1273

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: context-based prefetch - an optimization for implementing objects on relations

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: philip a. bernstein , shankar pal , david shutt
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: s [41, 42]. academia [1932, 1926, 2267, 2275]. academic [1908, 2441]. acceleration [1869]. Access [2370, 13, 38, 1530, 1620, 1555, 2106, 1643, 1410, 602, 1672, 1033, 531, 601, 803, 1532, 1531, 949, 2204, 1455, 657, 598, 1263, 1086, 749, 409, 1969, 319, 2119, 450, 872, 1808, 171, 1559, 238, 821, 812, 1209, 1198, 694, 126, 1357, 883, 2139]. accesses [406]. Accessing [1295, 127, 515]. Accommodating [626]. account [1889]. accounting [679]. accrued [2385]. Accurate [276, 855]. achieve [858]. Achievements [1238, 619, 627]. ACID [1688]. ACM [38, 2139, 2216, 14, 2106, 2290, 2291, 42, 2239, 2445, 13, 2472, 2475, 2476, 2477, 2479, 2480, 1580, 1099, 2478, 2130, 2474, 1946, 1697, 2213, 2029, 2143, 2319, 2397, 2262]. ACM-SIGFIDET [38, 14]. Acquisition [288]. Across [1645, 2393, 1936]. ACT [1322]. ACT-NET [1322]. ACTA [586]. action [761]. Actions [1329]. activation [851]. Active [1322, 1239, 970, 833, 1471, 272, 1574, 1407, 1071, 1321, 1236, 1174, 420, 1171, 546, 1914, 1178, 1470, 766, 1098, 521, 1228, 1444, 1126, 1065]. Activities [136, 984, 216, 156, 1965, 587, 67, 748, 820]. activity [1311, 196]. Actual [211]. Ad [1394, 2001, 2056, 3]. ad-hoc [2001]. Ada [249, 458]. ADAM [1897]. adapt [2406]. Adaptable [1807]. adaptation [2113, 2328, 1209]. Adapting [1121, 254]. Adaptive [1730, 1675, 1002, 1642, 1024, 1826, 1112, 1737, 304, 1282, 307, 1809, 567]. ADBIS’2001 [1931]. ADBMS [1321, 1322]. adding [463]. Addison [2043, 66]. Addison-Wesley [66]. Address [1148, 749, 1147, 642]. Addressing [1199]. ADEPT [1588]. Administering [221, 1303]. Administration [1478, 53]. Ado [1317]. ADO.NET [2236]. ADS [1071]. Advanced [1881, 952, 740, 1765, 2405, 1062, 678, 924, 1177]. Advancements [2105]. Advances [1227, 1931]. advantage [1163]. adventure [2310]. adversarial [2337]. Affiliation [2402]. after [1121]. again [1875]. against [471, 1939]. Agenda [1040]. Agent [1386, 1607, 2047, 1588, 1919, 1198, 2065]. Agent-Based [1386, 1607, 1588, 1198].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",y
"LEFT id: NA
RIGHT id: 2240

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 680

LEFT text: In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for minimizing tree pattern queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: prakash ramanan
",n
"LEFT id: NA
RIGHT id: 261

LEFT text: Xyleme is a dynamic warehouse for XML data of the Web supporting query evaluation, change control and data integration. We briefly present our motivations, the general architecture and some aspects of Xyleme. The project we describe here was completed at the end of 2000. A prototype has been implemented. This prototype is now being turned into a product by a start-up company also called Xyleme [14]. Xyleme: a complex tissue of wood cells, functions in conduction and storage ...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: monitoring xml data on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: benjamin nguyen , serge abiteboul , gr &#233; gory cobena , miha &#237; preda
",y
"LEFT id: NA
RIGHT id: 1235

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient schemes for managing multiversionxml documents

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s.-y . chien , v. j. tsotras , c. zaniolo
",n
"LEFT id: NA
RIGHT id: 326

LEFT text: The IBM Lotus Domino/Notes is excellent office electronic collaboration platform, and collaborative platforms, industry leading news IM solution integration with industry leading enterprises, to create a collaborative solution. This paper USES the ideas and methods of software engineering, based on Domino/Notes. First, the Domino/Notes platform architecture and composition; Then, the design by B/S and C/S combination of office automation system structure; Then, design a Domino database components, including access control list, logic and data; At last, the design structure of directory service, convenient storage, access, management and use of resources. In this paper, the research content, give full play to the advantages of the Domino/Notes platform, to improve the efficiency of software development and quality plays an important role.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mqseries and cics link for lotus notes

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: lotus development corp.
",y
"LEFT id: NA
RIGHT id: 1413

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",y
"LEFT id: NA
RIGHT id: 1483

LEFT text: The performance of data-parallel algorithms for spatial operations using data-parallel variants of the bucket PMR quadtree, R-tree, and R+-tree spatial data structures is compared. The studied operations are data structure build, polygonization, and spatial join in an application domain consisting of planar line segment data. The algorithms are implemented using the scan model of parallel computation on the hypercube architecture of the Connection Machine. The results of experiments reveal that the bucket PMR quadtree outperforms both the R-tree and R+-tree. This is primarily because the bucket PMR quadtree yields a regular disjoint decomposition of space while the Rtree and R+-tree do not.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance of data-parallel spatial operations

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: erik g. hoel , hanan samet
",y
"LEFT id: NA
RIGHT id: 384

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lambda-db : an odmg-based object-oriented dbms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier
",n
"LEFT id: NA
RIGHT id: 689

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 817

LEFT text: Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: probabilistic optimization of top n queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: donko donjerkovic , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 477

LEFT text: From the standpoint of satisfying human's information needs, the current digital library (DL) systems suffer from the following two shortcomings: (i) inadequate high-level cognition support; (ii) inadequate knowledge sharing facilities. In this article, we introduce a two-layered digital library architecture to support different levels of human cognitive acts. The model moves beyond simple information searching and browsing across multiple repositories, to inquiry of knowledge about the contents of digital libraries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: towards knowledge-based digital libraries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ling feng , marfred a. jeusfeld , jeroen hoppenbrouwers
",y
"LEFT id: NA
RIGHT id: 1785

LEFT text: In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 228

LEFT text: Microsoft’s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David’s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch’s vision, and, indeed, in Microsoft’s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch’s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital’s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft’s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the microsoft database research group

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya
",y
"LEFT id: NA
RIGHT id: 430

LEFT text: Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space. In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and extensible algorithms for multi query optimization

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: prasan roy , s. seshadri , s. sudarshan , siddhesh bhobe
",y
"LEFT id: NA
RIGHT id: 793

LEFT text: In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that traditional relational DBMSs can process eciently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to a relational DBMS, and the impact of the quality of these statistics on the retrieval eciency of the resulting scheme.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating top-k selection queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",y
"LEFT id: NA
RIGHT id: 1023

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 1715

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: Our experience with the SIFT [YGM95] information dissemination system (in use by over 7,000 users daily) has identified an important and generic dissemination problem: duplicate information. In this paper we explain why duplicates arise, we quantify the problem, and we discuss why it impairs information dissemination. We then propose a Duplicate Removal Module (DRM) for an information dissemination system. The removal of duplicates operates on a per user, per document basis each document read by a user generates a request, or a duplicate restraint. In wide-area environments, the number of restraints handled is very large. We consider the implementation of a DRM, examining alternative algorithms and data structures that may be used. We present a performance evaluation of the alternatives and answer important design questions such as: Which implementation is the best? With “best” scheme, how expensive will duplicate removal be? How much memory is required? How fast can restraints be processed?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 2272

LEFT text: In this paper we propose a distributed case-based approach to the problem of rewriting queries. According to this approach we use a case memory instead of static views, i.e. views that are deened a priori. As a consequence, the mediated schema is dynamically updated, strongly innuenced by the queries submitted by a consumer. This approach allows a mediator to face systems where consumers may change their customization needs and information sources may become unavailable, may be added, or may modify their schemas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: instance-based attribute identification in database integration

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: cecil eng h. chua , roger h. l. chiang , ee-peng lim
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1988

LEFT text: This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintaining database consistency in presence of value dependencies in multidatabase systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: claire morpain , mich &#233; le cart , jean ferri &#233; , jean-fran &#231; ois pons
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: Multidimensional data-base technology is a key factor in the interactive analysis of large amounts of data for decision-making purposes. In contrast to previous technologies, these databases view data as multidimensional cubes that are particularly well suited for data analysis. Multidimensional models categorize data either as facts with associated numerical measures or as textual dimensions that characterize the facts. Queries aggregate measure values over a range of dimension values to provide results such as total sales per month of a given product. Multidimensional database technology is being applied to distributed data and to new types of data that current technology often cannot adequately analyze. For example, classic techniques such as preaggregation cannot ensure fast query response times when data-such as that obtained from sensors or GPS-equipped moving objects-changes continuously. Multidimensional database technology will increasingly be applied where analysis results are fed directly into other systems, thereby eliminating humans from the loop. When coupled with the need for continuous updates, this context poses stringent performance requirements not met by current technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 598

LEFT text: In this column, we review these three books.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1045

LEFT text: We map an object model to a commercial relational multi-processor database system using replication and view materialisation to provide fast retrieval. To speed up complex update operations, we exploit intra-transaction parallelism by breaking such an operation down into shorter relational operations which are executed as parallel subtransactions of the update transaction. To ensure the correctness and recoverability of the operation’s execution, we use multi-level transactions. In addition, we minimise the resulting overhead for the logging of the compensating inverse operation required by the multi-level concept by logging the compensation for non-derived data only. In particular, we concentrate on the novel application of multi-level transaction management to efficiently maintain the replicated data and materialised views. We present a prototype implementation and give a performance evaluation of an exemplary set-oriented update statement.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: intra-transaction parallelism in the mapping of an object model to a relational multi-processor system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael rys , moira c. norrie , hans-j &#246; rg schek
",y
"LEFT id: NA
RIGHT id: 291

LEFT text: XQuery is the de facto standard XML query language, and it is important to have efficient query evaluation techniques available for it. A core operation in the evaluation of XQuery is the finding of matches for specified tree patterns, and there has been much work towards algorithms for finding such matches efficiently. Multiple XPath expressions can be evaluated by computing one or more tree pattern matches.    However, relatively little has been done on efficient evaluation of XQuery queries as a whole. In this paper, we argue that there is much more to XQuery evaluation than a tree pattern match. We propose a structure called generalized tree pattern (GTP) for concise representation of a whole XQuery expression. Evaluating the query reduces to finding matches for its GTP. Using this idea we develop efficient evaluation plans for XQuery expressions, possibly involving join, quantifiers, grouping, aggregation, and nesting.    XML data often conforms to a schema. We show that using relevant constraints from the schema, one can optimize queries significantly, and give algorithms for automatically inferring GTP simplifications given a schema. Finally, we show, through a detailed set of experiments using the TIMBER XML database system, that plans via GTPs (with or without schema knowledge) significantly outperform plans based on navigation and straightforward plans obtained directly from the query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of xml middle-ware queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mary fernandez , atsuyuki morishima , dan suciu
",n
"LEFT id: NA
RIGHT id: 1578

LEFT text: This annotated bibliography presents a collection of published papers, technical reports, Master's and PhD Theses that have investigated various aspects of object database performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on active databases ( short version )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ulrike jaeger , johann christoph freytag
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1945

LEFT text: It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: tail recursion elimination in deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 127

LEFT text: The MIROWeb Espris developped a unique technology to integrate multiple data sources through an object-relational model with semistructured data types. In addresses the problem of integrating irregular web sources and regular relational databases through a mediated architecture based on hybrid model, supporting relational object and semi-structured features. The project data exchange format is XML, the new standard of the Web and the pivot language is XMLQL, a query language based on XML templates from AT&T. The demonstration will show the data warehousing approach for mediation, based on Oracle 8 and a semi-structured cartridge developped in the project for supporting XML and XMLQL queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 1085

LEFT text: Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: facilitating multimedia database exploration through visual interfaces and perpetual query reformulations

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wen-syan li , k. sel &#231; uk candan , kyoji hirata , yoshinori hara
",n
"LEFT id: NA
RIGHT id: 199

LEFT text: Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 430

LEFT text: In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and extensible algorithms for multi query optimization

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: prasan roy , s. seshadri , s. sudarshan , siddhesh bhobe
",n
"LEFT id: NA
RIGHT id: 853

LEFT text: XML tries to bring to natural language documents (""texts"" for short), some of what databases have had for decades: explicit structure whose properties can be known; independence of data and structure from reporting (which we foreigners call ""formatting""); various kinds of isolation; and so on. But the data buried in those XML elements is weird stuff: deep hierarchies, arbitrary and unpredictable orderings and repetitions, a painful number of atomic types, and enough aggregates to make one's head hurt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what do those weird xml types want , anyway ?

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: steven j. derose
",y
"LEFT id: NA
RIGHT id: 1262

LEFT text: One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure  (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: indexing very high-dimensional sparse and quasi-sparse vectors for similarity searches

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: changzhou wang , x. sean wang
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: The latest, and the subject of this review, is a set of course notes published by the Mineralogical Association of Canada, compiled to accompany a short course on PGE exploration held at the recent PGE Symposium in Oulu, Finland.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 951

LEFT text: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: logical and physical versioning in main memory databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: rajeev rastogi , s. seshadri , philip bohannon , dennis w. leinbaugh , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 804

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: context-based prefetch for implementing objects on relations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: philip a. bernstein , shankar pal , david shutt
",y
"LEFT id: NA
RIGHT id: 1961

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: birch : an efficient data clustering method for very large databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tian zhang , raghu ramakrishnan , miron livny
",y
"LEFT id: NA
RIGHT id: 1804

LEFT text: In this paper we propose an approach that enables mobile clients to determine the validity of previous queries based on their current locations. In order to make this possible, the server returns in addition to the query result, a validity region around the client's location within which the result remains the same. We focus on two of the most common spatial query types, namely nearest neighbor and window queries, define the validity region in each case and propose the corresponding query processing algorithms. In addition, we provide analytical models for estimating the expected size of the validity region. Our techniques can significantly reduce the number of queries issued to the server, while introducing minimal computational and network overhead compared to traditional spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the dedale system for complex spatial queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: st &#233; phane grumbach , philippe rigaux , luc segoufin
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: With the advent of XML as a format for data ex-change and semistructured databases, query languagesfor XML and semistructured data have become in-creasingly popular.Many such query languages, like XPath andXQuery, are navigational in the sense that their vari-able binding paradigm requires the programmer tospecify path navigations through the document (ordata item). In contrast, some other languages – suchas UnQL [1] and Xcerpt [2] – are pattern-based: theirvariable binding paradigm is that of mathematical log-ics, i.e. the programmer speciﬁes patterns (or terms)including variables. Arguably, a pattern-based vari-able binding paradigm makes complex queries mucheasier to specify and to read, thus improving theprogramming eﬃciency. Sustaining this ﬁrst claimwith practical examples is one of the objectives of thepresent demonstration.Xcerpt [2] is an experimental pattern-based queryand transformation language for XML and semistruc-tured data. Xcerpt uses patterns both for bindingvariables in query expressions and for reassembling thevariables (bound to data items in query expressions) inso-called construct terms. Arguably, a pattern-baseddocument construction combined with a pattern-basedvariable binding results in a rather intuitive, userfriendly, and programming eﬃcient language. Sustain-ing this second claim is another objective of the presentdemonstration.Xcerpt is experimental in the sense that its purposeis to investigate and test another, non-navigational ap-proach to retrieve data from the Web than that ofthe widespread query languages XPath and XQuery.Nonetheless, Xcerpt has been prototypically imple-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 245

LEFT text: The MultiView Project is an ongoing 5-year NFS-ftrnded effort at the University of Michigan to develop and apply object-oriented view technology to address the needs of recently emerging applications such as data warehousing and workflow management systems that require the sharing, virtual restructuring, and caching of data [5]. Through Multi-View, users can dynamically create and modify virtual classes and schemata at any time .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multiview project : object-oriented view technology and applications

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: e. a. rundensteiner , h. a. kuno , y.-g . ra , v. crestana-taube , m. c. jones , p. j. marron
",y
"LEFT id: NA
RIGHT id: 1042

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a new sql-like operator for mining association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rosa meo , giuseppe psaila , stefano ceri
",n
"LEFT id: NA
RIGHT id: 1881

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: semantics for update rule programs and implementation in a relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: louiqa raschid , jorge lobo
",n
"LEFT id: NA
RIGHT id: 1778

LEFT text: A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimal multi-step k-nearest neighbor search

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 2109

LEFT text: Heavily used in both academic and corporate R&D settings, ACM Transactions on Database Systems (TODS) is a key publication for computer scientists working in data abstraction, data modeling, and designing data management systems. Topics include storage and retrieval, transaction management, distributed and federated databases, semantics of data, intelligent databases, and operations and algorithms relating to these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 1122

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: calibrating the query optimizer cost model of iro-db , an object-oriented federated database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , fei sha , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1440

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: generalized search trees for database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton , avi pfeffer
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 689

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 77

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mocha : a self-extensible database middleware system for distributed data sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 962

LEFT text: Structural queries constitute a special form of content-based retrieval where the user specifies a set of spatial constraints among query variables and asks for all configurations of actual objects that (totally or partially) match these constraints. Processing such queries can be thought of as a general form of spatial joins, i.e., instead of pairs, the result consists of n-tuples of objects, where n is the number of query variables. In this paper we describe a flexible framework which permits the representation of configurations in different resolution levels and supports the automatic derivation of similarity measures. We subsequently propose three algorithms for structural query processing which integrate constraint satisfaction with spatial indexing (R-trees). For each algorithm we apply several optimization techniques and experimentally evaluate performance using real data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for querying by spatial structure

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: dimitris papadias , nikos mamoulis , vasilis delis
",y
"LEFT id: NA
RIGHT id: 1354

LEFT text: There is a new emerging world of web services. In this world, services will be combined in innovative ways to form elaborate services out of building blocks of other services. This is predicated on having a common ground of vocabulary and communication protocols operating in a secured environment. Currently, massive standardization efforts [UDDI, WSDL, ebXML, RosettaNet] are aiming at achieving this common ground. We explore possible architectures for deploying computerized traders internal services. This encompasses both the structure and the functionalities of “traders” and “services” and the form in which these functionalities could be realized in actual implementations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: architectures for internal web services deployment

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: oded shmueli
",y
"LEFT id: NA
RIGHT id: 16

LEFT text: The proliferation of the Internet and intranets, advances in wireless and satellite networks, and the availability of asymmetric, high-bandwidth links to the home, have fueled the development of a wide range of new \dissemination-based"" applications. These applications involve the timely distribution of data to a large set of consumers, and include stock and sports tickers, tra c information systems, electronic personalized newspapers, and entertainment delivery. Dissemination-oriented applications have special characteristics that render traditional client-server data management approaches ine ective. These include: tremendous scale, signi cant overlap in user data needs, and asymmetric data ow from sources to consumers. The mismatch between the data access characteristics of these applications and the technology used to implement them on the WWW results in scalability problems [Fran98]. For example, WWW based applications employ the HTTP protocol which uses a request-response (or client-server), unicast method of data delivery. Using request-response, each user sends requests for data to the server. The large audience for a popular event can generate huge spikes in the load at servers, resulting in long delays and overloaded servers. Compounding the situation is that users must continually poll the server to obtain the most current data, resulting in multiple requests for the same data items from each user. In an application such as an election result server, where the interests of a large part of the population are known a priori, most of these requests are unnecessary. In order to address the needs of this new class of applications, we are developing a Dissemination-Based Information Systems (DBIS) toolkit. The toolkit serves as an adaptable middleware layer that incorporates several di erent data delivery mechanisms and provides an architecture for deploying them in a networked environment. The toolkit also includes facilities for performance monitoring, which can allow a system developer to examine the impact of using di erent data delivery mechanisms. We have implemented an initial version of this toolkit and have used it to develop a weather map dissemination application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dbis-toolkit : adaptable middleware for large scale data delivery

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mehmet altinel , demet aksoy , thomas baby , michael franklin , william shapiro , stan zdonik
",y
"LEFT id: NA
RIGHT id: 1667

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 331

LEFT text: Although there are many applications where an object-oriented data model is a good way of representing and querying data, current object database systems are unable to handle objects whose attributes are uncertain. In this article, we extend previous work by Kornatzky and Shimony to develop an algebra to handle object bases with uncertainty. We propose concepts of consistency for such object bases, together with an NP-completeness result, and classes of probabilistic object bases for which consistency is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions of events, and as the probability of conjunctive and disjunctive events depends both on the probabilities of the primitive events involved as well as on what is known (if anything) about the relationship between the events, we show how all our algebraic operations may be performed under arbitrary probabilistic conjunction and disjunction strategies. We also develop a host of equivalence results in our algebra, which may be used as rewrite rules for query optimization. Last but not least, we have developed a prototype probabilistic object base server on top of ObjectStore. We describe experiments to assess the efficiency of different possible rewrite rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probabilistic temporal databases , i : algebra

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alex dekhtyar , robert ross , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 591

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 1754

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semistructured and structured data in the web : going back and forth

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 622

LEFT text: In traditional software systems, significant attention is devoted to keeping modules well separated and coherent with respect to functionality, thus ensuring that changes in the system are localized to a handful of modules. Reuse is seen as the key method in reaching that goal. Ontology-based systems on the Semantic Web are just a special class of software systems, so the same principles apply. In this article, we present an integrated framework for managing multiple and distributed ontologies on the Semant ic Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the &#961; operator : discovering and ranking associations on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kemafor anyanwu , amit sheth
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 110

LEFT text: Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distance browsing in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1451

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast similarity search in the presence of noise , scaling , and translation in time-series databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rakesh agrawal , king-ip lin , harpreet s. sawhney , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 535

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , eamonn keogh , sharad mehrotra , michael pazzani
",n
"LEFT id: NA
RIGHT id: 2175

LEFT text: A new access method, called M-tree, is proposed to organize and search large data sets from a generic “metric space”, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O’s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 1905

LEFT text: Global information systems have the potential of providing decision makers with timely spatial information about earth systems. This information will come from diverse sources, including field monitoring, remotely sensed imagery, and environmental models. Of the three the latter has the greatest potential of providing regional and global scale information on the behavior of environmental systems, which may be vital for setting multi-governmental policy and for making decisions that are critical to quality of life. However, environmental models have limited prootocol for quality control and standardization. They tend to have weak or poorly defined semantics and so their output is often difficult to interpret outside a very limited range of applications for which they are designed. This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: uis-management of data and services in the environmental information systems of baden-w &#252; rttemberg

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wolf-fritz riekert , roland mayer-f &#246; ll , gerlinde wiest
",n
"LEFT id: NA
RIGHT id: 1699

LEFT text: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a hypertext query language for images

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: li yang
",n
"LEFT id: NA
RIGHT id: 1857

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probview : a flexible probabilistic database system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , nicola leone , robert ross , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: The cost of a query plan depends on many parameters, such as predicate selectivities and available memory, whose values may not be known at optimization time. Parametric query optimization (PQO) optimizes a query into a number of candidate plans, each optimal for some region of the parameter space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: To support efficient similarity searches in an NDDS, we propose a new dynamic indexing technique, called the ND-tree. The key idea is to extend the relevant geometric concepts as well as some indexing strategies used in CDSs to NDDSs. Efficient algorithms for ND-tree construction are presented. Our experimental results on synthetic and genomic sequence data demonstrate that the performance of the ND-tree is significantly better than that of the linear scan and M-tree in high dimensional NDDSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1888

LEFT text: Dynamic queries constitute a very powerful mechanism for information visualization; some universe of data is visualized, and this visualization is modified on-the-fly as users modify the range of interest within the domains of the various attributes of the visualized information. In this paper, we analyze dynamic queries and offer some natural generalizations of the original concept by establishing a connection to SQL. We also discuss some implementation ideas that should make these generalizations efficient as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text:  The database systems have nowadays an increasingly important role in the knowledge-based society, in which computers have penetrated all fields of activity and the Internet tends to develop worldwide. In the current informatics context, the development of the applications with databases is the work of the specialists. Using databases, reach a database from various applications, and also some of related concepts, have become accessible to all categories of IT users. This paper aims to summarize the curricular area regarding the fundamental database systems issues, which are necessary in order to train specialists in economic informatics higher education. The database systems integrate and interfere with several informatics technologies and therefore are more difficult to understand and use. Thus, students should know already a set of minimum, mandatory concepts and their practical implementation: computer systems, programming techniques, programming languages, data structures. The article also presents the actual trends in the evolution of the database systems, in the context of economic informatics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 1431

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pc database systems - present and future

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: The chair of ACRP’s Association Board of Trustees recounts how he left manufacturing and research and development a decade ago to take a position as a clinical research administrator. It was a move that place him into a role he knew little about, having not been engaged in clinical research beforehand. If this sounds like a familiar experience to others, the lessons shared in this column highlight the importance of the individual’s ongoing will to learn, and of organizational support for that learning.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 1933

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: highly concurrent cache consistency for indices in client-server database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: markos zaharioudakis , michael j. carey
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: Multidimensional discrete data (MDD) i.e., arrays of arbitrary size, dimension, and base type appear in a variety of business, technical, and scientific application fields. RasDaMan is an effort to give comprehensive domain-independent MDD database support. Based on a formal algebraic array model, RasDaMan offers declarative array operators embedded in standard SQL; key DBMS components are an MDD query optimizer and a streamlined storage manager for efficient access to subsets of huge arrays. We present the RasDaMan approach to MDD management based on the medical and geographic application fields addressed in the project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 1353

LEFT text: Information becomes a more and more valuable asset in today’s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome in the design and implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II , which is based on the wrapper-mediator approach. The main focus of this work lies on data model transformation and query processing. The following extensions to the AMOS II system are described in this thesis: • A framework for transforming various data models into the objectoriented model of AMOS II is presented. • The roles and tasks of wrappers are described. In particular their participation in query processing and query optimization is discussed. • A way for describing and utilizing the query capabilities of the different data sources is proposed. • Two different approaches to query processing over external data sources are developed and analyzed. All the proposed techniques are implemented in the AMOS II system, which runs on a Windows NT platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1513

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: applying update streams in a soft real-time database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: b. adelberg , h. garcia-molina , b. kao
",n
"LEFT id: NA
RIGHT id: 2098

LEFT text: Association Rule Mining algorithms operate on a data matrix (e.g., customers products) to derive association rules [2, 23]. We propose a new paradigm, namely, Ratio Rules, which are quanti able in that we can measure the \goodness"" of a set of discovered rules. We propose to use the \guessing error"" as a measure of the \goodness"", that is, the rootmean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can \guess"" the amount spent on, say, butter. Thus, we can perform a variety of important tasks such as forecasting, answering \what-if"" scenarios, detecting outliers, and visualizing the data. Moreover, we show how to compute Ratio Rules in a single pass over the dataset with small memory requirements (a few small matrices), in contrast to traditional association rule mining methods that require multiple passes and/or large memory. ExperWork performed while at the University of Maryland. This research was partially funded by the Institute for Systems Research (ISR), and by the National Science Foundation under Grants No. EEC-94-02384, IRI-9205273 and IRI-9625428. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 24th VLDB Conference New York, USA, 1998 iments on several real datasets (e.g., basketball and baseball statistics, biological data) demonstrate that the proposed method consistently achieves a \guessing error"" of up to 5 times less than the straightforward competitor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: active rules for xml : a new paradigm for e-services

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: angela bonifati , stefano ceri , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 1351

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: intelligent rollups in multidimensional olap data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: gayatri sathe , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 894

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intervals efficiently in object-relational databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: hans-peter kriegel , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 2054

LEFT text: Recent work on querying data streams has focused on systems where newly arriving data is processed and continuously streamed to the user in real-time. In many emerging applications, however, ad hoc queries and/or intermittent connectivity also require the processing of data that arrives prior to query submission or during a period of disconnection. For such applications, we have developed PSoup, a system that combines the processing of ad-hoc and continuous queries by treating data and queries symmetrically, allowing new queries to be applied to old data and new data to be applied to old queries. PSoup also supports intermittent connectivity by separating the computation of query results from the delivery of those results. PSoup builds on adaptive query processing techniques developed in the Telegraph project at UC Berkeley. In this paper, we describe PSoup and present experiments that demonstrate the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpath queries on streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: feng peng , sudarshan s. chawathe
",n
"LEFT id: NA
RIGHT id: 647

LEFT text: In this paper, we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. Our estimation algorithms are probabilistic in nature and rely on a novel, hash-based synopsis data structure, termed ""2-level hash sketch. We demonstrate how our 2-level hash sketch synopses can be used to provide low-error, high-confidence estimates for the cardinality of set expressions (including operators such as set union, intersection, and difference) over continuous update streams, using only small space and small processing time per update. Furthermore, our estimators never require rescanning or resampling of past stream items, regardless of the number of deletions in the stream. We also present lower bounds for the problem, demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. Preliminary experimental results verify the effectiveness of our approach

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: continuously adaptive continuous queries over streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , mehul shah , joseph m. hellerstein , vijayshankar raman
",n
"LEFT id: NA
RIGHT id: 2197

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xxl - a library approach to supporting efficient implementations of advanced database queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jochen van den bercken , bj &#246; rn blohsfeld , jens-peter dittrich , j &#252; rgen kr &#228; mer , tobias sch &#228; fer , martin schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1031

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 1780

LEFT text: Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scrambling. These approaches vary, for example, in whether they optimize for total work or response-time, and whether they construct partial or complete alternative plans. Using a two-phase randomized query optimizer, a distributed query processing simulator, and a workload derived from queries of the TPCD benchmark, we evaluate these different approaches and compare their ability to cope with initial delays in accessing remote sources. The results show that cost-based scrambling can effectively hide initial delays, but that in the absence of good predictions of expected delay durations, there are fundamental tradeoffs between risk aversion and effectiveness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based query scrambling for initial delays

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: tolga urhan , michael j. franklin , laurent amsaleg
",y
"LEFT id: NA
RIGHT id: 1593

LEFT text: Extended transaction model (ETM) is a powerful mechanism to ensure the consistency and reliability of complicated enterprise applications. However, there is few implementation of ETM in J2EE. The existing research is deficient in supporting range and requires some special database supporting. This paper explores the obstacle which prevents J2EE from supporting ETMs, and argues it is because of the limitation of J2EE XAResource interface and underlying databases. To overcome the obstacle, we propose a new approach, which processes concurrency control inside J2EE application server instead of in database. Furthermore, we implement TX/E service in JBoss to validate the approach, which is an enhanced J2EE transaction service supporting extended transaction models. Compared to existing work, TX/E supports user-defined transaction models and does not require any special database supporting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a critique of ansi sql isolation levels

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: hal berenson , phil bernstein , jim gray , jim melton , elizabeth o'neil , patrick o'neil
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: Data warehousing and On-Line Analytical Processing (OLAP) are becoming critical components of decision support as advances in technology are improving the ability to manage and retrieve large volumes of data. Data warehousing refers to \a collection of decision support technologies aimed at enabling the knowledge worker (executive, manager, analyst) to make better and faster decisions"" [1]. OLAP refers to the technique of performing complex analysis over the information stored in a data warehouse. It is often used by management analysts and decision makers in a variety of functional areas such as sales and marketing planning. Typically, OLAP queries look for speci c trends and anomalies in the base information by aggregating, ranging, ltering and grouping data in many di erent ways [8]. E cient query processing is a critical requirement for OLAP because the underlying data warehouse is very large, queries are often quite complex, and decision support applications typically require in-

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: Motivation  The field of data warehousing has emerged over the last decades. A data warehouse is developed at a moment in time to support business intelligence for an indefinite period of time. During the life of the data warehouse the world around it evolves, including the systems that are a source for the data warehouse. In order for a data warehouse to remain functioning and guarantee the quality of its data, it needs to be adjusted to the evolving world around it. The concept of Delta Impact Analysis is used by the company BI4U for the activities of analysing the impact of specific changes. This concept is important because it provides insight into how a data warehouse can be adjusted to the evolving world around it. The motivation to perform this research was the fact that a clear definition on the concept of DIA and what it comprehends was lacking.  Goals  The main goals of the research were to examine the topic of DIA in practice, to gather insights from literature and other research, to design and develop a practical model for DIA, to test the DIA model, and to provide recommendations to better support changes in data warehouse source systems. These goals resulted in the following main problem statement: How can a Delta Impact Analysis model be designed that supports the process of analyzing the impact of changes in a data warehouse source system situation?  Approach  The research approach is based on the design science framework by Hevner in combination with action science theory to validate the resulting DIA model from the research. The design science perspective resulted in an approach that is both rigorous, by performing a literature study, and relevant, by applying the research to practice. The research started by investigating the concept of DIA in practice at BI4U, in order to provide more insight into what it comprehends and what was relevant for the focus of the research. Next a thorough literature study was performed. Finally an artifact was proposed, a model for the process of DIA, which was validated in practice with a field study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",n
"LEFT id: NA
RIGHT id: 1579

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography of benchmarks for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 1881

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: semantics for update rule programs and implementation in a relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: louiqa raschid , jorge lobo
",n
"LEFT id: NA
RIGHT id: 488

LEFT text: We present an approach to database interoperation that exploits the semantic information provided by integrity constraints defined on the component databases. We identify two roles of integrity constraints in database interoperation. First, a set of integrity constraints describing valid states of the integrated view can be derived from the constraints defined on the underlying databases. Moreover, local integrity constraints can be used as a semantic check on the validity of the specification of the integrated view. We illustrate our ideas in the context of an instance-based database interoperation paradigm, where objects rather than classes are the unit of integration. We introduce the notions of objectivity and subjectivity as an indication of whether a constraint is valid beyond the context of a specific database, and demonstrate the impact of these notions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mlpq/gis constraint database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang
",n
"LEFT id: NA
RIGHT id: 1550

LEFT text: In the past decade, advances in the speed of commodity CPUs have far out-paced advances in memory latency. Main-memory access is therefore increasingly a performance bottleneck for many computer applications, including database systems. In this article, we use a simple scan test to show the severe impact of this bottleneck. The insights gained are translated into guidelines for database architecture, in terms of both data structures and algorithms. We discuss how vertically fragmented data structures optimize cache performance on sequential data access. We then focus on equi-join, typically a random-access operation, and introduce radix algorithms for partitioned hash-join. The performance of these algorithms is quantified using a detailed analytical model that incorporates memory access cost. Experiments that validate this model were performed on the Monet database system. We obtained exact statistics on events such as TLB misses and L1 and L2 cache misses by using hardware performance counters found in modern CPUs. Using our cost model, we show how the carefully tuned memory access pattern of our radix algorithms makes them perform well, which is confirmed by experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: one size fits all database architectures do not work for dss

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: clark d. french
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: In recent years, new developments in genetics have generated a lot of interest in genomic and proteomic data, investing international significance (and competition) in the fledgling discipline of bioinformatics. Researchers in pharmaceutical and biotech companies have found that database products can bring a wide range of relevant technologies to bear on their problems. Benefiting from a number of new technology enhancements, Oracle has emerged as a popular platform for pharmaceutical knowledge management and bioinformatics. We look at four powerful technologies that show promise for solving hitherto intractable problems in bioinformatics: the extensibility architecture to store gene sequence data natively and perform high-dimensional structure-searches in the database; warehousing technologies and data mining on genetic patterns; data integration technologies to enable heterogeneous queries across distributed biological sources, and internet portal technologies that allow life sciences information to be published and managed across intranets and the internet.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: The National Technical University of Athens (NTUA) is the leading Technical University in Greece. The Computer Science Division of the Electrical and Computer Engineering Department covers several fields of practical, theoretical and technical computer science and is involved in several research projects supported by the EEC, the government and industrial companies. The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media. The KDBS Laboratory employs one full-time research engineer and several graduate students. Its infrastructure includes a LAN with several DECstation 5000/200 and 5000/240 workstations, an HP Multimedia Workstation, several PCs and software for database and multimedia applications. The basic research interests of our Laboratory include: Spatial Database Systems, Multimedia Database Systems and Active Database Systems. Apart from the above database areas, interests of the KDBS Laboratory span several areas of Information Systems, such as Software Engineering Databases, Transactional Systems, Image Databases, Conceptual Modeling, Information System Development, Temporal Databases, Advanced Query Processing and Optimization Techniques.  The group's efforts on Spatial Database Systems, include the study of new data structures, storage techniques, retrieval mechanisms and user interfaces for large geographic data bases. In particular, we look at specialized, spatial data structures (R-Trees and their variations) which allow for the direct access of the data based on their spatial properties, and not some sort of encoded representation of the objects' coordinates. We study implementation and optimization techniques of spatial data structures and develop models that make performance estimation. Finally, we are investigating techniques for the efficient representation of relationships and reasoning in space. The activities on Multimedia Database Systems, include the study of advanced data models, storage techniques, retrieval mechanisms and user interfaces for large multimedia data bases. The data models under study include the object-oriented model and the relational model with appropriate extensions to support multimedia data. We are also investigating content-based search techniques for image data bases. In a different direction, we are studying issues involved in the development of multimedia front-ends for conventional, relational data base systems. In the area of Active Database Systems, we are developing new mechanisms for implementing triggers in relational databases. Among the issues involved, we address the problem of efficiently finding qualifying rules against updates in large sets of triggers. This problem is especially critical in database system implementations of triggers, where large amounts of data may have to be searched in order to find out if a particular trigger may qualify to run or not.  Continuing work that started at the Foundation for Research and Technology (FORTH), Institute of Computer Science, the group is investigating reuse-oriented approaches to information systems application development. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 1455

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 2011

LEFT text: STREAM is a general-purpose relational Data Stream Management System (DSMS). STREAM supports a declarative query language and flexible query execution plans. It is designed to cope with high data rates and large numbers of continuous queries through careful resource allocation and use, and by degrading gracefully to approximate answers as necessary. A description of language design, algorithms, system design, and implementation as of late 2002 can be found in [3]. The demonstration focuses on two aspects:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: stream : the stanford stream data manager ( demonstration description )

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arvind arasu , brian babcock , shivnath babu , mayur datar , keith ito , itaru nishizawa , justin rosenstein , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1318

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: vxmlr : a visual xml-relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: aoying zhou , hongjun lu , shihui zheng , yuqi liang , long zhang , wenyun ji , zengping tian
",n
"LEFT id: NA
RIGHT id: 1499

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-oriented , rapid application development in a pc database environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate fox development team microsoft
",n
"LEFT id: NA
RIGHT id: 1377

LEFT text: A function-based approach and mechanism to support sharing among the component database systems in a federation is described. In the context of a functional object-based database model, a technique to support inter-component information unit and behavior sharing is presented. An experimental system that implements the function-based sharing mechanism is described, its underlying algorithms are outlined, and its practical utility and effectiveness are assessed. This work is couched in the framework of the Remote-Exchange research project and experimental system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: rainbow : distributed database system for classroom education and experimental research

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: abdelsalam helal , hua li
",n
"LEFT id: NA
RIGHT id: 658

LEFT text: This paper describes the architecture of OPERA, a generic platform for building distributed systems over stand alone applications. The main contribution of this research effort. is t,o propose a “kernel” system providing the “essentials” for distributed processing and to show the important role database technology may play in supporting such functionality. These include a powerful process management environment. created as a generalization of workflow ideas and incorporating transactional notions such as spheres of isolation, atomicit.y, and persistence and a transactional engine enforcing correctness based on the nested and multi-level models. It also includes a tool-kit providing externalized database functionality enabling physical database design over heterogeneous data repositories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: databases and transaction processing : an application-oriented approach

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: philip m. lewis , arthur bernstein , michael kifer
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1632

LEFT text: Internet, Web and distributed computing infrastructures continue to gain in popularity as a means of communication for organizations, groups and individuals alike. In such an environment, characterized by large distributed, autonomous, diverse, and dynamic information sources, access to relevant and accurate information is becoming increasingly complex. This complexity is exacerbated by the evolving system, semantic and structural heterogeneity of these potentially global, cross-disciplinary, multicultural and rich-media technologies. Clearly, solutions to these challenges require addressing directly a variety of interoperability issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing disjunctive queries with expensive predicates

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: a. kemper , g. moerkotte , k. peithner , m. steinbrunn
",n
"LEFT id: NA
RIGHT id: 1216

LEFT text: Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo atzeni , alberto o. mendelzon
",n
"LEFT id: NA
RIGHT id: 514

LEFT text: Clustering is an unsupervised process since there are no predefined classes and no examples that would indicate grouping properties in the data set. The majority of the clustering algorithms behave differently depending on the features of the data set and the initial assumptions for defining groups. Therefore, in most applications the resulting clustering scheme requires some sort of evaluation as regards its validity. Evaluating and assessing the results of a clustering algorithm is the main subject of cluster validity. In this paper we present a review of the clustering validity and methods. More specifically, Part I of the paper discusses the cluster validity approaches based on external and internal criteria.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: clustering validity checking methods : part ii

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: maria halkidi , yannis batistakis , michalis vazirgiannis
",n
"LEFT id: NA
RIGHT id: 630

LEFT text: The analytic prediction of buffer hit probability, based on the characterization of database accesses from real reference traces, is extremely useful for workload management and system capacity planning. The knowledge can be helpful for proper allocation of buffer space to various database relations, as well as for the management of buffer space for a mixed transaction and query environment. Access characterization can also be used to predict the buffer invalidation effect in a multi-node environment which, in turn, can influence transaction routing strategies. However, it is a challenge to characterize the database access pattern of a real workload reference trace in a simple manner that can easily be used to compute buffer hit probability. In this article, we use a characterization method that distinguishes three types of access patterns from a trace: (1) locality within a transaction, (2) random accesses by transactions, and (3) sequential accesses by long queries. We then propose a concise way to characterize the access skew across randomly accessed pages by logically grouping the large number of data pages into a small number of partitions such that the frequency of accessing each page within a partition can be treated as equal. Based on this approach, we present a recursive binary partitioning algorithm that can infer the access skew characterization from the buffer hit probabilities for a subset of the buffer sizes. We validate the buffer hit predictions for single and multiple node systems using production database traces. We further show that the proposed approach can predict the buffer hit probability of a composite workload from those of its component files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: characterization of database access pattern for analytic prediction of buffer hit probability

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: asit dan , philip s. yu , jen yao chung
",y
"LEFT id: NA
RIGHT id: 1217

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 1465

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: duplicate removal in information system dissemination

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: tak w. yan , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 993

LEFT text: We propose a new class of algorithms that can be used to speed up the execution of multi-way join queries or of queries that involve one or more joins and a group-by. These new evaluation techniques allow to perform several hash-based operations (join and grouping) in one pass without repartitioning intermediate results. These techniques work particularly well for joining hierarchical structures, e.g., for evaluating functional join chains along key/foreign-key relationships. The idea is to generalize the concept of hash teams as proposed by Graefe et.al [GBC98] by indirectly partitioning the input data. Indirect partitioning means to partition the input data on an attribute that is not directly needed for the next hash-based operation, and it involves the construction of bitmaps to approximate the partitioning for the attribute that is needed in the next hash-based operation. Our performance experiments show that such generalized hash teams perform significantly better than conventional strategies for many common classes of decision support queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: diag-join : an opportunistic join algorithm for 1 : n relationships

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sven helmer , till westmann , guido moerkotte
",y
"LEFT id: NA
RIGHT id: 2010

LEFT text: In this paper we present a prototype system for the management of earth science data which is novel in that it takes a DBMS centric view of the the task. Our prototype -called ""BigSur"" -is shown in the context of its use by two geographically distributed scientific groups with demanding data storage and processing requirements. BigSur currently stores 1 Terabyte of data, about one thousandth of the volume EOSDIS must store. We claim that the design principles embodied in BigSur provide sufficient flexibility to achieve the difficult scientific and technical objectives of Mission to Planet Earth.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 1553

LEFT text: Since the last issue of this column six months ago, there have been many interesting program announcements, some of which have already passed deadline. We'll go over these announcements anyway, with the hope that they can get the readers better prepared for future funding opportunities. But first, we'll talk about the continuing budget battle at Congress, and the recent turmoil at NASA.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: turmoil at nasa , and numerous funding announcements

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: xiaolei qian
",y
"LEFT id: NA
RIGHT id: 1462

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 2106

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query mapping : accounting for translation closeness

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1958

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries over multimedia repositories

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 1555

LEFT text: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia’s Programming Systems Laboratory (Section 7).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: florida international university high performance database research center

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: naphtali rishe , wei sun , david barton , yi deng , cyril orji , michael alexopoulos , leonardo loureiro , carlos ordonez , mario sanchez , artyom shaposhnikov
",n
"LEFT id: NA
RIGHT id: 1665

LEFT text: In this first article of the regular column on data base standardization activities, I give an overview of topic areas under active development in the formal national and international standardization bodies. I solicit contributions on these active topics so that standardizers and researchers can cooperate in the near term, before irreversible decisions are made, to produce the most useful and highest quality database standards.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a language based multidatabase system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eva k &#252; hn , thomas tschernko , konrad schwarz
",n
"LEFT id: NA
RIGHT id: 859

LEFT text: George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: research directions in biodiversity informatics

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: john l. schnase
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1384

LEFT text: Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: temporal queries in olap

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alberto o. mendelzon , alejandro a. vaisman
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Unfortunately, there is very little money to support the health promotion initiatives in this plan. Health promotion receives very little of the $17 billion NIH research budget and few health promotion procedures are covered by the $400+ billion spent annually for Medicare and Medicaid. The Office of Health Promotion and Disease Prevention has a budget so small that very few health promotion professionals ever encounter this office directly during their careers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",y
"LEFT id: NA
RIGHT id: 1167

LEFT text: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency control in hierarchical multidatabase systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sharad mehrotra , henry f. korth , avi silberschatz
",n
"LEFT id: NA
RIGHT id: 1

LEFT text: The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a user-centered interface for querying distributed multimedia databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: isabel f. cruz , kimberly m. james
",n
"LEFT id: NA
RIGHT id: 1440

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: generalized search trees for database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton , avi pfeffer
",n
"LEFT id: NA
RIGHT id: 828

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for maintaining replica consistency in lazy master replicated databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: esther pacitti , pascale minet , eric simon
",n
"LEFT id: NA
RIGHT id: 1713

LEFT text: The analysis of web usage has mostly focused on sites composed of conventional static pages. However, huge amounts of information available in the web come from databases or other data collections and are presented to the users in the form of dynamically generated pages. The query interfaces of such sites allow the specification of many search criteria. Their generated results support navigation to pages of results combining cross-linked data from many sources. For the analysis of visitor navigation behaviour in such web sites, we propose the web usage miner (WUM), which discovers navigation patterns subject to advanced statistical and structural constraints. Since our objective is the discovery of interesting navigation patterns, we do not focus on accesses to individual pages. Instead, we construct conceptual hierarchies that reflect the query capabilities used in the production of those pages. Our experiments with a real web site that integrates data from multiple databases, the German SchulWeb, demonstrate the appropriateness of WUM in discovering navigation patterns and show how those discoveries can help in assessing and improving the quality of the site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: staggered striping in multimedia information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: steven berson , shahram ghandeharizadeh , richard muntz , xiangyu ju
",n
"LEFT id: NA
RIGHT id: 1106

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: processing object-oriented queries with invertible late bound functions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: staffan flodin , tore risch
",n
"LEFT id: NA
RIGHT id: 2050

LEFT text: We have introduced a Multi-Dimensional Clustering (MDC) physical layout scheme in DB2 version 8.0 for relational tables. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. Each clustering key is allocated one or more blocks of physical storage with the aim of storing the multiple records belonging to the cluster in almost contiguous fashion. Block oriented indexes are created to access these blocks. In this paper, we describe novel techniques for query processing operations that provide significant performance improvements for MDC tables. Current database systems employ a repertoire of access methods including table scans, index scans, index ANDing, and index ORing. We have extended these access methods for efficiently processing the block based MDC tables. One important concept at the core of processing MDC tables is the block oriented access technique. In addition, since MDC tables can include regular record oriented indexes, we employ novel techniques to combine block and record indexes. Block oriented processing is extended to nested loop joins and star joins as well. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: contorting high dimensional data for efficient main memory knn processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: bin cui , beng chin ooi , jianwen su , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1938

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 1002

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",n
"LEFT id: NA
RIGHT id: 747

LEFT text: Data Preprocessing for Data Mining addresses one of the most important issues within the well-known Knowledge Discovery from Data process. Data directly taken from the source will likely have inconsistencies, errors or most importantly, it is not ready to be considered for a data mining process. Furthermore, the increasing amount of data in recent science, industry and business applications, calls to the requirement of more complex tools to analyze it. Thanks to data preprocessing, it is possible to convert the impossible into possible, adapting the data to fulfill the input demands of each data mining algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data mining : concepts and techniques by jiawei han and micheline kamber

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: fernando berzal , nicolf &#225; s mat &#237; n
",y
"LEFT id: NA
RIGHT id: 936

LEFT text: This paper proposes an extension of the multiversion two phase locking protocol, called EMVZPL, which enables update transactions to use versions while guaranteeing the serializability of all transactions. The use of the protocol is restricted to transactions, called write-then-read transactions that consist of two consecutive parts: a write part containing both read and write operations in some arbitrary order, and an abusively called read part, containing read operations or write operations on data items already locked in the write part of the transaction. With EMVZPL, read operations in the read part use versions and read locks acquired in the write part can be released just before entering the read part. We prove the correctness of our protocol, and show that its implementation requires very few changes to classical implementations of MVZPL. After presenting various methods used by application developers to implement integrity checking, we show how EMV2PL can be effectively used to optimize the processing of update transactions that perform integrity checks. Finally, performance studies show the benefits of our protocol compared to a (strict) two phase locking protocol.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using versions in update transactions : application to integrity checking

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fran &#231; ois llirbat , eric simon , dimitri tombroff
",y
"LEFT id: NA
RIGHT id: 159

LEFT text: A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: free parallel data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bin li , dennis shasha
",n
"LEFT id: NA
RIGHT id: 1883

LEFT text: Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 448

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: using quantitative information for efficient association rule generation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: b. p &#244; ssas , m. carvalho , r. resende , w. meita , jr.
",n
"LEFT id: NA
RIGHT id: 590

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the demarcation protocol : a technique for maintaining constraints in distributed database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: daniel barbar &#225; - mill &#225; , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 300

LEFT text: In the past decade, advances in the speed of commodity CPUs have far out-paced advances in memory latency. Main-memory access is therefore increasingly a performance bottleneck for many computer applications, including database systems. In this article, we use a simple scan test to show the severe impact of this bottleneck. The insights gained are translated into guidelines for database architecture, in terms of both data structures and algorithms. We discuss how vertically fragmented data structures optimize cache performance on sequential data access. We then focus on equi-join, typically a random-access operation, and introduce radix algorithms for partitioned hash-join. The performance of these algorithms is quantified using a detailed analytical model that incorporates memory access cost. Experiments that validate this model were performed on the Monet database system. We obtained exact statistics on events such as TLB misses and L1 and L2 cache misses by using hardware performance counters found in modern CPUs. Using our cost model, we show how the carefully tuned memory access pattern of our radix algorithms makes them perform well, which is confirmed by experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing multidimensional index trees for main memory access

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kihong kim , sang k. cha , keunjoo kwon
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: In this paper we present a tool for enhanced exploration of OLAP data that is adaptive to a user’s prior knowledge of the data. The tool continuously keeps track of the parts of the cube that a user has visited. The information in these scattered visited parts of the cube is pieced together to form a model of the user’s expected values in the unvisited parts. The mathematical foundation for this modeling is provided by the classical Maximum Entropy principle. At any time, the user can query for the most surprising unvisited parts of the cube. The most surprising values are dened as those which if known to the user would bring the new expected values closest to the actual values. This process of updating the user’s context based on visited parts and querying for regions to explore further continues in a loop until the user’s mental model perfectly matches the actual cube.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 1135

LEFT text: The optimization capabilities of RDBMSs make them attractive for executing data transformations. However, despite the fact that many useful data transformations can be expressed as relational queries, an important class of data transformations that produce several output tuples for a single input tuple cannot be expressed in that way. To overcome this limitation, we propose to extend Relational Algebra with a new operator named data mapper. In this paper, we formalize the data mapper operator and investigate some of its properties. We then propose a set of algebraic rewriting rules that enable the logical optimization of expressions with mappers and prove their correctness. Finally, we experimentally study the proposed optimizations and identify the key factors that influence the optimization gains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: schemasql - a language for interoperability in relational multi-database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , fereidoon sadri , iyer n. subramanian
",n
"LEFT id: NA
RIGHT id: 2247

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: investigation of algebraic query optimisation techniques for database programming languages

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 1954

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sql query optimization : reordering for a general class of queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 1246

LEFT text: Keyword indices, topic directories, and link-based rankings are used to search and structure the rapidly growing Web today. Surprisingly little use is made of years of browsing experience of millions of people. Indeed, this information is routinely discarded by browsers. Even deliberate bookmarks are stored in a passive and isolated manner. All this goes against Vannevar Bush’s dream of the Memex : an enhanced supplement to personal and community memory. We propose to demonstrate the beginnings of a ‘Memex’ for the Web: a browsing assistant for individuals and groups with focused interests. Memex blurs the articial distinction between browsing history and deliberate bookmarks. The resulting glut of data is analyzed in a number of ways at the individual and community levels. Memex constructs a topic directory customized to the community, mapping their interests naturally to nodes in this directory. This lets the user recall topic-based browsing contexts by asking questions like \What trails was I following when I was last surng about classical music?"" and \What are some popular pages in or near my community’s recent trail graph related to music?"" 1 Motivation Three paradigms have emerged for exploring the Web: keyword search, directory browsing, and following links. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: memex : a browsing assistant for collaborative archiving and mining of surf trails

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: soumen chakrabarti , sandeep srivastava , mallela subramanyam , mitul tiwari
",y
"LEFT id: NA
RIGHT id: 1226

LEFT text: In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L 1 norm) is consistently more preferable than the Euclidean distance metric (L 2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: distance-based outliers : algorithms and applications

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: edwin m. knorr , raymond t. ng , vladimir tucakov
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 184

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wavelet-based histograms for selectivity estimation

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 2051

LEFT text: The widespread distribution and availability of small-scale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.In this paper, we introduce the Cougar approach to tasking sensor networks through declarative queries. Given a user query, a query optimizer generates an efficient query plan for in-network query processing, which can vastly reduce resource usage and thus extend the lifetime of a sensor network. In addition, since queries are asked in a declarative language, the user is shielded from the physical characteristics of the network. We give a short overview of sensor networks, propose a natural architecture for a data management system for sensor networks, and describe open research problems in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the design of an acquisitional query processor for sensor networks

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: samuel madden , michael j. franklin , joseph m. hellerstein , wei hong
",n
"LEFT id: NA
RIGHT id: 1181

LEFT text: In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: scalable feature selection , classification and signature generation for organizing large text databases into hierarchical topic taxonomies

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: soumen chakrabarti , byron dom , rakesh agrawal , prabhakar raghavan
",y
"LEFT id: NA
RIGHT id: 1662

LEFT text: This article describes a novel way of combining data mining techniques on Internet data in order to discover actionable marketing intelligence in electronic commerce scenarios. The data that is considered not only covers various types of server and web meta information, but also marketing data and knowledge. Furthermore, heterogeneity resolution thereof and Internet- and electronic commerce-specific pre-processing activities are embedded. A generic web log data hypercube is formally defined and schematic designs for analytical and predictive activities are given. From these materialised views, various online analytical web usage data mining techniques are shown, which include marketing expertise as domain knowledge and are specifically designed for electronic commerce purposes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: hodfa : an architectural framework for homogenizing heterogeneous legacy databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kamalakar karlapalem , qing li , chung-dak shum
",n
"LEFT id: NA
RIGHT id: 1284

LEFT text: To overcome current bottlenecks in business-to-business (B2B) electronic commerce, we need intelligent solutions for mechanizing the process of structuring, standardizing, aligning and personalizing data. This article surveys the overall content management process and discusses requirements for its scalable support.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data management for pervasive computing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mitch cherniack , michael j. franklin , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: Disk-based database systems benefit from concurrency among transactions - usually with marginal overhead. For main-memory database systems, however, locking overhead can have a serious impact on performance. This paper proposes SP, a serial protocol for the execution of transactions in main-memory systems, and evaluates its performance against that of strict two-phase locking. The novelty of SP lies in the use of timestamps and mutexes to allow one transaction to begin before its predecessors' commit records have been written to disk, while also ensuring that no committed transactions read uncommitted data. We demonstrate seven-fold and two-fold increases in maximum throughput for read-and update-intensive workloads, respectively. At fixed loads, we demonstrate ten-fold and two-fold improvements in response time for the same transaction mixes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 1864

LEFT text: Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the whips prototype for data warehouse creation and maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wilburt j. labio , yue zhuge , janet l. wiener , himanshu gupta , h &#233; ctor garc &#237; a-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 252

LEFT text: Imagine that you are a “knowledge worker” in the coming millenium. That means you must synthesize information and make decisions such as “Which benefits plan to use?” “What do the regulations say about this course of action?” “How does my job fit into the corporate business plan?” “What should I be careful about when I approach this client?” or even “HOW does this program work?” If the dream of digital libraries is to bring you all material relevant to your task, you may find yourself drowning before long. Reading is harder than talking to people who know the relevant documents and can tell you what you’re interested in. That is what many current knowledge workers do, giving rise to professions such as insurance consultant, lawyer, benefits specialist, and so on. Imagine by contrast that the documents you retrieve could be tailored precisely to your needs. That is, imagine that the document might ask you questions and produce a document filtered and organized according to those you have answered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: thinksheet : a tool for tailoring complex documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: peter piatko , roman yangarber , daoi lin , dennis shasha
",y
"LEFT id: NA
RIGHT id: 1462

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregate-query processing in data warehousing environments

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashish gupta , venky harinarayan , dallan quass
",n
"LEFT id: NA
RIGHT id: 527

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross , theodore johnson , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 755

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",n
"LEFT id: NA
RIGHT id: 1895

LEFT text: Since it quotes extensively from writings of my own, I feel obliged to respond to the article “Domains, Relations and Religious Wars,” by R. Camps (SIGMOD Record 25, No. 3, September 1996). In that article, Camps is clearly suggesting (among other things) that my definition of the term “domain” has changed over the years. I agree, it has! But Camps goes on to say: “… considering that [Date's book An Introduction to Database Systems] was the bible [Camps' italics] where most university graduates all over the world learnt, I believe that Date can be held partly responsible for the lack of implementation of domains [in today's SQL DBMSs].”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: in reply to domains , relations and religious wars

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hugh darwen
",n
"LEFT id: NA
RIGHT id: 2211

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: including group-by in query optimization

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 434

LEFT text: Content-based retrieval of images is the ability to retrieve images that are similar to a query image. Oracle8i Visual Information Retrieval provides this facility based on technology licensed from Virage, Inc. This product is built on top of Oracle8i interMedia which enables storage, retrieval and management of images, audios and videos. Images are matched using attributes such as color, texture and structure and efficient content-based retrieval is provided using indexes of an image index type. The design of the index type is based on a multi-level filtering algorithm. The filters reduce the search space so that the expensive comparison algorithm operates on a small subset of the data. Bitmap indexes are used to evaluate the first filter resulting in a design which performs well and is scalable. The image index type is built using Oracle8i extensible indexing technology, allowing users to create, use, and drop instances of this index type as they would any other standard index. In this paper we present an overview of the product, the design of the image index type, and some performance results of our product.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: indexing images in oracle8i

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: melliyal annamalai , rajiv chopra , samuel defazio , susan mavris
",y
"LEFT id: NA
RIGHT id: 1304

LEFT text: Non-conventional database management systems are used to achieve a better performance when dealing with complex data. One fundamental concept of these systems is object identity (OID), because each object in the database has a unique identifier that is used to access and reference it in relationships to other objects. Two approaches can be used for the implementation of OIDs: physical or logical OIDs. In order to manage complex data, was proposed the Multimedia Data Manager Kernel (NuGeM) that uses a logical technique, named Indirect Mapping. This paper proposes an improvement to the technique used by NuGeM, whose original contribution is management of OIDs with a fewer number of disc accesses and less processing, thus reducing management time from the pages and eliminating the problem with exhaustion of OIDs. Also, the technique presented here can be applied to others OODBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an evaluation of generic bulk loading techniques

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 2009

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qxtract : a building block for efficient information extraction from text databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: eugene agichtein , luis gravano
",n
"LEFT id: NA
RIGHT id: 118

LEFT text: Currently, the Internet provides access to a very large number and wide variety of information sources (e.g., textual databases, sites containing technical reports, directory listings), and systems to access these sources (e.g., World Wide Web, Gopher, WAIS). The challenge is to provide easy, efficient, robust and secure access to this information and other kinds (e.g., relational and object oriented databases). This aim of this panel is to explore whether there are any new technical problems, relevant to the Database field, that need to be solved in order to realize such global information systems. In particular, we debate whether existing techniques from database systems (e.g., multidatabases and distributed databases) can be applied or straigtitforwardly extended to global information systems. Furthermore, we attempt to establish realistic goals for database technologies in global information systems. Some of the specific issues discussed are the following:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in global information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: a. m. ouksel , a. sheth
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: We present an approach to database interoperation that exploits the semantic information provided by integrity constraints defined on the component databases. We identify two roles of integrity constraints in database interoperation. First, a set of integrity constraints describing valid states of the integrated view can be derived from the constraints defined on the underlying databases. Moreover, local integrity constraints can be used as a semantic check on the validity of the specification of the integrated view. We illustrate our ideas in the context of an instance-based database interoperation paradigm, where objects rather than classes are the unit of integration. We introduce the notions of objectivity and subjectivity as an indication of whether a constraint is valid beyond the context of a specific database, and demonstrate the impact of these notions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1854

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: delaunay : a database visualization system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang
",n
"LEFT id: NA
RIGHT id: 2049

LEFT text: While the XML Stylesheet Language for Transformations (XSLT) was not designed as a query language, it is well-suited for many query-like operations on XML documents including selecting and restructuring data. Further, it actively fulfills the role of an XML query language in modern applications and is widely supported by application platform software. However, the use of database techniques to optimize and execute XSLT has only recently received attention in the research community. In this paper, we focus on the case where XSL transformations are to be run on XML documents defined as views of relational databases. For a subset of XSLT, we present an algorithm to compose a transformation with an XML view, eliminating the need for the XSLT execution. We then describe how to extend this algorithm to handle several additional features of XSLT, including a proposed approach for handling recursion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: composing xsl transformations with xml publishing views

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chengkai li , philip bohannon , p. p. s. narayan
",y
"LEFT id: NA
RIGHT id: 1802

LEFT text: Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental distance join algorithms for spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 215

LEFT text: We consider the problem of mapping data in peer-to-peer data-sharing systems. Such systems often rely on the use of mapping tables listing pairs of corresponding values to search for data residing in different peers. In this paper, we address semantic and algorithmic issues related to the use of mapping tables. We begin by arguing why mapping tables are appropriate for data mapping in a peer-to-peer environment. We discuss alternative semantics for these tables and we present a language that allows the user to specify mapping tables under different semantics. Then, we show that by treating mapping tables as constraints (called mapping constraints) on the exchange of information between peers it is possible to reason about them. We motivate why reasoning capabilities are needed to manage mapping tables and show the importance of inferring new mapping tables from existing ones. We study the complexity of this problem and we propose an efficient algorithm for its solution. Finally, we present an implementation along with experimental results that show that mapping tables may be managed efficiently in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 1015

LEFT text: DTL’s DataSpot is a database publishing tool that enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation. DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a hyperbase. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and userdefined associations, and creates the hyperbase. The DataSpot Search Server performs searches and navigation against the hyperbase, returning answers to the user either in HTML pages or through an object API. The DataSpot product has been successfilly deployed in diverse application areas including electronic catalogs, yellow pages, classified ads, help desks and finance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dtl 's dataspot : database exploration using plain language

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shaul dar , gadi entin , shai geva , eran palmon
",y
"LEFT id: NA
RIGHT id: 1637

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries on files

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mariano p. consens , tova milo
",n
"LEFT id: NA
RIGHT id: 1818

LEFT text: To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an array-based algorithm for simultaneous multidimensional aggregates

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yihong zhao , prasad m. deshpande , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: The paper describes the ARANEUS Wel-Base Management System [l, 5, 4, 61, a system developed at Universitb di Roma Tre, which represents a proposal towards the definition of a new kind of data-repository, designed to manage Web data in the database style. We call a WebBase a collection of data of heterogeneous nature, and more specifically: (i) highly structured data, such as the ones typically stored in relational or objectoriented database systems; (G) semistructured data, in the Web style.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 2013

LEFT text: In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: peerdb : peering into personal databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , kian-lee tan , aoying zhou , chin hong goh , yingguang li , chu yee liau , bo ling , wee siong ng , yanfeng shu , xiaoyu wang , ming zhang
",n
"LEFT id: NA
RIGHT id: 258

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: communication-efficient distributed mining of association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: assaf schuster , ran wolff
",n
"LEFT id: NA
RIGHT id: 1775

LEFT text: The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda — broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: predator : a resource for database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 623

LEFT text: Today the problem of semantic interoperability in information search on the Internet is solved mostly by means of centralization, both at a system and at a logical level. This approach has been successful to a certain extent. Peer-to-peer systems as a new brand of system architectures indicate that the principle of decentralization might offer new solutions to many problems that scale well to very large numbers of users.In this paper we outline how the peer-to-peer system architectures can be applied to tackle the problem of semantic interoperability in the large, driven in a bottom-up manner by the participating peers. Such a system can readily be used to study semantic interoperability as a global scale phenomenon taking place in a social network of information sharing peers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a framework for semantic gossiping

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer , philippe cudr &#233; - mauroux , manfred hauswirth
",y
"LEFT id: NA
RIGHT id: 2258

LEFT text: In this paper we investigate the co-authorship graph obtained from all papers published at SIGMOD between 1975 and 2002. We find some interesting facts, for instance, the identity of the authors wh...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: analysis of sigmod 's co-authorship graph

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: mario a. nascimento , j &#246; rg sander , jeffrey pound
",y
"LEFT id: NA
RIGHT id: 1367

LEFT text: In this paper we propose a distributed case-based approach to the problem of rewriting queries. According to this approach we use a case memory instead of static views, i.e. views that are deened a priori. As a consequence, the mediated schema is dynamically updated, strongly innuenced by the queries submitted by a consumer. This approach allows a mediator to face systems where consumers may change their customization needs and information sources may become unavailable, may be added, or may modify their schemas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: model-based information integration in a neuroscience mediator system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bertram lud &#228; scher , amarnath gupta , maryann e. martone
",n
"LEFT id: NA
RIGHT id: 1718

LEFT text: We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier, real point sets: (a) violate consistently the “uniformity” and “independence” assumptions, (b) can often be described as “fractals”, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called “Correlation Dimension” Dz is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with LI& as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for “biased” range queries (i.e., queries whose centers prefer areas of high point density).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive selectivity estimation using query feedback

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chungmin melvin chen , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: We have extended Rainbow, our existing XML data management system, as shown in Figure 1. Rainbow accepts an XQuery query or an update request in an extended XQuery syntax from the user. The XQuery is parsed into an algebraic representation, called XML Algebra Tree (XAT). The XAT is then optimized by the global query optimizer using algebraic rewrite rules. We have introduced a separate phase of XAT cleanup which includes the XAT table schema cleanup and cutting of unnecessary XML operators. This optimization often significantly improves the query performance. The optimized XAT is then executed by the query manager.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 689

LEFT text: In this paper, we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implemanting these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend that strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator TUR, resulting in a set of fixpoints for UR. The monotionicity of the operator is maintained nby explicitly representing the effect of asserting and retracting tuples in the database. A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relaions, i.e., those relations which are not update by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is “find the libraries that are within 10 miles of schools”. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call “pair-count exponent” (PC). We show that this law also holds for self-spatial-joins (“find schools within 5 miles of other schools”) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy). In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",n
"LEFT id: NA
RIGHT id: 205

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional selectivity estimation using compressed histogram information

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ju-hong lee , deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1671

LEFT text: speculated about the need and design of "" Information Refineries "" , machines capable of taking massive amounts of data and converting it into knowledge. In order to investigate the design of a database machine which would be capable of acting as the data storage engine of Gelemter's "" Information Refineries "" the authors initiated the MEDUSA Project as a joint undertaking between the Hardware Architecture The current MEDUSA prototype utilises a shared-nothing architecture based on the INMOS Transputer. Each of the three processing nodes used in the prototype consists of two T805 transputers with a T222 SCSI interface to a Maxtor 180 MByte disk unit as shown in figure 1. Principle Goals of the Project The principle goals of the MEDUSA Project are to develop a prototype database machine based on a shared-nothing architecture using low cost "" off the shelf "" components to SUpport research in the following areas: autonomous data management user data interfaces; backup and security systems. If the full potential of these machines is to be exploited they must exhibit a level of operational autonomy similar to the existing file server technology used on local area networks. That is, they should be capable of seamless integration into a network without requiring changes to existing software development practices or additional specialised staff to maintain their operational efficiency. Operational autonomy can be achieved by a self-organising or self-tuning database, In a self-organising database environment the database management system (DBMS) is responsible for the system tuning and data re-organisation, to achieve optimal performance. Human intervention in the system's maintenance task is reduced to no more than the changing of backup media or carrying out hardware maintenance. The traditional approach taken to performance tuning is normally heuristically bas~ with the DBMS providing the administrator with a number of tuning parameters which are adjusted based on the DBAs experience. The lack of an experienced DBA forces most sites to rely on intuition or guesswork the latter being more common. so L Interface An SQL interface, MedusaSQL, is available for data retrieval on MEDUSA. Currently, a reduced version of SQL92 standard is operational. This version allows retrieval of selected attribute values from tuples which meet criteria specified in the WHERE clause. Simple projections, joins and selections are possible using the features of the SELECT, FROM and WHERE clauses implemented thus far. Full implementation of SQL92 is planned to be completed by

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the medusa project : autonomous data management in a shared-nothing parallel database machine

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: g. m. bryan , w. e. moore , b. j. curry , k. w. lodge , j. geyer
",y
"LEFT id: NA
RIGHT id: 2109

LEFT text: We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 1742

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aries/csa : a method for database recovery in client-server architectures

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: c. mohan , inderpal narang
",n
"LEFT id: NA
RIGHT id: 480

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",y
"LEFT id: NA
RIGHT id: 2045

LEFT text: It is striking that the optimization of disjunctive queries-i.e. those which contain at least one OR-connective in the query predicate-has been vastly neglected in the literature, as well as in commercial systems. In this paper, we propose a novel technique, called bypass processing, for evaluating such disjunctive queries. The bypass processing technique is based on new selection and join operators that produce two output streams: the TRUE-stream with tuples satisfying the selection (join) predicate and the FALSE-stream with tuples not satisfying the corresponding predicate. Splitting the tuple streams in this way enables us to ""bypass"" costly predicates whenever the ""fate"" of the corresponding tuple (stream) can be determined without evaluating this predicate. In the paper, we show how to systematically generate bypass evaluation plans utilizing a bottom-up building-block approach. We show that our evaluation technique allows us to incorporate the standard SQL semantics of null values. For this, we devise two different approaches: one is based on explicitly incorporating three-valued logic into the evaluation plans; the other one relies on two-valued logic by ""moving"" all negations to atomic conditions of the selection predicate. We describe how to extend an iterator-based query engine to support bypass evaluation with little extra overhead. This query engine was used to quantitatively evaluate the bypass evaluation plans against the traditional evaluation techniques utilizing a CNFor DNF-based query predicate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: factorizing complex predicates in queries to exploit indexes

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: surajit chaudhuri , prasanna ganesan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1811

LEFT text: Today’s object-relational DBMSs (ORDBMSs) are designed to support novel application domains by providing an extensible architecture, supplemented by domain-specific database extensions supplied by external vendors. An important aspect of ORDBMSs is support for extensible indexing, which allows the core database server to be extended with external access methods (AMs). This paper describes a new approach to extensible indexing implemented in Informix Dynamic Server with Universal Data Option (IDS/UDO). The approach is is based on the generalized search tree, or GiST, which is a template index structure for abstract data types that supports an extensible set of queries. GiST encapsulates core database indexing functionality including search, update, concurrency control and recovery, and thereby relieves the external access method (AM) of the burden of dealing with these issues. The IDS/UDO implementation employs a newly designed GiST API that reduces the number of user defined function calls, which are typically expensive to execute, and at the same time makes GiST a more flexible data structure. Experiments show that GiST-based AM extensibility can offer substantially better performance than built-in AMs when indexing userdefined data types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: high-performance sorting on networks of workstations

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: andrea c. arpaci-dusseau , remzi h. arpaci-dusseau , david e. culler , joseph m. hellerstein , david a. patterson
",n
"LEFT id: NA
RIGHT id: 940

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries across diverse data sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laura m. haas , donald kossmann , edward l. wimmers , jun yang
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1265

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: supporting efficient multimedia database exploration

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: wen-syan li , k.sel &#231; uk candan , kyoji hirata , yoshinori hara
",n
"LEFT id: NA
RIGHT id: 1677

LEFT text: The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: quest : a project on database mining

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. agrawal , m. carey , c. faloutsos , s. ghosh , m. houtsma , t. imieli &#324; ski , b. iyer , a. mahboob , h. miranda , r. srikant , a. swami
",y
"LEFT id: NA
RIGHT id: 1600

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 811

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: relational databases for querying xml documents : limitations and opportunities

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 530

LEFT text: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 2253

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a multidatabase system for tracking and retrieval of financial data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munir cochinwala , john bradley
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: We propose a framework for integrating data from multiple relational sources into an XML document that both conforms to a given DTD and satisfies predefined XML constraints. The framework is based on a specification language, AIG, that extends a DTD by (1) associating element types with semantic attributes (inherited and synthesized, inspired by the corresponding notions from Attribute Grammars), (2) computing these attributes via parameterized SQL queries over multiple data sources, and (3) incorporating XML keys and inclusion constraints. The novelty of AIG consists in semantic attributes and their dependency relations for controlling context-dependent, DTD-directed construction of XML documents, as well as for checking XML constraints in parallel with document-generation. We also present cost-based optimization techniques for efficiently evaluating AIGs, including algorithms for merging queries and for scheduling queries on multiple data sources. This provides a new grammar-based approach for data integration under both syntactic and semantic constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 800

LEFT text: Abstract. The rapid growth of the Internet and support for interoperability protocols has increased the number of Web accessible sources, WebSources. Current wrapper mediator architectures need to be extended with a wrapper cost model (WCM) for WebSources that can estimate the response time (delays) to access sources as well as other relevant statistics. In this paper, we present a Web prediction tool (WebPT), a tool that is based on learning using query feedback from WebSources. The WebPT uses dimensions time of day, day, and quantity of data, to learn response times from a particular WebSource, and to predict the expected response time (delay) for some query. Experiment data was collected from several sources, and those dimensions that were significant in estimating the response time were determined. We then trained the WebPT on the collected data, to use the three dimensions mentioned above, and to predict the response time, as well as a confidence in the prediction. We describe the WebPT learning algorithms, and report on the WebPT learning for WebSources. Our research shows that we can improve the quality of learning by tuning the WebPT features, e.g., training the WebPT using a logarithm of the input training data; including significant dimensions in the WebPT; or changing the ordering of dimensions. A comparison of the WebPT with more traditional neural network (NN) learning has been performed, and we briefly report on the comparison. We then demonstrate how the WebPT prediction of delay may be used by a scrambling enabled optimizer. A scrambling algorithm identifies some critical points of delay, where it makes a decision to scramble (modify) a plan, to attempt to hide the expected delay by computing some other part of the plan that is unaffected by the delay. We explore the space of real delay at a WebSource, versus the WebPT prediction of this delay, with respect to critical points of delay in specific plans. We identify those cases where WebPT overestimation or underestimation of the real delay results in a penalty in the scrambling enabled optimizer, and those cases where there is no penalty. Using the experimental data and WebPT learning, we test how good the WebPT is in minimizing these penalties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost models do matter : providing cost information for diverse data sources in a federated system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mary tork roth , fatma ozcan , laura m. haas
",y
"LEFT id: NA
RIGHT id: 2186

LEFT text: In this paper, we introduce an approach that supports querying for Semantic Associations on the Semantic Web. Semantic Associations capture complex relationships between entities involving sequences of predicates, and sets of predicate sequences that interact in complex ways. Detecting such associations is at the heart of many research and analytical activities that are crucial to applications in national security and business intelligence. This in combination with the improving ability to identify entities in documents as part of automatic semantic annotation, gives a very powerful capability for semantic analysis of large amounts of heterogeneous content.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: managing multiple and distributed ontologies on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: a. maedche , b. motik , l. stojanovic
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1794

LEFT text: Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: new sampling-based summary statistics for improving approximate query answers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias
",n
"LEFT id: NA
RIGHT id: 839

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: active storage hierarchy , database systems and applications - socratic exegesis

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: felipe cari &#241; o , william o'connell , john burgess , joel h. saltz
",n
"LEFT id: NA
RIGHT id: 1342

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing : taming the terabytes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: minos n. garofalakis , phillip b. gibbon
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: This is a beautifully simple paper that I feel encompasses many ideas that keep reappearing in different guises every decade or so! The paper proposes the replication of a dictionary (basically a set of key and value pairs) to all relevant sites in a distributed system. Updates and deletes are propagated in a lazy manner through the system as sites communicate with each other using a simple notion of a log. Queries are answered based on the local copy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1203

LEFT text: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: mariposa : a wide-area distributed database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael stonebraker , paul m. aoki , witold litwin , avi pfeffer , adam sah , jeff sidell , carl staelin , andrew yu
",n
"LEFT id: NA
RIGHT id: 2269

LEFT text: SchemaSQL is a recently proposed extension to SQL for enabling multi-database interoperability. Several recently identi ed applications for SchemaSQL, however, mainly rely on its ability to treat data and schema labels in a uniform manner, and call for an e cient implementation of it on a single RDBMS. We rst develop a logical algebra for SchemaSQL by combining classical relational algebra with four restructuring operators { unfold, fold, split, and unite { originally introduced in the context of the tabular data model by Gyssens et al. [GLS96], and suitably adapted to t the needs of SchemaSQL. We give an algorithm for translating SchemaSQL queries/views involving restructuring, into the logical algebra above. We also provide physical algebraic operators which are useful for query optimization. Using the various operators as a vehicle, we give several alternate implementation strategies for SchemaSQL queries/views. All the proposed strategies can be implemented non-intrusively on top of existing relational DBMS, in that they do not require any additions to the existing set of plan operators. We conducted a series of performance experiments based on TPC-D benchmark data, using the IBM DB2 DBMS running on Windows/NT. In addition to showing the relative tradeo s between various alternate strategies, our experiments show the feasibility of implementing SchemaSQL on top of traditional 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementing lazy database updates for an object database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1334

LEFT text: A range-sum query is very popular and becomes important in finding trends and in discovering relationships between attributes in diverse database applications. It sums over the selected cells of an OLAP data cube where target cells are decided by the specified query ranges. The direct method to access the data cube itself forces too many cells to be accessed, therefore it incurs a severe overhead. The response time is very crucial for OLAP applications which need interactions with users. In the recent dynamic enterprise environment, data elements in the cube are frequently changed. The response time is affected in such an environment by the update cost as well as the search cost of the cube. In this paper, we propose an efficient algorithm to reduce the update cost significantly while maintaining reasonable search efficiency, by using an index structure called the ∆ -tree. In addition, we propose a hybrid method to provide either an approximate result or a precise one to reduce the overall cost of queries. It is useful for various applications that need a quick approximate answer rather than an accurate one, such as decision support systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic update cube for range-sum queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: seok-ju chun , chin-wan chung , ju-hong lee , seok-lyong lee
",y
"LEFT id: NA
RIGHT id: 1041

LEFT text: Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 2101

LEFT text: Wireless and mobile computing have advanced significantly in the last decade. In particular, we now face the challenge to spontaneously establish wireless self-organizing networks, such as ad hoc, disruption-tolerant, sensor, and wireless mesh networks. These spontaneous self-organizing networks have been the focus of intensive research activity in recent years. Spontaneous networks arise from the cooperation of mobile devices in an ad hoc fashion requiring no previous infrastructure in place. A key point to couple research and real-life applications in this context is to understand how mobility (of devices, users, and applications) impacts practical networking aspects

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amr el abbadi , gunter schlageter , kyu-young whang
",n
"LEFT id: NA
RIGHT id: 1864

LEFT text: A data warehouse is an integrated database whose data is collected from several data sources, and supports on-line analytical processing (OLAP). Typically, a query to the data warehouse tends to be complex and involves a large volume of data. To keep the data at the warehouse consistent with the source data, changes to the data sources should be propagated to the data warehouse periodically. Because the propagation of the changes (maintenance) is batch processing, it takes long time. Since both query transactions and maintenance transactions are long and involve large volumes of data, traditional concurrency control mechanisms such as two-phase locking are not adequate for a data warehouse environment. We propose a multi-version concurrency control mechanism suited for data warehouses which use multi-dimensional OLAP (MOLAP) servers. We call the mechanism multiversion concurrency control for data warehouses (MVCCDW). To our knowledge, our work is the first attempt to exploit versions for online data warehouse maintenance in a MOLAP environment. MVCC-DW guarantees the serializability of concurrent transactions. Transactions running under the mechanism do not block each other and do not need to place locks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the whips prototype for data warehouse creation and maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wilburt j. labio , yue zhuge , janet l. wiener , himanshu gupta , h &#233; ctor garc &#237; a-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 2188

LEFT text: Service composition is gaining momentum as the potential silver bullet for the envisioned Semantic Web. It purports to take the Web to unexplored efficiencies and provide a flexible approach for promoting all types of activities in tomorrow’s Web. Applications expected to heavily take advantage of Web service composition include B2B E-commerce and E-government. To date, enabling composite services has largely been an ad hoc, time-consuming, and error-prone process involving repetitive low-level programming. In this paper, we propose an ontology-based framework for the automatic composition of Web services. We present a technique to generate composite services from high-level declarative descriptions. We define formal safeguards for meaningful composition through the use of composability rules. These rules compare the syntactic and semantic features of Web services to determine whether two services are composable. We provide an implementation using an E-government application offering customized services to indigent citizens. Finally, we present an exhaustive performance experiment to assess the scalability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: learning to match ontologies on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: anhai doan , jayant madhavan , robin dhamankar , pedro domingos , alon halevy
",n
"LEFT id: NA
RIGHT id: 1485

LEFT text: This paper describes issues and solutions related to the creation of a product information database in the enterprise, and using this database as a foundation for deploying an electronic catalog. Today, product information is typically managed in document composition systems and communicated on paper. In the new wired world, these processes are undertaking fundamental changes to cope with the time to market pressure and the need for accurate, complete, and structured presentation of product information. Electronic catalogs are the answer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: memory-contention responsive hash joins

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: diane l. davison , goetz graefe
",y
"LEFT id: NA
RIGHT id: 1884

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 108

LEFT text: Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically diffcult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT { Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT's outperform previous data structures in a number of applications. Keywords { near neighbor, metric space, approximate queries, data mining, Dirichlet domains, Voronoi regions

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: indexing large metric spaces for similarity search queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 900

LEFT text: Computers running database management applications often manage large amounts of data. Typically, the price of the I/O subsystem is a considerable portion of the computing hardware. Fierce price competition demands every possible savings. Lossless data compression methods, when appropriately integrated with the dbms, yield signiflcant savings. Roughly speaking, a slight increase in cpu cycles is more than offset by savings in I/O subsystem. Various design issues arise in the use of data compression in the dbms from the choice of algorithm, statistics collection, hardware versus software based compression, location of the compression function in the overall computer system architecture, unit of compression, update in place, and the application of log’ to compressed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performing joins without decompression in a compressed database system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: s. j. o'connell , n. winterbottom
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 134

LEFT text: E-commerce is not a static field, but is constantly evolving to discover new and more effective ways of supporting businesses. Data management is an integral part of this effort, This special issue aims to report on some of the recent developments and identify some research directions in this area. Initially, e-commeree involved the use of ED] and intranets. Today we see the dominance of XML. Almost all recent elecn'onic commerce standards are based on X1VD... As a consequence, the amount of XML data being stored is large, and it is increasing. This naturally leads to the question of how to store and query the XML documents. The paper by Tian, DeWitt, Chen and Zhang describes the design and performance evaluation of alternative XM]., storage strategies. The results of this performance study provide valuable hints on how to store the XM1., files depending on the application. Personalization in e-commerce is about building customer loyalty by understanding and thus addressing the needs of each individual. E-commerce systems need customers' profiles to provide better services, 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: towards a theory of cost management for digital libraries and electronic commerce

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson , yelena yesha , robert sloan
",n
"LEFT id: NA
RIGHT id: 583

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs. Design principles have been proposed for persistent systems. By following these principles, languages that provide persistence as a basic abstraction have been developed. In this paper, the motivation for orthogonal persistence is reviewed along with the above mentioned design principles. The concepts for integrating programming languages and databases through the persistence abstraction, and their benefits, are given. The technology to support persistence, the achievements, and future directions of persistence research are then discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on prototypes of deductive database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: k. ramamohanarao
",n
"LEFT id: NA
RIGHT id: 1790

LEFT text: From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining and pruning optimizations of constrained associations rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang
",y
"LEFT id: NA
RIGHT id: 1724

LEFT text: Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: alphasort : a risc machine sort

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chris nyberg , tom barclay , zarka cvetanovic , jim gray , dave lomet
",y
"LEFT id: NA
RIGHT id: 925

LEFT text: Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: jim gray speaks out

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: marianne winslett
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 208

LEFT text: Clustering is one of the most important tasks performed in Data Mining applications. This paper presents an efficient SQL implementation of the EM algorithm to perform clustering in very large databases. Our version can effectively handle high dimensional data, a high number of clusters and more importantly, a very large number of data records. We present three strategies to implement EM in SQL: horizontal, vertical and a hybrid one. We expect this work to be useful for data mining programmers and users who want to cluster large data sets inside a relational DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast algorithms for projected clustering

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: charu c. aggarwal , joel l. wolf , philip s. yu , cecilia procopiuc , jong soo park
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system's performance and scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1637

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries on files

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mariano p. consens , tova milo
",n
"LEFT id: NA
RIGHT id: 790

LEFT text: Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability. This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments. A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator, whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL (C-SQL), in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: quality-driven integration of heterogenous information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: felix naumann , ulf leser , johann christoph freytag
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",y
"LEFT id: NA
RIGHT id: 1641

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as an efficient deductive database engine

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 1239

LEFT text: Web services are increasingly gaining acceptance as a framework for facilitating application-to-application interactions within and across enterprises. It is commonly accepted that a service description should include not only the interface, but also the business protocol supported by the service. The present work focuses on the formalization of an important category of protocols that includes time-related constraints (called timed protocols), and the impact of time on compatibility and replaceability analysis. We formalized the following timing constraints: C-Invoke constraints define time windows within which a service operation can be invoked while M-Invoke constraints define expiration deadlines. We extended techniques for compatibility and replaceability analysis between timed protocols by using a semantic-preserving mapping between timed protocols and timed automata, leading to the identification of a novel class of timed automata, called protocol timed automata (PTA). PTA exhibit a particular kind of silent transition that strictly increase the expressiveness of the model, yet they are closed under complementation, making every type of compatibility or replaceability analysis decidable. Finally, we implemented our approach in the context of a larger project called ServiceMosaic, a model-driven framework for Web service life-cycle management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 328

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1088

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sampling large databases for association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: hannu toivonen
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 726

LEFT text: Physical database design is important for query performance in a shared-nothing parallel database system, in which data is horizontally partitioned among multiple independent nodes. We seek to automate the process of data partitioning. Given a workload of SQL statements, we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal (or close to optimal) performance for that workload. Previous attempts use heuristic rules to make those decisions. These approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizers.We present a comprehensive solution to the problem that has been tightly integrated with the optimizer of a commercial shared-nothing parallel database system. Our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload, and to evaluate various combinations of these candidates. We compare a rank-based enumeration method with a random-based one. Our experimental results show that the former is more effective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automating physical database design in a parallel database

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jun rao , chun zhang , nimrod megiddo , guy lohman
",y
"LEFT id: NA
RIGHT id: 1953

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining using two-dimensional optimized association rules : scheme , algorithms , and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasukiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: The problem of indexing path queries in semistructured/XML databases has received considerable attention recently, and several proposals have advocated the use of structure indexes as supporting data structures for this problem. In this paper, we investigate efficient update algorithms for structure indexes. We study two kinds of updates -- the addition of a subgraph, intended to represent the addition of a new file to the database, and the addition of an edge, to represent a small incremental change. We focus on three instances of structure indexes that are based on the notion of graph bisimilarity. We propose algorithms to update the bisimulation partition for both kinds of updates and show how they extend to these indexes. Our experiments on two real world data sets show that our update algorithms are an order of magnitude faster than dropping and rebuilding the index. To the best of our knowledge, no previous work has addressed updates for structure indexes based on graph bisimilarity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1280

LEFT text: We describe the design and implementation of a new data layout scheme, called multi-dimensional clustering, in DB2 Universal Database Version 8. Many applications, e.g., OLAP and data warehousing, process a table or tables in a database using a multi-dimensional access paradigm. Currently, most database systems can only support organization of a table using a primary clustering index. Secondary indexes are created to access the tables when the primary key index is not applicable. Unfortunately, secondary indexes perform many random I/O accesses against the table for a simple operation such as a range query. Our work in multi-dimensional clustering addresses this important deficiency in database systems. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. We describe novel techniques for maintaining this physical layout efficiently and methods of processing database operations that provide significant performance improvements. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining multi-dimensional constrained gradients in data cubes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: guozhu dong , jiawei han , joyce m. w. lam , jian pei , ke wang
",n
"LEFT id: NA
RIGHT id: 1401

LEFT text: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be ""perfectly"" translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable ""closeness"" metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units. We show that, under practical assumptions, our algorithm generates the best approximate translations with respect to the closeness metric of choice. We also present a case study to show how our technique may be applied in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query translation across heterogeneous information sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 2225

LEFT text: A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient and effective clustering methods for spatial data mining

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: raymond t. ng , jiawei han
",n
"LEFT id: NA
RIGHT id: 140

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. In semantic integration, attributes are compared in a pairwise fashion to determine their equivalence. Automation is critical to integration as the volume of data or the number of databases to be integrated increase. Semiut “discovers” how to match equivalent attributes from information that can be automatically extracted from databases; as opposed to requiring human lmowledge to predefine what makes attributes equivalent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multimediaminer : a system prototype for multimedia data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: osmar r. za &#239; ane , jiawei han , ze-nian li , sonny h. chee , jenny y. chiang
",n
"LEFT id: NA
RIGHT id: 1839

LEFT text: Materialized views and view maintenance are important for data warehouses, retailing, banking, and billing applications. We consider two related view maintenance problems: 1) how to maintain views after the base tables have already been modified, and 2) how to minimize the time for which the view is inaccessible during maintenance.Typically, a view is maintained immediately, as a part of the transaction that updates the base tables. Immediate maintenance imposes a significant overhead on update transactions that cannot be tolerated in many applications. In contrast, deferred maintenance allows a view to become inconsistent with its definition. A refresh operation is used to reestablish consistency. We present new algorithms to incrementally refresh a view during deferred maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line warehouse view maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dallan quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 184

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wavelet-based histograms for selectivity estimation

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 1867

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: geominer : a system prototype for spatial data mining

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jaiwei han , krzysztof koperski , nebojsa stefanovic
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 619

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 2087

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: d ( k ) - index : an adaptive structural summary for graph-structured data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: qun chen , andrew lim , kian win ong
",n
"LEFT id: NA
RIGHT id: 1747

LEFT text: Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and dis...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the five-minute rule ten years later , and other computer storage rules of thumb

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jim gray , goetz graefe
",y
"LEFT id: NA
RIGHT id: 179

LEFT text: Object-relational database systems, a.k.a. “universal servers,” are emerging as the next major generation of commercial database system technology. Products from relational DBMS vendors including IBM, Informix, Oracle, UniSQL, and others, include object-relational features today, and all of the major vendors appear to be on course to delivering full object-relational support in their products over the next few years. In addition, the SQL3 standard is rapidly solidifying in this area. The goal of this tutorial is to explain what the key features are of object-relational database systems, review what today's products provide, and then look ahead to where these systems are heading. The presentation will be aimed at general SIGMOD audience, and should therefore be appropriate for users, practitioners, and/or researchers who want to learn about object-relational database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sap r/3 ( tutorial ) : a database application system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann , florian matthes
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 1478

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimating the selectivity of spatial queries using the ` correlation ' fractal dimension

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: alberto belussi , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 883

LEFT text: Information Dissemination applications are gaining increasing popularity due to dramatic improvements in communications bandwidth and ubiquity. The sheer volume of data available necessitates the use of selective approaches to dissemination in order to avoid overwhelming users with unnecessaryinformation. Existing mechanisms for selective dissemination typically rely on simple keyword matching or “bag of words” information retrieval techniques. The advent of XML as a standard for information exchangeand the development of query languages for XML data enables the development of more sophisticated filtering mechanisms that take structure information into account. We have developed several index organizations and search algorithms for performing efficient filtering of XML documents for large-scale information dissemination systems. In this paper we describe these techniques and examine their performance across a range of document, workload, and scale scenarios.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents for selective dissemination of information

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mehmet altinel , michael j. franklin
",y
"LEFT id: NA
RIGHT id: 873

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: Catalog management in websphere commerce suite. Share on. Author: Thomas Maguire. IBM, Hawthorne. IBM, Hawthorne. Search about this author. Authors Info & Affiliations. Publication: SIGMOD '01: Proceedings of the 2001 ACM SIGMOD international conference on Management of dataMay 2001 https://doi.org/10.1145/375663.375742. 0citation; 268 Downloads. Metrics. Total Citations0. Total Downloads268. Last 12 Months5. Last 6 weeks4. Get Citation Alerts New Citation Alert added! This alert has been successfully added

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 904

LEFT text: A book title cannot be more timely or accurate. Information rules society and it always has. The key difference is, that in our generation, the manner in which information is managed is more apparent to the everyday person and as more information becomes readily available the curse is that information can overload and intimidate us with little or no effort. Prior to the personal computer the everyday person could more easily manage the flow—such is not the case today. Throw into this fray the fact that information is a force in economics and the everyday person may become bewildered and perplexed. Many of these concerns are addressed in this excellent new book that focuses on the information economy and its effect on society and culture. In ten engaging chapters, key concepts such as pricing, versioning, rights management, recognizing and managing lock-in, networks, cooperation and compatibility, standards, and information policy are dissected, discussed, and explained. Most chapters end with lessons that reflect key points made in the chapter. The first chapter presents the foundation of the thesis of the book—the material is relatively general in nature—and sets the stage for the following nine interesting chapters. In discussing pricing, the authors cite the case of Encyclopedia Britannica and its inability to compete with the more popular and less expensive Microsoft product, Encarta. An associated concept, “versioning” is discussed and the authors show how a business can offer information products in different versions for differing markets to the benefit of the bottom line. The heady and confusion issue of copyright management, especially as related to internet economy is examined in chapter four of the book. Another issue of concern, lock-in, which results from switching from one technology to another, is discussed in chapters five and six. In chapter seven the authors discuss how the old industrial economy was driven by economies of scale whereas the information economy is driven by economics of networks. The last three chapters push the envelope and advise the reader how to affect real changes in their relationship with the information economy. The last chapter is key in that it discusses current government information policies in light of advice provided earlier in the book. This book may be one of the best to examine the theory and implications of the information economy. Although written by heavyweights in the field of economics and information management, the authors present a well written and thoughtful treatment of a subject that non-academics and academics alike should enjoy and refer to often. More importantly, this book offers direct advice that could well affect the bottom line of many entrepreneurs and existing companies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 608

LEFT text: One of the fundamental aspects of information and database systems is that they change. Moreover, in so doing they evolve, although the manner and quality of this evolution is highly dependent on the mechanisms in place to handle it. While changes in data are handled well, changes in other aspects, such as structure, rules, constraints, the model, etc., are handled to varying levels of sophistication and completeness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on semantic web and data management

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: robert meersman , amit sheth
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: Many database applications require the storage and manipulation of different versions of data objects. To satisfy the diverse needs of these applications, current database systems support versioning at a very low level. This article demonstrates that application-independent versioning can be supported at a significantly higher level. In particular, we extend the EXTRA data model and EXCESS query language so that configurations can be specified conceptually and non-procedurally. We also show how version sets can be viewed multidimensionally, thereby allowing configurations to be expressed at a higher level of abstraction. The resulting model integrates and generalizes ideas in CAD systems, CASE systems, and temporal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 249

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 986

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1660

LEFT text: We present a framework for designing, in a declarative and flexible way, efficient migration programs and an undergoing implementation of a migration tool called RelOO whose targets are any ODBC compliant system on the relational side and the 02 system on the object side. The framework consists of (i) a declarative language to specify database transformations from relations to objects, but also physical properties on the object database (clustering and sorting) and (ii) an algebrabased program rewriting technique which optimizes the migration processing time while taking into account physical properties and transaction decomposition.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: application of oodb and sgml techniques in text database : an electronic dictionary system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jian zhang
",n
"LEFT id: NA
RIGHT id: 276

LEFT text: We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting constraint-like data characterizations in query optimization

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: parke godfrey , jarek gryz , calisto zuzarte
",n
"LEFT id: NA
RIGHT id: 1041

LEFT text: Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 374

LEFT text: Abstract A Real-Time DataBase System (RTDBS) can be viewed as an amalgamation of a conventional DataBase Management System (DBMS) and a real-time system. Like a DBMS, it has to process transactions and guarantee ACID database properties. Furthermore, it has to operate in real-time, satisfying time constraints imposed on transaction commitments. A RTDBS may exist as a stand-alone system or as an embedded component in a larger multidatabase system. The publication in 1988 of a special issue of ACM SIGMOD Record on Real-Time DataBases signaled the birth of the RTDBS research area---an area that brings together researchers from both the database and real-time systems communities. Today, almost eight years later, I am pleased to present in this special section of ACM SIGMOD Record a review of recent advances in RTDBS research. There were 18 submissions to this special section, of which eight papers were selected for inclusion to provide the readers of ACM SIGMOD Record with an overview of current and future research directions within the RTDBS community. In this paper, I will summarize these directions and provide the reader with pointers to other publications for further information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: real-time index concurrency control

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jayant r. haritsa , s. seshadri
",y
"LEFT id: NA
RIGHT id: 303

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: eamonn keogh , kaushik chakrabarti , michael pazzani , sharad mehrotra
",n
"LEFT id: NA
RIGHT id: 1623

LEFT text: We present a comprehensive performance evaluation of transitive closure (reachability) algorithms for databases. The study is based upon careful implementations of the algorithms, measures page I/O, and covers algorithms for full transitive closure as well as partial transitive closure (finding all successors of each node in a set of given source nodes). We examine a wide range of acyclic graphs with varying density and “locality” of arcs in the graph. We also consider query parameters such as the selectivity of the query, and system parameters such as the buffer size and the page and successor list replacement policies. We show that significant cost tradeoffs exist between the algorithms in this spectrum and identify the factors that influence the performance of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a performance study of transitive closure algorithms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: shaul dar , raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 215

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 1427

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 84

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: chorochronos : a research network for spatiotemporal database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: andrew frank , stephane grumbach , ralf hartmut g &#252; ting , christian s. jensen , manolis koubarakis , nikos lorentzos , yannis manolopoulos , enrico nardelli , barbara pernici , hans-j &#246; rg schek , michel scholl , timos sellis , babis theodoulidis , peter widmayer
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 187

LEFT text: We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose a new error metric which has a reliable estimator and can still be exploited by query optimizers to influence the choice of execution plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: random sampling for histogram construction : how much is enough ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: surajit chaudhuri , rajeev motwani , vivek narasayya
",y
"LEFT id: NA
RIGHT id: 1719

LEFT text: We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 464

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql and management of external data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jim melton , jan-eike michels , vanja josifovski , krishna kulkarni , peter schwarz , kathy zeidenstein
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Currently, the Internet provides access to a very large number and wide variety of information sources (e.g., textual databases, sites containing technical reports, directory listings), and systems to access these sources (e.g., World Wide Web, Gopher, WAIS). The challenge is to provide easy, efficient, robust and secure access to this information and other kinds (e.g., relational and object oriented databases). This aim of this panel is to explore whether there are any new technical problems, relevant to the Database field, that need to be solved in order to realize such global information systems. In particular, we debate whether existing techniques from database systems (e.g., multidatabases and distributed databases) can be applied or straigtitforwardly extended to global information systems. Furthermore, we attempt to establish realistic goals for database technologies in global information systems. Some of the specific issues discussed are the following:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 1684

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the impact of database research on industrial products ( panel )

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , dan fishman , david lomet , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 1101

LEFT text: Integrity constraint checking for stratifiable deductive databases has been studied by many authors. However, most of these methods may perform unnecessary checking if the update is irrelevant to the constraints. [Lee94] proposed a set called relevant set which can be incorporated in these works to reduce unnecessary checking. [Lee94] adopts a top-down approach and makes use of constants and evaluable functions in the constraints and deductive rules to reduce the search space. In this paper, we further extend this idea to make use of relational predicates, instead of only constants and evaluable functions in [Lee94]. We first show that this extension is not a trivial one as extra database retrieval cost is incurred. We then present a new method to construct a pre-test which can be incorporated in most existing methods to reduce the average checking costs in terms of database accesses by a significant factor. Our method also differs from other partial checking methods as we can handle multiple updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: further improvements on integrity constraint checking for stratifiable deductive databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sin yeung lee , tok wang ling
",y
"LEFT id: NA
RIGHT id: 614

LEFT text: Many of today’s applications need massive real-time data processing. In-memory database systems have become a good alternative for these requirements. These systems maintain the primary copy of the database in the main memory to achieve high throughput rates and low latency. However, a database in RAM is more vulnerable to failures than in traditional disk-oriented databases because of the memory volatility. DBMSs implement recovery activities (logging, checkpoint, and restart) for recovery proposes. Although the recovery component looks similar in disk- and memory-oriented systems, these systems differ dramatically in the way they implement their architectural components, such as data storage, indexing, concurrency control, query processing, durability, and recovery. This survey aims to provide a thorough review of in-memory database recovery techniques. To achieve this goal, we reviewed the main concepts of database recovery and architectural choices to implement an in-memory database system. Only then, we present the techniques to recover in-memory databases and discuss the recovery strategies of a representative sample of modern in-memory databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 954

LEFT text: Overall performance can be improved by algorithms that enable operations to adjust their memory usage at run time in response to the actual size of their inputs and fluctuations in total memory demand. Sorting is a frequent operation in database systems. It is used not only to produce sorted output, but also in many sort-based algorithms, such as grouping with aggregation, duplicate removal, sort-merge join and set operations. Sorting can also improve the efficiency of algorithms like nested-loop joins and row retrieval via an index. This paper concentrates on dynamic memory adjustment for sorting but the same approach can be applied to other memory intensive operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic memory adjustment for external mergesort

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: weiye zhang , per - &#197; ke larson
",y
"LEFT id: NA
RIGHT id: 694

LEFT text: Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting statistics on query expressions for optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 540

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the cougar approach to in-network query processing in sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong yao , johannes gehrke
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 984

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the discovery of interesting patterns in association rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , sameer mahajan , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 1530

LEFT text: QBI is an icon-based query processing and exploration facility for large distributed databases [3]. As opposed to other interactive query interfaces, it combines (1) a pure iconic specification, i.e., no diagrams of any form, only icon manipulation, with (2) intensional browsing or metaquery tools that assist in the formulation of complete queries without involving path specification or access to the actual data in the database.Path expressions are automatically generated by QBI and irrespective of their length, represented by a single icon, allowing for better use of the screen. It requires no special knowledge of the content of the underlying database nor understanding of the details of the database schema. Hence, QBI is domain independent and equally useful to both unsophisticated and expert users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qbi : query by icons

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: antonio massari , stefano pavani , lorenzo saladini , panos k. chrysanthis
",y
"LEFT id: NA
RIGHT id: 84

LEFT text: Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: chorochronos : a research network for spatiotemporal database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: andrew frank , stephane grumbach , ralf hartmut g &#252; ting , christian s. jensen , manolis koubarakis , nikos lorentzos , yannis manolopoulos , enrico nardelli , barbara pernici , hans-j &#246; rg schek , michel scholl , timos sellis , babis theodoulidis , peter widmayer
",n
"LEFT id: NA
RIGHT id: 1684

LEFT text: Data security issues today go far beyond the traditional questions of grant/revoke in an RDBMS. We will discuss what the new research agenda should be.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the impact of database research on industrial products ( panel )

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , dan fishman , david lomet , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 93

LEFT text: Semantic Binary Object-oriented Data Model (Sem-ODM) provides an expressive data model (similar to Object-oriented Data Models) with a well-known declarative query facility - SQL (similar to relational databases). Advantages of using Sem-ODM include (i.) friendlier and more intelligent generic user interfaces; (ii.) comprehensive enforcement of integrity constraints; (iii.) greater flexibility; (iv.) substantially shorter application programs; and (v.) easier query facility. SemanticAccess is a set of tools developed to provide a semantic interface to Semantic Binary Object-oriented Databases (Sem-ODB) as well as relational databases. This presentation focuses on the system architecture of SemanticAccess including Semantic Binary Object-oriented Data Model, Semantic SQL query language, Semantic Binary Database and a wrapper developed for relational databases. 1. Purpose

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 1163

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a database model for object dynamics

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: m. p. papazoglou , b. j. kr &#228; mer
",n
"LEFT id: NA
RIGHT id: 1047

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views. The study shows that the MiniCon scales up well and significantly outperforms the previous algorithms. We describe an extension of the MiniCon to handle comparison predicates, and show its performance experimentally. Finally, we describe how the MiniCon can be extended to the context of query optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering queries with aggregation using views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: divesh srivastava , shaul dar , h. v. jagadish , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: This paper describes the design and implementation of PEST0 (Portable Explorer of Snuctured Objects), a user interface that supports browsing and querying of object databases. PEST0 allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (“query-in-place”) that presents querying as a natural extension of browsing. PEST0 is designed to be portable to any object database system that supports a high-level query language; in addition, PEST0 is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text). uniformly and manipulated using an object-oriented dialect of SQL. One component of this project, which is joint work between IBM Almaden and the University of Wisconsin, is the development of a graphical user interface called PEST0 (Portable Explorer of STructured Objects). We refer to the PEST0 interface as a query/browser, as it marries navigational object browsing’ with declarative querying; it integrates browsing and querying via a “query-in-place” paradigm that provides a powerful yet natural user interface for exploring the contents of object databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 141

LEFT text: TimesTen Performance Software's Front-Tier product is an application-tier data cache that inter-operates with disk-based relational database management systems (RDBMSs) to achieve breakthrough response time and throughput, scalability in transaction load, high availability, and ease of administration and deployment. Front-Tier caches frequently used subsets of the corporate database on multiple servers in the application tier and supports SQL queries and updates to the caches. The caches may or may not be overlapping, are kept synchronized with the corporate database and with each other, and may be dynamically reconfigured to contain different subsets of the corporate database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xmas : an extensible main-memory storage system for high-performance applications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jang ho park , yong sik kwon , ki hong kim , sang ho lee , byoung dae park , sang kyun cha
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text:  In addition it constructs a special node Authors() and connects it to all pages corresponding to ""Author""s. The output graph is called SiteGraph. One way to write this in StruQL is: input DataGraph where Root(x); x ! ! y; y ! l ! z; l in f""Paper"", ""TechReport"", ""Title"", ""Abstract"", ""Author""g create Authors(); Page(y); Page(z) link Page(y) ! l ! Page(z) where x ! ! y1; y1 ! ""Author"" ! z1 link Authors() ! ""Author"" ! Page(z1) output SiteGraph 2 In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph. Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together: input DataGraph where Root

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",y
"LEFT id: NA
RIGHT id: 1125

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1840

LEFT text: In order to access information from a variety of heterogeneous information sources, one has to be able to translate queries and data from one data model into another. This functionality is provided by so-called (source) wrappers [4,8] which convert queries into one or more commands/queries understandable by the underlying source and transform the native results into a format understood by the application. As part of the TSIMMIS project [1, 6] we have developed hard-coded wrappers for a variety of sources (e.g., Sybase DBMS, WWW pages, etc.) including legacy systems (Folio). However, anyone who has built a wrapper before can attest that a lot of effort goes into developing and writing such a wrapper. In situations where it is important or desirable to gain access to new sources quickly, this is a major drawback. Furthermore, we have also observed that only a relatively small part of the code deals with the specific access details of the source. The rest of the code is either common among wrappers or implements query and data transformation that could be expressed in a high level, declarative fashion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: template-based wrappers in the tsimmis system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joachim hammer , h &#233; ctor garc &#237; a-molina , svetlozar nestorov , ramana yerneni , marcus breunig , vasilis vassalos
",y
"LEFT id: NA
RIGHT id: 301

LEFT text: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptable query optimization and evaluation in temporal middleware

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1294

LEFT text: Computational grids provide access to distributed compute resources and distributed data resources, creating unique opportunities for improved access to information. When data repositories are accessible from any platform, applications can be developed that support nontraditional uses of computing resources. Environments thus enabled include knowledge networks, in which researchers collaborate on common problems by publishing results in digital libraries, and digital government, in which policy decisions are based on knowledge gleaned from teams of experts accessing distributed data repositories. In both cases, users access data that has been turned into information through the addition of metadata that describes its origin and quality. Information-based computing within computational grids will enable collective advances in knowledge [396]. In this view of the applications that will dominate in the future, application development will be driven by the need to process and analyze information , rather than the need to simulate a physical process. In addition to accessing specific data sets, applications will need to use information discovery interfaces [138] and dynamically determine which data sets to process. In Section 5.1, we discuss how these applications will evolve, and we illustrate their new capabilities by presenting projects now under way that use some concepts implicit within grid environments. Data-intensive applications that will require the manipulation of terabytes of data aggregated across hundreds of files range from comparisons of numerical simulation output, to analyses of satellite observation data streams, to searches for homologous structures

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: enabling end-users to construct data-intensive web-sites from xml repositories : an example-based approach

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: atsuyuki morishima , seiichi koizumi , hiroyuki kitagawa , satoshi takano
",y
"LEFT id: NA
RIGHT id: 1883

LEFT text: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable “views” of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1519

LEFT text: We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be ""10% of married people between age 50 and 60 have at least 2 cars"". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a ""greater-than-expected-value"" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an effective hash-based algorithm for mining association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jong soo park , ming-syan chen , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 529

LEFT text: Authors and publishers who wish their publications to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4. All relevant books received will be listed, but not all can be reviewed. Technical reports (other than dissertations) will not be listed or reviewed. Authors should be aware that some publishers will not send books for review (even when instructed to do so); authors wishing to inquire as to whether their book has been received for review may contact the book review editor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: combining fuzzy information : an overview

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ronald fagin
",n
"LEFT id: NA
RIGHT id: 1297

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: automated resolution of semantic heterogeneity in multidatabases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. w. bright , a. r. hurson , s. pakzad
",n
"LEFT id: NA
RIGHT id: 1854

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: delaunay : a database visualization system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang
",n
"LEFT id: NA
RIGHT id: 1189

LEFT text: Research work in programming languages and in database systems is combating the same problems of scale, change and complexity. This paper looks at the present difficulties of relating persistent data with changing programs. It illustrates the influence that the present interfaces have on programming methodology and algorithm design. It recognises the need for new language primitives to encapsulate database concepts and a few putative primitives are examined. It is suggested that such primitives could simplify the use of databases by programmers. These ideas are illustrated with examples from geometric modelling using Algol 68.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: fabio casati , ming-chien shan , dimitrios georgakopoulos
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: Traditional indexes aim at optimizing the node accesses during query processing, which, however, does not necessarily minimize the total cost due to the possibly large number of random accesses. In this paper, we propose a general framework for adaptive indexes that improve overall query cost. The performance gain is achieved by allowing index nodes to contain a variable number of disk pages. Update algorithms dynamically re-structure adaptive indexes depending on the data and query characteristics. Extensive experiments show that adaptive B- and R-trees significantly outperform their conventional counterparts, while incurring minimal update overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 551

LEFT text: I first came across the AMS paper when I started getting interested in the data-streaming area, in the spring of 2001. Reading this paper was a real eye-opener for me. It was just amazing to see how simple randomization ideas and basic probabilistic tools (like the Chebyshev inequality and the Chernoff bound) can come together to provide elegant, space-efficient randomized approximation algorithms for estimation problems that, at first glance, would seem impossible to solve. The second-moment method described in the AMS paper is essentially the father of all “sketch-based” techniques for data-stream management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 649

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1084

LEFT text: Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",y
"LEFT id: NA
RIGHT id: 342

LEFT text: Current State of Health Promotion The science and art of health promotion has made very impressive progress in the past two decades. It has evolved from :in innovative idea that made conceptual sense, but had no scientific backing to a maturing field supported by over ],000 empirical studies which demonstrate the positive health and financial impact of programs, and practiced by virtually all major employers in the US. Despite this progress, health promotion is not a part of mainstream medicine. Only a small fraction, probably less than 1%, of the $1.149 trillion spent annually on medical care is spent on health promotion. Despite the progress we h~.ve made on developing the science of health promotion, it is not recognized as a mature science by any respected scientific group. Repeated analyses conclude that roughly half of all prematu:ve deaths in the United States are from lifestyle related causes. Indeed, conservative estimates are that tobacco kills 450,000; obesity kills 300,000; and alcohol kills 100,000. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 301

LEFT text: Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptable query optimization and evaluation in temporal middleware

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 1667

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1990

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cost-based optimization for magic : algebra and implementation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , joseph m. hellerstein , hamid pirahesh , t. y. cliff leung , raghu ramakrishnan , divesh srivastava , peter j. stuckey , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1104

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: applying database technology in the adsm mass storage system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: luis-felipe cabrera , robert rees , wayne hineman
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: LeSelect is a mediator system which allows scientists to publish their resources (data and programs) so they can be transparently accessed. The scientists can typically issue queries which access distributed published data and involve the execution of expensive functions (corresponding to programs). Furthermore, the queries can involve large objects, such as images (e.g. archived meteorological satellite data). In this context, the costs of transmitting large objects and invoking expensive functions are the dominant factors of execution time. In this paper, we first propose three query execution techniques which minimize these costs by taking full advantage of the distributed architecture of mediator systems like LeSelect. Then we devise parallel processing strategies for queries including expensive functions. Based on experimentation, we show that it is hard to predict the optimal execution order when dealing with several functions. We propose a new hybrid parallel technique to solve this problem and give some experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 664

LEFT text: One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a thery and practice of intellectual property cost management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: business data management for business-to-business electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: christoph quix , mareike schoop , manfred jeusfeld
",n
"LEFT id: NA
RIGHT id: 493

LEFT text: We study the problem of efficient maintenance of materialized views that may contain duplicates. This problem is particularly important when queries against such views involve aggregate functions, which need duplicates to produce correct results. Unlike most work on the view maintenance problem that is based on an algorithmic approach, our approach is algebraic and based on equational reasoning. This approach has a number of advantages: it is robust and easily extendible to new language constructs, it produces output that can be used by query optimizers, and it simplifies correctness proofs.We use a natural extension of the relational algebra operations to bags (multisets) as our basic language. We present an algorithm that propagates changes from base relations to materialized views. This algorithm is based on reasoning about equivalence of bag-valued expressions. We prove that it is correct and preserves a certain notion of minimality that ensures that no unnecessary tuples are computed. Although it is generally only a heuristic that computing changes to the view rather than recomputing the view from scratch is more efficient, we prove results saying that under normal circumstances one should expect, the change propagation algorithm to be significantly faster and more space efficient than complete recomputing of the view. We also show that our approach interacts nicely with aggregate functions, allowing their correct evaluation on views that change.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental maintenance of recursive views using relational calculus/sql

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: guozhu dong , jianwen su
",n
"LEFT id: NA
RIGHT id: 810

LEFT text:  In this paper we present a second enhancement: a single operator that lets the analyst get summarized reasons for drops or increases observed at an aggregated level. This eliminates the need to manually drill-down for such reasons. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design a dynamic programming algorithm that requires only one pass of the data improving significantly over our initial greedy algorithm that required multiple passes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: explaining differences in multidimensional aggregates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sunita sarawagi
",y
"LEFT id: NA
RIGHT id: 1789

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel mining algorithms for generalized association rules with classification hierarchy

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: takahiko shintani , masaru kitsuregawa
",n
"LEFT id: NA
RIGHT id: 249

LEFT text: DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentation of large tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework is being implemented in a tool that has been already successfully applied to a variety of real applications by a number of user groups.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 1870

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1849

LEFT text: The popularity of the World-Wide Web (WWW) has made it a prime vehicle for disseminating information. The relevance of database concepts to the problems of managing and querying this information has led to a signi cant body of recent research addressing these problems. Even though the underlying challenge is the one that has been traditionally addressed by the database community { how to manage large volumes of data { the novel context of the WWW forces us to signi cantly extend previous techniques. The primary goal of this survey is to classify the di erent tasks to which database concepts have been applied, and to emphasize the technical innovations that were required to do so.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases on the web : technologies for federation architectures and case studies

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be ""10% of married people between age 50 and 60 have at least 2 cars"". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a ""greater-than-expected-value"" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 1958

LEFT text: OptimizingQueriesOnCompressedBitmapsSihem Amer-YahiaAT&T Labs{Researchsihem@research.att.comTheo doreJohnsonjohnsont@research.att.comAbstractBitmap indices are used by DBMS's to accelerate decision supp ort queries.A signi cant advantage ofbitmap indices is that complex logical selection op erations can b e p erformed very quickly, by p erformingbit-wiseAND,OR,andNOTop erators.Althoughbitmapindicescanb espaceinecientforhighcardinalityattributes,the space use of compressed bitmapscompares well to other indexingmetho ds.Oracle and Sybase IQ are two commercial pro ducts that make extensive use of compressed bitmap indices.Our recent research showed that there are several fast algorithmsfor evaluatingBo oleanop eratorson compressedbitmaps.Dep endingon the natureof the op erandbitmaps(theirformat, densityandclusterdness) and the op eration to b e p erformed (AND, NOT, ...), these algorithms can have executiontimes that are orders of magnitude di erent.Cho osing an algorithm for p erforming a Bo olean op erationhas global e ects in the Bo olean query expression, requiring global optimization.We present a linear timedynamicprogrammingsearch strategy based on a cost mo delto optimizequeryexpressionevaluationplans.We alsopresentrewritingheuristicsthat rewritethe queryexpressionto anequivalenonetoencourage b etter algorithmsassignments.Our p erformance results show that the optimizerrequiresanegligibl e amount of time to execute, and that optimized complex queries can execute up to three timesfaster than unoptimized queries on real data.1Intro ductionAbitmap indexis a bit string in which each bit is mapp ed to a record ID (RID) of a relation.A bit in thebitmap index is set (to 1) if the corresp onding RID has prop ertyP(i.e., the RID represents a customer thatlives in New York), and is reset (to 0) otherwise.In typical usage, the predicatePis true for a record if it hasthe valueafor attributeA.One such predicate is asso ciated to one bitmap index for each unique value ofthe attributeA.The predicates can b e more complex, for example bitslice indices [OQ97] and precomputedcomplex selection predicates [HEP99].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries over multimedia repositories

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 1426

LEFT text: One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure  (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: near neighbor search in large metric spaces

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sergey brin
",n
"LEFT id: NA
RIGHT id: 2038

LEFT text: Elasticity is highly desirable for stream systems to guarantee low latency against workload dynamics, such as surges in arrival rate and fluctuations in data distribution. Existing systems achieve elasticity using a resource-centric approach that repartitions keys across the parallel instances, i.e., executors, to balance the workload and scale operators. However, such operator-level repartitioning requires global synchronization and prohibits rapid elasticity. We propose an executor-centric approach that avoids operator-level key repartitioning and implements executors as the building blocks of elasticity. By this new approach, we design the Elasticutor framework with two level of optimizations: i) a novel implementation of executors, i.e., elastic executors, that perform elastic multi-core execution via efficient intra-executor load balancing and executor scaling and ii) a global model-based scheduler that dynamically allocates CPU cores to executors based on the instantaneous workloads. We implemented a prototype of Elasticutor and conducted extensive experiments. We show that Elasticutor doubles the throughput and achieves up to two orders of magnitude lower latency than previous methods for dynamic workloads of real-world applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: chain : operator scheduling for memory minimization in data stream systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , shivnath babu , rajeev motwani , mayur datar
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: Since relational database management systems typically support only diadic join operators as primitive operations, a query optimizer must choose the “best” scquence of two-way joins to achieve the N-way join of tables requested by a query. The computational complexity of this optimization process is dominated by the number of such possible sequences that must bc evaluated by the optimizer. This paper describes and measures the performance of the Starburst join enumerator, which can parameterically adjust for each query the space of join sequences that arc evaluated by the optimizer to allow or disallow (I) composite tables (i.e., tables that are themselves the result of a join) as the inner operand of a join and (2) joins between two tables having no join predicate linking them (i.e., Cartesian products). To limit the size of their optimizer’s search space, most earlier systems excludcd both of these types of plans, which can exccutc significantly faster for some queries. Dy experimentally varying the parameters of the Starburst join enumerator, we have validated analytic formulas for the number of join sequcnccs under a variety of conditions, and have proven their dependence upon the “shape” of the query. Specifically, ‘linear” queries, in which tables arc connectcd by binary predicates in a straight lint, can hc optimized in polynomial time. llence the dynamic programming techniques of System R and R* can still be used to optimize linear queries of as many as 100 tables in a reasonable amount of time! A query optimizer in a relational DRMS translates non-procedural queries into a pr0cedura.l plan for execution, typically hy generating many alternative plans, estimating the execution cost of each, and choosing the plan having the lowest estimated cost. Increasing this set offeasilile plans that it evaluates improves the chances but dots not guarantee! that it will find a bcttct plan, while increasing the (compile-time) cost for it to optimize the query. A major challenge in the design of a query optimizer is to ensure that the set of feasible plans contains cflicient plans without making the :set too big to he gcncratcd practically.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 858

LEFT text: A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for ""Eager Compensating Algorithm""), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra ""compensating"" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance issues in incremental warehouse maintenance

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wilburt labio , jun yang , yingwei cui , hector garcia-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 178

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: electronic commerce : tutorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: nabil r. adam , yelena yesha
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: In late 2000, work was completed on yet another part of the SQL standard, to which we introduced our readers in an earlier edition of this column.Although SQL database systems manage an enormous amount of data, it certainly has no monopoly on that task. Tremendous amounts of data remain in ordinary operating system files, in network and hierarchical databases, and in other repositories. The need to query and manipulate that data alongside SQL data continues to grow. Database system vendors have developed many approaches to providing such integrated access.In this (partly guested) article, SQL's new part, Management of External Data (SQL/MED), is explored to give readers a better notion of just how applications can use standard SQL to concurrently access their SQL data and their non-SQL data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 951

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: logical and physical versioning in main memory databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: rajeev rastogi , s. seshadri , philip bohannon , dennis w. leinbaugh , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1203

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: mariposa : a wide-area distributed database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael stonebraker , paul m. aoki , witold litwin , avi pfeffer , adam sah , jeff sidell , carl staelin , andrew yu
",n
"LEFT id: NA
RIGHT id: 126

LEFT text: The goal of the InfoSleuth project at MCC is to exploit and synthesize new technologies into a unified system that retrieves and processes information in an ever-changing network of information sources. InfoSleuth has its roots in the Carnot project at MCC, which specialized in integrating heterogeneous information bases. However, recent emerging technologies such as internetworking and the World Wide Web have significantly expanded the types, availability, and volume of data available to an information management system. Furthermore, in these new environments, there is no formal control over the registration of new information sources, and applications tend to be developed without complete knowledge of the resources that will be available when they are run. Federated database projects such as Carnot that do static data integration do not scale up and do not cope well with this ever-changing environment. On the other hand, recent Web technologies, based on keyword search engines, are scalable but, unlike federated databases, are incapable of accessing information based on concepts. In this experience paper, we describe the architecture, design, and implementation of a working version of InfoSleuth. We show how InfoSleuth integrates new technological developments such as agent technology, domain ontologies, brokerage, and internet computing, in support of mediated interoperation of data and services in a dynamic and open environment. We demonstrate the use of information brokering and domain ontologies as key elements for scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: agent-based semantic interoperability in infosleuth

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jerry fowler , brad perry , marian nodine , bruce bargmeyer
",y
"LEFT id: NA
RIGHT id: 1715

LEFT text: In the early 1980's, researchers recognized that semantic information stored in databases as integrity constraints could be used for query optimization. A new set of techniques called semantic query optimization (SQO) was developed. Some of the ideas developed for SQO have been used commercially, but to the best of our knowledge, no extensive implementations of SQO exist today. In this paper, we describe an implementation of two SQO techniques, Predicate Introduction and Join Elimination, in DB2 Universal Database. We present the implemented algorithms and performance results using the TPCD and APB-1 OLAP benchmarks. Our experiments show that SQO can lead to dramatic query performance improvements. A crucial aspect of our implementation of SQO is the fact that it does not rely on complex integrity constraints (as many previous SQO techniques did); we use only referential integrity constraints and check constraints.  

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementation of magic-sets in a relational database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: inderpal singh mumick , hamid pirahesh
",n
"LEFT id: NA
RIGHT id: 780

LEFT text: This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: proximity search in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roy goldman , narayanan shivakumar , suresh venkatasubramanian , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 436

LEFT text: Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of cube automatic summary tables

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wolfgang lehner , richard sidle , hamid pirahesh , roberta wolfgang cochrane
",n
"LEFT id: NA
RIGHT id: 147

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the multidimensional database system rasdaman

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. baumann , a. dehmel , p. furtado , r. ritsch , n. widmann
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: Abstract.This paper contains an overview of the technology used in the query processing and optimization component of Oracle Rdb, a relational database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation. Oracle Rdb is a production system that supports the most demanding database applications, runs on multiple platforms and in a variety of environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 0

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the wasa2 object-oriented workflow management system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: gottfried vossen , mathias weske
",y
"LEFT id: NA
RIGHT id: 755

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mpeg-7 and multimedia database systems

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: harald kosch
",y
"LEFT id: NA
RIGHT id: 1785

LEFT text: Publisher Summary  This chapter presents the first XPath query evaluation algorithm that runs in polynomial time with respect to the size of both the data and of the query. XPath has been proposed by the W3C as a practical language for selecting nodes from XML document trees. XPath is important because of its potential application as an XML query language per se, it being at the core of several other XML-related technologies, such as XSLT, XPointer, and XQuery, and the great and well-deserved interest such technologies receive. Since XPath and related technologies will be tested in ever-growing deployment scenarios, its implementations need to scale well both with respect to the size of the XML data and the growing size and intricacy of the queries (usually referred to as combined complexity).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1373

LEFT text: Electronic commerce systems (retail, auction, etc.) are good examples of data-based systems that operate under correctness and resilience requirements of a transactional nature but go beyond conventional databases, as they are formed by the aggregation of heterogeneous, autonomous components. We introduce a framework to specify, analyze and reason about the behavior of such systems, focusing on how they are designed to make consistent progress in spite of failures. The contributions are: (a) the introduction of the Guarantee abstraction to deal with transactional applications; (b) a framework based on guarantees and protocols to specify the behaviors of systems and their components and reason about the properties of systems and their components; and (c) application of the framework to a common e-commerce scenario. The framework allows the hierarchical composition of transactional systems and their properties, as well as the proofs of these properties: we specify a system's behavior at its most abstract level, and proceed to decompose the specification mirroring the structure of the system's components, considering the role of guarantee-preserving component systems and recovery in each case. In particular we show how the lower-level properties are supported by the component systems, which we also characterize within the same framework.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximating aggregate queries about web pages via random walks

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ziv bar-yossef , alexander berg , steve chien , jittat fakcharoenphol , dror weitz
",n
"LEFT id: NA
RIGHT id: 2076

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xrank : ranked keyword search over xml documents

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: lin guo , feng shao , chavdar botev , jayavel shanmugasundaram
",n
"LEFT id: NA
RIGHT id: 333

LEFT text: Publisher Summary  Traditional databases allow for the storage and retrieval of large amounts of data, but do not make any concessions for uncertainty in the data. In many domains, it is difficult, if not impossible, to state all information with 100% certainty. Scientific research, for example, is subject to a great deal of uncertainty and error that cannot be modeled by traditional database systems. Error-prone experimental machinery, polluted samples, and simple human error are a few of the many possible sources of this uncertainty. With the recent importance of the web, and the many textual (and HTML encoded) sources of information that it makes available, information extraction has become a hot area. The idea is to use natural language analysis tools to create structured representations of free-form text documents. This information extraction is an error-prone endeavor: even the best systems can only hope to be right part of the time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probabilistic object bases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: thomas eiter , james j. lu , thomas lukasiewicz , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 266

LEFT text: Catalog management in websphere commerce suite. Share on. Author: Thomas Maguire. IBM, Hawthorne. IBM, Hawthorne. Search about this author. Authors Info & Affiliations. Publication: SIGMOD '01: Proceedings of the 2001 ACM SIGMOD international conference on Management of dataMay 2001 https://doi.org/10.1145/375663.375742. 0citation; 268 Downloads. Metrics. Total Citations0. Total Downloads268. Last 12 Months5. Last 6 weeks4. Get Citation Alerts New Citation Alert added! This alert has been successfully added

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: catalog management in websphere commerce suite

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: thomas maguire
",y
"LEFT id: NA
RIGHT id: 441

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximating multi-dimensional aggregate range queries over real attributes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dimitrios gunopulos , george kollios , vassilis j. tsotras , carlotta domeniconi
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 1653

LEFT text: ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a survey and critique of advanced transaction models

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: This paper describes the XSB system, and its use as an in-memory deductive database engine. XSB began from a Prolog foundation, and traditional Prolog systems are known to have serious deficiencies when used as database systems. Accordingly, XSB has a fundamental bottom-up extension, introduced through tabling (or memoing)[4], which makes it appropriate as an underlying query engine for deductive database systems. Because it eliminates redundant computation, the tabling extension makes XSB able to compute all modularly stratified datalog programs finitely and with polynomial data complexity. For non-stratified programs, a meta-interpreter with the same properties is provided. In addition XSB significantly extends and improves the indexing capabilities over those of standard Prolog. Finally, its syntactic basis in HiLog [2], lends it flexibility for data modelling. The implementation of XSB derives from the WAM [25], the most common Prolog engine. XSB inherits the WAM's efficiency and can take advantage of extensive compiler technology developed for Prolog. As a result, performance comparisons indicate that XSB is significantly faster than other deductive database systems for a wide range of queries and stratified rule sets. XSB is under continuous development, and version 1.3 is available through anonymous ftp.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 1975

LEFT text: In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial hash-joins

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ming-ling lo , chinya v. ravishankar
",n
"LEFT id: NA
RIGHT id: 971

LEFT text: Operators of large networks and providers of network services need to monitor and analyze the network traffic flowing through their systems. Monitoring requirements range from the long term (e.g., monitoring link utilizations, computing traffic matrices) to the ad-hoc (e.g. detecting network intrusions, debugging performance problems). Many of the applications are complex (e.g., reconstruct TCP/IP sessions), query layer-7 data (find streaming media connections), operate over huge volumes of data (Gigabit and higher speed links), and have real-time reporting requirements (e.g., to raise performance or intrusion alerts).We have found that existing network monitoring technologies have severe limitations. One option is to use TCPdump to monitor a network port and a user-level application program to process the data. While this approach is very flexible, it is not fast enough to handle gigabit speeds on inexpensive equipment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: heterogeneous database query optimization in db2 universal datajoiner

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shivakumar venkataraman , tian zhang
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: There are two key motivations for this work. First, the implementation of object-oriented databases has grown to a significant number. Second, there has been a need for integrated access of information from multiple data sources. The multidatabase system has been proposed as a solution for integrated access of data from multiple distributed, heterogeneous, and autonomous database systems. To present a single database illusion to its users, a multidatabase system maintains a single global database schema, which is the integration of all component database schemas and against which its users will issue queries and updates. Many approaches to schema integration have been proposed in the literature. Most of the previous approaches are concerned with relational databases. In this paper, we propose an approach to the integration of database schemas between object-oriented databases in a multidatabase system environment. The underlying principle of our approach is to facilitate the automation of the schema integration process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1500

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a database interface for file update

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: serge abiteboul , sophie cluet , tova milo
",n
"LEFT id: NA
RIGHT id: 211

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: update propagation protocols for replicated databates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yuri breitbart , raghavan komondoor , rajeev rastogi , s. seshadri , avi silberschatz
",y
"LEFT id: NA
RIGHT id: 1849

LEFT text: Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases on the web : technologies for federation architectures and case studies

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer
",n
"LEFT id: NA
RIGHT id: 2129

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1784

LEFT text: In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficiently mining long patterns from databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roberto j. bayardo , jr.
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 1061

LEFT text: The transaction concept in computing goes back to the early days of computerized data processing. It has developed and evolved over the years both in terms of formal theory and practical application. This evolutionary process has been driven in large part by applications that require transactionlike properties. Newly emerging applications include several that involve people in a time-dependent role. The new forms of human involvement in transaction processing required by these applications are generating new systemslevel challenges. Likewise, these needs present challenges and opportunities from a theoretical standpoint. This talk reviews the history of synergy between theory and practice in the area of transaction processing, and considers currently emerging needs from that perspective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the double life of the transaction abstraction : fundamental principle and evolving system concept

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: henry f. korth
",y
"LEFT id: NA
RIGHT id: 378

LEFT text: The education industry has a very poor record of productivity gains. In this brief article, I outline some of the ways the teaching of a college course in database systems could be made more efficient, and staff time used more productively. These ideas carry over to other programming-oriented courses, and many of them apply to any academic subject whatsoever. After proposing a number of things that could be done, I concentrate here on a system under development, called OTC (On-line Testing Center), and on its methodology of ""root questions.""

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 334

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 759

LEFT text: trees that minimize the computation and communication costs of parallel execution. We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: bringing order to query optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 982

LEFT text: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: inferring function semantics to optimize queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mitch cherniack , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Computers running database management applications often manage large amounts of data. Typically, the price of the I/O subsystem is a considerable portion of the computing hardware. Fierce price competition demands every possible savings. Lossless data compression methods, when appropriately integrated with the dbms, yield signiflcant savings. Roughly speaking, a slight increase in cpu cycles is more than offset by savings in I/O subsystem. Various design issues arise in the use of data compression in the dbms from the choice of algorithm, statistics collection, hardware versus software based compression, location of the compression function in the overall computer system architecture, unit of compression, update in place, and the application of log’ to compressed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1847

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: languages for multi-database interoperability

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 38

LEFT text: Video is composed of audio-visual information. Providing content based access to video data is essential for the sucessful integration of video into computers. Organizing video for content based access requires the use of video metadata. This paper explores the nature video metadata. A data model for video databases is presented based on a study of the applications of video, the nature of video retrieval requests, and the features of video. The data model is used in the architectural framework of a video database. The current state of technology in video databases is summarized and research issues are highlighted.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: petabyte databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: dirk d &#252; llmann
",n
"LEFT id: NA
RIGHT id: 354

LEFT text: Data management in a networked world presents us with some of the same challenges that we’ve seen in the past, but emphasizes our ability to deal with scale, in that there are several orders of magnitude more database users, and database sizes are rising more quickly than Moore’s law. We have considerably less control over the structure of the data than in the past and must efficiently operate over poorly or weakly specified schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial data management for computer aided design

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: hans-peter kriegel , andreas m &#252; ller , marco p &#246; tke , thomas seidl
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1030

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating reliable memory in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wee teck ng , peter m. chen
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: Our goal is to enhance multidimensional database systems with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. In this paper, we present a key component of our system that characterizes the information content of a cell based on a user's prior familiarity with the cube and provides a context-sensitive exploration of the cube. There are three main modules of this component. A Tracker, that continuously tracks the parts of the cube that a user has visited. A Modeler, that pieces together the information in the visited parts to model the user's expected values in the unvisited parts. An Informer, that processes user's queries about the most informative unvisited parts of the cube. The mathematical basis for the expected value modeling is provided by the classical maximum entropy principle. Accordingly, the expected values are computed so as to agree with every value that is already visited while reducing assumptions about unvisited values to the minimum by maximizing their entropy. The most informative values are defined as those that bring the new expected values closest to the actual values. We believe and prove through experiments that such a user-in-the-loop exploration will enable much faster assimilation of all significant information in the data compared to existing manual explorations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 694

LEFT text: In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fashion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting statistics on query expressions for optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the different conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best-first search strategy in order to produce a first complete plan early in the search. We describe experiments to illustrate the performance of our algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: EFIS 2000 was held at Dublin City University in June 2000. The principal aim of this third workshop was to bring together new insights from academic research with industry-driven developments and perspectives in the area of federated information systems. This report describes the observations of the workshop together with the outcome and future research possibilities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 313

LEFT text: Parallel implementations based on OpenMP or MapReduce also adopt the pruning policy and do not solve the problem thoroughly. In this context, taking into account features of document datasets, we propose 2Step-SSJ, which solves the document similarity self-join in CUDA environment on GPUs. 2Step-SSJ performs the similarity self-join in two steps, i.e., similarity computing on the inverted list and similarity computing on the forward list, which compromises between the memory visiting and dot-product computation. The experimental results show that 2Step-SSJ could solve the problem much faster than existing methods on three benchmark text corpora, achieving the speedup of 2x-23x against the state-of-the-art parallel algorithm in general, while keep a relatively stable running time with different values of the threshold.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: epsilon grid order : an algorithm for the similarity join on massive high-dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian b &#246; hm , bernhard braunm &#252; ller , florian krebs , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 817

LEFT text: Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: probabilistic optimization of top n queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: donko donjerkovic , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1209

LEFT text: Many interesting examples in view maintenance involve semijoin and outerjoin queries. In this paper we develop algebraic change propagation algorithms for the following operators: semijoin, anti-semijoin, left outerjoin, right outerjoin, and full outerjoin.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: algebraic query optimisation for database programming languages

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: OptimizingQueriesOnCompressedBitmapsSihem Amer-YahiaAT&T Labs{Researchsihem@research.att.comTheo doreJohnsonjohnsont@research.att.comAbstractBitmap indices are used by DBMS's to accelerate decision supp ort queries.A signi cant advantage ofbitmap indices is that complex logical selection op erations can b e p erformed very quickly, by p erformingbit-wiseAND,OR,andNOTop erators.Althoughbitmapindicescanb espaceinecientforhighcardinalityattributes,the space use of compressed bitmapscompares well to other indexingmetho ds.Oracle and Sybase IQ are two commercial pro ducts that make extensive use of compressed bitmap indices.Our recent research showed that there are several fast algorithmsfor evaluatingBo oleanop eratorson compressedbitmaps.Dep endingon the natureof the op erandbitmaps(theirformat, densityandclusterdness) and the op eration to b e p erformed (AND, NOT, ...), these algorithms can have executiontimes that are orders of magnitude di erent.Cho osing an algorithm for p erforming a Bo olean op erationhas global e ects in the Bo olean query expression, requiring global optimization.We present a linear timedynamicprogrammingsearch strategy based on a cost mo delto optimizequeryexpressionevaluationplans.We alsopresentrewritingheuristicsthat rewritethe queryexpressionto anequivalenonetoencourage b etter algorithmsassignments.Our p erformance results show that the optimizerrequiresanegligibl e amount of time to execute, and that optimized complex queries can execute up to three timesfaster than unoptimized queries on real data.1Intro ductionAbitmap indexis a bit string in which each bit is mapp ed to a record ID (RID) of a relation.A bit in thebitmap index is set (to 1) if the corresp onding RID has prop ertyP(i.e., the RID represents a customer thatlives in New York), and is reset (to 0) otherwise.In typical usage, the predicatePis true for a record if it hasthe valueafor attributeA.One such predicate is asso ciated to one bitmap index for each unique value ofthe attributeA.The predicates can b e more complex, for example bitslice indices [OQ97] and precomputedcomplex selection predicates [HEP99].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 2138

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information rules

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: dale a. stirling
",y
"LEFT id: NA
RIGHT id: 1647

LEFT text: Our results show that the policies are effective at achieving user-specified levels of I/O operations and database garbage percentage. We also investigate the sensitivity of the policies over a range of object connectivities. The evaluation demonstrates that semi-automatic, self-adaptive policies are a practical means for flexibly controlling garbage collection rate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partition selection policies in object database garbage collection

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jonathan e. cook , alexander l. wolf , benjamin g. zorn
",n
"LEFT id: NA
RIGHT id: 445

LEFT text: While content-based image retrieval (CBIR) is an expanding field, and new approaches to ever more effective retrieval are frequently proposed, relatively little attention has so far been paid to the process of evaluating the effectiveness of CBIR methods. Most of the reported evaluations use standard IR evaluation methodologies, with little consideration of their statistical significance or appropriateness for CBIR, which makes it difficult to assess the precise impact of individual methods. In this paper, we present a new approach for evaluating CBIR systems which provides both efficient and statistically-sound performance evaluation. The approach is based on stratified sampling, and provides a significant improvement over existing evaluation approaches. Comprehensive experiments using our approach to evaluate a range of CBIR methods have shown that the approach reduces not only the estimation error, but also reduces the size of the test data set required to achieve specific estimation error levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and cost-effective techniques for browsing and indexing large video databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: junghwan oh , kien a. hua
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: Database management is one of the main areas of research of the School of Computer Science at The University of Oklahoma (OU). The objective of the database research team at OU (OUDB) is to help solve the many issues and challenges facing the database research community, especially with respect to emerging technology. Currently, many projects are being conducted in the following areas: real-time databases, object-oriented databases, mobile databases, multimedia databases, data mining and data warehouses. These projects have been funded by federal and state agencies as well as private industries such as National Science Foundation, the U.S. Department of Education, Oklahoma State Department of Environmental Quality, and Objectivity, Inc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 614

LEFT text: Performance needs of many database applications dictate that the entire database be stored in main memory. The Dali system is a main memory storage manager designed to provide the persistence, availability and safety guarantees one typically expects from a diskresident database, while at the same time providing very high performance by virtue of being tuned to support in-memory data. Dali follows the philosophy of treating all data, including system data, uniformly as database files that can be memory mapped and directly accessed/updated by user processes. Direct access provides high performance; slower, but more secure, access is also provided through the use of a server process.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not suÆcient to associate \any"" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 873

LEFT text: Our results show that the policies are effective at achieving user-specified levels of I/O operations and database garbage percentage. We also investigate the sensitivity of the policies over a range of object connectivities. The evaluation demonstrates that semi-automatic, self-adaptive policies are a practical means for flexibly controlling garbage collection rate.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 639

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tigukat : a uniform behavioral objectbase management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu , randal peters , duane szafron , boman irani , anna lipka , adriana mu &#241; oz
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1418

LEFT text: While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: form-based proxy caching for database-backed web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: qiong luo , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1918

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management for earth system science

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james frew , jeff dozier
",n
"LEFT id: NA
RIGHT id: 961

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: nd-sql : a multi-dimensional language for interoperability and olap

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1846

LEFT text: We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative formalism for specifying these kinds of probabilistic information, and we propose algorithms for ordering the information sources. Finally, we discuss a preliminary experimental evaluation of these algorithms on the domain of bibliographic sources available on the WWW.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infomaster : an information integration system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael r. genesereth , arthur m. keller , oliver m. duschka
",n
"LEFT id: NA
RIGHT id: 2184

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",y
"LEFT id: NA
RIGHT id: 420

LEFT text: A unifying model for the study of database performance is proposed. Applications of the model are shown to relate and extend important work concerning batched searching, transposed files, index selection, dynamic hash-based files, generalized access path structures, differential files, network databases, and multifile query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a data model and data structures for moving objects databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: luca forlizzi , ralf hartmut g &#252; ting , enrico nardelli , markus schneider
",n
"LEFT id: NA
RIGHT id: 1469

LEFT text: Although many extended transaction models have been proposed [Elm93], few practical implementations exist and even fewer can support more than one model. We present the Reflective Transaction Framework, as a practical and modular method to implement extended transaction models. We achieve modularity by applying the Open Implementation approach [Kic92] (also known as meta-object protocol [KdRBSl]) to the design of the reflective transaction framework. We achieve practicality by implementing on top of a commercial transaction processing monitor. For our implementation of the reflective transaction framework, we introduce transaction adapters, add-on modules built on top of existing commercial TP components, such as Encina, that extend their functionality to support extended transaction features and semantics. Since our framework design is based on the transaction processing monitor architecture [GR93], it is widely applicable to many modern TP monitors. The reflective transaction framework enables us to implement a wide range of independently proposed extended transaction models, which we demonstrate by implementing the split/join model [PKH88] and cooperative transaction groups [MP92, RC92].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a practical and modular implementation of extended transaction models

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: roger s. barga , calton pu
",y
"LEFT id: NA
RIGHT id: 1150

LEFT text: In system monitoring, one is often interested in checking properties of aggregated data. Current policy monitoring approaches are limited in the kinds of aggregations they handle. To rectify this, we extend an expressive language, metric first-order temporal logic, with aggregation operators. Our extension is inspired by the aggregation operators common in database query languages like SQL. We provide a monitoring algorithm for this enriched policy specification language. We show that, in comparison to related data processing approaches, our language is better suited for expressing policies, and our monitoring algorithm has competitive performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 562

LEFT text: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of illinois at urbana-champaign

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: m. winslett , k. chang , a. doan , j. han , c. zhai , y. zhou
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",y
"LEFT id: NA
RIGHT id: 2054

LEFT text: Our method relies on randomizing techniques that compute small ""sketch"" summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. We also demonstrate how existing statistical information on the base data (e.g., histograms) can be used in the proposed framework to improve the quality of the approximation provided by our algorithms. The key idea is to intelligently partition the domain of the underlying attribute(s) and, thus, decompose the sketching problem in a way that provably tightens our guarantees. Results of our experimental study with real-life as well as synthetic data streams indicate that sketches provide significantly more accurate answers compared to histograms for aggregate queries. This is especially true when our domain partitioning methods are employed to further boast the accuracy of the final estimates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xpath queries on streaming data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: feng peng , sudarshan s. chawathe
",n
"LEFT id: NA
RIGHT id: 2227

LEFT text: DataJoiner (DJ) is a heterogeneous database system that provides a single database image of multiple databases. It provides transparent access to tables at remote databases through user defined aliases (nicknames) that can be accessed as if they were local tables. DJ is also a fully functional relational database system. A couple of salient features of the DataJoiner query optimizer are: (1) A query submitted to DataJoiner is optimized using a cost model that takes into account the remote optimizer’s capabilities in addition to the remote query processing capabilities and (2) If a remote database system lacks some functionality (eg: sorting), DataJoiner compensates for it. In this paper, we present the design of the Datajoiner query optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic integration in heterogeneous databases using neural networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 990

LEFT text: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable “views” of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: is web-site management a database problem ?

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon y. levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1957

LEFT text: Relational OLAP tools and other database applications generate sequences of SQL statements that are sent to the database server as result of a single information request provided by a user. Unfortunately, these sequences cannot be processed efficiently by current database systems because they typically optimize and process each statement in isolation. We propose a practical approach for this optimization problem, called ""coarse-grained optimization,"" complementing the conventional query optimization phase. This new approach exploits the fact that statements of a sequence are correlated since they belong to the same information request. A lightweight heuristic optimizer modifies a given statement sequence using a small set of rewrite rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fundamental techniques for order optimization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: david simmen , eugene shekita , timothy malkemus
",n
"LEFT id: NA
RIGHT id: 1547

LEFT text: The purpose of a textual database is to store textual documents. These documents have not only textual contents, but also structure. Many traditional text database systems have focused only on querying by contents or by structure. Recently, a number of models integrating both types of queries have appeared. We argue in favor of that integration, and focus our attention on these recent models, covering a representative sampling of the proposals in the field. We pay special attention to the tradeoffs between expressiveness and efficiency, showing the compromises taken by the models. We argue in favor of achieving a good compromise, since being weak in any of these two aspects makes the model useless for many applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: use of a component architecture in integrating relational and non-relational

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: robert atkinson
",n
"LEFT id: NA
RIGHT id: 1583

LEFT text: The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database group at university of hagen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gunter schlageter , thomas berkel , eberhard heuel , silke mittrach , andreas scherer , wolfgang wilkes
",n
"LEFT id: NA
RIGHT id: 1892

LEFT text: Camps rightly focuses on certain important facets of the evolution of relational theory since 1969, in particular our ideas about what our revered originator, E.F. Codd, chose to call doma/ns. Camps's tone at times suggests that we (in the relational camp) have been guilty of waging war over issues on which we have subsequently recanted, too late. I think this is an exaggeration, and that we could make a reasonable counter-claim to the effect that all of the clarifications we have been able to suggest, after very careful study, over those many years, are compatible with what we said before. I would not strongly object if Camps retorted that that, too, is something of an exaggeration, but obviously I think my way of expressing it is closer to the troth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a response to r. camps ' article domains , relations and religious wars

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: c. j. date
",n
"LEFT id: NA
RIGHT id: 1741

LEFT text: We have been developing a mobile passenger support system for public transport. Passengers can make their travel plans and purchase necessary tickets by accessing databases via the system. After starting the travel, a mobile terminal checks the travel schedule of its user by accessing several databases and gathering various kinds of information. In this application field, many kinds of data must be handled. Examples of such data are route information, fare information, area map, station map, planned operation schedule, real-time operation schedule, vehicle facilities and so on. Depending on the user's situation, different information should be supplied and personalized. In this paper, we propose a new mechanism to support passengers using the multi-channel data communication environments. On the other hand, transport systerns can gather information about situations and demands of users and modify their services offered for the users. We also describe a prototype system developed for visually handicapped passengers and the results of tests in an actual railway station.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: asset : a system for supporting extended transactions

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: a. biliris , s. dar , n. gehani , h. v. jagadish , k. ramamritham
",n
"LEFT id: NA
RIGHT id: 2286

LEFT text: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: re-tree : an efficient index structure for regular expressions

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chee-yong chan , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 679

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hierarchical subspace sampling : a unified framework for high dimensional data reduction , selectivity estimation and nearest neighbor search

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: charu c. aggarwal
",n
"LEFT id: NA
RIGHT id: 744

LEFT text: In this column, we review these three books.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 1782

LEFT text: Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space. In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient mid-query re-optimization of sub-optimal query execution plans

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 2196

LEFT text: A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: analysis of predictive spatio-temporal queries

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: yufei tao , jimeng sun , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 991

LEFT text: A software architecture is presented that allows client application programs to interact with a DBMS server in a flexible and powerful way, using either direct, volatile messages, or messages sent via recoverable queues. Normal requests from clients to the server and replies from the server to clients can be transmitted using direct or recoverable messages. In addition, an application event notification mechanism is provided, whereby client applications running anywhere on the network can register for events, and when those events are raised, the clients are notified. A novel parameter passing mechanism allows a set of tuples to be included in an event notification. The event mechanism is particularly useful in an active DBMS, where events can be raised by triggers to signal running application programs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database system for real-time event aggregation in telecommunication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , stephen blott , henry f. korth , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 1144

LEFT text: In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing techniques for multiversion access methods

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jochen van den bercken , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 1494

LEFT text: Businesses today are searching for information solutions that enable them to compete in the global marketplace. To minimize risk, these solutions must build on existing investments, permit the best technology to be applied to the problem, and be manageable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: directv and oracle rdb : the challenge of vldb transaction processing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: william l. gettys
",n
"LEFT id: NA
RIGHT id: 1609

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1708

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: evaluation of remote backup algorithms for transaction-processing systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christos a. polyzois , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1267

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: combining multi-visual features for efficient indexing in a large image database

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: anne h. h. ngu , quan z. sheng , du q. huynh , ron lei
",n
"LEFT id: NA
RIGHT id: 163

LEFT text: The proliferation of data in RDF format has resulted in the emergence of a plethora of specialized management systems. While the ability to adapt to the complexity of a SPARQL query -- given their inherent diversity -- is crucial, current approaches do not scale well when faced with substantially complex, non-selective joins, resulting in exponential growth of execution times. In this demonstration we present H2 RDF+, an RDF store that efficiently performs distributed Merge and Sort-Merge joins using a multiple-index scheme over HBase indexes. Through a greedy planner that incorporates our cost-model, it adaptively commands for either single or multi-machine query execution based on join complexity. In this paper, we present its key scientific contributions and allow participants to interact with an H2RDF+ deployment over a Cloud infrastructure. Using a web-based GUI we allow users to load different datasets (both real and synthetic), apply any query (custom or predefined) and monitor its execution. By allowing real-time inspection of cluster status, response times and committed resources the audience will evaluate the validity of H2RDF+'s claims and perform direct comparisons to two other state-of-the-art RDF stores.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: about quark digital media system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: kamar aulakh
",y
"LEFT id: NA
RIGHT id: 1752

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: management of semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1032

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 336

LEFT text: Spatial joins are one of the most important operations for combining spatial objects of several relations. In this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year's conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidate which is handled by the following two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaining candidates has to be tested against the join predicate. The time required for computing spatial join predicates can essentially be reduced when objects are adequately organized in main memory. In our approach, objects are first decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: multiway spatial joins

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: nikos mamoulis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: Web repositories, such as the Stanford WebBase repository, manage large heterogeneous collections of Web pages and associated indexes. For effective analysis and mining, these repositories must provide a declarative query interface that supports complex expressive Web queries. Such queries have two key characteristics: (i) They view a Web repository simultaneously as a collection of text documents, as a navigable directed graph, and as a set of relational tables storing properties of Web pages (length, URL, title, etc.). (ii) The queries employ application-specific ranking and ordering relationships over pages and links to filter out and retrieve only the ""best"" query results. In this paper, we model a Web repository in terms of ""Web relations"" and describe an algebra for expressing complex Web queries. Our algebra extends traditional relational operators as well as graph navigation operators to uniformly handle plain, ranked, and ordered Web relations. In addition, we present an overview of the cost-based optimizer and execution engine that we have developed, to efficiently execute Web queries over large repositories.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1312

LEFT text: In this paper we describe an application of the lexical resource JurWordNet and of the Core Legal Ontology as a descriptive vocabulary for modeling legal domains. It can be viewed as the semantic component of a global standardisation framework for digital governments. A content description model provides a repository of structured knowledge aimed at supporting the semantic interoperability between sectors of Public Administration and the communication processes towards citizen. Specific conceptual models built from this base will act as a cognitive interface able to cope with specific digital government issues and to improve the interaction between citizen and Public Bodies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: ontology-based support for digital government

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: athman bouguettaya , ahmed k. elmagarmid , brahim medjahed , m. ouzzani
",y
"LEFT id: NA
RIGHT id: 986

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 694

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploiting statistics on query expressions for optimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 2273

LEFT text: We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time ""twist"": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of ""big data"". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a ""big data"" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle ""big"" as well as ""fast"" data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental indexing for full-text information retrieval

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eric w. brown , james p. callan , w. bruce croft
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 19

LEFT text: Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: domino : databases for moving objects tracking

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ouri wolfson , prasad sistla , bo xu , jutai zhou , sam chamberlain
",y
"LEFT id: NA
RIGHT id: 874

LEFT text: Formation and evolution of galaxies have been a central driving force in the studies of galaxies and cosmology. Recent studies provided a global picture of cosmic star formation history. However, what drives the evolution of star formation activities in galaxies has long been a matter of debate. The key factor of the star formation is the transition of hydrogen from atomic to molecular state, since the star formation is associated with the molecular phase. This transition is also strongly coupled with chemical evolution, because dust grains, i.e., tiny solid particles of heavy elements, play a critical role in molecular formation. Therefore, a comprehensive understanding of neutral-molecular gas transition, star formation and chemical enrichment is necessary to clarify the galaxy formation and evolution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: toto , we 're not in kansas anymore : on transitioning from research to the real ( invited industrial talk )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael j. carey
",y
"LEFT id: NA
RIGHT id: 595

LEFT text: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 756

LEFT text: This paper introduces a new approach to database disk buffering, called the LRU-K method. The basic idea of LRU-K is to keep track of the times of the last K references to popular database pages, using this information to statistically estimate the interarrival times of references on a page by page basis. Although the LRU-K approach performs optimal statistical inference under relatively standard assumptions, it is fairly simple and incurs little bookkeeping overhead. As we demonstrate with simulation experiments, the LRU-K algorithm surpasses conventional buffering algorithms in discriminating between frequently and infrequently referenced pages. In fact, LRU-K can approach the behavior of buffering algorithms in which page sets with known access frequencies are manually assigned to different buffer pools of specifically tuned sizes. Unlike such customized buffering algorithms however, the LRU-K method is self-tuning, and does not rely on external hints about workload characteristics. Furthermore, the LRU-K algorithm adapts in real time to changing patterns of access.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database tuning : principles , experiments , and troubleshooting techniques ( part ii )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dennis shasha , philippe bonnet
",y
"LEFT id: NA
RIGHT id: 207

LEFT text: Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 1764

LEFT text: A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an extensible notation for spatiotemporal index queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: vassilis j. tsotras , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1833

LEFT text: We introduce a new algorithm to compute the spatial join of two or more spatial data sets, when indexes are not available on them. Size Separation Spatial Join (S<3J<) imposes a hierarchical decomposition of the data space and, in contrast with previous approaches, requires no replication of entities from the input data sets. Thus its execution time depends only on the sizes of the joined data sets. We describe S<3J< and present an analytical evaluation of its I/O and processor requirements comparing them with those of previously proposed algorithms for the same problem. We show that S<3J< has relatively simple cost estimation formulas that can be exploited by a query optimizer. S<3J< can be efficiently implemented using software already present in many relational systems. In addition, we introduce Dynamic Spatial Bitmaps< (DSB), a new technique that enables S<3J< to dynamically or statically exploit bitmap query processing techniques. Finally, we present experimental results for a prototype implementation of S<3J< involving real and synthetic data sets for a variety of data distributions. Our experimental results are consistent with our analytical observations and demonstrate the performance benefits of S<3J< over alternative approaches that have been proposed recently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: size separation spatial join

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nick koudas , kenneth c. sevcik
",y
"LEFT id: NA
RIGHT id: 1317

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 414

LEFT text: We present HOMER, a CASE tool for building and maintaining complex, data-intensive Web sites. In HOMER the processes of creation and maintenance of a Web site are completely based on the adoption of suitable models, to describe the various aspects of the site (content~ navigation structure, presentation). The development of a site does not require any code writing activity: based on the results of the design process, the system automatically creates programs to implement the site, statically and/or dynamically, as needed; also, the system does not depend on any specific tool or language: it has a modular architecture, which integrates external servers for specific tasks; finally, the system supports site administrators for several maintenance activities, which can involve changes over the site at different levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: homer : a model-based case tool for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo merialdo , paolo atzeni , marco magnante , giansalvatore mecca , marco pecorone
",y
"LEFT id: NA
RIGHT id: 1215

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an experimental object-based sharing system for networked databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: doug fang , shahram ghandeharizadeh , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 1648

LEFT text: Solid state disks (SSDs) provide much faster random access to data compared to conventional hard disk drives. Therefore, the response time of a database engine could be improved by moving the objects that are frequently accessed in a random fashion to the SSD. Considering the price and limited storage capacity of solid state disks, the database administrator needs to determine which objects (tables, indexes, materialized views, etc.), if placed on the SSD, would most improve the performance of the system. In this paper we propose a tool called ""Object Placement Advisor"" for making a wise decision for the object placement problem. By collecting profile inputs from workload runs, the advisor utility provides a list of objects to be placed on the SSD by applying heuristics like the greedy knapsack technique or dynamic programming. To show that the proposed approach is effective in conventional database management systems, we have conducted experiments on IBM DB2 with queries and schemas based on the TPC-H and TPC-C benchmarks. The results indicate that using a relatively small amount of SSD storage, the response time of the system can be reduced significantly by considering the recommendation of the advisor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu object-oriented dbms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: asuman dogac , budak arpinar , cem evrendilek , cetin ozkan , ilker altintas , ilker durusoy , mehmet altinel , tansel okay , yuksel saygin
",n
"LEFT id: NA
RIGHT id: 2119

LEFT text: Corporations worldwide are finding that understanding and managing rapidly growing, enterprisewide data is critical for making timely decisions and responding to changing business conditions. To manage and use business information competitively, many companies are establishing decision support systems built around a data warehouse of subject-oriented, integrated, historical information. In order to understand why the data warehouse must replace old legacy applications for effective information processing, it is necessary to understand the root causes of the difficulty in getting information in the first place. The first difficulty in getting information from the base of old applications is that those old applications were shaped around business requirements that were relevant as much as twenty-five years ago. These applications that were shaped yesterday do not reflect today’s business. The second reason why older applications are so hard to use as a basis for information is that those applications were shaped around the clerical needs of the corporation. A clerically focused application of necessity does not have the historical foundation required to support a long-term view. Another reason why the clerical perspective of applications does not support management’s need for information is that the clerical community focuses on detailed data. While detailed data is tine for the day-to-day clerical needs of the organization, management needs to see summary data in order to identify trends, challenges and opportunities. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: y. cui , j. widom
",n
"LEFT id: NA
RIGHT id: 1108

LEFT text: Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining generalized association rules

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 651

LEFT text: The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. In this paper we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in this paper have been previously explored in [BERN96].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: too much middleware

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: michael stonebraker
",y
"LEFT id: NA
RIGHT id: 51

LEFT text: WALRUS employs a novel similarity model in which each image is first decomposed into its regions and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying sizes and then clusters them based on the proximity of their signatures. An efficient dynamic programming algorithm is used to compute wavelet-based signatures for the sliding windows. Experimental results on real-life data sets corroborate the effectiveness of WALRUS'S similarity model.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: walrus : a similarity retrieval algorithm for image databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: apostol natsev , rajeev rastogi , kyuseok shim
",y
"LEFT id: NA
RIGHT id: 1898

LEFT text: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 1687

LEFT text: In this paper, a new probe-based distributed deadlock detection algorithm is proposed. It is an enhanced version of the algorithm originally proposed by Chandy's et al.. The new algorithm has proven to be error free and suffers very little performance degradation from the additional deadlock detection overhead. The algorithm has been compared with the modified probe-based and timeout methods. It is found that under high data contention, it has the best performance. Results also indicate that the rate of probe initiation is significantly reduced in the new algorithm

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performance evaluation of a new distributed deadlock detection algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chim-fu yeung , sheung-lun hung , kam-yiu lam
",y
"LEFT id: NA
RIGHT id: 1719

LEFT text: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",y
"LEFT id: NA
RIGHT id: 140

LEFT text: The increasing performance and decreasing cost of processors and memory are causing system intelligence to move into peripherals from the CPU. Storage system designers are using this trend toward “excess” compute power to perform more complex processing and optimizations inside storage devices. To date, such optimizations have been at relatively low levels of the storage protocol. At the same time, trends in storage density, mechanics, and electronics are eliminating the bottleneck in moving data off the media and putting pressure on interconnects and host processors to move data more efficiently. We propose a system called Active Disks that takes advantage of processing power on individual disk drives to run application-level code. Moving portions of an application’s processing to execute directly at disk drives can dramatically reduce data traffic and take advantage of the storage parallelism already present in large systems today. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multimediaminer : a system prototype for multimedia data mining

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: osmar r. za &#239; ane , jiawei han , ze-nian li , sonny h. chee , jenny y. chiang
",n
"LEFT id: NA
RIGHT id: 1534

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 303

LEFT text: Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: locally adaptive dimensionality reduction for indexing large time series databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: eamonn keogh , kaushik chakrabarti , michael pazzani , sharad mehrotra
",y
"LEFT id: NA
RIGHT id: 127

LEFT text: Semi-structured documents (e.g. journal art,icles, electronic mail, television programs, mail order catalogs, . ..) a.re often not explicitly typed; the only available t,ype information is the implicit structure. An explicit t,ype, however, is needed in order to a.pply objectoriented technology, like type-specific methods. In this paper, we present a.n experimental vector space cla.ssifier for determining the type of semi-structured documents. Our goal was to design a. high-performa.nce classifier in t,erms of accuracy (recall and precision), speed, and extensibility.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 1799

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 291

LEFT text: As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, τXQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a τXQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach: selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of xml middle-ware queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mary fernandez , atsuyuki morishima , dan suciu
",n
"LEFT id: NA
RIGHT id: 1891

LEFT text: A dynamic query interface (DQI) is a database access mechanism that provides continuous real-time feedback to the user during query formulation. Previous work shows that DQIs are elegant and powerful interfaces to small databases. Unfortunately, when applied to large databases, previous DQI algorithms slow to a crawl. We present a new incremental approach to DQI algorithms and display updates that work well with large databases, both in theory and in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: incremental data structures and algorithms for dynamic query interfaces

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: egemen tanin , richard beigel , ben shneiderman
",y
"LEFT id: NA
RIGHT id: 316

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: generating efficient plans for queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: foto n. afrati , chen li , jeffrey d. ullman
",n
"LEFT id: NA
RIGHT id: 258

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: communication-efficient distributed mining of association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: assaf schuster , ran wolff
",n
"LEFT id: NA
RIGHT id: 1356

LEFT text: Data warehouses are used to collect and analyze data from remote sources. The data collected often originate from transactional information and can become very large. This paper presents a framework for incrementally removing warehouse data (without a need to fully recompute offering two choices. One is to expunge data, in which case the result is as if the data had never existed. The second is to expire data, in which case views defined over the data are not necessarily affected. the framework, a user or administrator can specify what data to expire or expunge, what auxiliary data is to be kept for facilitating incremental view maintenance, what type of updates are expected from external sources, and how the system should compensate when data is expired or other parameters changed. We present algorithms for the various expiration and compensation actions, and we show how our framework can be implemented on top of a conventional RDBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrency in the data warehouse

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: richard taylor
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 2245

LEFT text: Data analytics technologies and techniques are widely used in commercial industries to enable organizations to make more-informed business decisions and by scientists and researchers to verify or disprove scientific models, theories and hypotheses. Depending on the specific application, the information may be historical records or new data that has been processed in real-time or it may be the result of a mix data channels. To this end, this book provides an in-depth report of data-enabled methods for analyzing Intelligent Transportation Systems (ITS), including detailed coverage of the tools needed to implement these methods, using big data analytics and other computing techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: challenges for global information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova
",n
"LEFT id: NA
RIGHT id: 59

LEFT text: The emergence and growing popularity of Internet-based electronic market-places, in their various forms, has raised the challenge to explore genericity in market design. In this paper we present a domain-specific software architecture that delineates the abstract components of a generic market and specifies control and data-flow constraints between them, and a framework that allows convenient pluggability of components that implement specific market policies. The framework was realized in the GEM system. GEM provides infrastructure services that allow market designers to focus solely on market-issues. In addition, it allows dynamic (re)configuration of components. This functionality can be used to change market-policies as the environment or market trends change, adding another level of flexibility to market designers and administrators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a layered architecture for querying dynamic web content

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: hasan davulcu , juliana freire , michael kifer , i. v. ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1102

LEFT text: We propose and evaluate two indexing schemes for improving the efficiency of data retrieval in high-dimensional databases that are incomplete. These schemes are novel in that the search keys may contain missing attribute values. The first is a multi-dimensional index structure, called the Bitstring-augmented R-tree (BR-tree), whereas the second comprises a family of multiple one-dimensional one-attribute (MOSAIC) indexes. Our results show that both schemes can be superior over exhaustive search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: obtaining complete answers from incomplete databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy
",n
"LEFT id: NA
RIGHT id: 2128

LEFT text: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1687

LEFT text: Non-conventional database management systems are used to achieve a better performance when dealing with complex data. One fundamental concept of these systems is object identity (OID), because each object in the database has a unique identifier that is used to access and reference it in relationships to other objects. Two approaches can be used for the implementation of OIDs: physical or logical OIDs. In order to manage complex data, was proposed the Multimedia Data Manager Kernel (NuGeM) that uses a logical technique, named Indirect Mapping. This paper proposes an improvement to the technique used by NuGeM, whose original contribution is management of OIDs with a fewer number of disc accesses and less processing, thus reducing management time from the pages and eliminating the problem with exhaustion of OIDs. Also, the technique presented here can be applied to others OODBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performance evaluation of a new distributed deadlock detection algorithm

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: chim-fu yeung , sheung-lun hung , kam-yiu lam
",n
"LEFT id: NA
RIGHT id: 200

LEFT text: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: boat-optimistic decision tree construction

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: johannes gehrke , venkatesh ganti , raghu ramakrishnan , wei-yin loh
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in POSTGRES.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 992

LEFT text: In this demo, we show how database-style declarative queries can be executed over data streaming from sensor networks. Our demo consists of two major components: a set of Berkeley TinyOS battery-powered, wireless sensor ""motes"" (see Figure 1) that produce and process data, and a desktop-based query processor which parses queries, distributes them over motes, and collects and displays answers. Specifically, we allow conference attendees standing at our query processing workstation to query a number of motes distributed throughout the demo space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a raster approximation for processing of spatial joins

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: geraldo zimbrao , jano moreira de souza
",n
"LEFT id: NA
RIGHT id: 406

LEFT text: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mocha : a database middleware system featuring automatic deployment of application-specific functionality

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos , john m. mcgann , stephen kelley , vadim katz , zhexuan song , joseph j &#225; j &#225;
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 2129

LEFT text: SIGMOD Record encourages readers to join the conversation by writing reviews on recent publications in the field. For more information, please contact the Contributing Editor Paul King at paul.h.king@vanderbilt.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1197

LEFT text: In this demo, we show how database-style declarative queries can be executed over data streaming from sensor networks. Our demo consists of two major components: a set of Berkeley TinyOS battery-powered, wireless sensor ""motes"" (see Figure 1) that produce and process data, and a desktop-based query processor which parses queries, distributes them over motes, and collects and displays answers. Specifically, we allow conference attendees standing at our query processing workstation to query a number of motes distributed throughout the demo space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: searching in metric spaces by spatial approximation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: gonzalo navarro
",y
"LEFT id: NA
RIGHT id: 785

LEFT text: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: performance measurements of compressed bitmap indices

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: theodore johnson
",n
"LEFT id: NA
RIGHT id: 943

LEFT text: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be “perfectly” translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable “closeness” metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: merging ranks from heterogeneous internet sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: luis gravano , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 118

LEFT text: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in global information systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: a. m. ouksel , a. sheth
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 66

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization in object databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mohana k. lakhamraju , rajeev rastogi , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 642

LEFT text: The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient integration and aggregation of historical information

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mirek riedewald , divyakant agrawal , amr el abbadi
",n
"LEFT id: NA
RIGHT id: 179

LEFT text: The key driving force behind general-purpose enterprise directory services is for providing a central repository for commonly and widely used information such as users, groups, network service access information and profiles, security information, etc. Acceptance of the Lightweight Directory Access Protocol (LDAP) as an access protocol has facilitated widespread integration of these directory services into the network infrastructure and applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sap r/3 ( tutorial ) : a database application system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann , florian matthes
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: This is my first issue as associate editor of software reviews for The American Statistician. In this column, I will introduce myself, comment on the types of software reviews that can be published in this section of The American Statistician, and encourage others in the profession to consider taking on the task of reviewing statistical software packages. I first began reading statistical software reviews in this journal when enrolled in my doctoral program in biostatistics. I read a number of excellent reviews and found the information they contained helpful in my own research, in class projects, and in collaborative work that I was doing at the time. I became interested in writing software reviews while completing my dissertation and contacted Dr. Richard Goldstein, at that time the journal?s software reviews editor. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 2154

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on fqas 2002 : fifth international conference on flexible query answering systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: amihai motro , troels andreasen
",n
"LEFT id: NA
RIGHT id: 1203

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: mariposa : a wide-area distributed database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael stonebraker , paul m. aoki , witold litwin , avi pfeffer , adam sah , jeff sidell , carl staelin , andrew yu
",y
"LEFT id: NA
RIGHT id: 1284

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data management for pervasive computing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mitch cherniack , michael j. franklin , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 1160

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data placement in shared-nothing parallel database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1416

LEFT text: This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: sit-in : a real-life spatio-temporal information system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giuseppe sindoni , leonardo tininini , amedea ambrosetti , cristina bedeschi , stefano de francisci , orietta gargano , rossella molinaro , mario paolucci , paola patteri , pina ticca
",n
"LEFT id: NA
RIGHT id: 618

LEFT text: Service composition is gaining momentum as the potential silver bullet for the envisioned Semantic Web. It purports to take the Web to unexplored efficiencies and provide a flexible approach for promoting all types of activities in tomorrow’s Web. Applications expected to heavily take advantage of Web service composition include B2B E-commerce and E-government. To date, enabling composite services has largely been an ad hoc, time-consuming, and error-prone process involving repetitive low-level programming. In this paper, we propose an ontology-based framework for the automatic composition of Web services. We present a technique to generate composite services from high-level declarative descriptions. We define formal safeguards for meaningful composition through the use of composability rules. These rules compare the syntactic and semantic features of Web services to determine whether two services are composable. We provide an implementation using an E-government application offering customized services to indigent citizens. Finally, we present an exhaustive performance experiment to assess the scalability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the grid : an application of the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: carole goble , david de roure
",n
"LEFT id: NA
RIGHT id: 955

LEFT text: Our paper addresses the problem by proposing the MV3R-tree, a structure that utilizes the concepts of multi-version B-trees and 3D-Rtrees. Extensive experimentation proves that MV3R-trees compare favorably with specialized structures aimed at timestamp and interval window queries, both in terms of time and space requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: m-tree : an efficient access method for similarity search in metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo ciaccia , marco patella , pavel zezula
",n
"LEFT id: NA
RIGHT id: 1437

LEFT text: Commercial parallel database systems such as DB2 Parallel Edition (DB2 PE) [l, 21 are delivering the ability to execute complex queries on very large databases. However, the serial application interface to these database systems can become a bottleneck for a growing list of applications such as mailing list generation and data propagation from a warehouse to smaller data marts. In this abstract, we describe the CURRENT NODE and NODENUMBER functions provided by DB2 PE and show how these two functions can be used to retrieve data in parallel in a linearly scalable manner with respect to the number of nodes in the system. Before proceeding further, we should point out that DB2 PE uses a hash partitioning strategy to distribute rows of a table to nodes in a nodegroup which is a user-specified subset of system nodes. We apply a system-specified hashing function on the user-specified partitioning key values to generate a partition number. This number is used as an index into a partition map (which can be modified by users) to find the node number where the row will be stored.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing a db2 parallel edition database

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gilles fecteau
",n
"LEFT id: NA
RIGHT id: 903

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a multi-paradigm querying approach for a generic multimedia database management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ji-rong wen , qing li , wei-ying ma , hong-jiang zhang
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: In this paper we propose a distributed case-based approach to the problem of rewriting queries. According to this approach we use a case memory instead of static views, i.e. views that are deened a priori. As a consequence, the mediated schema is dynamically updated, strongly innuenced by the queries submitted by a consumer. This approach allows a mediator to face systems where consumers may change their customization needs and information sources may become unavailable, may be added, or may modify their schemas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 214

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: querying network directories

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , laks v. s. lakshmanan , tova milo , divesh srivastava , dimitra vista
",n
"LEFT id: NA
RIGHT id: 1435

LEFT text: This paper presents a model of a nomadic middleware system with support for temporal consistency of structures semantically associated to XML-documents. Specially defined high-level operations commute with each other in most cases reducing amount of transaction aborts and increasing system availability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: towards a cooperative transaction model - the cooperative activity model

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: marek rusinkiewicz , wolfgang klas , thomas tesch , j &#252; rgen w &#228; sch , peter muth
",y
"LEFT id: NA
RIGHT id: 2240

LEFT text: In this paper, we give an overview of the semantic integrity support in the most recent SQL-standard SQL:1999, and we show to what extent the different concepts and language constructs proposed in this standard can be found in major commercial (object-)relational database management systems. In addition, we discuss general design guidelines that point out how the semantic integrity features provided by these systems should be utilized in order to implement an effective integrity enforcing subsystem for a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1374

LEFT text: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries on compressed bitmaps

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sihem amer-yahia , theodore johnson
",n
"LEFT id: NA
RIGHT id: 451

LEFT text: In June 1997, an international workshop on engineering of federated database systems has been held in Barcelona in conjunction with the 9th Conference on Advanced Information Systems Engineering (CAiSE'97). This paper reports on the results of this workshop and summarises the identified open issues for future research in this area.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research and practice in federated information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: w. hasselbring , w.-j . van den heuvel , g. j. houben , r.-d . kutsche , b. rieger , m. roantree , k. subieta
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 347

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1534

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 1040

LEFT text: In this demonstration, we present a prototype peer-topeer (P2P) application called PeerDB that provides database capabilities. This system has been developed at the National University of Singapore in collaboration with Fudan University, and is being enhanced with more features and applications. The concept behind PeerDB is similar to the analogy of publishing personal web sites, except that it is now applied to personal databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating triggers and declarative constraints in sql database sytems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: roberta cochrane , hamid pirahesh , nelson mendon &#231; a mattos
",n
"LEFT id: NA
RIGHT id: 948

LEFT text: We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to find the N tuples that satisfy the query the best but not necessarily completely in an efficient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The first step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four different techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare different methods and very promising results are obtained using the method that requires no cooperation from local databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: distributed processing over stand-alone systems and applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustavo alonso , claus hagen , hans-j &#246; rg schek , markus tresch
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 2017

LEFT text: XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: timber : a native system for querying xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: stelios paparizos , shurug al-khalifa , adriane chapman , h. v. jagadish , laks v. s. lakshmanan , andrew nierman , jignesh m. patel , divesh srivastava , nuwee wiwatwattana , yuqing wu , cong yu
",y
"LEFT id: NA
RIGHT id: 1156

LEFT text: This paper presents an approach to object view management for relational databases. Such a view mechanism makes it possible for users to transparently work with data in a relational database as if it was stored in an object-oriented (OO) database. A query against the object view is translated to one or several queries against the relational database. The results of these queries are then processed to form an answer to the initial query. The approach is not restricted to a ‘pure’ object view mechanism for the relational data, since the object view can also store its own data and methods. Therefore it must be possible to process queries that combine local data residing in the object view with data retrieved from the relational database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",y
"LEFT id: NA
RIGHT id: 1904

LEFT text: Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: www-udk : a web-based environmental meta-information system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ralf kramer , ralf nicholai , arne koschel , claudia rolker , peter lockemann , andree keitel , rudolf legat , konrad tirm
",n
"LEFT id: NA
RIGHT id: 196

LEFT text: A query Q is said to be effectively bounded if for all datasets D, there exists a subset DQ of D such that Q(D) = Q(DQ), and the size of DQ and time for fetching DQ are independent of the size of D. The need for studying such queries is evident, since it allows us to compute Q(D) by accessing a bounded dataset DQ, regardless of how big D is. This paper investigates effectively bounded conjunctive queries (SPC) under an access schema A, which specifies indices and cardinality constraints commonly used. We provide characterizations (sufficient and necessary conditions) for determining whether an SPC query Q is effectively bounded under A. We study several problems for deciding whether Q is bounded, and if not, for identifying a minimum set of parameters of Q to instantiate and make Q bounded. We show that these problems range from quadratic-time to NP-complete, and develop efficient (heuristic) algorithms for them. We also provide an algorithm that, given an effectively bounded SPC query Q and an access schema A, generates a query plan for evaluating Q by accessing a bounded amount of data in any (possibly big) dataset. We experimentally verify that our algorithms substantially reduce the cost of query evaluation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in the presence of limited access patterns

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniela florescu , alon levy , ioana manolescu , dan suciu
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1804

LEFT text: Abstract Spatial-Query-by-Sketch is the design of a query language for geographic information systems. It allows a user to formulate a spatial query by drawing the desired configuration with a pen on a touch-sensitive computer screen and translates this sketch into a symbolic representation that can be processed against a geographic database. Since the configurations queried usually do not match exactly the sketch, it is necessary to relax the spatial constraints drawn. This paper describes the representation of a sketch and outlines the design of the constraint relaxation methods used during query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the dedale system for complex spatial queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: st &#233; phane grumbach , philippe rigaux , luc segoufin
",n
"LEFT id: NA
RIGHT id: 384

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lambda-db : an odmg-based object-oriented dbms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1679

LEFT text: Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dblearn : a system prototype for knowledge discovery in relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu , yue huang , yandong cai , nick cercone
",n
"LEFT id: NA
RIGHT id: 358

LEFT text: Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dna-miner : a system prototype for mining dna sequences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jiawei han , hasan jamil , ying lu , liangyou chen , yaqin liao , jian pei
",n
"LEFT id: NA
RIGHT id: 1773

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",n
"LEFT id: NA
RIGHT id: 658

LEFT text: When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: databases and transaction processing : an application-oriented approach

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: philip m. lewis , arthur bernstein , michael kifer
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1592

LEFT text: The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. In this paper we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in this paper have been previously explored in [BERN96].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the new middleware

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rich finkelstein
",n
"LEFT id: NA
RIGHT id: 727

LEFT text: The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sdss skyserver : public access to the sloan digital sky server data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander s. szalay , jim gray , ani r. thakar , peter z. kunszt , tanu malik , jordan raddick , christopher stoughton , jan vandenberg
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 1489

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: carnot and infosleuth : database technology and the world wide web

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: d. woelk , b. bohrer , n. jacobs , k. ong , c. tomlinson , c. unnikrishnan
",n
"LEFT id: NA
RIGHT id: 291

LEFT text: In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of xml middle-ware queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: mary fernandez , atsuyuki morishima , dan suciu
",n
"LEFT id: NA
RIGHT id: 1146

LEFT text: The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 2280

LEFT text:  The challenge of peer-to-peer computing goes beyond simple file sharing. In the DBGlobe project, we view the multitude of peers carrying data and services as a superdatabase. Our goal is to develop a data management system for modeling, indexing and querying data hosted by such massively distributed, autonomous and possibly mobile peers. We employ a service-oriented approach, in that data are encapsulated in services. Direct querying of data is also supported by an XML-based query language. In this paper, we present our research results along the following topics: (a) infrastructure support, including mobile peers and the creation of context-dependent communities, (b) metadata management for services and peers, including locationdependent data, (c) filters for efficiently routing path queries on hierarchical data, and (d) querying using the AXML language that incorporates service calls inside XML documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dbglobe : a service-oriented p2p system for global computing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: evaggelia pitoura , serge abiteboul , dieter pfoser , george samaras , michalis vazirgiannis
",y
"LEFT id: NA
RIGHT id: 649

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1441

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a scalable architecture for autonomous heterogeneous database interactions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: steven milliner , athman bouguettaya , mike p. papazoglou
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1189

LEFT text: Wireless and mobile computing have advanced significantly in the last decade. In particular, we now face the challenge to spontaneously establish wireless self-organizing networks, such as ad hoc, disruption-tolerant, sensor, and wireless mesh networks. These spontaneous self-organizing networks have been the focus of intensive research activity in recent years. Spontaneous networks arise from the cooperation of mobile devices in an ad hoc fashion requiring no previous infrastructure in place. A key point to couple research and real-life applications in this context is to understand how mobility (of devices, users, and applications) impacts practical networking aspects

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: fabio casati , ming-chien shan , dimitrios georgakopoulos
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1083

LEFT text: Information becomes a more and more valuable asset in today’s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome in the design and implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II , which is based on the wrapper-mediator approach. The main focus of this work lies on data model transformation and query processing. The following extensions to the AMOS II system are described in this thesis: • A framework for transforming various data models into the objectoriented model of AMOS II is presented. • The roles and tasks of wrappers are described. In particular their participation in query processing and query optimization is discussed. • A way for describing and utilizing the query capabilities of the different data sources is proposed. • Two different approaches to query processing over external data sources are developed and analyzed. All the proposed techniques are implemented in the AMOS II system, which runs on a Windows NT platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: describing and using query capabilities of heterogeneous sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vasilis vassalos , yannis papakonstantinou
",y
"LEFT id: NA
RIGHT id: 1115

LEFT text: Workflow management systems are among the most interesting concepts for supporting modern organizations with a focus on processes rather than on structure. Workflow management systems offer different degrees of automation of business processes. We classify workflow management systems according to the features they provide and the types of processes they support. Database systems facilitate the realization of workflow management systems in several ways. They can provide the necessary functionality to keep the workflow relevant data, business data as well as process data. The dynamic execution of workflows can be handled by triggers of active database systems. Furthermore, the transaction concept can be extended to develop workflow transactions for consistent execution of workflows and intelligent treatment of exceptions and errors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: databases and workflow management : what is it all about ? ( panel )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: andreas reuter , stefano ceri , jim gray , betty salzberg , gerhard weikum
",y
"LEFT id: NA
RIGHT id: 268

LEFT text: High speed computer and telephone networks carry large amounts of data and signalling traffic. The engineers who build and maintain these networks use a combination of hardware and software tools to monitor the stream of network traffic. Some of these tools operate directly on the live network; others record data on magnetic tape for later offline analysis by software. Most analysis tasks require tens to hundreds of gigabytes of data. Traffic analysis applications include protocol performance analysis, conformance testing, error monitoring and fraud detection.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the network is the database : data management for highly distributed systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: julio c. navas , michael wynblatt
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: We have built a multidatabase system to support a financial application that stores historical data used by traders to identify trends in the market. The application has an update rate (append-only) of 500 inserts per second and also has sub-second response requirements for queries. A typical query requests between 100-1000 records. In this paper we define the characteristics of the application, the multidatabase system we used to support the applications and the extensions we made in t.he application to achieve the required functionality and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1271

LEFT text: In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: one-dimensional and multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 914

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at ut arlington

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sharma chakravarthy , alp aslandogan , ramez elmasri , leonidas fegaras , junghwan oh
",n
"LEFT id: NA
RIGHT id: 1236

LEFT text: We investigate the optimization and evaluation of queries with universal quantification in the context of the object-oriented and object-relational data models. The queries are classified into 16 categories depending on the variables referenced in the so-called range and quantifier predicates. For the three most important classes we enumerate the known query evaluation plans and devise some new ones. These alternative plans are primarily based on anti-semijoin, division, generalized grouping with count aggregation, and set difference. In order to evaluate the quality of the many different evaluation plans a thorough performance analysis on some sample database configurations was carried out.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: description logics for semantic query optimization in object-oriented database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , claudio sartori
",n
"LEFT id: NA
RIGHT id: 560

LEFT text: Data cube computation is one of the most essential but expensive operations in data warehousing. The latter, represented by two algorithms: BUC and H-Cubing, computes the iceberg cube bottom-up and facilitates Apriori pruning. BUC explores fast sorting and partitioning techniques; whereas H-Cubing explores a data structure, H-Tree, for shared computation. However, none of them fully explores multi-dimensional simultaneous aggregation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: interviewing during a tight job market

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: zachary g. ives , qiong luo
",y
"LEFT id: NA
RIGHT id: 1413

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scienti c and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at di erent levels to ultimately nd a set of high-quality queryanswering plans.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 900

LEFT text: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: performing joins without decompression in a compressed database system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: s. j. o'connell , n. winterbottom
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: 1. Motivation Internet search engines have popularized keyword based search. While relational database systems offer powerfifl structured query languages such as SQL, there is no support for keyword search over databases. The simplicity of keyword search as a querying paradigm offers compelling values for data exploration. Specifically, keyword search does not require a priori knowledge of the schema. The above is significant as much information in a corporation is increasingly being available at its intranet. However, it is unrealistic to expect users who would browse and query such information to have detailed knowledge of the schema of available databases. Therefore, just as keyword search and classification hierarchies complement each other for document search, keyword search over databases can be effective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 1271

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: one-dimensional and multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1419

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: naos - efficient and modular reactive capabilities in an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: christine collet , thierry coupaye , t. svensen
",n
"LEFT id: NA
RIGHT id: 1306

LEFT text: Non-conventional database management systems are used to achieve a better performance when dealing with complex data. One fundamental concept of these systems is object identity (OID), because each object in the database has a unique identifier that is used to access and reference it in relationships to other objects. Two approaches can be used for the implementation of OIDs: physical or logical OIDs. In order to manage complex data, was proposed the Multimedia Data Manager Kernel (NuGeM) that uses a logical technique, named Indirect Mapping. This paper proposes an improvement to the technique used by NuGeM, whose original contribution is management of OIDs with a fewer number of disc accesses and less processing, thus reducing management time from the pages and eliminating the problem with exhaustion of OIDs. Also, the technique presented here can be applied to others OODBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast evaluation techniques for complex similarity queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , michael mlivoncic , hans-j &#246; rg schek , roger weber
",n
"LEFT id: NA
RIGHT id: 693

LEFT text: This paper introduces a new efficient join algorithm to increase the speed of the join relational operation. Using a divide and conquer strategy, stack oriented filter technique in the new join algorithm filters unwanted tuples as early as possible while none of the currently existing join algorithms takes advantage of any filtering concept. Other join algorithms may carry the unnecessary tuples up to the last moment of join attribute comparisons.Four join algorithms are described and discussed in this paper: the nested-loop join algorithm, the sort-merge join algorithm, the hash join algorithm, and the new join algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a scalable hash ripple join algorithm

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: gang luo , curt j. ellmann , peter j. haas , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1369

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: xperanto : middleware for publishing object-relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael j. carey , jerry kiernan , jayavel shanmugasundaram , eugene j. shekita , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1690

LEFT text: The XQuery formalization is an ongoing effort of the W3C XML Query working group to define a precise formal semantics for XQuery. This paper briefly introduces the current state of the formalization and discusses some of the more demanding remaining challenges in formally describing an expressive query language for XML.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: medical information systems : characterization and challenges

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jorge c. g. ramirez , lon a. smith , lynn l. peterson
",n
"LEFT id: NA
RIGHT id: 1152

LEFT text: Client-server object-oriented database management systems differ significantly from traditional centralized systems in terms of their architecture and the applications they target. In this paper, we present the client-server architecture of the EOS storage manager and we describe the concurrency control and recovery mechanisms it employs. EOS offers a semi-optimistic locking scheme based on the multi-granularity two-version two-phase locking protocol. Under this scheme, multiple concurrent readers are allowed to access a data item while it is being updated by a single writer. Recovery is based on write-ahead redo-only logging.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: synchronization and recovery in a client-server storage system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: e. panagos , a. biliris
",y
"LEFT id: NA
RIGHT id: 127

LEFT text: To resolve the syntax, structure and semantic heterogeneity for sharing information resources, the representative technologies are XML and Metadata. XML is used to represent the syntax and structure of information resources but the various XML schema definitions that have been developed by independent organizations without any standards or guidelines, make it difficult to share the semantic meaning of XML encoded information resources. In this paper, we propose a mechanism, named MSDL that represents the exact meaning of XML tags by describing the structural and semantic differences with standard metadata in metadata registries. MSDL overcomes the limitations of other approaches with respect to exactness, flexibility and standardization, and provides an environment for business partners using different metadata to share their XML encoded information resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 1123

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the structured information manager : a database system for sgml documents

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ron sacks-davis
",n
"LEFT id: NA
RIGHT id: 1100

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing in hierarchical parallel database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: luc bouganim , daniela florescu , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 125

LEFT text: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in information services : experiencing with coopware

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: avigdor gal
",n
"LEFT id: NA
RIGHT id: 2279

LEFT text: Object-oriented databases enforce behavioral schema consistency rules to guarantee type safety, i.e., that no run-time type error can occur. When the schema must evolve, some schema updates may violate these rules. In order to maintain complete behavioral schema consistency, traditional solutions require significant changes to the types, the type hierarchy and the code of existing methods. Such operations are very expensive in a database context. To ease schema evolution, we propose to support exceptions to the behavioral consistency rules without sacrificing type safety for all that. The basic idea is to detect unsafe statements at compile-time and check them at run-time. The run-time check is performed by a specific clause that is automatically inserted around unsafe statements. This check clause warns the programmer of the safety problem and lets him provide exception-handling code. Schema updates can therefore be performed with only minor changes to the code of methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting exceptions to schema consistency to ease schema evolution in oodbms

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eric amiel , marie-jo bellosta , eric dujardin , eric simon
",y
"LEFT id: NA
RIGHT id: 1848

LEFT text: We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications. The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data warehousing and olap for decision support

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 1367

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: model-based information integration in a neuroscience mediator system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bertram lud &#228; scher , amarnath gupta , maryann e. martone
",n
"LEFT id: NA
RIGHT id: 1053

LEFT text: This paper describes the design and implementation of PEST0 (Portable Explorer of Snuctured Objects), a user interface that supports browsing and querying of object databases. PEST0 allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (“query-in-place”) that presents querying as a natural extension of browsing. PEST0 is designed to be portable to any object database system that supports a high-level query language; in addition, PEST0 is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text). uniformly and manipulated using an object-oriented dialect of SQL. One component of this project, which is joint work between IBM Almaden and the University of Wisconsin, is the development of a graphical user interface called PEST0 (Portable Explorer of STructured Objects). We refer to the PEST0 interface as a query/browser, as it marries navigational object browsing’ with declarative querying; it integrates browsing and querying via a “query-in-place” paradigm that provides a powerful yet natural user interface for exploring the contents of object databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pesto : an integrated query/browser for object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael j. carey , laura m. haas , vivekananda maganty , john h. williams
",y
"LEFT id: NA
RIGHT id: 4

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",y
"LEFT id: NA
RIGHT id: 1640

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 281

LEFT text: In the last few years, many active database models have been proposed. Some of them have been implemented as research prototypes. The use and study of these prototypes shows that it is difficult to get a clear idea of the proposed approaches and to compare them. More generally there are some unquestionable difficulties in understanding, reasoning about and teaching behavior of active database systems. We think there is a need for formal descriptions of the semantics of such systems in order to describe and to understand them with less ambiguities, to compare them and to come up with some progress in defining standard concepts and functionalities for active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the prototype of the dare system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tiziana catarci , giuseppe santucci
",n
"LEFT id: NA
RIGHT id: 1913

LEFT text: Abstract. The analysis of web usage has mostly focused on sites composed of conventional static pages. However, huge amounts of information available in the web come from databases or other data collections and are presented to the users in the form of dynamically generated pages. The query interfaces of such sites allow the specification of many search criteria. Their generated results support navigation to pages of results combining cross-linked data from many sources. For the analysis of visitor navigation behaviour in such web sites, we propose the web usage miner (WUM), which discovers navigation patterns subject to advanced statistical and structural constraints. Since our objective is the discovery of interesting navigation patterns, we do not focus on accesses to individual pages. Instead, we construct conceptual hierarchies that reflect the query capabilities used in the production of those pages. Our experiments with a real web site that integrates data from multiple databases, the German SchulWeb, demonstrate the appropriateness of WUM in discovering navigation patterns and show how those discoveries can help in assessing and improving the quality of the site.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an overview of data warehousing and olap technology

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , umeshwar dayal
",n
"LEFT id: NA
RIGHT id: 1859

LEFT text: Client-server database systems based on a data shipping model can exploit client memory resources by caching copies of data items across transaction boundaries. Caching reduces the need to obtain data from servers or other sites on the network. In order to ensure that such caching does not result in the violation of transaction semantics, a transactional cache consistency maintenance algorithm is required. Many such algorithms have been proposed in the literature and, as all provide the same functionality, performance is a primary concern in choosing among them. In this article we present a taxonomy that describes the design space for transactional cache consistency maintenance algorithms and show how proposed algorithms relate to one another. We then investigate the performance of six of these algorithms, and use these results to examine the tradeoffs inherent in the design choices identified in the taxonomy. The results show that the interactions among dimensions of the design space impact performance in many ways, and that classifications of algorithms as simply “pessimistic” or “optimistic” do not accurately characterize the similarities and differences among the many possible cache consistency algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: transactional client-server cache consistency : alternatives and performance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. franklin , michael j. carey , miron livny
",y
"LEFT id: NA
RIGHT id: 1609

LEFT text: A load, such as a logic network of the TTL type, is connected in parallel across a plurality of direct-current sources designed to maintain a substantially constant operating voltage. Each source includes a control unit which compares the load voltage with a reference level in order to stabilize the output voltage of an associated current generator at that level. If the generator current drops below a certain minimum value, however, a threshold sensor in the control unit raises the reference level up to an amount equaling about twice the maximum divergence possible between the reference levels of different control units, thereby ensuring that all sources contribute simultaneously to the load current.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: When an update to a view is requested by a user, there may be no unique way of up dating the stored relations in the database to realize the requested update. Chosing one of the alternatives for updating stored relations may not reflect the change that has actually taken place in the real world; in the presence of other derived views, the database may actually present a very wrong model of the world to the user. The problem is even more severe in the case of deductive databases. For avoiding this problem, we introduce a new notion of view updates, called cumulative updates. The key idea behind cumulative updates is that update mechanisms should wait for further update requests to resolve ambiguities. Equivalently, current update requests must also take into account previous requests made to the knowledge base. Cumulative updates, therefore, subsume conventional updates in which only the current update request is considered. In this paper, we motivate the need for cumulative updates and formally define the notion of such updates as well as the different classes therein. We then give methods for computing one particular class of cumulative updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1929

LEFT text: Materialized views (or Automatic Summary Tables—ASTs) are commonly used to improve the performance of aggregation queries by orders of magnitude. In contrast to regular tables, ASTs are synchronized by the database system. In this paper, we present techniques for maintaining cube ASTs. Our implementation is based on IBM DB2 UDB.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of data cubes and summary tables in a warehouse

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: inderpal singh mumick , dallan quass , barinderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 1850

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: object-relational database systems ( tutorial ) : principles , products and challenges

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: michael j. carey , nelson m. mattos , anil k. nori
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 636

LEFT text: This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: fibonacci : a programming language for object databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: antonio albano , giorgio ghelli , renzo orsini
",n
"LEFT id: NA
RIGHT id: 1335

LEFT text: The amount of scientific and technical information is growing exponentially. As a result, the scientific community has been overwhelmed by the information published in number of new books, journal articles, and conference proceedings. In addition to increasing number of publications, advances in information technology have dramatically reduced the barriers in electronic publishing and distribution of information over networks virtually anywhere in the world. As a result, the scientific community is facing the problem of locating relevant or interesting information. To address the problem of information overload and to sift all available information sources for useful information, recommender systems or filtering systems have emerged. Generally, recommender systems are used online to suggest items that users find interesting, thereby, benefiting both the user and merchant. Recommender systems benefit the user by making him suggestions on items that he is likely to purchase and the business by increase of sales. Filtering information or generation of recommendatio ns by the recommender systems mimic the process of information retrieval systems by incorporating advanced profile building techniques, item/user representation techniques, filtering and recommendation techniques, and profile adaptation techniques. This paper addresses the application domain analysis, functional classification, advantages and disadvantages of various filtering and recommender systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: functional properties of information filtering

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rie sawai , masahiko tsukamoto , yin-huei loh , tsutomu terada , shojiro nishio
",y
"LEFT id: NA
RIGHT id: 2259

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: This tutorial presents the primary constructs of the consensus temporal query language TSQL2 via a media planning scenario. Media planning is a series of decisions involved in the delivery of a promotional message via mass media. We will follow the planning of a particular advertising campaign. We introduce the scenario by identifying the marketing objective. The media plan involves placing commercials, and is recorded in a temporal database. The media plan must then be evaluated; we show how TSQL2 can be used to derive information from the stored data. We then give examples that illustrate storing and querying indeterminate information, comparing multiple versions of the media plan, accommodating changes to the schema, and vacuuming a temporal database of old data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1458

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic multi-resource load balancing in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: erhard rahm , robert marek
",n
"LEFT id: NA
RIGHT id: 1790

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining and pruning optimizations of constrained associations rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang
",n
"LEFT id: NA
RIGHT id: 713

LEFT text: The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: commix : towards effective web information extraction , integration and query answering

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: tengjiao wang , shiwei tang , dongqing yang , jun gao , yuqing wu , jian pei
",n
"LEFT id: NA
RIGHT id: 2192

LEFT text: In this paper, we introduce an approach that supports querying for Semantic Associations on the Semantic Web. Semantic Associations capture complex relationships between entities involving sequences of predicates, and sets of predicate sequences that interact in complex ways. Detecting such associations is at the heart of many research and analytical activities that are crucial to applications in national security and business intelligence. This in combination with the improving ability to identify entities in documents as part of automatic semantic annotation, gives a very powerful capability for semantic analysis of large amounts of heterogeneous content.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: composing web services on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brahim medjahed , athman bouguettaya , ahmed k. elmagarmid
",n
"LEFT id: NA
RIGHT id: 1601

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 1995 international workshop on temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arie segev , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 939

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: database indexing for large dna and protein sequence collections

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ela hunt , malcolm p. atkinson , robert w. irving
",n
"LEFT id: NA
RIGHT id: 1264

LEFT text: In this paper, we propose HMAP (The term is the transliteration of an ancient Greek poetical word meaning “day”.), a temporal data model extending the capability of defining valid times with different granularity and/or with indeterminacy. In HMAP, absolute intervals are explicitly represented by their start,end, and duration: in this way, we can represent valid times as “in December 1998 for five hours”, “from July 1995, for 15 days”, “from March 1997 to October 15, 1997, between 6 and 6:30 p.m.”. HMAP is based on a three-valued logic, for managing uncertainty in temporal relationships. Formulas involving different temporal relationships between intervals, instants, and durations can be defined, allowing one to query the database with different granularities, not necessarily related to that of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: hmap - a temporal data model managing intervals with different granularities and indeterminacy from natural language sentences

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: carlo combi , giuseppe pozzi
",y
"LEFT id: NA
RIGHT id: 1873

LEFT text: The overall theme of the FQAS conferences is innovative query systems that are aimed at providing easy, flexible and intuitive access to information. Such systems are intended to facilitate retrieval from information repositories such as databases, libraries, and the World Wide Web. These repositories are typically equipped with standard query systems, which are often inadequate, and the focus of FQAS is the development of query systems that are more expressive, informative, cooperative and productive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 247

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sonar : system for optimized numeric association rules

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shinichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 1167

LEFT text: The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency control in hierarchical multidatabase systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sharad mehrotra , henry f. korth , avi silberschatz
",y
"LEFT id: NA
RIGHT id: 471

LEFT text: Clio is a system for managing and facilitating the complex tasks of heterogeneous data transformation and integration. In Clio, we have collected together a powerful set of data management techniques that have proven invaluable in tackling these difficult problems. In this paper, we present the underlying themes of our approach and present a brief case study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the clio project : managing heterogeneity

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ren &#233; e j. miller , mauricio a. hern &#225; ndez , laura m. haas , lingling yan , c. t. howard ho , ronald fagin , lucian popa
",y
"LEFT id: NA
RIGHT id: 1156

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: We discuss the design and implementation of the O 2 Views object-oriented database view mechanism, which allows the redeenition of both the structure and the behavior of objects stored in a database. The data model extended with views is rst given and then the functionalities of the prototype implementing it are presented. The paper focuses on the requirements for the implementation of an object-oriented view mechanism, ranging from the conception of a view deenition language to optimization strategies for querying and updating through a view such as view materialization and consistency maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 597

LEFT text: The increasing usage of location-aware devices, such as GPS and RFID, has made moving object management an important task. Especially, being demanded in real-world applications, continuous query processing on moving objects has attracted significant research efforts. However, little attention has been given to the design of concurrent continuous query processing for multi-user environments. In this paper, we propose a concurrency control protocol to efficiently process continuous queries over moving objects on a B-tree-based framework. The proposed protocol integrates link-based and lock-coupling strategies, and is proven to assure serializable isolation, data consistency, and deadlock-free for continuous query processing. Concurrent operations including continuous query, object movement, and query movement are protected under this protocol. Experimental results on benchmark data sets demonstrated the scalability and efficiency of the proposed concurrent framework.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: transactional information systems : theory , algorithms , and the practice of concurrency control and recovery

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: marc h. scholl
",n
"LEFT id: NA
RIGHT id: 317

LEFT text: The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules, and show their deficiencies. We then describe the MiniCon, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries using materialized views : a practical , scalable solution

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jonathan goldstein , per - &#197; ke larson
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1533

LEFT text: DataMine is a statistical database mining system with strong emphasis on interactiveness and nice graphical representation of information produced. It also supports an offline mode of discovery, and provides an extensive API which allows users to write ""mining applications"" just as easily as routine database applications, The central idea is to perform discovery with a ""human in the loop"" guiding the system using his initial hypothesis and the feedback from the system. Users can pose a rule-query against a rulebase and the system can generate all rules matching their query. The rulebase could either be pregenerated (using offline mode) or could be realized in real-time as the discovery progresses.Rules generated by the system are of the form:Body -&gt; Consequentwhere Body is a conjunction of the elementary predicates of the form (A=a), where A is an attribute and a is a value from the attribute domain of A. Consequent is a single elementary predicate. Each rule can have several parameters like support, confidence, atypicality, color etc. (the definitions have been left out) which can also be used by the user in framing the rule query.For continuous attributes, the system also allows the user some control in deciding how they are discretized. It also allows for the creation of extra attributes at run time which can then be used in queries like the rest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: datamine-interactive rule discovery system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: t. imielinski , a. virmani
",y
"LEFT id: NA
RIGHT id: 585

LEFT text: nvited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1531

LEFT text: With about 8.000 researchers and 40.000 students, RWTH Aachen is the largest technical university in Europe. The science and engineering departments and their industrial collaborators offer a lot of challenges for database research.The chair Informatik V (Information Systems) focuses on the theoretical analysis, prototypical development, and practical evaluation of meta information systems. Meta information systems, also called repositories, document and coordinate the distributed processes of producing, integrating, operating, and evolving database-intensive applications.Our research approaches these problems from a technological and from an application perspective.On the one hand, we pursue theory and system aspects of the integration of deductive and object-oriented technologies. One outcome of this work is a deductive object manager called ConceptBase which has been developed over the past eight years and is currently used by many research groups and industrial teams throughout the world.On the other hand, a wide range of application-driven projects aims at building a sound basis of empirical knowledge about the demands on meta information systems, and about the quality of proposed solutions. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infoharness : a system for search and retrieval of heterogeneous information

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leon shklar , amit sheth , vipul kashyap , satish thatte
",n
"LEFT id: NA
RIGHT id: 1146

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mineset ( tm ) : a system for high-end data mining and visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: While content-based image retrieval (CBIR) is an expanding field, and new approaches to ever more effective retrieval are frequently proposed, relatively little attention has so far been paid to the process of evaluating the effectiveness of CBIR methods. Most of the reported evaluations use standard IR evaluation methodologies, with little consideration of their statistical significance or appropriateness for CBIR, which makes it difficult to assess the precise impact of individual methods. In this paper, we present a new approach for evaluating CBIR systems which provides both efficient and statistically-sound performance evaluation. The approach is based on stratified sampling, and provides a significant improvement over existing evaluation approaches. Comprehensive experiments using our approach to evaluate a range of CBIR methods have shown that the approach reduces not only the estimation error, but also reduces the size of the test data set required to achieve specific estimation error levels.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: Multidimensional inter-transactional association rules extend the traditional association rules to describe more general associations among items with multiple properties across transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transactional association rules tends to be extremely large, mining inter-transactional associations poses more challenges on efficient processing than mining traditional intra-transactional associations. In order to make such association rule mining truly practical and computationally tractable, in this study we present a template model to help users declare the interesting multidimensional inter-transactional associations to be mined.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1987

LEFT text: We briefly outline the main characteristics of an efficient server-based algorithm for garbage collecting object-oriented databases in a client-server environment. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any locks on data and does not require callbacks to clients. It is fault tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing OODB systems, and therefore it works with standard implementation techniques such as two-phase locking and write-ahead-logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. The algorithm has been implemented in the EXODUS storage manager before being evaluated. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semi-automatic , self-adaptive control of garbage collection rates in object databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jonathan e. cook , artur w. klauser , alexander l. wolf , benjamin g. zorn
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: I'm happy to be able to share with you the following three reminiscences. I continue to invite unsolicited contributions. See http://www.acm.org/sigmod/record/author.html for submission guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1883

LEFT text: Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: outerjoin simplification and reordering for query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , arnon rosenthal
",n
"LEFT id: NA
RIGHT id: 608

LEFT text: This paper studies the Candy model, a marked point process introduced by Stoica et al. (2000). We prove Ruelle and local stability, investigate its Markov properties, and discuss how the model may be sampled. Finally, we consider estimation of the model parameters and present some examples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on semantic web and data management

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: robert meersman , amit sheth
",n
"LEFT id: NA
RIGHT id: 506

LEFT text: The cost functions are parameterized to accommodate various hardware characteristics appropriately. Combining the basic patterns, we can describe the memory access patterns of database operations. The cost functions of database operations can automatically be derived by combining the basic patterns' cost functions accordingly. To validate our approach, we performed experiments using our DBMS prototype Monet. The results presented here confirm the accuracy of our cost models for different operations. Aside from being useful for query optimization, our models provide insight to tune algorithms not only in a main-memory DBMS, but also in a disk-based DBMS with a large main-memory buffer cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a survey of logical models for olap databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: panos vassiliadis , timos sellis
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 792

LEFT text: We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds. Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: high-performance extensible indexing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: marcel kornacker
",n
"LEFT id: NA
RIGHT id: 1578

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on active databases ( short version )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ulrike jaeger , johann christoph freytag
",n
"LEFT id: NA
RIGHT id: 1272

LEFT text: Web services are increasingly gaining acceptance as a framework for facilitating application-to-application interactions within and across enterprises. It is commonly accepted that a service description should include not only the interface, but also the business protocol supported by the service. The present work focuses on the formalization of an important category of protocols that includes time-related constraints (called timed protocols), and the impact of time on compatibility and replaceability analysis. We formalized the following timing constraints: C-Invoke constraints define time windows within which a service operation can be invoked while M-Invoke constraints define expiration deadlines. We extended techniques for compatibility and replaceability analysis between timed protocols by using a semantic-preserving mapping between timed protocols and timed automata, leading to the identification of a novel class of timed automata, called protocol timed automata (PTA). PTA exhibit a particular kind of silent transition that strictly increase the expressiveness of the model, yet they are closed under complementation, making every type of compatibility or replaceability analysis decidable. Finally, we implemented our approach in the context of a larger project called ServiceMosaic, a model-driven framework for Web service life-cycle management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: malcolm p. atkinson
",n
"LEFT id: NA
RIGHT id: 525

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql/med : a status report

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jim melton , jan eike michels , vanja josifovski , krishna kulkarni , peter schwarz
",n
"LEFT id: NA
RIGHT id: 1915

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: converting relational to object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: joseph fong
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 948

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: distributed processing over stand-alone systems and applications

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustavo alonso , claus hagen , hans-j &#246; rg schek , markus tresch
",n
"LEFT id: NA
RIGHT id: 1340

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: smooth - a distributed multimedia database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: harald kosch , l &#225; szl &#243; b &#246; sz &#246; rm &#233; nyi , alexander bachlechner , christian hanin , christian hofbauer , margit lang , carmen riedler , roland tusch
",n
"LEFT id: NA
RIGHT id: 693

LEFT text: Recently, Haas and Hellerstein proposed the hash ripple join algorithm in the context of online aggregation. Although the algorithm rapidly gives a good estimate for many join-aggregate problem instances, the convergence can be slow if the number of tuples that satisfy the join predicate is small or if there are many groups in the output. Furthermore, if memory overflows (for example, because the user allows the algorithm to run to completion for an exact answer), the algorithm degenerates to block ripple join and performance suffers. In this paper, we build on the work of Haas and Hellerstein and propose a new algorithm that (a) combines parallelism with sampling to speed convergence, and (b) maintains good performance in the presence of memory overflow. Results from a prototype implementation in a parallel DBMS show that its rate of convergence scales with the number of processors, and that when allowed to run to completion, even in the presence of memory overflow, it is competitive with the traditional parallel hybrid hash join algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a scalable hash ripple join algorithm

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: gang luo , curt j. ellmann , peter j. haas , jeffrey f. naughton
",y
"LEFT id: NA
RIGHT id: 510

LEFT text: High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a cost model for query processing in high dimensional data spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christian b &#246; hm
",n
"LEFT id: NA
RIGHT id: 586

LEFT text: Data in relational databases is frequently stored and retrieved using B-Trees. In &cis,ion isugprt applications the key of the B-Tree frequently involves the concatenation of several fields of the relationdl’ table. During retrieval, it is desirable to be able to access a small subset of the table based’ on partial key information, where some fields of the key may either not be present, involve ranges, or lists ‘of values. It is also advantageous to altow. this type, of access-with gen&il expressions involving any combination of disjuncts on key columns. This paper &scribes a method whereby BTrees can be eficiently used to retrieve small subsets, thus avoiding large scans of potentially huge tables. Another benefit is the ability of this method to reduce the need for additional secondary indexes, thus saving space, maintenance cost, and random accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 1870

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: s3 : similarity search in cad database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1645

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 744

LEFT text: A book title cannot be more timely or accurate. Information rules society and it always has. The key difference is, that in our generation, the manner in which information is managed is more apparent to the everyday person and as more information becomes readily available the curse is that information can overload and intimidate us with little or no effort. Prior to the personal computer the everyday person could more easily manage the flow—such is not the case today. Throw into this fray the fact that information is a force in economics and the everyday person may become bewildered and perplexed. Many of these concerns are addressed in this excellent new book that focuses on the information economy and its effect on society and culture. In ten engaging chapters, key concepts such as pricing, versioning, rights management, recognizing and managing lock-in, networks, cooperation and compatibility, standards, and information policy are dissected, discussed, and explained. Most chapters end with lessons that reflect key points made in the chapter. The first chapter presents the foundation of the thesis of the book—the material is relatively general in nature—and sets the stage for the following nine interesting chapters. In discussing pricing, the authors cite the case of Encyclopedia Britannica and its inability to compete with the more popular and less expensive Microsoft product, Encarta. An associated concept, “versioning” is discussed and the authors show how a business can offer information products in different versions for differing markets to the benefit of the bottom line. The heady and confusion issue of copyright management, especially as related to internet economy is examined in chapter four of the book. Another issue of concern, lock-in, which results from switching from one technology to another, is discussed in chapters five and six. In chapter seven the authors discuss how the old industrial economy was driven by economies of scale whereas the information economy is driven by economics of networks. The last three chapters push the envelope and advise the reader how to affect real changes in their relationship with the information economy. The last chapter is key in that it discusses current government information policies in light of advice provided earlier in the book. This book may be one of the best to examine the theory and implications of the information economy. Although written by heavyweights in the field of economics and information management, the authors present a well written and thoughtful treatment of a subject that non-academics and academics alike should enjoy and refer to often. More importantly, this book offers direct advice that could well affect the bottom line of many entrepreneurs and existing companies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 156

LEFT text: The 1998 Nagano Olympic games had more intensive demands on data management than any previous Olympics in history. This talk will take you behind the scenes to talk about the technical challenges and the architectures that made it possible to handle 4.5 Terabytes of data and sustain a total of almost 650 million web requests, reaching a peak of over 103K per minute. We will discuss the overall structure of the most comprehensive and heavily used Internet technology application in history. Many products were involved, both hardware and software, but this talk will focus in on the database and web challenges, the technology that made it possible to support this tremendous workload. High availability, data integrity, high performance, support of both SMPs and clustered architectures were among the features and functions that were critical.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: olympic records for data at the 1998 nagano games

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: edwin r. lassettre
",y
"LEFT id: NA
RIGHT id: 428

LEFT text: We present the PPOST-architecture (Persistent Parallel Object Store) for main-memory database systems on parallel computers, that is suited for applications with challenging performance requirements. The architecture takes full advantage of parallelism, large main memories and fast switching networks. An important property of this architecture is its excellent scaling behavior.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards self-tuning data placement in parallel database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal
",n
"LEFT id: NA
RIGHT id: 1866

LEFT text:  In addition it constructs a special node Authors() and connects it to all pages corresponding to ""Author""s. The output graph is called SiteGraph. One way to write this in StruQL is: input DataGraph where Root(x); x ! ! y; y ! l ! z; l in f""Paper"", ""TechReport"", ""Title"", ""Abstract"", ""Author""g create Authors(); Page(y); Page(z) link Page(y) ! l ! Page(z) where x ! ! y1; y1 ! ""Author"" ! z1 link Authors() ! ""Author"" ! Page(z1) output SiteGraph 2 In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph. Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together: input DataGraph where Root

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1447

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: procedures in object-oriented query languages

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kazimierz subieta , yahiko kambayashi , jacek leszczylowski
",n
"LEFT id: NA
RIGHT id: 1156

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",n
"LEFT id: NA
RIGHT id: 1981

LEFT text: Scheduling query execution plans is a particularly complex problem in hierarchical parallel systems, where each site consists of a collection of local time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory) resources and communicates with remote sites by messagepassing. We develop a general approach to the problem, capturing the full complexity of scheduling distributed multi-dimensional resource units for all kinds of parallelism within and across queries and operators. We present heuristic algorithms for various forms of the problem, some of which are provably near-optimal. Preliminary experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional resource scheduling for parallel queries

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: minos n. garofalakis , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1160

LEFT text: In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data placement in shared-nothing parallel database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 501

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on second international workshop on advanced issues of e-commerce and web-based information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: Multiversion access methods have been emerged in the literature primarily to support queries on a transaction-time database where records are never physically deleted. For a popular class of efficient methods (including the multiversion Btree), data records and index entries are occasionally duplicated to separate data according to time. In this paper, we present techniques for improving query processing in multiversion access methods. In particular, we address the problem of avoiding duplicates in the response sets. We first discuss traditional approaches that eliminate duplicates using hashing and sorting. Next, we propose two new algorithms for avoiding duplicates without using additional data structures. The one performs queries in a depth-first order starting from a root, whereas the other exploits links between data pages. These methods are discussed in full details and their main properties are identitied. Preliminary performance results confirm the advantages of these methods in comparison to traditional ones according to CPU-time, disk accesses and storage.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 956

LEFT text: The two observations that 1) many XML documents are stored in a database or generated from data stored in a database and 2) processing these documents with XSL stylesheet processors is an important, often recurring task justify a closer look at the current situation. Typically, the XML document is retrieved or constructed from the database, exported, parsed, and then processed by a special XSL processor. This cumbersome process clearly sets the goal to incorporate XSL stylesheet processing into the database engine.    We describe one way to reach this goal by translating XSL stylesheets into algebraic expressions. Further, we present algorithms to optimize the template rule selection process and the algebraic expression resulting from the translation. Along the way, we present several undecidability results hinting at the complexity of the problem on hand.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating sql databases with content-specific search engines

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan de &#223; loch , nelson mendon &#231; a mattos
",n
"LEFT id: NA
RIGHT id: 1825

LEFT text: A data warehouse is a repository of integrated information from distributed, autonomous, and possibly heterogeneous, sources. In effect, the warehouse stores one or more materialized views of the source data. The data is then readily available to user applications for querying and analysis. Figure 1 shows the basic architecture of a warehouse: data is collected from each source, integrated with data from other sources, and stored at the warehouse. Users then access the data directly from the warehouse.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient view maintenance at data warehouses

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. agrawal , a. el abbadi , a. singh , t. yurek
",n
"LEFT id: NA
RIGHT id: 988

LEFT text: In a recent paper, we proposed adding aSTOP AFTER clause to SQL to permit the cardinality of a query result to be explicitly limited by query writers and query tools. We demonstrated the usefulness of having this clause, showed how to extend a traditional cost-based query optimizer to accommodateit, and demonstrated via DB2-basedsimulations that large performancegains are possible whenSTOP AFTER queries are explicitly supported by the database engine. In this paper, we present several new strategies for efficiently processing STOP AFTER queries. These strategies, based largely on the use of range partitioning techniques, offer significant additional savings for handling STOP AFTER queries that yield sizeable result sets. We describe classes of queries where such savings would indeed arise and present experimental measurements that show the benefits and tradeoffs associated with the new processing strategies

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: reducing the braking distance of an sql query engine

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael j. carey , donald kossmann
",y
"LEFT id: NA
RIGHT id: 328

LEFT text: Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5]. The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: data mining with optimized two-dimensional association rules

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: takeshi fukuda , yasuhiko morimoto , shimichi morishita , takeshi tokuyama
",n
"LEFT id: NA
RIGHT id: 2133

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1427

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 397

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: anatomy of a real e-commerce system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: anant jhingran
",n
"LEFT id: NA
RIGHT id: 1819

LEFT text: Derived data is maintained in a database system to correlate and summarize base data which records real world facts. As base data changes, derived data needs to be recomputed. This is often implemented by writing active rules that are triggered by changes to base data. In a system with rapidly changing base data, a database with a standard rule system may consume most of its resources running rules to recompute data. This paper presents the rule system implemented as part of the STandard Real-time Information Processor (STRIP). The STRIP rule system is an extension of SQL3-type rules that allows groups of rule actions to be batched together to reduce the total recomputation load on the system. In this paper we describe the syntax and semantics of the STRIP rule system, present an example set of rules to maintain stock index and theoretical option prices in a program trading application, and report the results of experiments performed on the running system. The experiments verify that STRIP's rules allow much more efficient derived data maintenance than conventional rules without batching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the strip rule system for efficiently maintaining derived data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: brad adelberg , hector garcia-molina , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1769

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. We investigate algebraic query optimisation techniques for DBPLs in the context of a purely declarative functional language that supports sets as first-class objects. Since the language is computationally complete issues such as non-termination of expressions and construction of infinite data structures can be investigated, whilst its declarative nature allows the issue of side effects to be avoided and a richer set of equivalences to be developed. The support of a set bulk data type enables much prior work on the optimisation of relational languages to be utilised. Finally, the language has a well-defined semantics which permits us to reason formally about the prop erties of expressions, such as their equivalence with other expressions and their termination

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applications of java programming language to database management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bradley f. burton , victor w. marek
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1801

LEFT text: The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries.    We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: simultaneous optimization and evaluation of multiple dimensional queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yihong zhao , prasad m. deshpande , jeffrey f. naughton , amit shukla
",n
"LEFT id: NA
RIGHT id: 287

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vqbd : exploring semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , thomas baby , jihwang yoo
",n
"LEFT id: NA
RIGHT id: 1985

LEFT text: Rule-based optimizers and optimizer generators use rules to specify query transformations. Rules act directly on query representations, which typically are based on query algebras. But most algebras complicate rule formulation, and rules over these algebras must often resort to calling to externally defined bodies of code. Code makes rules difficult to formulate, prove correct and reason about, and therefore compromises the effectiveness of rule-based systems.In this paper we present KOLA: a combinator-based algebra designed to simplify rule formulation. KOLA is not a user language, and KOLA's variable-free queries are difficult for humans to read. But KOLA is an effective internal algebra because its combinator-style makes queries manipulable and structurally revealing. As a result, rules over KOLA queries are easily expressed without the need for supplemental code. We illustrate this point, first by showing some transformations that despite their simplicity, require head and body routines when expressed over algebras that include variables. We show that these transformations are expressible without supplemental routines in KOLA. We then show complex transformations of a class of nested queries expressed over KOLA. Nested query optimization, while having been studied before, have seriously challenged the rule-based paradigm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rule languages and internal algebras for rule-based optimizers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mitch cherniack , stanley b. zdonik
",y
"LEFT id: NA
RIGHT id: 1023

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 665

LEFT text: In this paper, we introduce a multi-agent system architecture and an implemented prototype for software component market-place. We emphasize the ontological perspective by discussing the ontology modeling for component market-place, UML extensions for ontology modeling, and the idea of ontology transfer which makes the multi-agent system to adapt itself to the dynamically changing ontologies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a multi-agent system infrastructure for software component market-place : an ontological perspective

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: riza cenk erdur , o &#287; uz dikenelli
",y
"LEFT id: NA
RIGHT id: 2016

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rainbow : multi-xquery optimization using materialized xml views

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: xin zhang , katica dimitrova , ling wang , maged el sayed , brian murphy , bradford pielech , mukesh mulchandani , luping ding , elke a. rundensteiner
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1156

LEFT text: Nowadays parallel object-relational DBMS are envisioned as the next great wave, but there is still a lack of efficient implementation concepts for some parts of the proposed functionality. Thus one of the current goals for parallel object-relational DBMS is to move towards higher performance. In this paper we develop a framework that allows to process user-defined functions with data parallelism. We will describe the class of partitionable functions that can be processed parallelly. We will also propose an extension which allows to speed up the processing of another large class of functions by means of parallel sorting. Functions that can be processed by means of our techniques are often used in decision support queries on large data volumes, for example. Hence a parallel execution is indispensable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing over object views of relational data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: gustav fahl , tore risch
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1704

LEFT text: Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia’s Programming Systems Laboratory (Section 7).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database research group at eth zurich

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: moira c. norrie , stephen m. blott , hans-j &#246; rg schek , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 776

LEFT text: Many applications require the management of spatial data. Clustering large spatial databases is an important problem which tries to find the densely populated regions in the feature space to be used in data mining, knowledge discovery, or efficient information retrieval. A good clustering approach should be efficient and detect clusters of arbitrary shape. It must be insensitive to the outliers (noise) and the order of input data. We propose WaveCluster, a novel clustering approach based on wavelet transforms, which satisfies all the above requirements. Using multiresolution property of wavelet transforms, we can effectively identify arbitrary shape clusters at different degrees of accuracy. We also demonstrate that WaveCluster is highly efficient in terms of time complexity. Experimental results on very large data sets are presented which show the efficiency and effectiveness of the proposed approach compared to the other recent clustering methods. This research is supported by Xerox Corporation. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: wavecluster : a multi-resolution clustering approach for very large spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: gholamhosein sheikholeslami , surojit chatterjee , aidong zhang
",y
"LEFT id: NA
RIGHT id: 160

LEFT text: workflow, business process, HPPM (HP Process Manager), data analysis, visualization of data Business Process Cockpit (BPC) is a tool that supports real-time monitoring, analysis, management, and optimization of business processes running on top of HP Process Manager, the Business Process Management System developed by HewlettPackard. The main goal of the Business Process Cockpit is to enable business users to perform business-level quality analysis, monitoring, and management of business processes. The BPC visualizes process execution data according to different focus points that identify the process entities that are the focus of the analysis, and different perspectives that define a way to look at the information. The BPC also allows users to define new concepts, such as “slow” and “fast” executions, and use those concepts to categorize the viewed data and make it much easier for users to interpret.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: real business processing meets the web

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: james chong
",n
"LEFT id: NA
RIGHT id: 674

LEFT text: We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a monte carlo algorithm for fast projective clustering

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: cecilia m. procopiuc , michael jones , pankaj k. agarwal , t. m. murali
",y
"LEFT id: NA
RIGHT id: 1454

LEFT text: The property management database system under development at the Northern Ireland Housing Executive (NIHE) is a large relational database system. The application system has a high expected transaction processing rate approximately 37000 transactions per day (most of them accessing mutliple tables) from about 250 on-line users. Performance is of critical importance in its success. In this paper we consider the effect of the Ingres Search Accelerator on the transaction processing efficiency of the system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a performance study of workfile disk management for concurrent mergesorts in a multiprocessor database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu , jen-yao chung , james z. teng
",n
"LEFT id: NA
RIGHT id: 1229

LEFT text: Multiversion support for XML documents is needed in many critical applications, such as software configuration control, cooperative authoring, web information warehouses, and ”e-permanence” of web documents. In this paper, we introduce efficient and robust techniques for: (i) storing and retrieving; (ii) viewing and exchanging; and (iii) querying multiversion XML documents. We first discuss the limitations of traditional version control methods, such as RCS and SCCS, and then propose novel techniques that overcome their limitations. Initially, we focus on the problem of managing secondary storage efficiently, and introduce an edit-based versioning scheme that enhances RCS with an effective clustering policy based on the concept of page-usefulness. The new scheme drastically improves version retrieval at the expense of a small (linear) space overhead. However, the edit-based approach falls short of achieving objectives (ii) and (iii). Therefore, we introduce and investigate a second scheme, which is reference-based and preserves the structure of the original document. In the reference-based approach, a multiversion document can be represented as yet another XML document, which can be easily exchanged and viewed on the web; furthermore, simple queries are also expressed and supported well under this representation. To achieve objective (i), we extend the page-usefulness clustering technique to the reference-based scheme. After characterizing the asymptotic behavior of the new techniques proposed, the paper presents the results of an experimental study evaluating and comparing their performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient filtering of xml documents with xpath expressions

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: c.-y . chan , p. felber , m. garofalakis , r. rastogi
",n
"LEFT id: NA
RIGHT id: 221

LEFT text: Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. Observing that latching index nodes for concurrency control (CC) incurs the so-called coherence cache misses on shared-memory multiprocessors thus limiting the scalability of the index performance, this paper presents an optimistic, latch-free index traversal (OLFIT) CC scheme based on a pair of consistent node read and update primitives. An experiment with various index CC implementations for the B+tree and CSB+-tree shows that the proposed scheme shows the superior scalability on the multiprocessor system as well as the performance comparable to that of the sequential execution without CC on the uniprocessor system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient concurrency control in multidimensional access methods

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , sharad mehrotra
",y
"LEFT id: NA
RIGHT id: 2290

LEFT text: P2P computing has become an extremely popular topic in computer science. It affects diverse areas such as networking, distributed systems, information systems, algorithms and databases. The P2P paradigm introduces an architectural principle “replacing” the paradigm of client-server computing. It is based on the concepts of decentralization and resource sharing. By avoiding central bottlenecks and distributing workload it facilitates the deployment of applications at a global scale. The use of the P2P paradigm as a practical

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: philip a. bernstein , yannis ioannidis , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 2248

LEFT text: Data replication has recently become a topic of increased interest among customers. Several database vendors provide products that perform data replication, The capabilities of these products and the customer problems they solve vary widely. This talk starts by identifying some of the dimensions of the replication solution space including latency, concurrency, logical and physical units of replication, network link requirements, heterogeneity, replica topology, replica transparency, and data transformation requirements. Digital Equipment Corporation provides three products that allow customers to replicate data. The distributed, two-phase commit products allow customers to program and coordinate replicated updates. DECTM Reliable Transaction Router provides an OLTP environment with transactional data replication. Transactions succeed in the face of site and network failures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: are quorums an alternative for data replication ?

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ricardo jim &#233; nez-peris , m. pati &#241; o-mart &#237; nez , gustavo alonso , bettina kemme
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: This diploma thesis covers information system SAP R3 which dates in year 1972. During decades it has been improved and spread almost in all areas of data handling in companies. By help of different modules the system covers human resources, production planning, material management, plant maintenance, project system, etc. The basic navigations in SAP R3 run by help of shortcuts of programs, called transaction. The business applications of system are divided in standard and customer applications. Standard applications are complex, extensive and general thus for easier application usually customer programs are developed covering only smaller area of company’s needs. Thus, customer applications are easier to use for end-users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 2084

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rights protection for relational data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: radu sion , mikhail atallah , sunil prabhakar
",n
"LEFT id: NA
RIGHT id: 117

LEFT text: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of environmental models for application to global information systems and decision-making

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: d. scott mackay
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: This paper proposes a cache-conscious version of the R-tree called the CR-tree. To pack more entries in a node, the CR-tree compresses MBR keys, which occupy almost 80% of index data in the two-dimensional case. It first represents the coordinates of an MBR key relatively to the lower left corner of its parent MBR to eliminate the leading O's from the relative coordinate representation. Then, it quantizes the relative coordinates with a fixed number of bits to further cut off the trailing less significant bits. Consequently, the CR-tree becomes significantly wider and smaller than the ordinary R-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Current State of Health Promotion The science and art of health promotion has made very impressive progress in the past two decades. It has evolved from :in innovative idea that made conceptual sense, but had no scientific backing to a maturing field supported by over ],000 empirical studies which demonstrate the positive health and financial impact of programs, and practiced by virtually all major employers in the US. Despite this progress, health promotion is not a part of mainstream medicine. Only a small fraction, probably less than 1%, of the $1.149 trillion spent annually on medical care is spent on health promotion. Despite the progress we h~.ve made on developing the science of health promotion, it is not recognized as a mature science by any respected scientific group. Repeated analyses conclude that roughly half of all prematu:ve deaths in the United States are from lifestyle related causes. Indeed, conservative estimates are that tobacco kills 450,000; obesity kills 300,000; and alcohol kills 100,000. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1979

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: improved histograms for selectivity estimation of range predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: viswanath poosala , peter j. haas , yannis e. ioannidis , eugene j. shekita
",y
"LEFT id: NA
RIGHT id: 1160

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data placement in shared-nothing parallel database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L 1 norm) is consistently more preferable than the Euclidean distance metric (L 2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 613

LEFT text: A new sort algorithm, called AlphaSort, demonstrates that commodity processors and disks can handle commercial batch workloads. Using commodity processors, memory, and arrays of SCSI disks, AlphaSort runs the industrystandard sort benchmark in seven seconds. This beats the best published record on a 32-CPU 32-disk Hypercube by 8:1. On another benchmark, AlphaSort sorted more than a gigabyte in one minute. AlphaSort is a cache-sensitive, memoryintensive sort algorithm. We argue that modern architectures require algorithm designers to re-examine their use of the memory hierarchy. AlphaSort uses clustered data structures to get good cache locality, file striping to get high disk bandwidth, QuickSort to generate runs, and replacement-selection to merge the runs. It uses shared memory multiprocessors to break the sort into subsort chores. Because startup times are becoming a significant part of the total time, we propose two new benchmarks: (1) MinuteSort: how much can you sort in one minute, and (2) PennySort: how much can you sort for one penny.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: alphasort : a cache-sensitive parallel external sort

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chris nyberg , tom barclay , zarka cvetanovic , jim gray , dave lomet
",y
"LEFT id: NA
RIGHT id: 93

LEFT text: National Tsing Hua University (NTttU) was founded in 1911 and is located in a suburb of the city of Hsinehu, Taiwan, about 50 miles southwest of Taipei, the capital city. Its Computer Science Department was established in 1977, and currently has 23 faculty members

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the university of oklahoma

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: le gruenwald , leonard brown , ravi dirckze , sylvain guinepain , carlos sanchez , brian summers , sirirut vanichayobon
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 66

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on-line reorganization in object databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mohana k. lakhamraju , rajeev rastogi , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 62

LEFT text: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query rewriting for semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1719

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimization of dynamic query evaluation plans

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: richard l. cole , goetz graefe
",n
"LEFT id: NA
RIGHT id: 887

LEFT text: VideoAnywhere has developed such a capability in the form of an extensible architecture as well as a specific implementation using the latest in Internet programming (Java, agents, XML, etc.) and applicable standards. It automatically extracts and manages an extensible set of metadata of major types of videos that can be queried using either attribute-based or keyword-based search. It also provides user profiling that can be combined with the query processing for filtering. A user-friendly interface provides management of all system functions and capabilities. VideoAnywhere can also be used as a video search engine for the Web, and a servlet-based version has also been implemented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: temporal integrity constraints with indeterminacy

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wes cowley , dimitris plexousakis
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 1150

LEFT text: Recent research activities in the area of Temporal Databases have revealed some problems related to the definition of time. In this paper we discuss the problem arising from the definition of valid time and the assumptions about valid time, which exist in current Temporal Database approaches. For this problem we propose a solution, while we identify some consistency problems that may appear in Temporal Databases, and which require further investigation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 344

LEFT text: Technology and education have wandered many separate but rarely intersecting paths throughout the 20th Century. In the 21st Century, the convergence of cost effective computing and networking products, methodologies, and services is finally enabling more researchers and practitioners than ever before to explore innovative ways to use computer technologies to manage and enhance the teaching and learning experience. Recognizing the importance of these trends, this Special Section solicited submissions belonging to one or all of the three mainstream learning domains, i.e., contents, methodologies, and technologies, addressing the above convergence in matters related, for example, to openness (e.g., source, access, and educational resources), online and hybrid or blended individualized and group instruction, collaborative methodologies, adaptive learning, Big Data and cloud computing applications in education, mobile learning, educational technology standards and social issues (e.g., privacy and security).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce : guest editor 's introduction

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 419

LEFT text: With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on effective multi-dimensional indexing for strings

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1506

LEFT text: Incremental refresh of a materialized join view is often less expensive than a full, non-incremental refresh. However, it is still a potentially costly atomic operation. This paper presents an algorithm that performs incremental view maintenance as a series of small, asynchronous steps. The size of each step can be controlled to limit contention between the refresh process and concurrent operations that access the materialized view or the underlying relations. The algorithm supports point-in-time refresh, which allows a materialized view to be refreshed to any time between the last refresh and the present.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 284

LEFT text: In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ominisearch : a method for searching dynamic content on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: david buttler , ling liu , calton pu , henrique paques , wei han , wei tang
",y
"LEFT id: NA
RIGHT id: 1854

LEFT text: MineSetTM is a highly integrated suite of client-server tools for the high-end mining and visualization of very large enterprise databases. MineSet represents the confluence of several important software and hardware technologies: data mining algorithms, fast multiprocessing database servers, novel techniques for interactive 3-D data visualization, and powerful graphics workstations. MineSet provides integrated facilities for the extraction of data from varied sources, algorithms for mining the extracted data, and tools for the 3-D visualization of results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: delaunay : a database visualization system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: isabel f. cruz , m. averbuch , wendy t. lucas , melissa radzyminski , kirby zhang
",n
"LEFT id: NA
RIGHT id: 1699

LEFT text:  In addition it constructs a special node Authors() and connects it to all pages corresponding to ""Author""s. The output graph is called SiteGraph. One way to write this in StruQL is: input DataGraph where Root(x); x ! ! y; y ! l ! z; l in f""Paper"", ""TechReport"", ""Title"", ""Abstract"", ""Author""g create Authors(); Page(y); Page(z) link Page(y) ! l ! Page(z) where x ! ! y1; y1 ! ""Author"" ! z1 link Authors() ! ""Author"" ! Page(z1) output SiteGraph 2 In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph. Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together: input DataGraph where Root

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a hypertext query language for images

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: li yang
",n
"LEFT id: NA
RIGHT id: 1505

LEFT text: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient maintenance of materialized mediated views

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 924

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1978

LEFT text: Success of commercial query optimizers and database management systems (object-oriented or relational) depend on accurate cost estimation of various query reordering [BGI]. Estimating predicate selectivity, or the fraction of rows in a database that satisfy a selection predicate, is key to determining the optimal join order. Previous work has concentrated on estimating selectivity for numeric fields [ASW, HaSa, IoP, LNS, SAC, WVT]. With the popularity of textual data being stored in databases, it has become important to estimate selectivity accurately for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the SQL like predicate [Dat]. Techniques used for estimating numeric selectivity are not suited for estimating alphanumeric selectivity.In this paper, we study for the first time the problem of estimating alphanumeric selectivity in the presence of wildcards. Based on the intuition that the model built by a data compressor on an input text encapsulates information about common substrings in the text, we develop a technique based on the suffix tree data structure to estimate alphanumeric selectivity. In a statistics generation pass over the database, we construct a compact suffix tree-based structure from the columns of the database. We then look at three families of methods that utilize this structure to estimate selectivity during query plan costing, when a query with predicates on alphanumeric attributes contains wildcards in the predicate.We evaluate our methods empirically in the context of the TPC-D benchmark. We study our methods experimentally against a variety of query patterns and identify five techniques that hold promise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: estimating alphanumeric selectivity in the presence of wildcards

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: p. krishnan , jeffrey scott vitter , bala iyer
",y
"LEFT id: NA
RIGHT id: 1697

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constructing the next 100 database management systems : like the handyman or like the engineer ?

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: andreas geppert , klaus r. dittrich
",n
"LEFT id: NA
RIGHT id: 99

LEFT text: This dynamic, data-centered approach opens up opportunities for personalizations: each user can be mapped to an individual hypertextual view of the Web site (called site view), and business rules may be used to change site views, both statically and dynamically. We argue that personalization of Web access (also called oneto-one Web delivery) is naturally supported by the proposed data-driven approach, and is We acknowledge the support of ESPRIT Project 28771 W3I3, MURST Project Interdata, CNR-CESTIA, and the HP Internet Philanthropic Initiative.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design principles for data-intensive web sites

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",n
"LEFT id: NA
RIGHT id: 197

LEFT text: The AQR-Toolkit divides the query routing task into two cooperating processes: query refinement and source selection. It is well known that a broadly defined query inevitably produces many false positives. Query refinement provides mechanisms to help the user formulate queries that will return more useful results and that can be processed efficiently. As a complimentary process, source selection reduces false negatives by identifying and locating a set of relevant information providers from a large collection of available sources. By pruning irrelevant information sources, source selection also reduces the overhead of contacting the information servers that do not contribute to the answer of the query. The system architecture of AQR-Toolkit consists of a hierarchical network (a directed acyclic graph) with external information providers at the leaves and query routers as mediating nodes. The end-point information providers support query-based access to their documents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an adaptive query execution system for data integration

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: zachary g. ives , daniela florescu , marc friedman , alon levy , daniel s. weld
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (XopY) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ∈{<, ≤, =, ≠, >, ≥). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ≤, =, ≥, >} and {<, ≤, =, ≠, ≥, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from <archive.fiu.edu>.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: A matrix comprising a water-insoluble beta -1,3-glucan gel in the shape of beads with diameters within the range of about 5 to 1000 mu is prepared by, for example, dispersing an alkaline aqueous solution of a water-soluble beta -1,3-glucan in a water-immiscible organic solvent, and adding an organic acid to the resultant dispersion. The matrix is useful as carrier materials for immobilized enzymes, affinity chromatography, gel filtration, ion exchange and other applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 38

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: petabyte databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: dirk d &#252; llmann
",n
"LEFT id: NA
RIGHT id: 1746

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information systems research at george mason university

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sushil jajodia , daniel barbar &#225; , alex brodsky , larry kerschberg , ami motro , edgar sibley , x. sean wang
",n
"LEFT id: NA
RIGHT id: 1020

LEFT text: Data warehouses and recording systems typically have a large continuous stream of incoming data, that must be stored in a manner suitable for future access. Access to stored records is usually based on a key. Organizing the data on disk as the data arrives using standard techniques would result in either (a) one or more I/OS to store each incoming record (to keep the data clustered by the key), which is too expensive when data arrival rates are very high, or (b) many I/OS to locate records for a particular customer (if data is stored clustered by arrival order). We study two techniques, inspired by external sorting algorithms, to store data incrementally as it arrives, simultaneously providing good performance for recording and querying. We present concurrency control and recovery schemes for both techniques. We show the benefits of our techniques both analytically and experimentally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental clustering for mining in a data warehousing environment

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: martin ester , hans-peter kriegel , j &#246; rg sander , michael wimmer , xiaowei xu
",n
"LEFT id: NA
RIGHT id: 722

LEFT text: The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: web caching for database applications with oracle web cache

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jesse anton , lawrence jacobs , xiang liu , jordan parker , zheng zeng , tie zhong
",n
"LEFT id: NA
RIGHT id: 2124

LEFT text: The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1698

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research perspectives for time series management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",n
"LEFT id: NA
RIGHT id: 668

LEFT text: In this column, we review these three books.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: Constraints provide a flexible and uniform way to represent diverse data capturing spatio-temporal behavior, complex modeling requirements, partial and incomplete information etc, and have been used in a wide variety of application domains. Constraint databases have recently emerged to deeply integrate data captured by constraints in databases. This paper reports on the development of the first constraint object-oriented database system, CCUBE, and describes its specification, design and implementation. The CCUBE system is designed to be used for the implementation and optimization of high-level constraint object-oriented query languages as well as for directly building software systems requiring extensible use of constraint database features. The CCUBE data manipulation language, Constraint Comprehension Calculus, is an integration of a constraint calculus for extensible constraint domains within monoid comprehensions, which serve as an optimization-level language for object-oriented queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 49

LEFT text: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",y
"LEFT id: NA
RIGHT id: 1704

LEFT text: The database area has been one of those areas of computerscience which have very directly been driven by applicationrequirements; this is true today in three ways: First, the userswant more application specific support from the database, and theyexpect the DBMS to have more semantic application knowledge.Second, users want database support for new applications which aresometimes far from the traditional database applications andintroduce completely new requirements as well as the need tosmoothly integrate database technology with other advancedtechnologies (e.g. neural nets) in one application. Finally, theembedding of databases into interactive work environments - forinstance, the use of databases in cooperative environments(computer supported cooperative work) - forces the databasecommunity to reconsider some of the traditional beliefs aboutdatabases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the database research group at eth zurich

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: moira c. norrie , stephen m. blott , hans-j &#246; rg schek , gerhard weikum
",n
"LEFT id: NA
RIGHT id: 1598

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ofl : a functional execution model for object query languages

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , fernando machuca , philippe pucheral
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 669

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1769

LEFT text: Various types of computer systems are used behind the scenes in many parts of the telecommunications network to ensure its efficient and trouble-free operation. These systems are large, complex, and expensive real-time computer systems that are mission critical, and contains a database engine as a critical component. These systems share some of common database issues with conventional applications, but they also exhibit rather unique characteristics that present challenging database issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applications of java programming language to database management

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bradley f. burton , victor w. marek
",n
"LEFT id: NA
RIGHT id: 1225

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 625

LEFT text: We describe the conceptual model of SORAC, a data modeling system developed at the University of Rhode Island. SORAC supports both semantic objects and relationships, and provides a tool for modeling databases needed for complex design domains. SORAC's set of built-in semantic relationships permits the schema designer to specify enforcement rules that maintain constraints on the object and relationship types. SORAC then automatically generates C++ code to maintain the specified enforcement rules, producing a schema that is compatible with Ontos. This facilitates the task of the schema designer, who no longer has to ensure that all methods on object classes correctly maintain necessary constraints. In addition, explicit specification of enforcement rules permits automated analysis of enforcement propagations. We compare the interpretations of relationships within the semantic and object-oriented models as an introduction to the mixed model that SORAC supports. Next, the set of built-in SORAC relationship types is presented in terms of the enforcement rules permitted on each relationship type. We then use the modeling requirements of an architectural design support system, called ArchObjects, to demonstrate the capabilities of SORAC. The implementation of the current SORAC prototype is also briefly discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data model for extensible support of explicit relationships in design databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joan peckham , bonnie mackellar , michael doherty
",y
"LEFT id: NA
RIGHT id: 97

LEFT text: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not suÆcient to associate \any"" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the oasis multidatabase prototype

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: mark roantree , john murphy , wilhelm hasselbring
",n
"LEFT id: NA
RIGHT id: 1831

LEFT text: Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand only

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 2253

LEFT text: We have built a multidatabase system to support a financial application that stores historical data used by traders to identify trends in the market. The application has an update rate (append-only) of 500 inserts per second and also has sub-second response requirements for queries. A typical query requests between 100-1000 records. In this paper we define the characteristics of the application, the multidatabase system we used to support the applications and the extensions we made in t.he application to achieve the required functionality and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a multidatabase system for tracking and retrieval of financial data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munir cochinwala , john bradley
",y
"LEFT id: NA
RIGHT id: 1988

LEFT text: In this paper, we present four approaches to providing highly concurrent B+-tree indices in the context of a data-shipping, client-server OODBMS architecture. The first performs all index operations at the server, while the other approaches support varying degrees of client caching and usage of index pages. We have implemented the four approaches, as well as the 2PL approach, in the context of the SHORE OODB system at Wisconsin, and we present experimental results from a performance study based on running SHORE on an IBM SP2 multicomputer. Our results emphasize the need for non-2PL approaches and demonstrate the tradeoffs between 2PL, no-caching, and the three caching alternatives.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintaining database consistency in presence of value dependencies in multidatabase systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: claire morpain , mich &#233; le cart , jean ferri &#233; , jean-fran &#231; ois pons
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 746

LEFT text: Authors and publishers who wish their publications to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4. All relevant books received will be listed, but not all can be reviewed. Technical reports (other than dissertations) will not be listed or reviewed. Authors should be aware that some publishers will not send books for review (even when instructed to do so); authors wishing to inquire as to whether their book has been received for review may contact the book review editor.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining the world wide web : an information search approach by george chang , marcus j. healey ( editor ) , james a. m. mchugh , jason t. l. wang

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: aris ouksel
",y
"LEFT id: NA
RIGHT id: 1831

LEFT text: We show how careful interspersing of queries and informed cache management can achieve rema.rkable reductions in access time compared 1x1 conventional methods. Our algoril(hms use a few model pa.rameters for each tertiary memory device and are thus designed to be portable across a wide variety of tert,ia.ry memory devices and da,tnhase t,ypes. We arc extending the PoS’TGR.ES database system to implements the new query processing strategics. Jnit,ial mea.surements on the prototype yield impressive results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 1033

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient cost-driven index selection tool for microsoft sql server

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: The problem of indexing path queries in semistructured/XML databases has received considerable attention recently, and several proposals have advocated the use of structure indexes as supporting data structures for this problem. In this paper, we investigate efficient update algorithms for structure indexes. We study two kinds of updates -- the addition of a subgraph, intended to represent the addition of a new file to the database, and the addition of an edge, to represent a small incremental change. We focus on three instances of structure indexes that are based on the notion of graph bisimilarity. We propose algorithms to update the bisimulation partition for both kinds of updates and show how they extend to these indexes. Our experiments on two real world data sets show that our update algorithms are an order of magnitude faster than dropping and rebuilding the index. To the best of our knowledge, no previous work has addressed updates for structure indexes based on graph bisimilarity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 454

LEFT text: Users of mobile computers will soon have online access to a large number of databases via wireless networks. Because of limited bandwidth, wireless communication is more expensive than wire communication. In this paper we present and analyze various static and dynamic data allocation methods. The objective is to optimize the communication cost between a mobile computer and the stationary computer that stores the online database. Analysis is performed in two cost models. One is connection (or time) based, as in cellular telephones, where the user is charged per minute of connection. The other is message based, as in packet radio networks, where the user is charged per message. Our analysis addresses both, the average case and the worst case for determining the best allocation method. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cache invalidation scheme for mobile computing systems with real-time data

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: joe chun-hung yuen , edward chan , kam-yiu lam , h. w. leung
",n
"LEFT id: NA
RIGHT id: 468

LEFT text: We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) >= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: iceberg-cube computation with pc clusters

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: raymond t. ng , alan wagner , yu yin
",n
"LEFT id: NA
RIGHT id: 2111

LEFT text: At the heart of all OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. This paper presents fast algorithms for computing a collection of group bys. We focus on a special case of the aggregation problem - computation of the CUBE operator. The CUBE operator requires computing group-bys on all possible combinations of a list of attributes, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations. Our algorithms extend sort-based and hashbased grouping methods with several .optimizations, like combining common operations across multiple groupbys, caching, and using pre-computed group-by8 for computing other groupbys. Empirical evaluation shows that the resulting algorithms give much better performance compared to straightforward meth

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: user-cognizant multidimensional analysis

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1278

LEFT text: This paper presents the framework of an Intelligent Information Presentation System (IIPS), which provides intelligent interface presentation support for data-intensive web-based applications through the use of ontologies to drive the web site generation and maintenance process. IIPS defines a comprehensive set of ontologies to model the navigational structure, the compositional structure, and the user interfaces of data-intensive web sites, and provides a suit of tools to support site generation, maintenance, and personalization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: declarative specification of web sites with s

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mary fern &#225; ndez , daniela florescu , alon levy , dan suciu
",y
"LEFT id: NA
RIGHT id: 296

LEFT text: We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:<ul><li>Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and effective metasearch for text databases incorporating linkages among documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: clement yu , weiyi meng , wensheng wu , king-lup liu
",n
"LEFT id: NA
RIGHT id: 510

LEFT text: In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a cost model for query processing in high dimensional data spaces

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christian b &#246; hm
",n
"LEFT id: NA
RIGHT id: 369

LEFT text: The chair of ACRP’s Association Board of Trustees recounts how he left manufacturing and research and development a decade ago to take a position as a clinical research administrator. It was a move that place him into a role he knew little about, having not been engaged in clinical research beforehand.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: temporal statement modifiers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , christian s. jensen , richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 1541

LEFT text: The rate of increase in database size and response-time requirements has outpaced advancements in processor and mass storage technology. One way to satisfy the increasing demand for processing power and input/output bandwidth in database applications is to have a number of processors, loosely or tightly coupled, serving database requests concurrently. Technologies developed during the last decade have made commercial parallel database systems a reality, and these systems have made an inroad into the stronghold of traditionally mainframe-based large database applications. This paper describes the DB2® Parallel Edition product that evolved from a prototype developed at IBM Research in Hawthorne, New York, and now is being jointly developed with the IBM Toronto laboratory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an overview of db2 parallel edition

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chaitanya baru , gilles fecteau
",n
"LEFT id: NA
RIGHT id: 1021

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: selectivity estimation in extensible databases - a neural network approach

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: m. seetha lakshmi , shaoyu zhou
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: The National Technical University of Athens (NTUA) is the leading Technical University in Greece. The Computer Science Division of the Electrical and Computer Engineering Department covers several fields of practical, theoretical and technical computer science and is involved in several research projects supported by the EEC, the government and industrial companies. The Knowledge and Data Base Systems (KDBS) Laboratory was established in 1992 at the National Technical University of Athens. It is recognised internationally, evidenced by its participation as a central node in the Esprit Network of Excellence IDOMENEUS. The Information and Data on Open MEdia for NEtworks of USers, project aims to coordinate and improve European efforts in the development of next-generation information environments which will be capable of maintaining and communicating a largely extended class of information in an open set of media. The KDBS Laboratory employs one full-time research engineer and several graduate students. Its infrastructure includes a LAN with several DECstation 5000/200 and 5000/240 workstations, an HP Multimedia Workstation, several PCs and software for database and multimedia applications. The basic research interests of our Laboratory include: Spatial Database Systems, Multimedia Database Systems and Active Database Systems. Apart from the above database areas, interests of the KDBS Laboratory span several areas of Information Systems, such as Software Engineering Databases, Transactional Systems, Image Databases, Conceptual Modeling, Information System Development, Temporal Databases, Advanced Query Processing and Optimization Techniques.  The group's efforts on Spatial Database Systems, include the study of new data structures, storage techniques, retrieval mechanisms and user interfaces for large geographic data bases. In particular, we look at specialized, spatial data structures (R-Trees and their variations) which allow for the direct access of the data based on their spatial properties, and not some sort of encoded representation of the objects' coordinates. We study implementation and optimization techniques of spatial data structures and develop models that make performance estimation. Finally, we are investigating techniques for the efficient representation of relationships and reasoning in space. The activities on Multimedia Database Systems, include the study of advanced data models, storage techniques, retrieval mechanisms and user interfaces for large multimedia data bases. The data models under study include the object-oriented model and the relational model with appropriate extensions to support multimedia data. We are also investigating content-based search techniques for image data bases. In a different direction, we are studying issues involved in the development of multimedia front-ends for conventional, relational data base systems. In the area of Active Database Systems, we are developing new mechanisms for implementing triggers in relational databases. Among the issues involved, we address the problem of efficiently finding qualifying rules against updates in large sets of triggers. This problem is especially critical in database system implementations of triggers, where large amounts of data may have to be searched in order to find out if a particular trigger may qualify to run or not.  Continuing work that started at the Foundation for Research and Technology (FORTH), Institute of Computer Science, the group is investigating reuse-oriented approaches to information systems application development. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 1823

LEFT text: MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: infosleuth : agent-based semantic integration of information in open and dynamic environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. bayardo , jr. , w. bohrer , r. brice , a. cichocki , j. fowler , a. helal , v. kashyap , t. ksiezyk , g. martin , m. nodine , m. rashid , m. rusinkiewicz , r. shea , c. unnikrishnan , a. unruh , d. woelk
",y
"LEFT id: NA
RIGHT id: 1689

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at nthu and itri

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arbee l. p. chen
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: The strength of commercial query optimizers like DB2 comes from their ability to select an optimal order by generating all equivalent reorderings of binary operators. However, there are no known methods to generate all equivalent reorderings for a SQL query containing joins, outer joins, and groupby aggregations. Consequently, some of the reorderings with significantly lower cost may be missed. Using hypergraph model and a set of novel identities, we propose a method to reorder a SQL query containing joins, outer joins, and groupby aggregations. While these operators are sufficient to capture the SQL semantics, it is during their reordering that we identify a powerful primitive needed for a dbms. We report our findings of a simple, yet fundamental operator, generalized selection, and demonstrate its power to solve the problem of reordering of SQL queries containing joins, outer joins, and groupby aggregations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 98

LEFT text: A data warehouse is a redundant collection of data replicated from several possibly distributed and loosely coupled source databases, organized to answer OLAP queries. Relational views are used both as a specification technique and as an execution plan for the derivation of the warehouse data. In this position paper, we summarize the versatility of relational views and their potential.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: efficient materialization and use of views in data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m &#225; rcio farias de souza , marcus costa sampaio
",n
"LEFT id: NA
RIGHT id: 1413

LEFT text: XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 109

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: specification and implementation of exceptions in workflow management systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi
",y
"LEFT id: NA
RIGHT id: 570

LEFT text: Over the years, the connection between database theory and database practice has weakened. We argue here that the new challenges posed by XML and its applications are strengthening this connection today. We illustrate three examples of theoretical problems arising from XML applications, based on our own research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: on database theory and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: dan suciu
",y
"LEFT id: NA
RIGHT id: 1261

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: deadlock detection in distributed database systems : a new algorithm and a comparative performance analysis

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: natalija krivokapi &#263; , alfons kemper , ehud gudes
",n
"LEFT id: NA
RIGHT id: 1773

LEFT text: The OOPSLA '97 Workshop on Experiences Using Object Data Management in the Real-World was held at the Cobb Galleria Centre in Atlanta, Georgia on Monday 6 October 1997. This report summarises some of the commercial case-study presentations made by workshop participants.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: workshop report on experiences using object data management in the real-world

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: akmal b. chaudhri
",y
"LEFT id: NA
RIGHT id: 589

LEFT text: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 836

LEFT text: This dynamic, data-centered approach opens up opportunities for personalizations: each user can be mapped to an individual hypertextual view of the Web site (called site view), and business rules may be used to change site views, both statically and dynamically. We argue that personalization of Web access (also called oneto-one Web delivery) is naturally supported by the proposed data-driven approach, and is We acknowledge the support of ESPRIT Project 28771 W3I3, MURST Project Interdata, CNR-CESTIA, and the HP Internet Philanthropic Initiative.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data-driven , one-to-one web site generation for data-intensive applications

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 537

LEFT text: Our system architecture to manage sensor data is described. Our data mining applications require past history of the sensor data. Therefore, unlike most present systems that focus on streaming data, and cache a small window of historic data, we store the entire historic data. Several interesting problems arise in these scenarios. We study two of them: (a) Given that a sensor can send data corresponding to its current configuration at any particular instant, how do we define the data that should be stored in the database? (b) Sensors try to minimize the amount of data transmitted. Also there could be data loss in the network. So the data stored will have lots of ""holes"". In this case, how can an application make sense of the stored data? In this paper, we describe our approach to solve these problems that enables an application to recreate the environment that generated the data as precisely as possible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: understanding the global semantics of referential actions using logic rules

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: wolfgang may , bertram lud &#228; scher
",n
"LEFT id: NA
RIGHT id: 1254

LEFT text: Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices.Our first contribution is a practical scheme that models magic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM's DB2 C/S V2 database system. Our performance measurements demonstrate that the cost-based magic optimization technique performs well, and that without it, several poor decisions could be made.Our second contribution is a formal algebraic model of magic sets rewriting, based on an extension of the multiset relational algebra, which cleanly defines the search space and can be used in a rule-based optimizer. We introduce the multiset &theta;-semijoin operator, and derive equivalence rules involving this operator. We demonstrate that magic sets rewriting for non-recursive SQL queries can be modeled as a sequential composition of these equivalence rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1444

LEFT text: Based on this probabilistic model, we develop three utility-theoretic based types of prefetching algorithms that anticipate how users will interact with the presentation. These prefetching algorithms allow efficient visualization of the query results in accordance with the underlying specification. We have built a prototype system that incorporates these algorithms. We report on the results of experiments conducted on top of this implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: document management as a database problem

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rudolf bayer
",n
"LEFT id: NA
RIGHT id: 2283

LEFT text: We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a low-cost storage server for movie on demand databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: banu &#214; zden , alexandros biliris , rajeev rastogi , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 1125

LEFT text: Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next functional join. Consequently, the algorithm is termed P(PM)*M where P stands for partitioning and M denotes merging. Our prototype implementation as well as an analytical assessment based on a cost model prove that this new algorithm performs superior in almost all database configurations. *This work was supported in part by the German National Research Foundation DFG under contracts Ke 401/6-2 and Ke 40117-I.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1432

LEFT text: Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dali : a high performance main memory storage manager

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. v. jagadish , daniel f. lieuwen , rajeev rastogi , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 992

LEFT text: Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a raster approximation for processing of spatial joins

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: geraldo zimbrao , jano moreira de souza
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",n
"LEFT id: NA
RIGHT id: 1831

LEFT text: The experimental results show that distributed commit processing can have considerably more influence than distributed data processing on the throughput performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best transaction throughput performance for a variety of workloads and system configurations. In fact, OPT's peak throughput is often close to the upper bound on achievable performance. Even more interestingly, a three-phase (i.e., non-blocking) version of OPT provides better peak throughput performance than all of the standard two-phase (i.e., blocking protocols evaluated in our study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",y
"LEFT id: NA
RIGHT id: 2250

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 556

LEFT text: The abstraction approach describes each object in the symbolic image by using a vector consisting of the values of some of its features (e.g., shape, genus, etc.). The approaches differ in the way in which responses to queries are computed. In the classification approach, images are retrieved on the basis of whether or not they contain objects that have the same classification as the objects in the query. On the other hand, in the abstraction approach, retrieval is on the basis of similarity of feature vector values of these objects. Methods of integrating these two approaches into a relational multimedia database management system so that symbolic images can be stored and retrieved based on their content are described. Schema definitions and indices that support query specifications involving spatial as well as contextual constraints are presented. Spatial constraints may be based on both locational information (e.g., distance) and relational information (e.g., north of). Different strategies for image retrieval for a number of typical queries using these approaches are described.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: detection and classification of intrusions and faults using sequences of system calls

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jo &#227; o b. d. cabrera , lundy lewis , raman k. mehra
",n
"LEFT id: NA
RIGHT id: 81

LEFT text: XML parsing is generally known to have poor performance characteristics relative to transactional database processing. Yet, its potentially fatal impact on overall database performance is being underestimated. We report real-word database applications where XML parsing performance is a key obstacle to a successful XML deployment. There is a considerable share of XML database applications which are prone to fail at an early and simple road block: XML parsing. We analyze XML parsing performance and quantify the extra overhead of DTD and schema validation. Comparison with relational database performance shows that the desired response times and transaction rates over XML data can not be achieved without major improvements in XML parsing technology. Thus, we identify research topics which are most promising for XML parser performance in database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: improving database design through the analysis of relationships

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: debabrata dey , veda c. storey , terence m. barron
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 1534

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the algres testbed of chimera : an active object-oriented database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , giuseppe psaila
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1614

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 1552

LEFT text: The MITRE Corporation provides technical assistance, system engineering, and acquisition support to large organizations, especially U.S. Government agencies. We help our customers to plan complex systems based on emerging technologies, and to implement systems based on commercial-off-the-shelf products. In MITRE's research program, instead of emphasizing concerns of DBMS or CASE vendors, our research emphasizes the issues of organizations who need to use such products. For example, we favor areas where we can build over commercial products, rather than changing their internals.Data management at MITRE goes beyond research, to include technology transition, system engineering, product evaluation, prototypes, tutorials, advice on customers' strategic directions, and participation in standards efforts. We use prototyping to illustrate potential improvements in customer systems, to understand vendors' capabilities, or both. There are close connections with efforts in object management, real-time systems, reengineering, artificial intelligence, and security.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management research at the mitre corporation

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: arnon rosenthal , len seligman , catherine mccollum , barbara blaustein , bhavani thuraisingham , edward lafferty
",y
"LEFT id: NA
RIGHT id: 1023

LEFT text: In this paper we present FeedbackBypass, a new approach to interactive similarity query processing. It complements the role of relevance feedback engines by storing and maintaining the query parameters determined with feedback loops over time, using a wavelet-based data structure (the Simplex Tree). For each query, a favorable set of query parameters can be determined and used to either “bypass” the feedback loop completely for already-seen queries, or to start the search process from a near-optimal configuration. FeedbackBypass can be combined well with all state-of-the-art relevance feedback techniques working in high-dimensional vector spaces. Its storage requirements scale linearly with the dimensionality of the query space, thus making even sophisticated query spaces amenable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 526

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1490

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 1949

LEFT text: In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a probabilistic relational model and algebra

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: debabrata dey , sumit sarkar
",n
"LEFT id: NA
RIGHT id: 1182

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 640

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rate-based query optimization for streaming information sources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: stratis d. viglas , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 2261

LEFT text: Current object database management systems support user-defined conversion functions to update the database once the schema has been modified. Two main strategies are possible when implementing such database conversion functions: immediate or lasy database updates. In this paper, we concentrate our attention to the definition of implementation strategies for conversion functions implemented as lasy database updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating a structured-text retrieval system with an object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: tak w. yan , jurgen annevelink
",n
"LEFT id: NA
RIGHT id: 76

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: spatial join selectivity using power laws

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: christos faloutsos , bernhard seeger , agma traina , caetano traina , jr.
",n
"LEFT id: NA
RIGHT id: 1641

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as an efficient deductive database engine

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",n
"LEFT id: NA
RIGHT id: 379

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: deeds towards a distributed and active real-time database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. f. andler , j. hansson , j. eriksson , j. mellin , m. berndtsson , b. eftring
",n
"LEFT id: NA
RIGHT id: 1935

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: meaningful change detection in structured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 1523

LEFT text: IP network operators collect aggregate traffic statistics on network interfaces via the Simple Network Management Protocol (SNMP). This is part of routine network operations for most ISPs; it involves a large infrastructure with multiple network management stations polling information from all the network elements and collating a real time data feed. This demo will present a tool that manages the live SNMP data feed on a fully operational large ISP at industry scale. The tool primarily serves to study correlations in the network traffic, by providing a rich mix of ad-hoc querying based on a user-friendly correlation interface and as well as canned queries, based on the expertise of the network operators with field experience.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: shore : combining the best features of oodbms and file systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the shore team
",y
"LEFT id: NA
RIGHT id: 1614

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",y
"LEFT id: NA
RIGHT id: 1800

LEFT text: In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: caching multidimensional queries using chunks

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: prasad m. deshpande , karthikeyan ramasamy , amit shukla , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1445

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bigsur : a system for the management of earth science data

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: paul brown , michael stonebraker
",n
"LEFT id: NA
RIGHT id: 1894

LEFT text: Documents stored in a database system can have complex internal structure described by a language such as SGML. How to take advantage of this structure presents challenges for database system implementors. We provide a classi cation of the types of queries that need to be supported by SGML conformant database systems. We then describe several data models that have been proposed for representing documents in a database system and the degree to which these models provide support for SGML. Finally we turn to the issue of evaluating queries considering the classic solutions for indexing text for Boolean and ranked retrieval. We describe techniques for improving the space and time e ciency of both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the aggregate data problem : a system for their definition and management

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: m. rafanelli , a. bezenchek , l. tininini
",n
"LEFT id: NA
RIGHT id: 1969

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language for multidimensional arrays : design , implementation , and optimization techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: leonid libkin , rona machlin , limsoon wong
",n
"LEFT id: NA
RIGHT id: 1539

LEFT text:  With increasing global exposure, today's enterprises must react quickly to changes, rapidly develop new services and products, and at the same time improve productivity and quality and reduce cost. Business process re-engineering and workflow automation to coordinate activities throughout the enterprise are recognized as important emerging technologies to support these requirements. Rosy estimates of a multi-billion dollar marketplace for workflow software has resulted in significant commercial activities in the area, with nearly hundred products now claiming to support workflow automation. While many help to automate document- and image-driven office applications

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: workflow automation : applications , technology and research

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: amit sheth
",y
"LEFT id: NA
RIGHT id: 1320

LEFT text: Protecting rights over relational data is of ever increasing interest, especially considering areas where sensitive, valuable content is to be outsourced. A good example is a data mining application, where data is sold in pieces to parties specialized in mining it.Different avenues for rights protection are available, each with its own advantages and drawbacks. Enforcement by legal means is usually ineffective in preventing theft of copyrighted works, unless augmented by a digital counter-part, for example watermarking.Recent research of the authors introduces the issue of digital watermarking for generic number sets. In the present paper we expand on this foundation and introduce a solution for relational database content rights protection through watermarking.Our solution addresses important attacks, such as data re-sorting, subset selection, linear data changes (applying a linear transformation on arbitrary subsets of the data). Our watermark also survives up to 50% and above data loss.Finally we present wmdb.*, a proof-of-concept implementation of our algorithm and its application to real life data, namely in watermarking the outsourced Wal-Mart sales data that we have available at our institute.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: Data in relational databases is frequently stored and retrieved using B-Trees. In &cis,ion isugprt applications the key of the B-Tree frequently involves the concatenation of several fields of the relationdl’ table. During retrieval, it is desirable to be able to access a small subset of the table based’ on partial key information, where some fields of the key may either not be present, involve ranges, or lists ‘of values. It is also advantageous to altow. this type, of access-with gen&il expressions involving any combination of disjuncts on key columns. This paper &scribes a method whereby BTrees can be eficiently used to retrieve small subsets, thus avoiding large scans of potentially huge tables. Another benefit is the ability of this method to reduce the need for additional secondary indexes, thus saving space, maintenance cost, and random accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1777

LEFT text: In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dimensionality reduction for similarity searching in dynamic databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: k. v. ravi kanth , divyakant agrawal , ambuj singh
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 663

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 2159

LEFT text: Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: very large databases in a commercial application environment

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: karl-heinz hess
",n
"LEFT id: NA
RIGHT id: 2150

LEFT text: XML has established itself over-the recent years as THE standard for representing data in scientific and business applications. Starting out as a standard data exchange format for the Web, it has become instrumental in the development of electronic commerce applications and online information services, and draws in its tailwind a multitude of standardization efforts for all kinds of applications. Documents are not only used for representing multimedia information content but also for many other purposes, like the representation of meta-information and the specification of component interfaces, protocols, and processes. As a consequence, the amount of XML data being stored and processed is large and will be increasing at an astonishing rate. This has caused XML data management to become a focus of research efforts in the database conmmnity. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on advanced xml data processing

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: NA
",n
"LEFT id: NA
RIGHT id: 1515

LEFT text: In this paper, we introduce a new type of integrity constraint, which we call a statistical constraint, and discuss its applicability to enhancing database correctness. Statistical constraints manifest embedded relationships among current attribute values in the database and are characterized by their probabilistic nature. They can be used to detect potential errors not easily detected by the conventional constraints. Methods for extracting statistical constraints from a relation and enforcement of such constraints are described. Preliminary performance evaluation of enforcing statistical constraints on a real life database is also presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enhancing database correctness : a statistical approach

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-chi hou , zhongyang zhang
",y
"LEFT id: NA
RIGHT id: 2030

LEFT text: IP network operators collect aggregate traffic statistics on network interfaces via the Simple Network Management Protocol (SNMP). This is part of routine network operations for most ISPs; it involves a large infrastructure with multiple network management stations polling information from all the network elements and collating a real time data feed. This demo will present a tool that manages the live SNMP data feed on a fully operational large ISP at industry scale. The tool primarily serves to study correlations in the network traffic, by providing a rich mix of ad-hoc querying based on a user-friendly correlation interface and as well as canned queries, based on the expertise of the network operators with field experience. The tool is called IPSOFACTO for IP Stream-Oriented FAst Correlation TOol.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ipsofacto : a visual correlation tool for aggregate network traffic data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: flip korn , s. muthukrishnan , yunyue zhu
",y
"LEFT id: NA
RIGHT id: 1254

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: opt + + : an object-oriented implementation for extensible database query optimization

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: navin kabra , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 219

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of spatial join algorithms for processing multiple inputs

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: nikos mamoulis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1588

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the merge/purge problem for large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: mauricio a. hern &#225; ndez , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 849

LEFT text: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: what is the nearest neighbor in high dimensional spaces ?

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alexander hinneburg , charu c. aggarwal , daniel a. keim
",n
"LEFT id: NA
RIGHT id: 145

LEFT text: To address these issues, we developed amdb, a visual AM “debugging” tool to support the AM design and implementation process. It is based on the GiST (Generalized Search Tree, [HNP95]) framework for AM construction, which offers the designer an abstracted view of a tree-structured AM and factors out the mechanical aspects of an AM implementation, such as tree traversal, concurrency control and recovery. Amdb is a visual analysis, debugging and profiling tool for AMs that are written as extensions of libgist, a public-domain stand-alone C++ implementation of GiSTs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: amdb : an access method debugging tool

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: marcel kornacker , mehul shah , joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 1152

LEFT text: MTCache is a prototype midtier database caching solution for SQL server that achieves this transparency. It builds on SQL server's support for materialized views, distributed queries and replication. We describe MTCache and report experimental results on the TPC-W benchmark. The experiments show that a significant part of the query workload can be offloaded to cache servers, resulting in greatly improved scale-out on the read-dominated workloads of the benchmark. Replication overhead was small with an average replication delay of less than two seconds.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: synchronization and recovery in a client-server storage system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: e. panagos , a. biliris
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments where data is updated, not only information provided by v...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 1492

LEFT text: We have developed a web-based architecture and user interface for fast storage, searching and retrieval of large, distributed, files resulting from scientific simulations. We demonstrate that the new DATALINK type defined in the draft SQL Management of External Data Standard can help to overcome problems associated with limited bandwidth when trying to archive large files using the web. We also show that separating the user interface specification from the user interface processing can provide a number of advantages. We provide a tool to generate automatically a default user interface specification, in the form of an XML document, for a given database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: high availability of commercial applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kestutis ivinskis
",y
"LEFT id: NA
RIGHT id: 215

LEFT text: From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online association rule mining

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: christian hidber
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Opening a series of concrete works to follow, this vision paper identifies, motivates, and abstracts the problem of model management. It proposes to support “models” and their “mapping” as first-class constructs, with high-level algebraic operations to manipulate. In the winter of 2000, I was a starting faculty at UIUC, and this paper inspired me immensely at the time when I had to create a research agenda of my own. I have always been interested in information integration, on various topics like query translation and data mapping. The area was exciting to me, as it was full of “real-world” problems. However, it was also not hard to see that these problems seemed inherently messy and their solutions inherently heuristic.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 663

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 794

LEFT text: In this paper, we address the need to automatically classify text documents into topic hierarchies like those in ACM Digital Library and Yahoo!. The existing local approach constructs a classi er at each split of the topic hierarchy. However, the local approach does not address the closeness of classi cation in hierarchical classi cation where the concern often is how close a classi cation is, rather than simply correct or wrong. Also, the local approach puts its bet on classi cation at higher levels where the classi cation structure often diminishes. To address these issues, we propose the notion of class proximity and cast the hierarchical classi cation as a at classi cation with the class proximity modeling the closeness of classes. Our approach is global in that it constructs a single classi er based on the global informationabout all classes and class proximity. We leverage generalized association rules as the rule/feature space to address several other issues in hierarchical classi cation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: building hierarchical classifiers using class proximity

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ke wang , senqiang zhou , shiang chen liew
",y
"LEFT id: NA
RIGHT id: 704

LEFT text: High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hd-eye : visual clustering of high dimensional data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim , markus wawryniuk
",n
"LEFT id: NA
RIGHT id: 1052

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the design and implementation of a sequence database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 1051

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dwms : data warehouse management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: narendra mohan
",n
"LEFT id: NA
RIGHT id: 1032

LEFT text: The automatic reclamation of storage for unreferenced objects is very important in object databases. Existing language system algorithms for automatic storage reclamation have been shown to be inappropriate. In this paper, we investigate methods to improve the performance of algorithms for automatic for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. Specifically, we investigate the policy that is used to select what partition in the database should be collected. The policies that we propose and investigate are based on the intuition that the values of overwritten pointers provide good hints about  where to find garbage. Using trace-driven simulation, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy and performs close to a near-optimal policy over a wide range of database sizes and object connectivities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: garbage collection in object oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: srinivas ashwin , prasan roy , s. seshadri , abraham silberschatz , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 919

LEFT text: The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be  used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data page layouts for relational databases on deep memory hierarchies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: anastassia ailamaki , david j. dewitt , mark d. hill
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 1084

LEFT text: A well-known challenge in data warehousing is the efficient incremental maintenance of warehouse data in the presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype data warehousing system supporting incremental maintenance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 788

LEFT text: Beginning in 1989 an ad-hoc collection of senior DBMS researchers has gathered periodically to perform a "" group grope "" i.e. an assessment of the state of the art in DBMS research as well as a prediction concerning what problems and problem areas deserve additional focus. The fifth ad-hoc meeting was held May 4-6, 2003 in Lowell, Ma. A report on the meeting is in preparation, and this panel discussion will summarize the upcoming document and discuss its conclusions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: unrolling cycles to decide trigger termination

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: sin yeung lee , tok wang ling
",y
"LEFT id: NA
RIGHT id: 918

LEFT text: Abstract. The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views fit into a prespecified storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. In addition, view selection is a core problem for intelligent data placement over a wide-area network for data integration applications and data management for ubiquitous computing. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equality-selection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show a somewhat surprising result, namely, that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a formal perspective on the view selection problem

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: rada chirkova , alon y. halevy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1372

LEFT text: From about the mid-1980' s to the mid-1990' s there was a flurry of research activity in the area of database triggers and constraints, seeing the development of numerous research proposals and prototypes. Soon thereafter, most mainstream database products ramped up their support for constraints and triggers, with expressive constraint specifications appearing in the SQL-92 standard, and both constraints and triggers in the SQL-99 standard.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: practical applications of triggers and constraints : success and lingering issues ( 10-year award )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: stefano ceri , roberta cochrane , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1182

LEFT text: Although many of the problems that must be solved by an object-oriented database system are similar to problems solved by relational systems, there are also significant problems that are unique. In particular, an object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for object query optimizer to select an algorithm to process the query based on direct navigation or various combinations of joins. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: garbage collection in object-oriented databases using transactional cyclic reference counting

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: p. roy , s. seshadri , a. silberschatz , s. sudarshan , s. ashwin
",n
"LEFT id: NA
RIGHT id: 949

LEFT text: Oracle Materialized Views (MVs) are designed for data warehousing and replication. For data warehousing, MVs based on inner/outer equijoins with optional aggregation, can be refreshed on transaction boundaries, on demand, or periodically. Refreshes are optimized for bulk loads and can use a multi-MV scheduler. MVs based on subqueries on remote tables support bidirectional replication. Optimization with MVs includes transparent query rewrite based on costbased selection method. The ability to rewrite a large class of queries based on a small set of MVs is supported by using Dimensions (new Oracle object), losslessness of joins, functional dependency, column equivalence, join derivability, joinback and aggregate rollup.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views selection in a multidimensional database

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: elena baralis , stefano paraboschi , ernest teniente
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable “views” of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 692

LEFT text: Data mining research typically assumes that the data to be analyzed has been identified, gathered, cleaned, and processed into a convenient form. While data mining tools greatly enhance the ability of the analyst to make data-driven discoveries, most of the time spent in performing an analysis is spent in data identification, gathering, cleaning and processing the data. Similarly, schema mapping tools have been developed to help automate the task of using legacy or federated data sources for a new purpose, but assume that the structure of the data sources is well understood. However the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields, with little reliable documentation about primary keys or foreign keys.We are developing a system, Bellman, which performs data mining on the structure of the database. In this paper, we present techniques for quickly identifying which fields have similar values, identifying join paths, estimating join directions and sizes, and identifying structures in the database. The results of the database structure mining allow the analyst to make sense of the database content. This information can be used to e.g., prepare data for data mining, find foreign key joins for schema mapping, or identify steps to be taken to prevent the database from collapsing under the weight of its complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining database structure ; or , how to build a data quality browser

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: tamraparni dasu , theodore johnson , s. muthukrishnan , vladislav shkapenyuk
",y
"LEFT id: NA
RIGHT id: 1637

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries on files

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mariano p. consens , tova milo
",n
"LEFT id: NA
RIGHT id: 1334

LEFT text: The goal of this paper is to describe the MOMIS (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic update cube for range-sum queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: seok-ju chun , chin-wan chung , ju-hong lee , seok-lyong lee
",n
"LEFT id: NA
RIGHT id: 1023

LEFT text: Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1652

LEFT text: The problems of discussing Geographic Information Systems GIS, for short begin with defining this term. There are countless definitions for GIS, each based on the type of user and application domain. The more general definition would be “a digital information system whose records are somehow geographically referenced”. For more precise definitions, one may emphasize their functional capabilities (e.g., that GIS capture and process spatial data) or the applications supported (linking the definition to the type of problem solved). Others stress the fact that GIS are ultimately tools to be used for decision support. Last but not least, a GIS is database-dependent (“a database system that supports management of spatial data”). In the database community, GIS are primarily associated with spatial databases, and therefore a large amount of the research effort in databases for GIS is related to spatial structures and access methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases for networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. v. jagadish
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 899

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in information managment at dublin city university

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mark roantree , alan f. smeaton
",n
"LEFT id: NA
RIGHT id: 1413

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: storage and retrieval of xml data using relational databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1234

LEFT text: Tree pattern is at the core of XML queries. The tree patterns in XML queries typically contain redundancies, especially when broad integrity constraints (ICs) are present and considered. Apparently, tree pattern minimization has great significance for efficient XML query processing. Although various minimization schemes/algorithms have been proposed, none of them can exploit broad ICs for thoroughly minimizing the tree patterns in XML queries. The purpose of this research is to develop an innovative minimization scheme and provide a novel implementation algorithm.Design/methodology/approach – Query augmentation/expansion was taken as a necessary first‐step by most prior approaches to acquire XML query pattern minimization under the presence of certain ICs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tree pattern query minimization

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: s. amer-yahia , s. cho , l. v. s. lakshmanan , d. srivastava
",n
"LEFT id: NA
RIGHT id: 1847

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: languages for multi-database interoperability

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is 𝒩𝒫-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 480

LEFT text: The fourth International Conference on Flexible Query Answering Systems (FQAS'2000) was held at the Academy of Sciences in Warsaw, Poland on October, 25-27, 2000. This series of conferences was launched in 1994 by Troels Andreasen, Henning Christiansen and Henrik Larsen from Roskilde University in Denmark, who have been the main driving force behind this series ever since. The previous FQAS events were held in Denmark in 1994, 1996, mad 1998. The next conference in this series will return to Denmark in 2002.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 1969

LEFT text: In the early 1980's, researchers recognized that semantic information stored in databases as integrity constraints could be used for query optimization. A new set of techniques called semantic query optimization (SQO) was developed. Some of the ideas developed for SQO have been used commercially, but to the best of our knowledge, no extensive implementations of SQO exist today. In this paper, we describe an implementation of two SQO techniques, Predicate Introduction and Join Elimination, in DB2 Universal Database. We present the implemented algorithms and performance results using the TPCD and APB-1 OLAP benchmarks. Our experiments show that SQO can lead to dramatic query performance improvements. A crucial aspect of our implementation of SQO is the fact that it does not rely on complex integrity constraints (as many previous SQO techniques did); we use only referential integrity constraints and check constraints.  

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a query language for multidimensional arrays : design , implementation , and optimization techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: leonid libkin , rona machlin , limsoon wong
",n
"LEFT id: NA
RIGHT id: 762

LEFT text: Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. Consequently, I will also briefly introduce the related XML, Java and OMG technologies like SOAP, J2EE and CORBA. One of the most important features of ASs is their ability to integrate the modern application environments with legacy data sources like IMS, CICS, VSAM, etc. They provide a number of connectors for this purpose, typically using asynchronous transactional messaging technologies like MQSeries and JMS. Traditional TPM-style requirements for industrial strength features like scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state. Recently, the ECPerf benchmark has been developed via the Java Community Process to evaluate in a standardized way the cost performance of J2EE-compliant ASs. Several caching technologies have been developed to improve performance of ASs.Soon after this conference is over, the slides of this tutorial will be available on the web at the following URL: http://www.almaden.ibm.com/u/mohan/AppServersTutorial_SIGMOD2002_Slides.pdf

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: tutorial : application servers and associated technologies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 1637

LEFT text: OptimizingQueriesOnCompressedBitmapsSihem Amer-YahiaAT&T Labs{Researchsihem@research.att.comTheo doreJohnsonjohnsont@research.att.comAbstractBitmap indices are used by DBMS's to accelerate decision supp ort queries.A signi cant advantage ofbitmap indices is that complex logical selection op erations can b e p erformed very quickly, by p erformingbit-wiseAND,OR,andNOTop erators.Althoughbitmapindicescanb espaceinecientforhighcardinalityattributes,the space use of compressed bitmapscompares well to other indexingmetho ds.Oracle and Sybase IQ are two commercial pro ducts that make extensive use of compressed bitmap indices.Our recent research showed that there are several fast algorithmsfor evaluatingBo oleanop eratorson compressedbitmaps.Dep endingon the natureof the op erandbitmaps(theirformat, densityandclusterdness) and the op eration to b e p erformed (AND, NOT, ...), these algorithms can have executiontimes that are orders of magnitude di erent.Cho osing an algorithm for p erforming a Bo olean op erationhas global e ects in the Bo olean query expression, requiring global optimization.We present a linear timedynamicprogrammingsearch strategy based on a cost mo delto optimizequeryexpressionevaluationplans.We alsopresentrewritingheuristicsthat rewritethe queryexpressionto anequivalenonetoencourage b etter algorithmsassignments.Our p erformance results show that the optimizerrequiresanegligibl e amount of time to execute, and that optimized complex queries can execute up to three timesfaster than unoptimized queries on real data.1Intro ductionAbitmap indexis a bit string in which each bit is mapp ed to a record ID (RID) of a relation.A bit in thebitmap index is set (to 1) if the corresp onding RID has prop ertyP(i.e., the RID represents a customer thatlives in New York), and is reset (to 0) otherwise.In typical usage, the predicatePis true for a record if it hasthe valueafor attributeA.One such predicate is asso ciated to one bitmap index for each unique value ofthe attributeA.The predicates can b e more complex, for example bitslice indices [OQ97] and precomputedcomplex selection predicates [HEP99].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries on files

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mariano p. consens , tova milo
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 152

LEFT text: The goal of STRUDEL project is to extend and adapt these concepts to the problem of Web-site management. Consider several tasks required of a Web-site manager. Site managers often want to manage a single repository of site data, but present different browsable “views” of the site based on criteria such as the type of user accessing the site, e.g., external or internal, expert or novice. Morever, a manager might want to modify the data repository by editing simple text files or by updating external databases, to reorganize the structure of the pages by manipulating graphs that represent the linked pages, or to design multiple presentations of a single page by editing HTML files or by using a WYSIWYG HTML generator.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the araneus web-based management system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. mecca , p. atzeni , a. masci , g. sindoni , p. merialdo
",n
"LEFT id: NA
RIGHT id: 1135

LEFT text: We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits ""horizontal"" aggregation and even aggregation over more general ""blocks"" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: schemasql - a language for interoperability in relational multi-database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , fereidoon sadri , iyer n. subramanian
",n
"LEFT id: NA
RIGHT id: 663

LEFT text: There is not appropriate testing method for the purchasing process of sealing washer in hydraulic support producing company at present.In order to solve the problem,by using the performance testing system of sealing washer worked upright column for hydraulic support,the author designed a test bed used to test the seal performance of hydraulic cylinder in the mine hydraulic support to provide database supports for purchasing sealing washer for hydraulic support manufacturers.In this paper,there will be the introduction of the principles,constitutions and functions of the test bed.It is reflected by the practical applications that the test bed is operating stably,accurately and efficiently which could be used by testing sealing washer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 345

LEFT text: This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the design and performance evaluation of alternative xml storage strategies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: feng tian , david j. dewitt , jianjun chen , chun zhang
",y
"LEFT id: NA
RIGHT id: 1898

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 2152

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 1522

LEFT text: We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: paradise : a database system for gis applications

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the paradise team
",n
"LEFT id: NA
RIGHT id: 1317

LEFT text: Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be ""first-class citizens"" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the ""accuracy"" or ""robustness"" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is ""attribute-granularity"" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and ""merging"" of, proposed updates are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a taxonomy for secure object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: martin s. olivier , sebastiaan h. von solms
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 992

LEFT text: We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a raster approximation for processing of spatial joins

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: geraldo zimbrao , jano moreira de souza
",n
"LEFT id: NA
RIGHT id: 256

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: minimization of tree pattern queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sungran cho , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1783

LEFT text: Outlier detection is an integral component of statistical modelling and estimation. For high-dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. The cut-off value is obtained from the asymptotic distribution of the distance, which enables us to control the Type I error and deliver robust outlier detection. Simulation studies show that the proposed method behaves well for high-dimensional data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automatic subspace clustering of high dimensional data for data mining applications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: rakesh agrawal , johannes gehrke , dimitrios gunopulos , prabhakar raghavan
",n
"LEFT id: NA
RIGHT id: 1652

LEFT text: We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: databases for networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. v. jagadish
",n
"LEFT id: NA
RIGHT id: 777

LEFT text: Each tuple in a valid-time relation includes an interval attribute T that represents the tuple's valid time. The overlap join between two valid-time relations determines all pairs of tuples with overlapping intervals. Although overlap joins are common, existing partitioning and indexing schemes are inefficient if the data includes long-lived tuples or if intervals intersect partition boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 1233

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: anatomy of a native xml base management system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: t. fiebig , s. helmer , c.-c . kanne , g. moerkotte , j. neumann , r. schiele , t. westmann
",n
"LEFT id: NA
RIGHT id: 225

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the trigs active object-oriented database system - an overview

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g. kappel , w. retschitzegger
",n
"LEFT id: NA
RIGHT id: 1786

LEFT text: Rule-based optimizers are extensible because they consist of modifiable sets of rules. For modification to be straightforward, rules must be easily reasoned about (i.e., understood and verified). At the same time, rules must be expressive and efficient (to fire) for rule-based optimizers to be practical. Production-style rules (as in [15]) are expressed with code and are hard to reason about. Pure rewrite rules (as in [1]) lack code, but cannot atomically express complex transformations (e.g., normalizations). Some systems allow rules to be grouped, but sacrifice efficiency by providing limited control over their firing. Therefore, none of these approaches succeeds in making rules expressive, efficient and understandable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: changing the rules : transformations for rule-based optimizers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mitch cherniack , stan zdonik
",y
"LEFT id: NA
RIGHT id: 1239

LEFT text: Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation, cube-based feature extraction, and gradient analysis, and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",y
"LEFT id: NA
RIGHT id: 917

LEFT text: Wireless and mobile computing have advanced significantly in the last decade. In particular, we now face the challenge to spontaneously establish wireless self-organizing networks, such as ad hoc, disruption-tolerant, sensor, and wireless mesh networks. These spontaneous self-organizing networks have been the focus of intensive research activity in recent years. Spontaneous networks arise from the cooperation of mobile devices in an ad hoc fashion requiring no previous infrastructure in place. A key point to couple research and real-life applications in this context is to understand how mobility (of devices, users, and applications) impacts practical networking aspects

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: Previous studies on mining association rules find rules at single concept level, however, mining association rules at multiple concept levels may lead to the discovery of more specific and concrete knowledge from data. In this study, a top-down progressive deepening method is developed for mining multiplelevel association rules from large transaction databases by extension of some existing association rule mining techniques. A group of variant algorithms are proposed based on the ways of sharing intermediate results, with the relative performance tested on different kinds of data. Relaxation of the rule conditions for finding “level-crossing” association rules is also discussed in the paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1692

LEFT text: Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metadata for multimedia documents

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: klemens b &#246; hm , thomas c. rakow
",n
"LEFT id: NA
RIGHT id: 488

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mlpq/gis constraint database system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter revesz , rui chen , pradip kanjamala , yiming li , yuguo liu , yonghui wang
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: We demonstrate a visual based XML-Relational database system where XML data is managed by commercial RDBMS. A query interface enables users to form path expression based queries against stored data visually. Statistics about data and a special path directory are used to rewrite path expression based queries into efficient SQL statements involving less number of joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 2095

LEFT text: While work in recent years has demonstrated that wavelets can be efficiently used to compress large quantities of data and provide fast and fairly accurate answers to queries, little emphasis has been placed on using wavelets in approximating datasets containing multiple measures. Existing decomposition approaches will either operate on each measure individually, or treat all measures as a vector of values and process them simultaneously. We show in this paper that the resulting individual or combined storage approaches for the wavelet coefficients of different measures that stem from these existing algorithms may lead to suboptimal storage utilization, which results to reduced accuracy to queries. To alleviate this problem, we introduce in this work the notion of an extended wavelet coefficient as a flexible storage method for the wavelet coefficients, and propose novel algorithms for selecting which extended wavelet coefficients to retain under a given storage constraint. Experimental results with both real and synthetic datasets demonstrate that our approach achieves improved accuracy to queries when compared to existing techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extended wavelets for multiple measures

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: antonios deligiannakis , nick roussopoulos
",y
"LEFT id: NA
RIGHT id: 104

LEFT text: We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",y
"LEFT id: NA
RIGHT id: 1754

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semistructured and structured data in the web : going back and forth

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: paolo atzeni , giansalvatore mecca , paolo merialdo
",n
"LEFT id: NA
RIGHT id: 2234

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: guest editor 's introduction

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 139

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: optimization techniques for queries with expensive methods

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1761

LEFT text: OdeFS is a file-like interface to the Ode objectoriented database. OdeFS allows database objects to be accessed and manipulated with standard commands, just like files in a traditional file system. No recompilation is required, so proprietary applications can access Ode objects. OdeFS is implemented as a network file server, using the NFS protocol. This paper describes OdeFS and its implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: repositories and object oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 459

LEFT text: Although research on temporal database systems has been active for about 20 years, implementations have not appeared until recently. This is one reason why current commercial database systems provide only limited temporal functionality. This paper summarizes extant state of the art of temporal database implementations. Rather than being very specific about each system we have attempted to provide an indication of the functionality together with pointers to additional information. It is hoped that this leads to more efforts pushing the implementation of temporal database systems in the near future.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the implementation and performance of compressed databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: till westmann , donald kossmann , sven helmer , guido moerkotte
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 832

LEFT text: Microsoft SQL Server was successful for many years for transaction processing and decision sup- port workloads with neither merge join nor hash join, relying entirely on nested loops and index nested loops join. How much difference do addi- tional join algorithms really make, and how much system performance do they actually add? In a pure OLTP workload that requires only record-to-record navigation, intuition agrees that index nested loops join is sufficient. For a DSS workload, however, the question is much more complex. To answer this question, we have analyzed TPC-D query perform- ance using an internal build of SQL Server with merge-join and hash-join enabled and disabled. It shows that merge join and hash join are both re- quired to achieve the best performance for decision support workloads

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the value of merge-join and hash-join in sql server

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: goetz graefe
",y
"LEFT id: NA
RIGHT id: 620

LEFT text: The Semantic Web is a vision the idea of having data on the Web defined and linked in such a way that it can be used by machines not just for display purposes but for automation, integration and reuse of data across various applications. Technically, however, there is a widespread misconception that the Semantic Web is primarily a rehash of existing AI and database work focused on encoding knowledge representation formalisms in markup languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler, and Moran seek to dispel this notion by presenting the broad dimensions of this emerging Semantic Web and the multi-disciplinary technological underpinnings like machine learning, information retrieval, service-oriented architectures, and grid computing, thus combining the informational and computational aspects needed to realize the full potential of the Semantic Web vision.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: agents , trust , and information access on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: tim finin , anupam joshi
",n
"LEFT id: NA
RIGHT id: 464

LEFT text: In late 2000, work was completed on yet another part of the SQL standard, to which we introduced our readers in an earlier edition of this column.Although SQL database systems manage an enormous amount of data, it certainly has no monopoly on that task. Tremendous amounts of data remain in ordinary operating system files, in network and hierarchical databases, and in other repositories. The need to query and manipulate that data alongside SQL data continues to grow. Database system vendors have developed many approaches to providing such integrated access.In this (partly guested) article, SQL's new part, Management of External Data (SQL/MED), is explored to give readers a better notion of just how applications can use standard SQL to concurrently access their SQL data and their non-SQL data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sql and management of external data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jim melton , jan-eike michels , vanja josifovski , krishna kulkarni , peter schwarz , kathy zeidenstein
",y
"LEFT id: NA
RIGHT id: 501

LEFT text: An interdisciplinary research community needs to address challenging issues raised by applying workflow management technology in information systems. This conclusion results from the NSF workshop on Workflow and Process Automation in Information Systems which was held at the State Botanical Garden of Georgia during May 8-10, 1996. The workshop brought together active researchers and practitioners from several communities, with significant representation from database and distributed systems, software process and software engineering, and computer supported cooperative work. The presentations given at the workshop are available in the form of an electronic proceedings of this workshop at http://lsdis.cs.uga.edu/activities/). This report is the joint work of selected representatives from the workshop and it documents the results of significant group discussions and exchange of ideas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on second international workshop on advanced issues of e-commerce and web-based information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu
",n
"LEFT id: NA
RIGHT id: 2182

LEFT text: In this paper, we propose a monitoring service that could be offered by such database servers, and present algorithms for its implementation. In contrast to published view maintenance algorithms, we do not assume that the server has access to the original materialization when computing differential view changes to be notified. We also do not assume any database capabilities on the client side and therefore compute precisely the required differentials rather than just an approximation, as is done by cache coherence techniques in homogeneous clientserver databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: incremental computation and maintenance of temporal aggregates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun yang , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1805

LEFT text: Similarity retrieval mechanisms should utilize generalized quadratic form distance functions as well as the Euclidean distance function since ellipsoid queries parameters may vary with the user and situation. In this paper, we present the spatial transformation technique that yields a new search method for adaptive ellipsoid queries with quadratic form distance functions. The basic idea is to transform the bounding rectangles in the original space, wherein distance from a query point is measured by quadratic form distance functions, into spatial objects in a new space wherein distance is measured by Euclidean distance functions. Our method significantly reduces CPU cost due to the distance approximation by the spatial transformation; exact distance evaluations are avoided for most of the accessed bounding rectangles in the index structures. We also present the multiple spatial transformation technique as an extension of the spatial transformation technique. The multiple spatial transformation technique adjusts the tree structures to suit typical ellipsoid queries; the search algorithm utilizes the adjusted structure. This technique reduces both page accesses and CPU time for ellipsoid queries. Experiments using various matrices and index structures demonstrate the superiority of the proposed methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: similarity query processing using disk arrays

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: apostolos n. papadopoulos , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 2087

LEFT text: Traditional indexes aim at optimizing the node accesses during query processing, which, however, does not necessarily minimize the total cost due to the possibly large number of random accesses. In this paper, we propose a general framework for adaptive indexes that improve overall query cost. The performance gain is achieved by allowing index nodes to contain a variable number of disk pages. Update algorithms dynamically re-structure adaptive indexes depending on the data and query characteristics. Extensive experiments show that adaptive B- and R-trees significantly outperform their conventional counterparts, while incurring minimal update overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: d ( k ) - index : an adaptive structural summary for graph-structured data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: qun chen , andrew lim , kian win ong
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: Invited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.- A Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic Relationships and Their Propagation and Concurrency Semantics in Object-Oriented Databases.- Tracking Moving Objects Using Database Technology in DOMINO.- OLOG: A Deductive Object Database Language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 1393

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information integration : the momis project demonstration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: domenico beneventano , sonia bergamaschi , silvana castano , alberto corni , r. guidetti , g. malvezzi , michele melchiori , maurizio vincini
",n
"LEFT id: NA
RIGHT id: 384

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lambda-db : an odmg-based object-oriented dbms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier
",y
"LEFT id: NA
RIGHT id: 882

LEFT text: Like any other data, biological data is a very vast one. Due to emergence of system biology it is necessary to develop various platforms and techniques to analyze and organize the biological data in meaning full manner for which it to be mined and processed carefully. As the complexity associated with biological data is high ,it has to be studied considering various criteria’s and also it is mandatory to study all available databases and then has to undergo several processing mining techniques to finally put in a format which is easy to assess and produce the information of interest. There are various techniques and method for mining biological data. Here we will put forth all possible techniques and operations involved in data mining and will compare them in order to find the advantages and disadvantages of different methods

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data mining in the bioinformatics domain

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: shalom tsur
",y
"LEFT id: NA
RIGHT id: 1675

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 1217

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimizing multiple dimensional queries simultaneously in multidimensional databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: weifa liang , maria e. orlowska , jeffrey x. yu
",n
"LEFT id: NA
RIGHT id: 1831

LEFT text: The two observations that 1) many XML documents are stored in a database or generated from data stored in a database and 2) processing these documents with XSL stylesheet processors is an important, often recurring task justify a closer look at the current situation. Typically, the XML document is retrieved or constructed from the database, exported, parsed, and then processed by a special XSL processor. This cumbersome process clearly sets the goal to incorporate XSL stylesheet processing into the database engine.    We describe one way to reach this goal by translating XSL stylesheets into algebraic expressions. Further, we present algorithms to optimize the template rule selection process and the algebraic expression resulting from the translation. Along the way, we present several undecidability results hinting at the complexity of the problem on hand.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: revisiting commit processing in distributed database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ramesh gupta , jayant haritsa , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 643

LEFT text: Like experiments performed at a laboratory bench, the results of an e-science in silico experiment are of limited value if other scientists are not able to identify the origin, or provenance, of those results. For e-Science, we need more systematic provenance logs across a range of eScience activities and disciplines as well as a more informed understanding of the information in these provenance data. Semantic Web technology, which enables data to be linked and defined in a way for more effective discovery, integration and cooperation across computers and people, provides an appropriate solution for our current requirement. In this paper we show how we used the COHSE conceptual open hypermedia system to build a dynamically generated hypertext of web of provenance documents arising from the Grid project based on associated concepts and reasoning over the ontology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: archiving scientific data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter buneman , sanjeev khanna , keishi tajima , wang-chiew tan
",y
"LEFT id: NA
RIGHT id: 1347

LEFT text: An important challenge to web technologies such as proxy caching, web portals, and application servers is keeping cached data up-to-date. Clients may have different preferences for the latency and recency of their data. Some prefer the most recent data, others will accept stale cached data that can be delivered quickly. Existing approaches to maintaining cache consistency do not consider this diversity and may increase the latency of requests, consume excessive bandwidth, or both. Further, this overhead may be unnecessary in cases where clients will tolerate stale data that can be delivered quickly. This paper introduces latency-recency profiles, a set of parameters that allow clients to express preferences for their different applications. A cache or portal uses profiles to determine whether to deliver a cached object to the client or to download a fresh object from a remote server. We present an architecture for profiles that is both scalable and straightforward to implement at a cache. Experimental results using both synthetic and trace data show that profiles can reduce latency and bandwidth consumption compared to existing approaches, while still delivering fresh data in many cases. When there is insufficient bandwidth to answer all requests at once, profiles significantly reduce latencies for all clients.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: update propagation strategies for improving the quality of data on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alexandros labrinidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: We consider the problem of finding association rules that make nearly optimal binary segmentations of huge categorical databases. The optimality of segmentation is defined by an objective function suitable for the user’s objective. An objective function is usually defined in terms of the distribution of a given target attribute. Our goal is to find association rules that split databases into two subsets, optimizing the value of an objective function. The problem is intractable for general objective functions, because letting N be the number of records of a given database, there are 2N possible binary segmentations, and we may have to exhaustively examine all of them. However, when the objective function is convex, there are feasible algorithms for finding nearly optimal binary segmentations, and we prove that typical criteria, such as “entropy (mutual information),” “x2 (correlation) ,” and “gini index (mean squared error) ,” are actually convex. We propose practical algorithms that use computational geometry techniques to handle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 2056

LEFT text: In this demo, we will show the implementation of a content-based SPatial Image Retrieval Engine (SPIRE) for multimodal unstructured data. This architecture provides a framework for retrieving multi-modal data including image, image sequence, time series and parametric data from large archives. Dramatic speedup (from a factor of 4 to 35) has been achieved for many search operations such as template matching, texture feature extraction. This framework has been applied and validated in solar flares and petroleum exploration in which spatial and spatial-temporal phenomena are located.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qcluster : relevance feedback using adaptive clustering for content-based image retrieval

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: deok-hwan kim , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: We consider data to be semistructured when there is no schema fixed or known in advance and when the data may be incomplete or irregular. For example, HTML files on the World-Wide Web usually contain some structure, but often the data is irregular or In addition, data integrated from multiple, heterogeneous information sources often is semistructured. Storing and querying semistructured data poses considerably different problems and requirements than those for traditional databases, where data storage and query processing are dependent upon structured data. Relational, nested-relational, and object-oriented database systems, for example, all depend upon the data having a known and regular schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 2192

LEFT text: We show that adaptive agents on the Internet can learn to exploit bidding agents who use a (limited) number of fixed strategies. These learning agents can be generated by adapting a special kind of finite automata with evolutionary algorithms (EAs). Our approach is especially powerful if the adaptive agent participates in frequently occurring micro-transactions, where there is sufficient opportunity for the agent to learn online from past negotiations. More in general, results presented in this paper provide a solid basis for the further development of adaptive agents for Internet applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: composing web services on the semantic web

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brahim medjahed , athman bouguettaya , ahmed k. elmagarmid
",n
"LEFT id: NA
RIGHT id: 1494

LEFT text: The management of organizational knowledge is becoming a key requirement in many engineering organizations. In many cases, it is difficult to capture this knowledge directly, as it is hidden in the way-of-working followed by networks of highly qualified specialists. Moreover, much of this knowledge is strongly context-dependent, so rules to be followed must be augmented by adequate situation analysis. Hardware and software tools used to support these processes are strongly heterogeneous, involving significant effort of usage and very different kinds of data. In this paper, we propose SURFHVV GDWD ZDUH KRXVHV as a means to remedy these problems. A process data warehouse, according to our approach, is centered around a knowledge-based metadata repository which records and drives a heterogeneous engineering process, supported by selected materialized instance data. We follow a concept-centered approach expanding ideas from the European DWQ project and illustrate our solution with a prototypical process data warehouse for chemical engineering design developed within the Collaborative Research Centre IMPROVE at Aachen University of Technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: directv and oracle rdb : the challenge of vldb transaction processing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: william l. gettys
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: DataJoiner (DJ) is a heterogeneous database system that provides a single database image of multiple databases. It provides transparent access to tables at remote databases through user defined aliases (nicknames) that can be accessed as if they were local tables. DJ is also a fully functional relational database system. A couple of salient features of the DataJoiner query optimizer are: (1) A query submitted to DataJoiner is optimized using a cost model that takes into account the remote optimizer’s capabilities in addition to the remote query processing capabilities and (2) If a remote database system lacks some functionality (eg: sorting), DataJoiner compensates for it. In this paper, we present the design of the Datajoiner query optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 424

LEFT text: In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. non-trivial use of indexes and materialized views may be enabled only by semantic constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a chase too far ?

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: lucian popa , alin deutsch , arnaud sahuguet , val tannen
",y
"LEFT id: NA
RIGHT id: 1476

LEFT text: Many researchers have investigated the process of decomposing transactions into smaller pieces to increase concurrency. The research typically focuses on implementing a decomposition supplied by the database application developer, with relatively little attention to what constitutes a desirable decomposition and how the developer should obtain such a decomposition. In this paper, we argue that the decomposition process itself warrants attention. A decomposition generates a set of proof obligations that must be satisfied to show …

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using formal methods to reason about semantics-based decompositions of transactions

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: paul ammann , sushil jajodia , indrakshi ray
",y
"LEFT id: NA
RIGHT id: 1150

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coalescing in temporal databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen , richard thomas snodgrass , michael d. soo
",n
"LEFT id: NA
RIGHT id: 1588

LEFT text: Graph databases have aroused a large interest in the last years due to their large scope of potential applications (e.g., social networks, biomedical networks, data stemming from the web). However, much published data suffer from quality problems, and graph data are no exception. In this paper, we investigate the issue of dealing with quality information in graph databases, at querying time. A framework is provided that makes it possible to introduce fuzzy quality preferences into graph pattern queries. This question is answered first from a theoretical point of view and then with an application to the Neo4j database management system by the extension of the cypher query language, for which implementation issues are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the merge/purge problem for large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: mauricio a. hern &#225; ndez , salvatore j. stolfo
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 2047

LEFT text: When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting structured data from web pages

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arvind arasu , hector garcia-molina , stanford university
",n
"LEFT id: NA
RIGHT id: 1927

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: query previews for networked information systems : a case study with nasa environmental data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: khoa doan , catherine plaisant , ben shneiderman , tom bruns
",n
"LEFT id: NA
RIGHT id: 1862

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a query language for a web-site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 1888

LEFT text: We present new techniques for supervised wrapper generation and automated web information extraction, and a system called Lixto implementing these techniques. Our system can generate wrappers which translate relevant pieces of HTML pages into XML. Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface. In this convenient user-interface very expressive extraction programs can be created. Internally, this functionality is reected by the new logicbased declarative language Elog. Users never have to deal with Elog and even familiarity with HTML is not required. Lixto can be used to create an \XML-Companion"" for an HTML web page with changing content, containing the continually updated XML translation of the relevant information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 1918

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management for earth system science

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james frew , jeff dozier
",n
"LEFT id: NA
RIGHT id: 2010

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 930

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data manager for evolvable real-time command and control systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eric hughes , roman ginis , bhavani m. thuraisingham , peter c. krupp , john a. maurer
",n
"LEFT id: NA
RIGHT id: 2246

LEFT text: In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: iterative spatial join

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: edwin h. jacox , hanan samet
",n
"LEFT id: NA
RIGHT id: 1600

LEFT text: We study various kinds of operations in a database context, and show how the inner loop of the operations can be accelerated using SIMD instructions. The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions.We consider the most important database operations, including sequential scans, aggregation, index operations, and joins. We present techniques for implementing these using SIMD instructions. We show that there are significant benefits in redesigning traditional query processing algorithms so that they can make better use of SIMD technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 1055

LEFT text: Abstract.Most research on attribute identification in database integration has focused on integrating attributes using schema and summary information derived from the attribute values. No research has attempted to fully explore the use of attribute values to perform attribute identification. We propose an attribute identification method that employs schema and summary instance information as well as properties of attributes derived from their instances. Unlike other attribute identification methods that match only single attributes, our method matches attribute groups for integration. Because our attribute identification method fully explores data instances, it can identify corresponding attributes to be integrated even when schema information is misleading. Three experiments were performed to validate our attribute identification method. In the first experiment, the heuristic rules derived for attribute classification were evaluated on 119 attributes from nine public domain data sets. The second was a controlled experiment validating the robustness of the proposed attribute identification method by introducing erroneous data. The third experiment evaluated the proposed attribute identification method on five data sets extracted from online music stores. The results demonstrated the viability of the proposed method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the role of integrity constraints in database interoperation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: mark w. w. vermeer , peter m. g. apers
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: We introduce and study a new class of queries that we refer to as OPAC (optimization under parametric aggregation constraints) queries. Such queries aim to identify sets of database tuples that constitute solutions of a large class of optimization problems involving the database tuples. The constraints and the objective function are specified in terms of aggregate functions of relational attributes, and the parameter values identify the constants used in the aggregation constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 1865

LEFT text: The FileNet Integrated Document Management (IDM) products consists of a family of client applications and Imaging and Electronic Document Management (EDM) services. These services provide robust facilities for document creation, update, and deletion along with document search capabilities. Document properties are stored in an underlying relational database (RDBMS); document content is stored in files or in a specialized optical disk hierarchical storage manager. FileNet Corporation's Visual WorkFlo® and Ensemble® workflow products can be utilized in conjunction with FileNet's IDM technologies to automate production and ad hoc business processes respectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structural matching and discovery in document databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason tsong-li wang , dennis shasha , george j. s. chang , liam relihan , kaizhong zhang , girish patel
",n
"LEFT id: NA
RIGHT id: 1092

LEFT text: Deep pretrained transformer networks are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their computational expenses deem them cost-prohibitive in practice. Our proposed approach, called PreTTR (Precomputing Transformer Term Representations), considerably reduces the query-time latency of deep transformer networks (up to a 42x speedup on web document ranking) making these networks more practical to use in a real-time ranking scenario. Specifically, we precompute part of the document term representations at indexing time (without a query), and merge them with the query representation at query time to compute the final ranking score. Due to the large size of the token representations, we also propose an effective approach to reduce the storage requirement by training a compression layer to match attention scores. Our compression technique reduces the storage required up to 95% and it can be applied without a substantial degradation in ranking performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: effective & ; efficient document ranking without using a large lexicon

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: yasushi ogawa
",y
"LEFT id: NA
RIGHT id: 48

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 877

LEFT text: Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining frequent itemsets using support constraints

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ke wang , yu he , jiawei han
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: We consider an optimization technique for deductive and relational databases. The optimization technique is an extension of the magic templates rewriting, and it can improve the performance of query evaluation by not materializing the extension of intermediate views. Standard relational techniques, such as unfolding embedded view definitions, do not apply to recursively defined views, and so alternative techniques are necessary. We demonstrate the correctness of our rewriting. We define a class of “nonrepeating” view definitions, and show that for certain queries our rewriting performs at least as well as magic templates on nonrepeating views, and often much better. A syntactically recognizable property, called “weak right-linearity”, is proposed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 1885

LEFT text: In this paper, we give an overview of the semantic integrity support in the most recent SQL-standard SQL:1999, and we show to what extent the different concepts and language constructs proposed in this standard can be found in major commercial (object-)relational database management systems. In addition, we discuss general design guidelines that point out how the semantic integrity features provided by these systems should be utilized in order to implement an effective integrity enforcing subsystem for a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: unisql 's next-generation object-relational database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: albert d'andrea , phil janus
",n
"LEFT id: NA
RIGHT id: 2034

LEFT text: n the theoretical side, we prove that with high probability, it produces a result that is a (1 + ε) factor approximation to the Euclidean nearest neighbor. On the practical side, it turns out to be extremely efficient, often exploring no more than 5% of the data to obtain very high-quality results. This method is also database-friendly, in that it accesses data primarily in a pre-defined order without random accesses, and, unlike other methods for approximate nearest neighbors, requires almost no extra storage. Also, we extend our approach to deal with the k nearest neighbors.We conduct two sets of experiments to evaluate the efficacy of our methods. Our experiments include two scenarios where nearest neighbors are typically employed---similarity search and classification problems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient similarity search and classification via rank aggregation

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ronald fagin , ravi kumar , d. sivakumar
",y
"LEFT id: NA
RIGHT id: 2255

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the impact of global clustering on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1323

LEFT text: System developments and research on parallel query processing have concentrated either on “Shared Everything” or “Shared Nothing” architectures so far. While there are several commercial DBMS based on the “Shared Disk” alternative, this architecture has received very little attention with respect to parallel query processing. A comparison between Shared Disk and Shared Nothing reveals many potential benefits for Shared Disk with respect to parallel query processing. In particular, Shared Disk supports more flexible control over the communication overhead for intra-transaction parallelism, and a higher potential for dynamic load balancing and efficient processing of mixed OLTP/query workloads. We also sketch necessary extensions for transaction management (concurrency/coherency control, logging/recovery) to support intra-transaction parallelism in the Shared Disk environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: warlock : a data allocation tool for parallel warehouses

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: thomas st &#246; hr , erhard rahm
",n
"LEFT id: NA
RIGHT id: 1998

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 760

LEFT text: This paper describes algorithms for key deletion in B+-trees. There are published algorithms and pseudocode for searching and inserting keys, but deletion, due to its greater complexity and perceived lesser importance, is glossed over completely or left as an exercise to the reader. To remedy this situation, we provide a well documented flowchart, algorithm, and pseudo-code for deletion, their relation to search and insertion algorithms, and a reference to a freely available, complete B+-tree library written in the C programming language.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementing xquery

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: paul cotton
",n
"LEFT id: NA
RIGHT id: 2286

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: re-tree : an efficient index structure for regular expressions

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chee-yong chan , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 860

LEFT text: In this paper, we introduce a formalism for expressing and reasoning about order properties: ordering and grouping constraints that hold of physical representations of relations. In so doing, we can reason about how the relation is ordered or grouped, both in terms of primary and secondary orders. After formally defining order properties, we introduce a plan refinement algorithm that infers order properties for intermediate and final query results on the basis of those known to hold of query inputs, and then exploits these inferences to avoid unnecessary sorting and grouping.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: toward learning based web query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yanlei diao , hongjun lu , songting chen , zengping tian
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 411

LEFT text: The analysis of business data is often an ill-defined task characterized by large amounts of noisy data. Because of this, business data analysis must combine two kinds of intertwined tasks: exploration and analysis. Exploration is the process of finding the appropriate subset of data to analyze, and analysis is the process of measuring the data to provide the business answer. While there are many tools available both for exploration and for analysis, a single tool or set of tools may not provide full support for these intertwined tasks. We report here on a project that set out to understand a specific business data analysis problem and build an environment to support it. The results of this understanding are, first of all, a detailed list of requirements of this task; second, a set of capabilities that meet these requirements; and third, an implemented client-server solution that addresses many of these requirements and identifies others for future work. Our solution incorporates several novel perspectives on data analysis and combines a history mechanism with a graphical, re-usable representation of the analysis and exploration process. Our approach emphasizes using the database itself to represent as many of these functions as possible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: i3 : intelligent , interactive investigation of olap data cubes

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sunita sarawagi , gayatri sathe
",n
"LEFT id: NA
RIGHT id: 2002

LEFT text: The computation of multi-dimension cube in data warehouse is of much importance.Dwarf is a highly compressed structure for computing,storing data cubes which can be materialized completely.During the constructing process,each closed node is stored in disk,while the computing of unit ALL needs access the closed nodes in the disk frequently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: socqet : semantic olap with compressed cube and summarization

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , jian pei , yan zhao
",y
"LEFT id: NA
RIGHT id: 413

LEFT text: In this paper, we describe a novel Web query processing approach with learning capabilities. Under this approach, user queries are in the form of keywords and search engines are employed to find URLs of Web sites that might contain the required information. The first few URLs are presented to the user for browsing. Meanwhile, the query processor learns both the information required by the user and the way that the user navigates through hyperlinks to locate such information. With the learned knowledge, it processes the rest URLs and produces precise query results in the form of segments of Web pages without user involvement. The preliminary experimental results indicate that the approach can process a range of Web queries with satisfactory performance. The architecture of such a query processor, techniques of modeling HTML pages, and knowledge for query processing are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fact : a learning based web query processing system

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: songting chen , yanlei diao , hongjun lu , zengping tian
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1273

LEFT text: This paper describes the design and implementation of an OODBMS, namely the METU Object-Oriented DBMS (MOOD). MOOD [Dog 94b] is developed on the Exodus Storage Manager (ESM) [ESM 92] and therefore some of the kernel functions like storage management, concurrency control, backup and recovery of data were readily available through ESM. In addition ESM has a client-server architecture and each MOOD process is a client application in ESM. The kernel functions provided by MOOD are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management. SQL statements are interpreted whereas functions (which have been previously compiled with C++) within SQL statements are dynamically linked and executed. A query optimizer is implemented by using the Volcano Query Optimizer Generator. A graphical user interface, namely Mood-View [Arp 93a, Arp 93b], is developed using Motif. MoodView displays both the schema information and the query results graphically. Additionally it is possible to update the database schema and to traverse the references in query results graphically.The system is coded in GNU C++ on Sun Sparc 2 workstations. MOOD has a SQL-like object-oriented query language, namely MOODSQL [Ozk 93b, Dog 94c]. MOOD type system is derived from C++, thus eliminating the impedance mismatch between MOOD and C++. The users can also access the MOOD Kernel from their application programs written in C++. For this purpose MOOD Kernel defines a class named UserRequest that contains a method for the execution of MOODSQL statements. The MOOD source code is available both for anonymous ftp users from ftp.cs.wisc.edu and for the WWW users from the site http://www.srdc.metu.edu.tr along with its related documents.In MOOD, each object is given a unique Object Identifier (OID) at object creation time by the ESM which is the disk start address of the object returned by the ESM. Object encapsulation is considered in two parts, method encapsulation and attribute encapsulation. These encapsulation properties are similar to the public and private declarations of C++.Methods can be defined in C++ by users to manipulate user defined classes and after compilation, they are dynamically linked and executed during the interpretation of SQL statements. This late binding facility is essential since database environments enforce run-time modification of schema and objects. With our approach, the interpretation of functions are avoided thus increasing the efficiency of the system. Dynamic linking primitives are implemented by the use of the shared object facility of SunOS [Sun 90]. Overloading is realized by making use of the signature concept of C++.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: context-based prefetch - an optimization for implementing objects on relations

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: philip a. bernstein , shankar pal , david shutt
",n
"LEFT id: NA
RIGHT id: 112

LEFT text: A new menagerie of middleware is emerging. These products promise great flexibility in partitioning enterprise applications across the diverse corporate computing landscape. What factors should you consider when choosing a solution, and how do current products stack up? More important to the focus of this article, what role should Web servers play?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the middleware muddle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david ritter
",y
"LEFT id: NA
RIGHT id: 1353

LEFT text: We deal first with the case of perfectly declustered queries, i.e., queries which retrieve a fixed proportion of the answer from each disk. We show that the fraction of the dataset which must be allocated to each disk is affected by both the relative speed and capacity of the disk. Furthermore, the hierarchical structure of most distributed systems, where groups of disks are placed in servers, imposes further complications due to variations . in server and network bandwidths which may affect the actual achievable transfer rates. We propose an algorithm which determines the fraction of the dataset which must be loaded on each disk. The algorithm may be tailored to find disk loading for minimal response time for a given database size, or to compute a system profile showing the optimal loading of the disks for all possible ranges of database sizes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1186

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: query processing techniques for arrays

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: arunprasad p. marathe , kenneth salem
",n
"LEFT id: NA
RIGHT id: 1427

LEFT text: Abstract.Due to their expressive power, regular expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multidimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations including the effective splitting of RE-tree nodes and computing a ""tight"" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Preliminary experimental results with moderately large synthetic data sets indicate that the RE-tree is effective in pruning the search space and easily outperforms naive sequential search approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the hcc-tree : an efficient index structure for object oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: b. sreenath , s. seshadri
",n
"LEFT id: NA
RIGHT id: 1385

LEFT text: To bridge the gap between these two extremes, we propose a new class of replication systems called TRAPP (Tradeoff in Replication Precision and Performance). TRAPP systems give each user fine-grained control over the tradeoff between precision and performance: Caches store ranges that are guaranteed to bound the current data values, instead of storing stale exact values. Users supply a quantitative precision constraint along with each query. To answer a query, TRAPP systems automatically select a combination of locally cached bounds and exact master data stored remotely to deliver a bounded answer consisting of a range that is no wider than the specified precision constraint, that is guaranteed to contain the precise answer, and that is computed as quickly as possible. This paper defines the architecture of TRAPP replication systems and covers some mechanics of caching data ranges.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: offering a precision-performance tradeoff for aggregation queries over replicated data

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: chris olston , jennifer widom
",y
"LEFT id: NA
RIGHT id: 1938

LEFT text: The analysis of time series in financial and scientific applications requires database functionality with complex specialized modeling capabilities and at the same time an easy-to-use interface. We present the time series management system CALANDA which combines both, a powerful dedicated data model and an intuitive GUI. The focus of this paper and the demonstration is to show how CALANDA is accessed by end users.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using the calanda time series management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: werner dreyer , angelika kotz dittrich , duri schmidt
",y
"LEFT id: NA
RIGHT id: 809

LEFT text: In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size. This is a highly desirable property for any data reduction system since the problem itself is motivated by the large size of data sets. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. Furthermore, the subspace sampling technique is able to reveal important local subspace characteristics of high dimensional data which can be harnessed for effective solutions to problems such as selectivity estimation and approximate nearest neighbor search.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multi-dimensional substring selectivity estimation

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: h. v. jagadish , olga kapitskaia , raymond t. ng , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1225

LEFT text: The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: secure buffering in firm real-time database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: binto george , jayant r. haritsa
",n
"LEFT id: NA
RIGHT id: 2255

LEFT text: Global clustering has rarely been investigated in the area of spatial database systems although dramatic performance improvements can be achieved by using suitable techniques. In this paper, we propose a simple approach to global clustering called cluster organization. We will demonstrate that this cluster organization leads to considerable performance improvements without any algorithmic overhead. Based on real geographic data, we perform a detailed empirical performance evaluation and compare the cluster organization to other organization models not using global clustering. We will show that global clustering speeds up the processing of window queries as well as spatial joins without decreasing the performance of the insertion of new objects and of selective queries such as point queries. The spatial join is sped up by a factor of about 4, whereas non-selective window queries are accelerated by even higher speed up factors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the impact of global clustering on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel
",y
"LEFT id: NA
RIGHT id: 954

LEFT text: The elapsed time for external mergesort is normally dominated by I/O time. This paper is focused on reducing I/O time during the merge phase. Three new buffering and readahead strategies are proposed, called equal buffering, extended forecasting and clustering. They exploit the fact that virtually all modern disks perform caching and sequential readahead. The latter two also collect information during run formation (the last key of each run block) which is then used to preplan reading. For random input data, extended forecasting and clustering were found to reduce merge time by 30% compared with traditional double buffering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic memory adjustment for external mergesort

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: weiye zhang , per - &#197; ke larson
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers in large datasets can only deal efficiently with two dimensions/attributes of a dataset. Here, we study the notion of DB- (DistanceBased) outliers. While we provide formal and empirical evidence showing the usefulness of DB-outliers, we focus on the development of algorithms for computing such outliers. First, we present two simple algorithms, both having a complexity of O(k N’), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we present an optimized cell-based algorithm that has a complexity that is linear wrt N, but exponential wrt k. Third, for datasets that are mainly disk-resident, we present another version of the cell-based algorithm that guarantees at most 3 passes over a dataset. We provide

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 2046

LEFT text: Managing scientific data warehouses requires constant adaptations to cope with changes in processing algorithms, computing environments, database schemas, and usage patterns. We have faced this challenge in the RHESSI Experimental Data Center (HEDC), a datacenter for the RHESSI NASA spacecraft. In this paper we describe our experience in developing HEDC and discuss in detail the design choices made. To successfully accommodate typical adaptations encountered in scientific data management systems, HEDC (i) clearly separates generic from domain specific code in all tiers, (ii) uses a file system for the actual data in combination with a DBMS to manage the corresponding meta data, and (iii) revolves around a middle tier designed to scale if more browsing or processing power is required. These design choices are valuable contributions as they address common concerns in a wide range of scientific data management systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scientific data repositories : designing for a moving target

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: etzard stolte , christoph von praun , gustavo alonso , thomas gross
",y
"LEFT id: NA
RIGHT id: 848

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 12

LEFT text: Conduct of scientific and engineering research is becoming critically dependent on effective management of scientific and engineering data and technical information. The rapid advances in scientific instrumentation, computer and communication technologies enable the scientists to collect, generate, process, and share unprecedented volumes of data. For example, the Earth Observing System Data and Information System (EOSDIS) has the task to manage the data from NASA’s Earth science research satellites and field measurement programs, and other data essential for the interpretation of these measurements in support of global change research. Apart from being able to handle a stream of 1 terabyte of data daily by the year 2000, EOSDIS will also need to provide transparent access to heterogeneous data held in the archives of several US government agencies, organizations and countries. A single graphical user interface employing the Global Change Master Directory needs to help users locate data sets of interest among massive and diverse data sets, or find the appropriate data analysis tools, regardless of their location. Another major international effort in the area of human genome research faces some similar, as well as unique issues due to the complexity of the genome data, special querying requirements and much more heterogeneous collections of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: component-based e-commerce : assessment of current practices and future directions

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: martin bichler , arie segev , j. leon zhao
",n
"LEFT id: NA
RIGHT id: 644

LEFT text: We present a two-phase Web Query Optimizer (WQO). In a pre-optimization phase, the WQO selects one or more WSIs for a pre-plan; a pre-plan represents a space of query evaluation plans (plans) based on this choice of WSIs. The WQO uses cost-based heuristics to evaluate the choice of WSI assignment in the pre-plan and to choose a good pre-plan. The WQO uses the pre-plan to drive the extended relational optimizer to obtain the best plan for a pre-plan. A prototype of the WQO has been developed. We compare the effectiveness of the WQO, i.e., its ability to efficiently search a large space of plans and obtain a low cost plan, in comparison to a traditional optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of queries in a mediator for websources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vladimir zadorozhny , louiqa raschid , maria esther vidal , tolga urhan , laura bright
",y
"LEFT id: NA
RIGHT id: 2184

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: vijay atluri , anupam joshi , yelena yesha
",n
"LEFT id: NA
RIGHT id: 647

LEFT text: In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: continuously adaptive continuous queries over streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: samuel madden , mehul shah , joseph m. hellerstein , vijayshankar raman
",n
"LEFT id: NA
RIGHT id: 1889

LEFT text: When we design an object-oriented database schema, we need to normalize object classes as we do for relations when designing a relational database schema. However, the normalization process for an object class cannot be the same as that of a relation, because of the distinct characteristics of an object-oriented data model such as complex attributes, collection data types, and the usage of object identifiers in place of relational key attributes. We need only one kind of dependency proposed here -- the object functional dependency -- which specifies the dependency of object attributes with respect to the object identifier. We also propose the object normal form of an object class, for which all determinants of object functional dependencies are object identifiers. There is no risk of update anomalies as long as all object classes are in the object normal form.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information visualization

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tiziana catarci , isabel f. cruz
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 280

LEFT text: Enterprise-class databases require database administrators who are responsible for performance tuning. With large-scale deployment of databases, minimizing database administration function becomes important. One important task of a database administrator is selecting indexes that are appropriate for the workload on the system. In data intensive applications such as decision support and data warehousing picking the right set of indexes becomes crucial for performance. Moreover, the indexes chosen should track changes in the workload. While automating the process of index selection can greatly reduce administration cost, enterprise databases are simply too complex for the administrator to hit the “accept” button on the recommendations of an index selection tool without doing a quantitative impact analysis of the recommendations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view and index selection tool for microsoft sql server 2000

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1976

LEFT text: A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: structures for manipulating proposed updates in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael doherty , richard hull , mohammed rupawalla
",n
"LEFT id: NA
RIGHT id: 1664

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mosaico-a system for conceptual modeling and rapid prototyping of object-oriented database application

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. missikoff , m. toiati
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: The cost of a query plan depends on many parameters, such as predicate selectivities and available memory, whose values may not be known at optimization time. Parametric query optimization (PQO) optimizes a query into a number of candidate plans, each optimal for some region of the parameter space. We propose a heuristic solution for the PQO problem for the case when the cost functions may be nonlinear in the given parameters. This solution is minimally intrusive in the sense that an existing query optimizer can be used with minor modifications. We have implemented the heuristic and the results of the tests on the TPCD benchmark indicate that the heuristic is very effective. The minimal intrusiveness, generality in terms of cost functions and number of parameters and good performance (up to 4 parameters) indicate that our solution is of significant practical importance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 1455

LEFT text: Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database control and eliminates double buffering, but it may expose the buffer cache to database errors. Our third design reduces this exposure by write protecting the buffer pages. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This is because wild stores rarely touch dirty, committed pages written by previous transactions. As a result, we believe that databases should use a memory interface to reliable memory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query processing in tertiary memory databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 2175

LEFT text: One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure  (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: index-driven similarity search in metric spaces

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: gisli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 1680

LEFT text: Some problems connected with the handling of null values in SQL are discussed. A definition of sure answers to SQL queries is proposed which takes care of the “no information” meaning of null values in SQL. An algorithm is presented for modifying SQL queries such that answers are not changed for databases without null values but sure answers are obtained for arbitrary databases with standard SQL semantics.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: how to modify sql queries in order to guarantee sure answers

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h.-j . klein
",y
"LEFT id: NA
RIGHT id: 1442

LEFT text: We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering followed by parallelization. We focus on the parallelization phase and develop algorithms for exploiting pipelined parallelism. We formulate parallelization as scheduling a weighted operator tree to minimize response time. Our model of response time captures the fundamental tradeoff between parallel execution and its communication overhead. We assess the quality of an optimization algorithm by its performance ratio which is the ratio of the response time of the generated schedule to that of the optimal. We develop fast algorithms that produce near-optimal schedules the performance ratio is extremely close to 1 on the average and has a worst case bound of about 2 for many cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coloring away communication in parallel query optimization

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: waqar hasan , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1707

LEFT text: New telecommunication services and mobility networks have introduced databases in telecommunication networks. Compared with traditional use of databases, telecom databases must fulfill very tough requirements on response time, throughput, and availability. ClustRa is a telecom database prototype developed to run on standard workstations interconnected by an ATM switch. To meet the throughput and real-time response requirements, ClustRa is a main memory database with neighbor main, memory logging. Transactions are executed in parallel. To meet the availability requirements, we use a 2-safe replication scheme over two sites with independent failure modes, a novel declustering strategy, early detection of failures with fast takeover, and by on-line self-repair and maintenance. This paper gives an overview of ClustRa and includes a set of performance measurements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: databases for gis

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: claudia bauzer medeiros , fatima pires
",n
"LEFT id: NA
RIGHT id: 1035

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries with universal quantification in object-oriented and object-relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jens clau &#223; en , alfons kemper , guido moerkotte , klaus peithner
",n
"LEFT id: NA
RIGHT id: 1918

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management for earth system science

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james frew , jeff dozier
",n
"LEFT id: NA
RIGHT id: 642

LEFT text: Modern applications (Web portals, digital libraries, etc.) require integrated access to various information sources (from traditional DBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient XML query evaluation. We define a general purpose algebra suitable for semistructured on XML query languages. We show how this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally, we develop new optimization techniques for XML-based integration systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient integration and aggregation of historical information

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mirek riedewald , divyakant agrawal , amr el abbadi
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their verifiability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: Despite some early signs that our final FY 2002 fund balance could be considerably lower than the $142k we had originally projected for the end of FY 02 (see Tamer Ozsu's chair message in SIGMOD Record Vol. 31, No. 2), our financial situation remains sound. On June 30 at the end of FY02, our actual fund balance had risen to almost $160k, nearly $20K more than projected and $40k above the minimum fund balance required by ACM.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 1932

LEFT text: Three designs of hierarchical locking suitable for B-tree indexes are explored in detail and their advantages and disadvantages compared. Traditional hierarchies include index, leaf page, and key range or key value. Alternatively, locks on separator keys in interior B-tree pages can protect key ranges of different sizes. Finally, for keys consisting of multiple columns, key prefixes of different sizes permit a third form of hierarchical locking. Each of these approaches requires appropriate implementation techniques. The techniques explored here include node splitting and merging, lock escalation and lock de-escalation, and online changes in the granularity of locking. Those techniques are the first designs permitting introduction and removal of levels in a lock hierarchy on demand and without disrupting transaction or query processing. In addition, a simplification of traditional key range locking is introduced that applies principled hierarchical locking to keys in B-tree leaves. This new method of key range locking avoids counter-intuitive lock modes used in today’s highperformance database systems. Nonetheless, it increases concurrency among operations on individual keys and records beyond that enabled by traditional lock modes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: concurrency and recovery in generalized search trees

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marcel kornacker , c. mohan , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 88

LEFT text: In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: distributed transactions in practice

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: prabhu ram , lyman do , pamela drew
",y
"LEFT id: NA
RIGHT id: 1353

LEFT text: In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering xml queries on heterogeneous data sources

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana manolescu , daniela florescu , donald kossmann
",n
"LEFT id: NA
RIGHT id: 1170

LEFT text: In digital libraries image retrieval queries can be based on the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651.7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including weighted queries. Compared to Fagin’s algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical data indicate efficiency gains by a factor of 30. For very skewed data observed speed-up factors are even larger. These performance results scale through different database sizes and numbers of result sets to combine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: optimal clip ordering for multi-clip queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , paul shum
",n
"LEFT id: NA
RIGHT id: 1571

LEFT text: One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast algorithms for universal quantification in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: goetz graefe , richard l. cole
",n
"LEFT id: NA
RIGHT id: 614

LEFT text: QuickStore is a memory-mapped storage system for persistent C++, built on top of the EXODUS Storage Manager. QuickStore provides fast access to in-memory objects by allowing application programs to access objects via normal virtual memory pointers. This article presents the results of a detailed performance study using the OO7 benchmark. The study compares the performance of QuickStore with the latest implementation of the E programming language. The QuickStore and E systems exemplify the two basic approaches (hardware and software) that have been used to implement persistence in object-oriented database systems. In addition, both systems use the same underlying storage manager and compiler, allowing us to make a truly apples-to-apples comparison of the hardware and software techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",y
"LEFT id: NA
RIGHT id: 480

LEFT text: A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1909

LEFT text: Environmental Management Information Systems (EMIS) are socio-technological systems used as business applications to gather, process, and provide environmental information, inside companies and in exchange with other actors in industry. They help to identify environmental impacts and support measures avoiding these impacts or reducing them. EMIS provide the necessary information support for decision making in companies. Hence, EMIS can be viewed as certain Information Systems (IS) usually implemented in companies as a part of their Environmental Management Systems (EMS). In order to give a tangible example with practical implications, the developments that EMIS have passed the last years are described along the field of “online communication and sustainability reporting” and illustrated by a case study. This area represents an emerging digital and fully ICT (information and communication technologies) supported approach within EMIS, using current internet technologies and services. It makes clear the array of capabilities of latest EMIS to be exploited for the improvement of advanced environmental and sustainability management, finally to the benefit for companies and their various stakeholders. The case study describes the concept and implementation of a software tool with shopping cart functionality providing sustainability reports a la carte so that stakeholders (i.e. users, readers) can create their own report on the fly, exactly meeting their detailed information needs and preferred media out from a single publishing database. This software tool which represents a module of a comprehensive EMIS is implemented as a web-based ICT application. Its performance goes beyond the leadingedge approach of O2 who provides a personalized reporting feature on its website that could be regarded as best practice and pioneering effort in sustainability online reporting, so far.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 927

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 771

LEFT text: Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the problem of building an industry-strength tool for automated selection of materialized views and indexes for SQL workloads. In this paper, we present an end-to-end solution to the problem of selecting materialized views and indexes. We describe results of extensive experimental evaluation that demonstrate the effectiveness of our techniques. Our solution is implemented as part of a tuning wizard that ships with Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: materialized views in oracle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: randall g. bello , karl dias , alan downing , james j. feenan , jr. , james l. finnerty , william d. norcott , harry sun , andrew witkowski , mohamed ziauddin
",n
"LEFT id: NA
RIGHT id: 1951

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mining quantitative association rules in large relational tables

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramakrishnan srikant , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1668

LEFT text: Substantive changes in the business environment—and aggressive initiatives in business process reengineering—are driving corresponding changes in the information technology architectures of large enterprises. Those changes are enabled by the convergence of a long list of maturing new technologies. As one of its many implications, the new IT architecture demands revised assumptions about the design and deployment of databases. This paper reviews the components of the architectural shift now in process, and offers strategic planning assumptions for database professionals.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: enterprise information architectures-they 're finally changing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: wesley p. melling
",y
"LEFT id: NA
RIGHT id: 279

LEFT text: Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: orthogonal optimization of subqueries and aggregation

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c &#233; sar galindo-legaria , milind joshi
",n
"LEFT id: NA
RIGHT id: 904

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1057

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: information management for genome level bioinformatics

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: norman w. paton , carole a. goble
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1027

LEFT text: The automatic reclamation of storage for unreferenced objects is very important in object databases. Existing language system algorithms for automatic storage reclamation have been shown to be inappropriate. In this paper, we investigate methods to improve the performance of algorithms for automatic for automatic storage reclamation of object databases. These algorithms are based on a technique called partitioned garbage collection, in which a subset of the entire database is collected independently of the rest. Specifically, we investigate the policy that is used to select what partition in the database should be collected. The policies that we propose and investigate are based on the intuition that the values of overwritten pointers provide good hints about  where to find garbage. Using trace-driven simulation, we show that one of our policies requires less I/O to collect more garbage than any existing implementable policy and performs close to a near-optimal policy over a wide range of database sizes and object connectivities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrent garbage collection in o2

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marcin skubiszewski , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 184

LEFT text: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wavelet-based histograms for selectivity estimation

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",y
"LEFT id: NA
RIGHT id: 343

LEFT text: Editor's note: For this issue's ""From the Editors,"" I invited Robert Gephart of the University of Al-berta to reflect on his observations as a long-serving , award-winning reviewer of qualitative research for A!vII Over the past two and a half years, I have developed a tremendous respect for Bob's keen eye for evaluating qualitative research submissions , and great admiration for the painstaking advice he provides authors about how to improve their work. As a world-renowned qualitative author himself, Bob is in an excellent position to provide observations about how authors might increase the chances of having their qualitative research accepted for publication at AMI In a three-way electronic mail conversation about the challenges and opportunities of qualitative research , Bob, Tom Lee, and I all concluded that many authors with potentially very interesting data sets don't seem to know how to analyze them to their full potential. This is perhaps not surprising, gi ven the clear predominance of quantitative methods and statistics courses over qualitative ones, particularly in North America, as well as the inherently greater subjectivity involved in designing and analyzing qualitative research. As such, we encouraged Bob to provide a bit of a minitutorial-complete with reference citations and examples of high-quality papers that use particular qualitative approaches-in addition to his observations about qualitative research submitted to AMI The result is a longer-than-usual ""From the Edi-tors"" column. but one that we believe is well worth the extra reading time for anyone interested in producing , reviewing, or attempting to coax greater insights from qualitative research. We are fortunate to have someone with Bob's expertise share his observations, and we hope that his thoughts will prove useful to researchers for many years to come. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1194

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: building knowledge base management systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: john mylopoulos , vinay chaudhri , dimitris plexousakis , adel shrufi , thodoros topologlou
",n
"LEFT id: NA
RIGHT id: 1756

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database systems-breaking out of the box

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: avi silberschatz , stan zdonik
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 986

LEFT text: Views are a central component of both traditional database systems and new applications such as data warehouses. Very often the desired views (e.g. the transitive closure) cannot be defined in the standard language of the underlying database system. Fortunately, it is often possible to incrementally maintain these views using the standard language. For example, transitive closure of acyclic graphs, and of undirected graphs, can be maintained in relational calculus after both single edge insertions and deletions. Many such results have been published in the theoretical database community. The purpose of this survey is to make these useful results known to the wider database research and development community.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental maintenance for materialized views over semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: serge abiteboul , jason mchugh , michael rys , vasilis vassalos , janet l. wiener
",n
"LEFT id: NA
RIGHT id: 2152

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1513

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: applying update streams in a soft real-time database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: b. adelberg , h. garcia-molina , b. kao
",n
"LEFT id: NA
RIGHT id: 666

LEFT text: Events are at the core of reactive and proactive applications, which have become popular in many domains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: conceptual modeling and specification generation for b2b business processes based on ebxml

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: hyoungdo kim
",n
"LEFT id: NA
RIGHT id: 1215

LEFT text: Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an experimental object-based sharing system for networked databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: doug fang , shahram ghandeharizadeh , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 991

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database system for real-time event aggregation in telecommunication

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jerry baulier , stephen blott , henry f. korth , abraham silberschatz
",n
"LEFT id: NA
RIGHT id: 346

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a logical foundation for deductive object-oriented databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: mengchi liu , gillian dobbie , tok wang ling
",n
"LEFT id: NA
RIGHT id: 2067

LEFT text: The Web presents the database area with vast opportunities and commensurate challenges. Databases and the Web are organically connected at many lev els. Web sites are increasingly pow ered b y databases.Collections of linked Web pages distributed across the Internet are themselves tempting targets for a database. The emergence of XML as the lingua franc a of the Web brings some m uchneeded order and will greatly facilitate the use of database techniques to manage Web information. This paper will discuss some of the developments related to the Web from the viewpoint of database theory. As we shall see, the Web scenario requires revisiting some of the basic assumptions of the area. T o be sure, database theory remains as valid as ev er in the classical setting, and the database industry will continue to represent a multi-billion dollar target of applicability for the foreseeable future. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a web odyssey : from codd to xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: victor vianu
",y
"LEFT id: NA
RIGHT id: 1799

LEFT text: Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: extracting schema from semistructured data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: svetlozar nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1728

LEFT text: Video is composed of audio-visual information. Providing content based access to video data is essential for the sucessful integration of video into computers. Organizing video for content based access requires the use of video metadata. This paper explores the nature video metadata. A data model for video databases is presented based on a study of the applications of video, the nature of video retrieval requests, and the features of video. The data model is used in the architectural framework of a video database. The current state of technology in video databases is summarized and research issues are highlighted.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metadata in video databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ramesh jain , arun hampapur
",y
"LEFT id: NA
RIGHT id: 1926

LEFT text: Bioinformatics, the discipline concerned with biological information management is essential in the post-genome era, where the complexity of data processing allows for contemporaneous multi level research including that at the genome level, transcriptome level, proteome level, the metabolome level, and the integration of these -omic studies towards gaining an understanding of biology at the systems level. This research is also having a major impact on disease research and drug discovery, particularly through pharmacogenomics studies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: opportunities in information management and assurance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 913

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the first international workshop on efficient web-based information systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: zo &#233; lacroix , omar boucelma
",n
"LEFT id: NA
RIGHT id: 1508

LEFT text: Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hypergraph based reorderings of outer join queries with complex predicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gautam bhargava , piyush goel , bala iyer
",n
"LEFT id: NA
RIGHT id: 2104

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1004

LEFT text: Content-based retrieval of images is the ability to retrieve images that are similar to a query image. Oracle8i Visual Information Retrieval provides this facility based on technology licensed from Virage, Inc. This product is built on top of Oracle8i interMedia which enables storage, retrieval and management of images, audios and videos. Images are matched using attributes such as color, texture and structure and efficient content-based retrieval is provided using indexes of an image index type. The design of the index type is based on a multi-level filtering algorithm. The filters reduce the search space so that the expensive comparison algorithm operates on a small subset of the data. Bitmap indexes are used to evaluate the first filter resulting in a design which performs well and is scalable. The image index type is built using Oracle8i extensible indexing technology, allowing users to create, use, and drop instances of this index type as they would any other standard index. In this paper we present an overview of the product, the design of the image index type, and some performance results of our product.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: checkpointing in oracle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: ashok joshi , william bridge , juan loaiza , tirthankar lahiri
",n
"LEFT id: NA
RIGHT id: 1672

LEFT text: In a graphical interface which is used to display database objects, dynamic displays are updated automatically as modifications occur to the database objects being visualised. Approaches based on enlarging either the database system or the interface code to provide the appropriate communication, complicates the interaction between the two systems, as well as making later updates cumbersome. In this paper, an approach based on active rules is presented. The declarative and modular description of active rules enables active displays to be supported with minimal changes to the database or its graphical interface. Although this approach has been used to support the link between a database system and its graphical interface, it can easily be adapted to support dynamic interaction between an active database system and other external systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: eos : an extensible object store

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alexandros biliris , euthimios panagos
",n
"LEFT id: NA
RIGHT id: 1527

LEFT text: The information available on the internet is growing at very high rate. Especially, news articles are added and updated round-the-clock. News retrieval systems which are in use today, are not very much capable of handling such huge amounts of news articles effectively and accurately. Due to the need for frequent and intensive processing, a news retrieval system needs to be scalable, robust and fault tolerant. By the use of Cloud technology, this can achieved. A news retrieval system on the cloud can be used to fetch, process, organize and also be used for faster and accurate retrieval. It can be made to operate with less supervision or none at all. Cloud Press, a next generation news retrieval system presented here, is designed and implemented, to overcome most of the pit falls of the news retrieval systems, which are in place today. It uses MapReduce paradigm for fetching, processing and organizing all the news articles in a distributed fashion. MapReduce approach allows it to split the tasks into sub-tasks and then allows them to be assigned to various nodes present in the cloud, which are then finished and consolidated to give one final output. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: visdb : a system for visualizing large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1190

LEFT text: We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a template model for multidimensional inter-transactional association rules

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ling feng , jeffrey xu yu , hongjun lu , jiawei han
",n
"LEFT id: NA
RIGHT id: 577

LEFT text: Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently instantiating view-objects from remote relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: byung suk lee , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 243

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lore : a lightweight object repository for semistructured data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: dallan quass , jennifer widom , roy goldman , kevin haas , qingshan luo , jason mchugh , svetlozar nestorov , anand rajaraman , hugo rivero , serge abiteboul , jeff ullman , janet wiener
",n
"LEFT id: NA
RIGHT id: 1149

LEFT text: Providing content-based video query, retrieval and browsing is the most important goal of a video database management system (VDBMS). Video data is unique not only in terms of its spatial and temporal characteristics, but also in the semantic associations manifested by the entities present in the video. This paper introduces a novel video data model called Logical Hypervideo Data Model. In addition to multilevel video abstractions, the model is capable of representing video entities that users are interested in (defined as hot objects) and their semantic associations with other logical video abstractions, including hot objects themselves.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: supporting periodic authorizations and temporal reasoning in database access control

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: elisa bertino , claudio bettini , elena ferrari , pierangela samarati
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1667

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ores temporal database management system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: babis theodoulidis , aziz ait-braham , george andrianopoulos , jayant chaudhary , george karvelis , simon sou
",n
"LEFT id: NA
RIGHT id: 1836

LEFT text: This paper aims at classifying existing approaches which can be used to query heterogeneous data sources. We consider one of the approaches — the mediated query approach — in more detail and provide a classification framework for it as well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distance-based indexing for high-dimensional metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: Parallel database systems have to support the effective parallelization of complex queries in multi-user mode, i.e. in combination with inter-query~mter-transaction parallelism. For this purpose, dynamic scheduling and load balancing strategies’ are necessary that umsider the current system state for dekrminhg the degree of intra-query parallelism and for selecting the processors for executing subqueries. We study these issues for parallel hash joinprocessing and show that the two subproblems should be addressed in au integrated way. Even more importantly, however, is the use of a multimannce load balancing approach that considers all potential bottleneck resources. in particular memory, disk and CPU. We discuss basic performance tradeoffs to consider and evalGate the performauce of several oad balancing strategies by means of a detailed simulation model. Simulation results will be analyzed for multiuser configurations with both homogeneous andheterogeneous (query/OLTP) workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 1873

LEFT text: Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensiona...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on first international workshop on real-time database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros , kwei-jay lin , sang son
",n
"LEFT id: NA
RIGHT id: 1260

LEFT text: In query-intensive database application areas, like decision support and data mining, systems that use vertical fragmentation have a significant performance advantage. In order to support relational or object oriented applications on top of such a fragmented data model, a flexible yet powerful intermediate language is needed. This problem has been successfully tackled in Monet, a modern extensible database kernel developed by our group. We focus on the design choices made in the Monet interpreter language (MIL), its algebraic query language, and outline how its concept of tactical optimization enhances and simplifies the optimization of complex queries. Finally, we summarize the experience gained in Monet by creating a highly efficient implementation of MIL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: mil primitives for querying a fragmented world

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: peter a. boncz , martin l. kersten
",y
"LEFT id: NA
RIGHT id: 1664

LEFT text: The system Mosaico has been conceived to support the design, conceptual modeling, and rapid prototyping of data intensive applications based on Object-Oriented Databases (OODBS). The application is modeled through a graphical user interface and the produced model is encoded in TQL++, the design language on which Mosaico is based.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mosaico-a system for conceptual modeling and rapid prototyping of object-oriented database application

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: m. missikoff , m. toiati
",y
"LEFT id: NA
RIGHT id: 620

LEFT text: On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe GLUE, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology GLUE finds the most similar concept in the other ontology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: agents , trust , and information access on the semantic web

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: tim finin , anupam joshi
",n
"LEFT id: NA
RIGHT id: 1020

LEFT text: A data warehouse materializes views derived from data that may not reside at the warehouse. Maintaining these views effciently in response to base updates is diffcult, since it may involve querying external sources where the base data reside. This paper considers the problem of view self-maintenance, where the views are maintained without using all the base data. Without full use of the base data, however, maintaining a view unambiguously is not always possible. Thus, the two critical questions that must be addressed are to determine, in a given situation, whether a view is maintainable, and how to maintain it. W e provide algorithms that answer these questions for a general class of views, and for an important subclass, generate SQL queries that test whether a view is self-maintainable and update the view if it is. We improve significantly on previous work by solving the view self-maintenance problem in the presence of multiple views, with optional access to a subset of the base data, and under arbitrary mixes of insertions and deletions. We provide better insight into the problem by showing that view self-maintainability can be reduced to the problem of deciding query containment. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: incremental clustering for mining in a data warehousing environment

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: martin ester , hans-peter kriegel , j &#246; rg sander , michael wimmer , xiaowei xu
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1808

LEFT text: Today the problem of semantic interoperability in information search on the Internet is solved mostly by means of centralization, both at a system and at a logical level. This approach has been successful to a certain extent. Peer-to-peer systems as a new brand of system architectures indicate that the principle of decentralization might offer new solutions to many problems that scale well to very large numbers of users.In this paper we outline how the peer-to-peer system architectures can be applied to tackle the problem of semantic interoperability in the large, driven in a bottom-up manner by the participating peers. Such a system can readily be used to study semantic interoperability as a global scale phenomenon taking place in a social network of information sharing peers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a framework for implementing hypothetical queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: timothy griffin , richard hull
",n
"LEFT id: NA
RIGHT id: 2270

LEFT text: We propose an approach for indexing fuzzy data based on inverted files that speeds up retrieval considerably by stopping the traversal of postings lists early. This is possible because the entries in the postings lists are organized in a way that guarantees that there are no matching items beyond a certain point in a list. Consequently, we can reduce the number of false positives significantly, leading to an increase in retrieval performance. We have implemented our approach and evaluated it experimentally, including a test on skewed and real-world data, comparing it to an approach that has previously been shown to be superior to other methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a performance study of four index structures for set-valued attributes of low cardinality

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sven helmer , guido moerkotte
",y
"LEFT id: NA
RIGHT id: 1023

LEFT text: We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: improving adaptable similarity query processing by using approximations

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: mihael ankerst , bernhard braunm &#252; ller , hans-peter kriegel , thomas seidl
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: The constraint theory developed for traditional structured databases no longer applies to XML data. Thus, many efforts focus on the key constraints for XML. In the paper, motivated by the problem encountered in the evaluation of change detection for XML documents, we present the notion of multi-instance-based key, which can be fundamental to a highly efficient change detection algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 940

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries across diverse data sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laura m. haas , donald kossmann , edward l. wimmers , jun yang
",n
"LEFT id: NA
RIGHT id: 506

LEFT text: Text models focus on the manipulation of textual data. They describe texts by their structure, operations on the texts, and constraints on both structure and operations. In this article common characteristics of machine readable texts in general are outlined. Subsequently, ten text models are introduced. They are described in terms of the datatypes that they support, and the operations defined by these datatypes. Finally, the models are compared.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a survey of logical models for olap databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: panos vassiliadis , timos sellis
",n
"LEFT id: NA
RIGHT id: 14

LEFT text: As we embark on the information age the use of electronic information is spreading through all sectors of society, both nationally and internationally. As a result, commercial organizations, educational institutions and government agencies are finding it essential to be linked by world wide networks, and commercial Internet usage is growing at an accelerating pace.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: xml and electronic commerce : enabling the network economy

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: bart meltzer , robert glushko
",n
"LEFT id: NA
RIGHT id: 1017

LEFT text: Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: low-cost compensation-based query processing

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: &#216; ystein gr &#248; vlen , svein-olaf hvasshovd , &#216; ystein torbj &#248; rnsen
",n
"LEFT id: NA
RIGHT id: 2250

LEFT text: Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data grid management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: arun jagatheesan , arcot rajasekar
",y
"LEFT id: NA
RIGHT id: 1600

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: temporal database system implementations

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: michael h. b &#246; hlen
",n
"LEFT id: NA
RIGHT id: 919

LEFT text: We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: data page layouts for relational databases on deep memory hierarchies

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: anastassia ailamaki , david j. dewitt , mark d. hill
",n
"LEFT id: NA
RIGHT id: 1179

LEFT text: Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: This paper reports on experience obtained during the design, implementation and use of a multi-paradigm query interface to an object-oriented database. The specific system which has been developed allows equivalent data retrieval tasks to be expressed using textual, form-based and graph-based notations, and supports automatic translation of queries between these three paradigms. The motivation behind the development of such an interface is presented, as is the software architecture which supports the multi-paradigm functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1330

LEFT text: This paper proposes an effective key management scheme to harden embedded devices against side-channel attacks. This technique leverages the bandwidth limitation of side channels and employs an effective updating mechanism to prevent the keying materials from being exposed. This technique forces attackers to launch much more expensive and invasive attacks to tamper embedded devices and also has the potential of defeating unknown semi-invasive side-channel attacks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tavant system architecture for sell-side channel management

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: srinivasa narayanan , subbu n. subramanian
",y
"LEFT id: NA
RIGHT id: 2109

LEFT text: We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: answering queries using views : a survey

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 42

LEFT text: SQL Server 7.0 offers three different styles of replication that we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means that data changes can be performed at any replica, and that the changes performed at multiple replicas are later merged together. Because Merge Replication allows updates to disconnected replicas, it is particularly well suited to applications that require a lot of autonomy. A special process called the Merge Agent propagates changes between replicas, filters data as appropriate, and detects and handles conflicts according to user-specified rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: merge replication in microsoft 's sql server 7.0

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: brad hammond
",y
"LEFT id: NA
RIGHT id: 1909

LEFT text: Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: environmental information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oliver g &#252; nther
",n
"LEFT id: NA
RIGHT id: 306

LEFT text: We introduce and study a new class of queries that we refer to as OPAC (optimization under parametric aggregation constraints) queries. Such queries aim to identify sets of database tuples that constitute solutions of a large class of optimization problems involving the database tuples. The constraints and the objective function are specified in terms of aggregate functions of relational attributes, and the parameter values identify the constants used in the aggregation constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1100

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing in hierarchical parallel database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: luc bouganim , daniela florescu , patrick valduriez
",y
"LEFT id: NA
RIGHT id: 873

LEFT text: Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: automated selection of materialized views and indexes in sql databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek r. narasayya
",n
"LEFT id: NA
RIGHT id: 1641

LEFT text: This paper describes the XSB system, and its use as an in-memory deductive database engine. XSB began from a Prolog foundation, and traditional Prolog systems are known to have serious deficiencies when used as database systems. Accordingly, XSB has a fundamental bottom-up extension, introduced through tabling (or memoing)[4], which makes it appropriate as an underlying query engine for deductive database systems. Because it eliminates redundant computation, the tabling extension makes XSB able to compute all modularly stratified datalog programs finitely and with polynomial data complexity. For non-stratified programs, a meta-interpreter with the same properties is provided. In addition XSB significantly extends and improves the indexing capabilities over those of standard Prolog. Finally, its syntactic basis in HiLog [2], lends it flexibility for data modelling. The implementation of XSB derives from the WAM [25], the most common Prolog engine. XSB inherits the WAM's efficiency and can take advantage of extensive compiler technology developed for Prolog. As a result, performance comparisons indicate that XSB is significantly faster than other deductive database systems for a wide range of queries and stratified rule sets. XSB is under continuous development, and version 1.3 is available through anonymous ftp.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xsb as an efficient deductive database engine

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: konstantinos sagonas , terrance swift , david s. warren
",y
"LEFT id: NA
RIGHT id: 1371

LEFT text: We propose multi-precision similarity matching where the image is divided into a number of subblocks, each with its associated color histogram. We present experimental results showing that the spatial distribution information recorded by multiprecision color histograms helps to make similarity matching more precise. We also show that sub-image queries are much better supported with multi-precision color histograms. To minimize the overhead, we employ a filtering scheme based on the 3-dimensional average color vectors. We provide a formal result proving that filtering with multi-precision color histograms is complete. Finally, we develop a novel extendible hashing structure for indexing the average color vectors. We give experimental results showing that the proposed structure significantly outperforms the SR-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing multi-feature queries for image databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: ulrich g &#252; ntzer , wolf-tilo balke , werner kie &#223; ling
",n
"LEFT id: NA
RIGHT id: 1827

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal aggregation in active database rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: iakovos motakis , carlo zaniolo
",n
"LEFT id: NA
RIGHT id: 688

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: qursed : querying and reporting semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yannis papakonstantinou , michalis petropoulos , vasilis vassalos
",n
"LEFT id: NA
RIGHT id: 1431

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pc database systems - present and future

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: philip a. bernstein
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: 1. Motivation Internet search engines have popularized keyword based search. While relational database systems offer powerfifl structured query languages such as SQL, there is no support for keyword search over databases. The simplicity of keyword search as a querying paradigm offers compelling values for data exploration. Specifically, keyword search does not require a priori knowledge of the schema. The above is significant as much information in a corporation is increasingly being available at its intranet. However, it is unrealistic to expect users who would browse and query such information to have detailed knowledge of the schema of available databases. Therefore, just as keyword search and classification hierarchies complement each other for document search, keyword search over databases can be effective.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 1795

LEFT text: This paper describes a model that integrates the execution of triggers with the evaluation of declarative constraints in SQL database systems. This model achieves full compatibility with the 1992 international standard for SQL (SQL92). It preserves the set semantics for declarative constraint evaluation while allowing the execution of powerful procedural triggers. It was implemented in DB2 for common servers and was recently accepted as the model for the emerging SQL standard (SQW).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integrating association rule mining with relational database systems : alternatives and implications

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sunita sarawagi , shiby thomas , rakesh agrawal
",n
"LEFT id: NA
RIGHT id: 1650

LEFT text: The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the “blackbox” ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: declarative updates of relational databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: weidong chen
",n
"LEFT id: NA
RIGHT id: 789

LEFT text: This paper presents a set of aggregation algorithms on very large compressed data warehouses for multidimensional OLAP. These algorithms operate directly on compressed datasets without the need to first decompress them. They are applicable to data warehouses that are compressed using variety of data compression methods. The algorithms have different performance behavior as a function of dataset parameters, sizes of outputs and main memory availability. The analysis and experimental results show that the algorithms have better performance than the traditional aggregation algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: aggregation algorithms for very large compressed data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jianzhong li , doron rotem , jaideep srivastava
",y
"LEFT id: NA
RIGHT id: 171

LEFT text: The field of database systems research and development has been enormously successful over its 30 year history. It has led to a $10 billion industry with an installed base that touches virtually every major company in the world. It would be unthinkable to manage the large volume of valuable information that keeps corporations running without support from commercial database management systems (DBMS).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 1081

LEFT text: Large multimedia document archives hold most of their data in near-line tertiary storage libraries for cost reasons. This paper develops an integrated approach to the vertical data migration hetween the tertiary and secondary storage in that it reconciles speculative preloading, to mask the high latency of the tertiary storage, with the replacement policy of the secondary storage. In addition, it considers the interaction of these policies with the tertiary storage scheduling and controls preloading aggressiveness by taking contention for tertiary storage drives into account. The integrated migration policy is based on a continuous-time Markov-chain (CTMC) model,fijr predicting the expected number of accesses to a document within a specified time horizon. The parameters of the CTMC model, the probabilities of co-accessing certain documents and the interaction times between successive accesses, are dynamically estimated and adjusted to evolving workload patterns by keeping online statistics. The integrated policy for vertical data migration has been implemented in a prototype system. Detailed simulation studies with Web-server-like synthetic workloads indicate sign$cant gains in terms of client response time. The studies also show that the overhead of the statistical bookkeeping and the computations for the access predictions is affordable.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: vertical data migration in large near-line document archives based on markov-chain predictions

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: achim kraiss , gerhard weikum
",y
"LEFT id: NA
RIGHT id: 179

LEFT text: This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sap r/3 ( tutorial ) : a database application system

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: alfons kemper , donald kossmann , florian matthes
",n
"LEFT id: NA
RIGHT id: 2023

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive [1] to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations. TREX is based on the novel notion of XTG (XML Transformation Grammar), which extends a DTD by incorporating semantic rules de ned with XML queries (expressed in Quilt [5]). This allows one to specify how to extract relevant data from a source XML document via the queries, and to construct a target XML document directed by the embedded DTD. TREX supports XTGs using Kweelt [6] as the underlying engine for XML queries (the reason for choosing Quilt rather than XQuery/XSL is that we could access the source code of Kweelt to incorporate our optimization algorithms). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: trex : dtd-conforming xml to xml transformations

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: aoying zhou , qing wang , zhimao guo , xueqing gong , shihui zheng , hongwei wu , jianchang xiao , kun yue , wenfei fan
",y
"LEFT id: NA
RIGHT id: 1577

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: acm multimedia '94 conference workshop on multimedia database management systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bruce berra , kingsley nwosu , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 886

LEFT text: In this paper, our research objective is to develop a database virtualizat ion technique in order to let data analysts or other users who apply data mining methods to their jobs use all ubiquitous databases on the Internet as if they were recognized as a single database, thereby helping to reduce their workloads such as data collection from the Internet databases and data cleansing works. In this study, firstly we examine XML schema advantages and propose a database virtualizat ion method by which such ubiquitous databases as relational databases, object-oriented databases, and XML databases are accessed as if they all behave as a single database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integration of data mining with database technology

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: amir netz , surajit chaudhuri , jeff bernhardt , usama m. fayyad
",n
"LEFT id: NA
RIGHT id: 828

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for maintaining replica consistency in lazy master replicated databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: esther pacitti , pascale minet , eric simon
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 1442

LEFT text: In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fashion.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: coloring away communication in parallel query optimization

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: waqar hasan , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1790

LEFT text: Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining and pruning optimizations of constrained associations rules

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: raymond t. ng , laks v. s. lakshmanan , jiawei han , alex pang
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: The Laboratory of Database Applications Engineering (LIBD) is devoted to the development of models, techniques, methods and tools to support all the engineering activities related to databases and their applications. It also develops material and activities to transfer database knowledge towards industry. This report describes the main activities of the laboratory during the last ten years. It first discusses general resources and processes that form the baselines for the other research activities. The latter will be classified into reverse engineering, interoperability, advanced processes and CASE technology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 863

LEFT text: New types of data processing applications are no longer satisfied with the capabilities offered by the relational data model. One example of this phenomenon is the growing use of the Internet as a source of data. The data on the Internet is inherently non-relational. As a result, demand developed for database management systems natively built on advanced data models. The semantic binary data model (Rishe, 1992), satisfies the criteria for the models required for today’s applications by providing the ability to build rich schemas with arbitrarily flexible relationships between objects. In this paper, we discuss a new design for a semantic database management system which is based on the semantic binary data model. Our challenge was to design and implement a database engine which, while being native to the model, is reasonably efficient on a wide variety of industrial applications, and which surpasses relational systems in performance and flexibility on those applications that require non-relational modelling. Special attention is given to multi-platform support by the semantic database engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic access : semantic interface for querying databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: naphtali rishe , jun yuan , rukshan athauda , shu-ching chen , xiaoling lu , xiaobin ma , alexander vaschillo , artyom shaposhnikov , dmitry vasilevsky
",y
"LEFT id: NA
RIGHT id: 1596

LEFT text: This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the lyric language : querying constraint objects

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: alexander brodsky , yoram kornatzky
",n
"LEFT id: NA
RIGHT id: 1161

LEFT text: Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point   and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for   querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of locking behavior in three real database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: vigyan singhal , alan jay smith
",n
"LEFT id: NA
RIGHT id: 978

LEFT text: The buffer pool manager is a central component of ADABAS, a high performance scaleable database system for OLTP processing. High efficiency and scalability of the buffer pool manager is mandatory for ADABAS on all supported platforms. In order to allow a maximum of parallelism without facing the danger of deadlocks, a multi-version locking method is used. Partitioning of central data structures is another key to performance. Variable page sizes allow for flexible tuning, but make the buffer pool logic more sophisticated, in particular concerning parallelism.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the adabas buffer pool manager

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: harald sch &#246; ning
",y
"LEFT id: NA
RIGHT id: 1560

LEFT text: In this work, we utilize the knowledge of scope relationship of relations in multidatabases to identify the sites that will return the same results. Then, we propose a novel way of optimizing queries which takes advantage of the conflicts of schemas in searching for the execution plan with the least execution cost. We achieve the goal by first classifying various schema conflicts into different types. The costs of executing the same relational operation on relations of conflicting schemas are evaluated and a weight is assigned to each of the cases to reflect the complexity of executing the operation. As this method only involves simple iterative computations of the weights and the saving of a query execution time can be dramatic, the method developed here can be regarded as an effective way of optimizing query processing in a multidatabase environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",y
"LEFT id: NA
RIGHT id: 2042

LEFT text: Our goal is to understand redo recovery. We define an installation graph of operations in an execution, an ordering significantly weaker than conflict ordering from concurrency control. The installation graph explains recoverable system state in terms of which operations are considered installed. This explanation and the set of operations replayed during recovery form an invariant that is the contract between normal operation and recovery. It prescribes how to coordinate changes to system components such as the state, the log, and the cache. We also describe how widely used recovery techniques are modeled in our theory, and why they succeed in providing redo recovery.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a theory of redo recovery

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: david lomet , mark tuttle
",y
"LEFT id: NA
RIGHT id: 1902

LEFT text: The Carnot research project [CARN, WOEL93] at MCC was initiated in 1990 with the goal of addressing the problem of logically unifying physically-distributed, enterprisewide, heterogeneous information. A prototype has been implemented that provides services for enterprise modeling and model integration to create au enterprise-wide view, semantic expansion of queries on the view to queries on individual resources, and interresource consistency management. Carnot also includes technology for 3D visualization of large information spaecs, knowledge discovery in databases, and software application design recovery. The Camot prototype software has been used by the sponsors of the Carnot project to develop a number of applications. These applications have included worldtow management, heterogeneous database access, knowledge discovery in large databases, and integrated access to both text databases and structured databases from a single initial query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: applying database visualization to the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: masum z. hasan , alberto o. mendelzon , dimitra vista
",n
"LEFT id: NA
RIGHT id: 1857

LEFT text: In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probview : a flexible probabilistic database system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , nicola leone , robert ross , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1280

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mining multi-dimensional constrained gradients in data cubes

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: guozhu dong , jiawei han , joyce m. w. lam , jian pei , ke wang
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 1479

LEFT text: The UniSQL/X unified relational and object--cmented database system is designed to support application development in either a convenuonal host programming Ianguagc (such as C), or an Object-aiented programming language (such as C++ or Smalltalk). In particular, C++ programmers can take advantage of all the capabilities of UniSQL/X in C++ programming style by using the UniSQL/X C++ Interfi~ce. C programmers can access the UniSQL/X database by using the Embedded SQL/X (objectaiented SQL) Preprocessor and/or the UniSQL/X AH (call levcl interface).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a cost model for clustered object-oriented databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 65

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient resumption of interrupted warehouse loads

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: wilburt juan labio , janet l. wiener , hector garcia-molina , vlad gorelik
",n
"LEFT id: NA
RIGHT id: 229

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at columbia university

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shih-fu chang , luis gravano , gail e. kaiser , kenneth a. ross , salvatore j. stolfo
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: Much of business XML data has accompanying XSD specifications. In many scenarios, ""shredding such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries over such XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",n
"LEFT id: NA
RIGHT id: 728

LEFT text: A star schema is very popular for modeling data warehouses and data marts. Therefore, it is important that a database system which is used for implementing such a data warehouse or data mart is able to efficiently handle operations on such a schema. In this paper we will describe how one of these operations, the join operation --- probably the most important operation --- is implemented in the IBM Informix Extended Parallel Server (XPS).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient execution of joins in a star schema

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: andreas weininger
",y
"LEFT id: NA
RIGHT id: 1299

LEFT text: Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability. This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments. A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator, whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL (C-SQL), in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: using semantic values to facilitate interoperability among heterogeneous information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore , michael siegel , arnon rosenthal
",y
"LEFT id: NA
RIGHT id: 350

LEFT text: Personalization generally refers to making a Web site more responsive to the unique and individual needs of each user. We argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect the changes in user interests.Creating user profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. However a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessable when the same user works on a different computer. On the other hand, the integration of Internet with telecommunication networks have made it possible for the users to connect to Web with a variety of mobile devices as well as desk tops. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: supporting global user profiles through trusted authorities

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ibrahim cingil
",y
"LEFT id: NA
RIGHT id: 1506

LEFT text: Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental maintenance of views with duplicates

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: timothy griffin , leonid libkin
",n
"LEFT id: NA
RIGHT id: 1496

LEFT text: In this paper, we propose an efficient direct and indirect file transfer protocol (C2CFTP) that transfers files between clients in a client-server system. Existing file transfer methods use an indirect transfer method through a server to transfer files between sending and receiving clients or a direct transfer method that connects a direct data channel between clients. However, in the case of indirect transmission, unnecessary file input/output (I/O) is required by the server, and in the case of direct transmission, a problem arises in that the file transmission delay time is increased due to channel management cost. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: upsizing form file server to client server architectures

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: corporate the access team microsoft
",y
"LEFT id: NA
RIGHT id: 548

LEFT text: Recent work on querying data streams has focused on systems where newly arriving data is processed and continuously streamed to the user in real-time. In many emerging applications, however, ad hoc queries and/or intermittent connectivity also require the processing of data that arrives prior to query submission or during a period of disconnection. For such applications, we have developed PSoup, a system that combines the processing of ad-hoc and continuous queries by treating data and queries symmetrically, allowing new queries to be applied to old data and new data to be applied to old queries. PSoup also supports intermittent connectivity by separating the computation of query results from the delivery of those results. PSoup builds on adaptive query processing techniques developed in the Telegraph project at UC Berkeley. In this paper, we describe PSoup and present experiments that demonstrate the effectiveness of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: continuous queries over data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: shivnath babu , jennifer widom
",n
"LEFT id: NA
RIGHT id: 818

LEFT text: Fast indexing in time sequence databases for similarity searching has attracted a lot of research recently. Most of the proposals, however, typically centered around the Euclidean distance and its derivatives. We examine the problem of multimodal similarity search in which users can choose the best one from multiple similarity models for their needs. In this paper, we present a novel and fast indexing scheme for time sequences, when the distance function is any of arbitrary Lp norms (p = 1; 2; : : : ;1). One feature of the proposed method is that only one index structure is needed for all Lp norms including the popular Euclidean distance (L2 norm). Our scheme achieves significant speedups over the state of the art: extensive experiments on real and synthetic time sequences show that the proposed method is comparable to the best competitor forL2 andL1 norms, but significantly (up to 10 times) faster for L1 norm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: ghost : fine granularity buffering of indexes

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: cheng hian goh , beng chin ooi , d. sim , kian-lee tan
",y
"LEFT id: NA
RIGHT id: 1152

LEFT text: A software architecture is presented that allows client application programs to interact with a DBMS server in a flexible and powerful way, using either direct, volatile messages, or messages sent via recoverable queues. Normal requests from clients to the server and replies from the server to clients can be transmitted using direct or recoverable messages. In addition, an application event notification mechanism is provided, whereby client applications running anywhere on the network can register for events, and when those events are raised, the clients are notified. A novel parameter passing mechanism allows a set of tuples to be included in an event notification. The event mechanism is particularly useful in an active DBMS, where events can be raised by triggers to signal running application programs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: synchronization and recovery in a client-server storage system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: e. panagos , a. biliris
",n
"LEFT id: NA
RIGHT id: 1836

LEFT text: Existing studies on outliers focus only on the identiication aspect; none provides any inten-sional knowledge of the outliers|by which we mean a description or an explanation of why an identiied outlier is exceptional. For many applications, a description or explanation is at least as vital to the user as the identii-cation aspect. Speciically, intensional knowledge helps the user to: (i) evaluate the validity of the identiied outliers, and (ii) improve one's understanding of the data. The two main issues addressed in this paper are: what kinds of intensional knowledge to provide, and how to optimize the computation of such knowledge. With respect to the rst issue, we propose nding strongest and weak outliers and their corresponding structural intensional knowledge. With respect to the second issue, we rst present a naive and a semi-naive algorithm. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: distance-based indexing for high-dimensional metric spaces

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: tolga bozkaya , meral ozsoyoglu
",n
"LEFT id: NA
RIGHT id: 1745

LEFT text: Inasmuch for speed to customers desires change and large completion that describe day world. To lead technology and operations technology to form general, to achieve competitive advantage and special form design technology is master key to determine nature and form product, and what tolerable quality levels that work fit product to uses, and all of features and preferences determine through design technology. For importance CAD/CAM subject, we introduce in this research that offer primary components to CAM system, and styles this system to achieve details work steps, and details design steps. From within completely program in (AutoCAD) system with details steps to how design transportation to manufacturing operations to series achieve to desired product. With offer conclusions that fitness between CAD and CAM to introduce direction communication between design and manufacturing lead to mistakes reduction to large ratio.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues in federated database systems : report of efdbs '97 workshop

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: s. conrad , b. eaglestone , w. hasselbring , m. roantree , m. sch &#246; hoff , m. str &#228; ssler , m. vermeer , f. saltor
",n
"LEFT id: NA
RIGHT id: 1354

LEFT text: Data warehouses have been successfully employed for assisting decision making by offering a global view of the enterprise data and providing mechanisms for On-Line Analytical processing. Traditionally, data warehouses are utilized within the limits of an enterprise or organization. The growth of Internet and WWW however, has created new opportunities for data sharing among ad-hoc, geographically spanned and possibly mobile users. Since it is impractical for each enterprise to set up a worldwide infrastructure, currently such applications are handled by the central warehouse. This often yields poor performance, due to overloading of the central server and low transfer rate of the network. In this paper we propose an architecture for OLAP cache servers (OCS). An OCS is the equivalent of a proxy-server for web documents, but it is designed to accommodate data from warehouses and support OLAP operations. We allow numerous OCSs to be connected via an arbitrary network, and present a centralized, a semi-centralized and an autonomous control policy. We experimentally evaluate these policies and compare the performance gain against the existing systems where caching is performed only at the client side. Our architecture offers increased autonomy at remote clients, substantial network traffic savings, better scalability, lower response time and is complementary both to existing OLAP cache systems and distributed OLAP approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: architectures for internal web services deployment

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: oded shmueli
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: This is a bibliography on active databases and active database systems which reflects the various research activities in this field. We compiled this bibliography for our own use, but hopefully it might be useful to other people as well. All papers that appear in the following list, are generally accessible.We do not claim that the bibliography is exhaustive and covers the complete range of literature that deals with activities. We decided to focus on central approaches, concepts, methods, and systems in the area of active databases. It does not contain entries in the area of ""pure"" real-time, object-oriented, temporal, and deductive databases. But we did include publications related to those approaches, as long as they discuss active databases.We divided the material into various sections following our own personal perception of the field. The sections provide an overview on different projects in the area of active databases, followed by sections on relevant research topics. Each section contains a few remarks followed by a list of cross references into the annotated bibliography. Papers might appear in more than one section in case they discuss different topics relevant to different sections.Additionally, when relevant we also included unpublished, but publicly available material. For those papers we included information how to obtain them from the authors or from the organizations where the were produced.The beauty of our work is the individual annotation to almost all publications. Due to space limitations we are forced to leave out those annotations in the version published here. For a complete annotated bibliography we refer to the entry in our WWW server.The effort to build up such a bibliography is an endless task. Since we believe it now provides a comprehensive overview on the existing literature in the field, we decided to publish it. However, we invite all readers to add remarks, corrections, updates, additions (including further annotations).Part of this work was done while we were associated with the FORWISS Institute of the Technical University of Munich. We would like to thank our student, Markus Blaschka, who compiled many references during his master's thesis.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 1582

LEFT text: PAYOFF IDEA. Different styles of user interfaces can dramatically affect data base capabilities. In an environment comprising many different data bases, the goal is to select one data base management system (DBMS) that provides the best selection of design tools, minimizes development times, and enforces relational rules. This article presents a case study performed at the Hospital of the University of Pennsylvania, in which a test data base was developed for implementation with three DBMSs, each with a distinctly different user and programmer interface.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a comparison of three user interfaces to relational microcomputer data bases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: carl medsker , margaret christensen , il-yeol song
",y
"LEFT id: NA
RIGHT id: 1056

LEFT text: Literature search is an important part of any research and publication activity. In the era of electronic database and explosion of scientific publication, keywords play an immense role in digging out the relevant published material, since these keywords act as ""keys"" to unlock the desired scientific paper abstracts/full articles from a vast collection of related publications1. Hence it is important to include and select pertinent keywords which can easily identify and search relevant references and filter-out the large body of unwanted material. It is, therefore, important that certain words may be added to the abstract of the article which a future researcher might be expected to use as keywords in MEDLINE search. These words should be such that they would make an article which might have remained invisible to the researcher visible on search2. It has been observed that the keywords included along with the abstracts by some journals in the research reports frequently do not identify relevant papers. In view of this problem we have analysed an issue of the Indian Journal of Pharmacology [2002; Vol 34 (Issue 1): 3-58] for the relevance of the keywords used.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scientific journals : extinction or explosion ? ( panel )

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: raghu ramakrishnan , hector garcia-molina , gerhard rossbach , abraham silberschatz , gio wiederhold , jaco zijlstra
",y
"LEFT id: NA
RIGHT id: 1560

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an aspect of query optimization in multidatabase systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: chiang lee , chia-jung chen , hongjun lu
",n
"LEFT id: NA
RIGHT id: 564

LEFT text: I'm happy to be able to share with you the following three reminiscences. I continue to invite unsolicited contributions. See http://www.acm.org/sigmod/record/author.html for submission guidelines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1912

LEFT text: Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1414

LEFT text: Cleaning data of errors in structure and content is important for data warehousing and integration. Current solutions for data cleaning involve many iterations of data “auditing” to find errors, and long-running transformations to fix them. Users need to endure long waits, and often write complex transformation scripts. We present Potter’s Wheel, an interactive data cleaning system that tightly integrates transformation and discrepancy detection. Users gradually build transformations to clean the data by adding or undoing transforms on a spreadsheet-like interface; the effect of a transform is shown at once on records visible on screen. These transforms are specified either through simple graphical operations, or by showing the desired effects on example data values. In the background, Potter’s Wheel automatically infers structures for data values in terms of user-defined domains, and accordingly checks for constraint violations. Thus users can gradually build a transformation as discrepancies are found, and clean the data without writing complex programs or enduring long delays.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: potter 's wheel : an interactive data cleaning system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",y
"LEFT id: NA
RIGHT id: 49

LEFT text: One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 689

LEFT text: We demonstrate a visual based XML-Relational database system where XML data is managed by commercial RDBMS. A query interface enables users to form path expression based queries against stored data visually. Statistics about data and a special path directory are used to rewrite path expression based queries into efficient SQL statements involving less number of joins.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing and querying ordered xml using a relational database system

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: igor tatarinov , stratis d. viglas , kevin beyer , jayavel shanmugasundaram , eugene shekita , chun zhang
",n
"LEFT id: NA
RIGHT id: 1072

LEFT text: Detecting and extracting modifications from information sources is an integral part of data warehousing. For unsophisticated sources, in practice it is often necessary to infer modifications by periodically comparing snapshots of data from the source. Although this sapshot di/rem tial problem is closely related to traditional joins and outerjoins, there are significant differences, which lead to simple new algorithms. In particular, we present algorithms that perform (possibly lossy) compression of records. We also present a window algorithm that works very well if the snapshots are not ""very different."" The algorithms are studied via analysis and an implementation of two of them; the results illustrate the potential gains achievable with the new algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for materialized view design in data warehousing environment

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jian yang , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 655

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the 18th british national conference on databases ( bncod )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: carole goble , brian read
",n
"LEFT id: NA
RIGHT id: 1576

LEFT text: The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information systems research at rwth aachen

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 1401

LEFT text: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be “perfectly” translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable “closeness” metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query translation across heterogeneous information sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kevin chen-chuan chang , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 1806

LEFT text: A broad spectrum of data is available on the Web in distinct heterogeneous sources, and stored under different formats. As the number of systems that utilize this heterogeneous data grows, the importance of data translation and conversion mechanisms increases greatly. In this paper we present a new translation system, based on schema-matching, aimed at simplifying the intricate task of data conversion. We observe that in many cases the schema of the data in the source system is very similar to that of the target system. In such cases, much of the translation work can be done automatically, based on the schemas similarity. This saves a lot of effort for the user, limiting the amount of programming needed. We define common schema and data models, in which schemas and data (resp.) from many common models can be represented. Using a rule-based method, the source schema is compared with the target one, and each component in the source schema is matched with a corresponding component in the target schema. Then, based on the matching achieved, data instances of the source schema can be translated to instances of the target schema. We show that our schema-based translation system allows a convenient specification and customization of data conversions, and can be easily combined with the traditional data-based translation languages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: using schematically heterogeneous structures

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: re &#233; e j. miller
",n
"LEFT id: NA
RIGHT id: 259

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation using probabilistic models

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: lise getoor , benjamin taskar , daphne koller
",n
"LEFT id: NA
RIGHT id: 1283

LEFT text: The increasing ability to interconnect computers through internet-working, wireless networks, high-bandwidth satellite, and cable networks has spawned a new class of information-centered applications based on data dissemination. These applications employ broadcast to deliver data to very large client populations. We have proposed the Broadcast Disks paradigm [Zdon94, Acha95b] for organizing the contents of a data broadcast program and for managing client resources in response to such a program. Our previous work on Broadcast Disks focused exclusively on the “push-based” approach, where data is sent out on the broadcast channel according to a periodic schedule, in anticipation of client requests. In this paper, we study how to augment the push-only model with a “pull-based” approach of using a backchannel to allow clients to send explicit requests for data to the server. We analyze the scalability and performance of a broadcast-based system that integrates push and pull and study the impact of this integration on both the steady state and warm-up performance of clients. Our results show that a client backchannel can provide significant performance improvement in the broadcast environment, but that unconstrained use of the backchannel can result in scalability problems due to server saturation. We propose and investigate a set of three techniques that can delay the onset of saturation and thus, enhance the performance and scalability of the system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data staging for on-demand broadcast

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: demet aksoy , michael j. franklin , stanley b. zdonik
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 585

LEFT text: We propose a novel index structure, A-tree (Approximation tree), for similarity search of high-dimensional data. The basic idea of the A-tree is the introduction of Virtual Bounding Rectangles (VBRs), which contain and approximate MBRs and data objects. VBRs can be represented rather compactly, and thus affect the tree configuration both quantitatively and qualitatively. Firstly, since tree nodes can install large number of entries of VBRs, fanout of nodes becomes large, thus leads to fast search. More importantly, we have a free hand in arranging MBRs and VBRs in tree nodes. In the A-trees, nodes contain entries of an MBR and its children VBRs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the tv-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: king ip lin , h. v. jagadish , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 1875

LEFT text: Middle East Technical University (METU) is the leading technical university in Turkey. The Software Research and Development Center was established by the Scientific and Technical Research Council of Turkey (TUBITAK) at the Department of Computer Engineering of METU in October 1991. The aim of this center is twofold: to lead large scale software research and development projects, and to foster international cooperation. SRDC is involved in a number of research and development projects supported by the government, industrial companies and international organizations. Although SRDC projects also cover other fields of computer science, the main emphasis is on database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: middle east technical university software research and development center

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac
",y
"LEFT id: NA
RIGHT id: 1758

LEFT text: The ORES TDBMS will support the efficient and user friendly representation and manipulation of temporal knowledge and it will be developed as an extension of the relational database management system INGRES. The ORES project will result in a general purpose TDBMS, the development of which is based on a practical and yet theoretically sound approach. More specifically, the overall objectives of the ORES project are: i) to develop a formal foundation for temporal representation and reasoning, ii) to develop a temporal query language that will be upwards consistent with SQL2, iii) to develop models, techniques and tools for user friendly and effective definition, manipulation and validation of temporal database applications, and iv) to evaluate the ORES environment using a hospital case study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 125

LEFT text: Internet, Web and distributed computing infrastructures continue to gain in popularity as a means of communication for organizations, groups and individuals alike. In such an environment, characterized by large distributed, autonomous, diverse, and dynamic information sources, access to relevant and accurate information is becoming increasingly complex. This complexity is exacerbated by the evolving system, semantic and structural heterogeneity of these potentially global, cross-disciplinary, multicultural and rich-media technologies. Clearly, solutions to these challenges require addressing directly a variety of interoperability issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic interoperability in information services : experiencing with coopware

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: avigdor gal
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 943

LEFT text: Information becomes a more and more valuable asset in today’s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome in the design and implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II , which is based on the wrapper-mediator approach. The main focus of this work lies on data model transformation and query processing. The following extensions to the AMOS II system are described in this thesis: • A framework for transforming various data models into the objectoriented model of AMOS II is presented. • The roles and tasks of wrappers are described. In particular their participation in query processing and query optimization is discussed. • A way for describing and utilizing the query capabilities of the different data sources is proposed. • Two different approaches to query processing over external data sources are developed and analyzed. All the proposed techniques are implemented in the AMOS II system, which runs on a Windows NT platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: merging ranks from heterogeneous internet sources

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: luis gravano , hector garcia-molina
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: We confront the promises of active database systems with the result of their use by application developers. The main problems encountered are iusufficient methodological support in analysis and design, the lack of standardization, missing development and administration tools for triggers, and weak performance. We concentrate on performance because we discovered it is one the maiu reasons that makes users reluctant to use active rules iu the development of large applications. We show, using simple concrete examples, that optimizing large applications is rendered difficult by the separation of transactions and triggers and the misunderstanding of their subtle interactions. We argue that tools, which provide assistance to programmers, database administrators, and database designers to optimize their applications and master application evolution is strongly needed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1631

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: odmg-93 : a standard for object-oriented dbmss

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: r. g. g. cattell
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 582

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: versioning and configuration management in an object-oriented data model

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: edward sciore
",n
"LEFT id: NA
RIGHT id: 1837

LEFT text: Nearest neighbor search in high dimensional spaces is an interesting and important problem which is relevant for a wide variety of novel database applications. As recent results show, however, the problem is a very di cult one, not only with regards to the performance issue but also to the quality issue. In this paper, we discuss the quality issue and identify a new generalized notion of nearest neighbor search as the relevant problem in high dimensional space. In contrast to previous approaches, our new notion of nearest neighbor search does not treat all dimensions equally but uses a quality criterion to select relevant dimensions (projections) with respect to the given query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the sr-tree : an index structure for high-dimensional nearest neighbor queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: norio katayama , shin ` ichi satoh
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 1279

LEFT text: Multimedia information systems have emerged as an essential component of many application domains ranging from library information systems to entertainment technology. However, most implementations of these systems cannot support the continuous display of multimedia objects and suffer from frequent disruptions and delays termed hiccups. This is due to the low I/O bandwidth of the current disk technology, the high bandwidth requirement of multimedia objects, and the large size of these objects that almost always requires them to be disk resident. One approach to resolve this limitation is to decluster a multimedia object across multiple disk drives in order to employ the aggregate bandwidth of several disks to support the continuous retrieval (and display) of objects.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: analysis of navigation behaviour in web sites integrating multiple information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: bettina berendt , myra spiliopoulou
",n
"LEFT id: NA
RIGHT id: 1005

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: determining text databases to search in the internet

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: weiyi meng , king-lup liu , clement t. yu , xiaodong wang , yuhsi chang , naphtali rishe
",n
"LEFT id: NA
RIGHT id: 191

LEFT text: This paper describes XDB-IPG, an open and extensible database architecture that supports efficient and flexible integration of heterogeneous and distributed information resources. XDB-IPG provides a novel “schema-less” database approach using a document-centered object-relational XML database mapping. This enables structured, unstructured, and semi-structured information to be integrated without requiring document schemas or translation tables. XDB-IPG utilizes existing international protocol standards of the World Wide Web Consortium Architecture Domain and the Internet Engineering Task Force, primarily HTTP, XML and WebDAV .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure and portable database extensibility

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael godfrey , tobias mayr , praveen seshadri , thorsten von eicken
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 1765

LEFT text: System developments and research on parallel query processing have concentrated either on “Shared Everything” or “Shared Nothing” architectures so far. While there are several commercial DBMS based on the “Shared Disk” alternative, this architecture has received very little attention with respect to parallel query processing. A comparison between Shared Disk and Shared Nothing reveals many potential benefits for Shared Disk with respect to parallel query processing. In particular, Shared Disk supports more flexible control over the communication overhead for intra-transaction parallelism, and a higher potential for dynamic load balancing and efficient processing of mixed OLTP/query workloads. We also sketch necessary extensions for transaction management (concurrency/coherency control, logging/recovery) to support intra-transaction parallelism in the Shared Disk environment.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: t2 : a customizable parallel database for multi-dimensional data

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chialin chang , anurag acharya , alan sussman , joel saltz
",n
"LEFT id: NA
RIGHT id: 469

LEFT text: We study sliding window multi-join processing in continuous queries over data streams. Several algorithms are reported for performing continuous, incremental joins, under the assumption that all the sliding windows fit in main memory. The algorithms include multiway incremental nested loop joins (NLJs) and multi-way incremental hash joins. We also propose join ordering heuristics to minimize the processing cost per unit time. We test a possible implementation of these algorithms and show that, as expected, hash joins are faster than NLJs for performing equi-joins, and that the overall processing cost is influenced by the strategies used to remove expired tuples from the sliding windows.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on computing correlated aggregates over continual data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: johannes gehrke , flip korn , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1512

LEFT text: Many database applications need accountability and trace-ability that necessitate retaining previous database states. For a transaction-time database supporting this, the choice of times used to timestamp database records, to establish when records are or were current, needs to be consistent with a committed transaction serialization order. Previous solutions have chosen timestamps at commit time, selecting a time that agrees with commit order. However, SQL standard databases can require an earlier choice because a statement within a transaction may request “current time.” Managing timestamps chosen before a serialization order is established is the challenging problem we solve here. By building on two-phase locking concurrency control, we can delay a transaction’s choice of a timestamp, reducing the chance that transactions may need to be aborted in order keep timestamps consistent with a serialization order. Also, while timestamps stored with records in a transaction-time database make it possible to directly identify write-write and write-read conflicts, handling read-write conflicts requires more. Our simple auxiliary structure conservatively detects read-write conflicts, and hence provides transaction timestamps that are consistent with a serialization order.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic assumptions and query evaluation in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: claudio bettini , x. sean wang , elisa bertino , sushil jajodia
",n
"LEFT id: NA
RIGHT id: 1869

LEFT text: A workflow history manager maintains the information essential for workflow monitoring and data mining as well as for recovery and authorization purposes.Certain characteristics of workflow systems like the necessity to run these systems on heterogeneous, autonomous and distributed environments and the nature of data, prevent history management in workflows to be handled by the classical data management techniques like distributed DBMSs. We further demonstrate that multi-database query processing techniques are also not appropriate for the problem at hand.In this paper, we describe history management, i.e., the structure of the history and querying of the history, in a fully distributed workflow architecture realized in conformance with Object Management Architecture (OMA) of OMG.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the mentor workbench for enterprise-wide workflow management

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dirk wodtke , jeanine weissenfels , gerhard weikum , angelika kotz dittrich , peter muth
",n
"LEFT id: NA
RIGHT id: 1265

LEFT text: Since multimedia retrieval is based on similarity calculations of semantics and media-based search, exact matches are not expected. We view querying multimedia database as a combination of IR, image matching, and traditional database query processing and it should be conducted in a way of perpetual query reformulation for honing target results. In this paper we present a hybrid multimedia database system, which employs a hierarchical database statistics structure for both query optimization and reformulation analysis without adding additional query processing cost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: supporting efficient multimedia database exploration

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: wen-syan li , k.sel &#231; uk candan , kyoji hirata , yoshinori hara
",n
"LEFT id: NA
RIGHT id: 832

LEFT text: The query execution engine in Microsoft SQL Server employs hash-based algorithms for inner and outer joins, semi-joins, set operations (such as intersection), grouping, and duplicate removal. The implementation combines many techniques proposed individually in the research literature but never combined in a single implementation, neither in a product nor in a research prototype. One of the paper’s contributions is a design that cleanly integrates most existing techniques. One technique, however, which we call hash teams and which has previously been described only in vague terms, has not been implemented in prior research or product work. It realizes in hash-based query processing many of the benefits of interesting orderings in sort-based query processing. Moreover, we describe how memory is managed in complex and bushy query evaluation plans with multiple sort and hash operations. Finally, we report on the effectiveness of hashing using two very typical database queries, including the performance effects of hash teams.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the value of merge-join and hash-join in sql server

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: goetz graefe
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 458

LEFT text: This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraint databases : a tutorial introduction

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jan van den bussche
",n
"LEFT id: NA
RIGHT id: 641

LEFT text: Peer-to-Peer (P2P) systems are becoming increasingly popular as they enable users to exchange digital information by participating in complex networks. Such systems are inexpensive, easy to use, highly scalable and do not require central administration. Despite their advantages, however, limited work has been done on employing database systems on top of P2P networks.Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload. This paper describes the core components of PeerOLAP and presents our results both from simulation and a prototype installation running on geographically remote peers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an adaptive peer-to-peer network for distributed caching of olap results

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: panos kalnis , wee siong ng , beng chin ooi , dimitris papadias , kian-lee tan
",y
"LEFT id: NA
RIGHT id: 340

LEFT text: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: distributed query evaluation on semistructured data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 850

LEFT text: nvited Talk I.- Some Advances in Data-Mining Techniques.- Web Exploration.- Querying Semantically Tagged Documents on the World-Wide Web.- WWW Exploration Queries.- Strategies for Filtering E-mail Messages Combining Content-Based and Sociological Filtering with User-Stereotypes.- Interactive Query Expansion in a Meta-search Engine.- Database Technology.- On the Optimization of Queries containing Regular Path Expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the a-tree : an index structure for high-dimensional spaces using relative approximation

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yasushi sakurai , masatoshi yoshikawa , shunsuke uemura , haruhiko kojima
",n
"LEFT id: NA
RIGHT id: 2194

LEFT text: This paper describes the POESIA approach to systematic composition of Web services. This pragmatic approach is strongly centered in the use of domain-specific multidimensional ontologies. Inspired by applications needs and founded on ontologies, workflows, and activity models, POESIA provides well-defined operations (aggregation, specialization, and instantiation) to support the composition of Web services. POESIA complements current proposals for Web services definition and composition by providing a higher degree of abstraction with verifiable consistency properties. We illustrate the POESIA approach using a concrete application scenario in agroenvironmental planning.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: poesia : an ontological workflow approach for composing web services in agriculture

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: renato fileto , ling liu , calton pu , eduardo delgado assad , claudia bauzer medeiros
",y
"LEFT id: NA
RIGHT id: 1890

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: spotfire : an information exploration environment

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: christopher ahlberg
",n
"LEFT id: NA
RIGHT id: 2202

LEFT text: Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: karl aberer
",y
"LEFT id: NA
RIGHT id: 1443

LEFT text: Data mining technology has given us new capabilities to identify correlations in large data sets. This introduces risks when the data is to be made public, but the correlations are private. We introduce a method for selectively removing individual values from a database to prevent the discovery of a set of rules, while preserving the data for other applications. The efficacy and complexity of this method are discussed. We also present an experiment showing an example of this methodology.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of multiple-level association rules from large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu
",n
"LEFT id: NA
RIGHT id: 1615

LEFT text: From the Publisher:  The focus of Data Management for Mobile Computing is on the impact of mobile computing on data management beyond the networking level. The purpose is to provide a thorough and cohesive overview of recent advances in wireless and mobile data management. The book is written with a critical attitude. This volume probes the new issues introduced by wireless and mobile access to data and what are both their conceptual and practical consequences. Data Management for Mobile Computing provides a single source for researchers and practitioners who want to keep current on the latest innovations in the field. It can also serve as a textbook for an advanced course on mobile computing or as a companion text for a variety of courses including courses on distributed systems, database management, transaction management, operating or file systems, information retrieval or dissemination, and web computing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: digital library services in mobile computing

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: bharat bhargava , melliyal annamalai , evaggelia pitoura
",n
"LEFT id: NA
RIGHT id: 596

LEFT text: The use of social media in advocacy, and particularly transnational advocacy, raises concerns of privacy and security for those conducting the advocacy and their contacts on social media. This chapter presents high-level summaries of cases of social media in advocacy and activism from the perspectives of information warfare and information security. From an analysis of these, the impact and relationships of social media in transnational advocacy and information security is discussed. Whilst online advocacy can be considered to be a form of information warfare aligned to a Cyber Macht theory, it can be argued that social media advocacy negatively impacts information security as it encourages various actors to actively attempt to breach security.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: information warfare and security

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: h. v. jagadish
",y
"LEFT id: NA
RIGHT id: 742

LEFT text: This report is a summary of the First International Workshop on Active and Real-Time Database Systems (ARTDB-95), held at the University of Skovde in June 1995. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the acm fourth international workshop on data warehousing and olap ( dolap 2001 )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 347

LEFT text: One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a thery and practice of intellectual property cost management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1640

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sequence query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",n
"LEFT id: NA
RIGHT id: 2026

LEFT text: Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- ""losing"" customers, ""misplacing"" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data quality and data cleaning : an overview

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: theodore johnson , tamraparni dasu
",y
"LEFT id: NA
RIGHT id: 1440

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databasesmust be integrated: Since the system’s data is supposedto reflect the environment being controlled, theymust be updated frequently tomaintain temporal validity; Many activities, including those that perform the updates, work under time constraints; The occurrence of events, for example, emergency events, trigger actions. In these systems, meeting timeliness, predictability, and QoS guarantee requirements – through appropriate resource and overload management – become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling,concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: generalized search trees for database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: joseph m. hellerstein , jeffrey f. naughton , avi pfeffer
",n
"LEFT id: NA
RIGHT id: 293

LEFT text: Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: outlier detection for high dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: charu c. aggarwal , philip s. yu
",n
"LEFT id: NA
RIGHT id: 886

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integration of data mining with database technology

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: amir netz , surajit chaudhuri , jeff bernhardt , usama m. fayyad
",n
"LEFT id: NA
RIGHT id: 1345

LEFT text: Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. Consequently, I will also briefly introduce the related XML, Java and OMG technologies like SOAP, J2EE and CORBA. One of the most important features of ASs is their ability to integrate the modern application environments with legacy data sources like IMS, CICS, VSAM, etc. They provide a number of connectors for this purpose, typically using asynchronous transactional messaging technologies like MQSeries and JMS. Traditional TPM-style requirements for industrial strength features like scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state. Recently, the ECPerf benchmark has been developed via the Java Community Process to evaluate in a standardized way the cost performance of J2EE-compliant ASs. Several caching technologies have been developed to improve performance of ASs.Soon after this conference is over, the slides of this tutorial will be available on the web at the following URL: http://www.almaden.ibm.com/u/mohan/AppServersTutorial_SIGMOD2002_Slides.pdf

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching technologies for web applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 255

LEFT text: Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jiawei han
",n
"LEFT id: NA
RIGHT id: 824

LEFT text: While the desire to support fast, ad hoc query processing for large data warehouses has motivated the recent introduction of many new indexing structures, with a few notable exceptions (namely, the LSM-Tree and the Stepped Merge Method) little attention has been given to developing new indexing schemes that allow fast insertions. Since additions to a large warehouse may number in the millions per day, indices that require a disk seek (or even a significant fraction of a seek) per insertion are not acceptable. In this paper, we offer an alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequent insertions. The Y-tree is a new indexing structure supporting both point and range queries over a single attribute, with retrieval performance comparable to the B+-tree. For processing insertions, however, the Y-tree may exhibit a speedup of 100 times over batched insertions into a B+-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a novel index supporting high volume data warehouse insertion

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chris jermaine , anindya datta , edward omiecinski
",y
"LEFT id: NA
RIGHT id: 1179

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: enhanced abstract data types in object-relational databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 1459

LEFT text: Abstract A Real-Time DataBase System (RTDBS) can be viewed as an amalgamation of a conventional DataBase Management System (DBMS) and a real-time system. Like a DBMS, it has to process transactions and guarantee ACID database properties. Furthermore, it has to operate in real-time, satisfying time constraints imposed on transaction commitments. A RTDBS may exist as a stand-alone system or as an embedded component in a larger multidatabase system. The publication in 1988 of a special issue of ACM SIGMOD Record on Real-Time DataBases signaled the birth of the RTDBS research area---an area that brings together researchers from both the database and real-time systems communities. Today, almost eight years later, I am pleased to present in this special section of ACM SIGMOD Record a review of recent advances in RTDBS research. There were 18 submissions to this special section, of which eight papers were selected for inclusion to provide the readers of ACM SIGMOD Record with an overview of current and future research directions within the RTDBS community. In this paper, I will summarize these directions and provide the reader with pointers to other publications for further information.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: index concurrency control in firm real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: brajesh goyal , jayant r. haritsa , s. seshadri , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 1274

LEFT text: We present a structured, iterative methodology for user-centered design and evaluation of VE user interaction. We recommend performing (1) user task analysis followed by (2) expert guidelines-based evaluation, (3) formative user-centered evaluation, and finally (4) comparative evaluation. In this article we first give the motivation and background for our methodology, then we describe each technique in some detail. We applied these techniques to a real-world battlefield visualization VE. Finally, we evaluate why this approach provides a cost-effective strategy for assessing and iteratively improving user interaction in VEs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: view management in multimedia databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: k. sel &#231; uk candan , eric lemar , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1663

LEFT text: Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many ""security flaws"" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: implementation aspects of an object-oriented dbms

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: asuman dogac , mehmet altinel , cetin ozkan , ilker durusoy
",n
"LEFT id: NA
RIGHT id: 2227

LEFT text: One important step in integrating heterogeneous databases is matching equivalent attributes: Determining which fields in two databases refer to the same data. The meaning of information may be embodied within a. database model, a conceptual schema, application programs, or data contents. Integration involves extracting semantics, expressing them as metadata, and matching semantically equivalent data elements. We present a procedure using a classifier to categorize attributes according to their field specifications and data values, then train a neural network to recognize similar attributes. In our technique, the knowledge of how to match equivalent data elements is “discovered” from metadata , not “pre-programmed”.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: semantic integration in heterogeneous databases using neural networks

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",y
"LEFT id: NA
RIGHT id: 1512

LEFT text: This paper provides an overview of the 1995 International Workshop on Temporal Databases. It summarizes the technical papers and related discussions, and three panels: “Wither TSQL3?”, “Temporal Data Management in Financial Applications,” and “Temporal Data Management Infrastructure & Beyond.”

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic assumptions and query evaluation in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: claudio bettini , x. sean wang , elisa bertino , sushil jajodia
",n
"LEFT id: NA
RIGHT id: 1557

LEFT text: Making a database system active entails developing an expressive event specification language with well-defined semantics, algorithms for the detection of composite events, and an architecture for an event detector along with its implementation. Thii paper presents the semantics of composite events using the notion of a global event history (or a global event-log). Parameter contexts are introduced and precisely defined to facilitate efficient management and detection of composite events. Finally, an architecture and the implementation of a composite event, detector is analyzed in the context of an object-oriented active DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a structured approach for the definition of the semantics of active databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: piero fraternali , letizia tanca
",n
"LEFT id: NA
RIGHT id: 723

LEFT text: Many applications today run in a multi-tier environment with browser-based clients, mid-tier (application) servers and a backend database server. Mid-tier database caching attempts to improve system throughput and scalability by offloading part of the database workload to intermediate database servers that partially replicate data from the backend server. The fact that some queries are offloaded to an intermediate server should be completely transparent to applications one of the key distinctions between caching and replication. MTCache is a prototype mid-tier database caching solution for SQL Server that achieves this transparency. It builds on SQL Server's support for materialized views, distributed queries and replication. This paper describes MTCache and reports experimental results on the TPC-W benchmark. The experiments show that a significant part of the query workload can be offloaded to cache servers, resulting in greatly improved scale-out on the read-dominated workloads of the benchmark. Replication overhead was small with an average replication delay of less than two seconds.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: middle-tier database caching for e-business

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , honguk woo , bruce g. lindsay , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1409

LEFT text: Abstract. Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide  the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom
",n
"LEFT id: NA
RIGHT id: 444

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for mining outliers from large data sets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sridhar ramaswamy , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2061

LEFT text: We study sliding window multi-join processing in continuous queries over data streams. Several algorithms are reported for performing continuous, incremental joins, under the assumption that all the sliding windows fit in main memory. The algorithms include multiway incremental nested loop joins (NLJs) and multi-way incremental hash joins. We also propose join ordering heuristics to minimize the processing cost per unit time. We test a possible implementation of these algorithms and show that, as expected, hash joins are faster than NLJs for performing equi-joins, and that the overall processing cost is influenced by the strategies used to remove expired tuples from the sliding windows.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptive filters for continuous queries over distributed data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chris olston , jing jiang , jennifer widom
",n
"LEFT id: NA
RIGHT id: 528

LEFT text: To bridge the gap between these two extremes, we propose a new class of replication systems called TRAPP (Tradeoff in Replication Precision and Performance). TRAPP systems give each user fine-grained control over the tradeoff between precision and performance: Caches store ranges that are guaranteed to bound the current data values, instead of storing stale exact values. Users supply a quantitative precision constraint along with each query. To answer a query, TRAPP systems automatically select a combination of locally cached bounds and exact master data stored remotely to deliver a bounded answer consisting of a range that is no wider than the specified precision constraint, that is guaranteed to contain the precise answer, and that is computed as quickly as possible. This paper defines the architecture of TRAPP replication systems and covers some mechanics of caching data ranges.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: top-k selection queries over relational databases : mapping strategies and performance evaluation

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: nicolas bruno , surajit chaudhuri , luis gravano
",n
"LEFT id: NA
RIGHT id: 782

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization for xml

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jason mchugh , jennifer widom
",n
"LEFT id: NA
RIGHT id: 723

LEFT text: Multi-tier infrastructures have become common practice for implementing high volume web sites. Such infrastructures typically contain TCP load balancers, HTTP servers, application servers, transaction-processing monitors, and databases. Caching has been widely used at different layers of the infrastructure stack to improve scalability and response time of e-business applications. The majority of existing caching mechanisms target only static HTML pages or page fragments. However, as web applications become more dynamic through increased personalization, these caching techniques turn out to be less useful. Consequently, as more application requests result in increased querying and updating of backend database servers, scalability limits are often reached.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: middle-tier database caching for e-business

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: qiong luo , sailesh krishnamurthy , c. mohan , hamid pirahesh , honguk woo , bruce g. lindsay , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 917

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. In the first Long Range Planning special issue on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus was on methodological developments and their application (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a,b,c; Money et al., 2012; Rigdon, 2012). 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: peter apers , stefano ceri , richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1125

LEFT text: Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cost-based selection of path expression processing algorithms in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: georges gardarin , jean-robert gruser , zhao-hui tang
",n
"LEFT id: NA
RIGHT id: 232

LEFT text: A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisation. In the paper we first give the syntax of our archetypal DBPL and briefly discuss its semantics. We then define a small but powerful algebra of operators over the set data type, provide some key equivalences for expressions in these operators, and list transformation principles for optimising expressions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: safe query languages for constraint databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter z. revesz
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the soluticn space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1751

LEFT text: The VQBD project addresses the following problem: What is the best way to explore an XML document of unknown structure and content? We focus on XML documents that are too large to browse in their entirety, even with the assistance of pretty-printing software (e.g., multi-megabyte or larger XML documents). In this context, we use the term data exploration to refer to the process by which a user gathers the information needed to use the data for a speci c purpose (e.g., generating a report, writing queries, building user interfaces, writing applications). In a relational or object database, the schema (e.g., table de nitions, class de nitions, integrity constraints, and stored procedures) provides some of the information necessary for writing queries and applications. However, the schema is rarely su cient for these tasks. Typically, one must probe and browse the database to discover data coverage, typical and exceptional values, and other information required to gain a better understanding of the database. In an XML environment, the need for such data exploration is much greater because it is quite likely that the XML data of interest is not accompanied by a schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: inferring structure in semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: svetlozer nestorov , serge abiteboul , rajeev motwani
",n
"LEFT id: NA
RIGHT id: 1199

LEFT text: The paper proposes two extended R -trees that permit the indexing of data regions that grow continuously over time, by also letting the internal bounding regions grow. Internal bounding regions may be triangular as well as rectangular. New heuristics for the algorithms that govern the index structure are provided. As a result, dead space and overlap, now also functions of time, are reduced. Performance studies indicate that the best extended index is typically 3‐5 times faster than the existing R-tree based indices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: indexing of now-relative spatio-bitemporal data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: simonas &#352; altenis , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1669

LEFT text: Managing the transactions in real time distributed computing system is not easy, as it has heterogeneously networked computers to solve a single problem. If a transaction runs across some different sites, it may commit at some sites and may failure at another site, leading to an inconsistent transaction. The complexity is increase in real time applications by placing deadlines on the response time of the database system and transactions processing. Such a system needs to process transactions before these deadlines expired. A series of simulation study have been performed to analyze the performance under different transaction management under conditions such as different workloads, distribution methods, execution mode-distribution and parallel etc. The scheduling of data accesses are done in order to meet their deadlines and to minimize the number of transactions that missed deadlines. A new concept is introduced to manage the transactions in database size for originating site and remote site rather than database size computing parameters. With this approach, the system gives a significant improvement in performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: relaxed transaction processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: munindar p. singh , christine tomlinson , darrell woelk
",n
"LEFT id: NA
RIGHT id: 109

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: specification and implementation of exceptions in workflow management systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: fabio casati , stefano ceri , stefano paraboschi , guiseppe pozzi
",n
"LEFT id: NA
RIGHT id: 518

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: introduction to constraint databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: bart kuijpers
",n
"LEFT id: NA
RIGHT id: 2064

LEFT text: As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 49

LEFT text: Efficient user-adaptable similarity search more and more increases in its importance for multimedia and spatial database systems. As a general similarity model for multi-dimensional vectors that is adaptable to application requirements and user preferences, we use quadratic form distance functions which have been successfully applied to color histograms in image databases [Fal+ 94]. The components aij of the matrix A denote similarity of the components i and j of the vectors. Beyond the Euclidean distance which produces spherical query ranges, the similarity distance defines a new query type, the ellipsoid query. We present new algorithms to efficiently support ellipsoid query processing for various user-defined similarity matrices on existing precomputed indexes. By adapting techniques for reducing the dimensionality and employing a multi-step query processing architecture, the method is extended to high-dimensional data spaces. In particular, from our algorithm to reduce the similarity matrix, we obtain the greatest lowerbounding similarity function thus guaranteeing no false drops. We implemented our algorithms in C++ and tested them on an image database containing 12,000 color histograms. The experiments demonstrate the flexibility of our method in conjunction with a high selectivity and efficiency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 425

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wsq/dsq : a practical approach for combined querying of databases and the web

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1825

LEFT text: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient view maintenance at data warehouses

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: d. agrawal , a. el abbadi , a. singh , t. yurek
",y
"LEFT id: NA
RIGHT id: 814

LEFT text: Our initial studies of Broadcast Disks focused on the performance of the mechanism when the data being broadcast did not change. In this paper, we extend those results to incorporate the impact of updates. We first propose several alternative models for updates and examine the fundamental tradeoff that arises between the currency of data and performance. We then propose and analyze mechanisms for implementing these various models. The performance results show that, even in a model where updates must be transmitted immediately, the performance of the Broadcast Disks technique can be made quite mbust through the use of simple techniques for propagating and prefetching data items.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: exploiting versions for handling updates in broadcast disks

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: evaggelia pitoura , panos k. chrysanthis
",n
"LEFT id: NA
RIGHT id: 38

LEFT text: Publisher Summary  The Hippocratic Oath has guided the conduct of physicians for centuries. Inspired by its tenet of preserving privacy, it has been argued that future database systems must include responsibility for the privacy of data that they manage as a founding tenet. The explosive progress in networking, storage, and processor technologies is resulting in an unprecedented amount of digitization of information. It is estimated that the amount of information in the world is doubling every 20 months, and the size and number of databases are increasing even faster. In concert with this dramatic and escalating increase in digital data, concerns about the privacy of personal information have emerged globally. Privacy issues have been further exacerbated, now that the Internet makes it easy for new data to be automatically collected and added to databases. Privacy is the fight of individuals to determine for themselves when, how, and to what extent information about them is communicated to others. Privacy concerns are being fueled by an ever-increasing list of privacy violations, ranging from privacy accidents to illegal actions. Lax security for sensitive data is of equal concern.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: petabyte databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: dirk d &#252; llmann
",n
"LEFT id: NA
RIGHT id: 1752

LEFT text: This paper proposes an extensible framework for capturing and querying meta-data properties in a semistructured data model. Properties such as temporal aspects of data, prices associated with data access, quality ratings associated with the data, and access restrictions on the data are considered. Specifically, the paper defines an extensible data model and an accompanying query language that provides new facilities for matching, slicing, collapsing, and coalescing properties. It also briefly introduces an implemented, SQLlike query language for the extended data model that includes additional constructs for the effective querying of graphs with properties.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: management of semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 547

LEFT text: The wonderfully clean and beautiful scheme put ""on its head"" the world of query optimization I had assumed was the only one possible. In fact, this paper is all about questioning implicit assumptions behind classic query optimization. Is it always true that query-evaluation performance does not fluctuate during query execution?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 4

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ccube constraint object-oriented database system

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alexander brodsky , victor e. segal , jia chen , paval a. exarkhopoulo
",n
"LEFT id: NA
RIGHT id: 428

LEFT text: Int#ra-operator (or partitioned) parallelism is a well-established mechanism for achieving high performance in parallel database systems. However, the problem of how to exploit intra-operator parallelism in a multi-query environment is not well underst,ood. This paper presents a detailed performance evaluation of several algorithms for managing intra-operator parallelism in a parallel database system. A dynamic scheme based on the concept of matching the ra.te of flow of tuples between operat,ors is shown to perform well on a variety of workloads and configurations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards self-tuning data placement in parallel database systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: mong li lee , masaru kitsuregawa , beng chin ooi , kian-lee tan , anirban mondal
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 87

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cost estimation of user-defined methods in object-relational database systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jihad boulos , kinji ono
",n
"LEFT id: NA
RIGHT id: 48

LEFT text: Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible. The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODL<subscrpt>I<3</subscrpt> descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions.  ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: storing semistructured data with stored

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: alin deutsch , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 114

LEFT text: This contribution argues that electronic markets can serve as a powerful mechanism to entice providers to identify their customer base and to offer customer-oriented, high-quality and economical services and to induce customers to a more focused and price-conscious behavior. The paper claims that this should be particularly true for the provision and access to scientific literature where the tradition so far has been mostly free access by customers and non-transparent cost accounting and service procurement by university libraries. We report on a project for developing a technical network infrastructure that allows for a more cost-transparent access to scientific literature by campus users and attempts to add a competitive element to library services. Equally important, it provides added value to the users so that they can orient themselves in the vast expanses of scientific literature much faster and more economically. We cover three major elements of the infrastructure: user agents, traders and source wrappers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: electronic market : the roadmap for university libraries and members to survive in the information jungle

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael christoffel , sebastian pulkowski , bethina schmitt , peter c. lockemann
",y
"LEFT id: NA
RIGHT id: 2085

LEFT text: Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vist : a dynamic index method for querying xml data by tree structures

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: haixun wang , sanghyun park , wei fan , philip s. yu
",n
"LEFT id: NA
RIGHT id: 262

LEFT text: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on supporting containment queries in relational database management systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chun zhang , jeffrey naughton , david dewitt , qiong luo , guy lohman
",n
"LEFT id: NA
RIGHT id: 381

LEFT text: In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research : achievements and opportunities into the 1st century

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: avi silberschatz , mike stonebraker , jeff ullman
",y
"LEFT id: NA
RIGHT id: 2182

LEFT text: Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: incremental computation and maintenance of temporal aggregates

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jun yang , jennifer widom
",n
"LEFT id: NA
RIGHT id: 2146

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dms : a parallel data mining server

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: felicity george
",n
"LEFT id: NA
RIGHT id: 1884

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: olap , relational , and multidimensional database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: george colliat
",n
"LEFT id: NA
RIGHT id: 2177

LEFT text: This paper reports on the managerial experience, technical approach, and lessons learned from reengineering eight departmental large-scale information systems. The driving strategic objective of each project was to migrate these systems into a set of enterprise-wide systems, which incorporate current and future requirements, drastically reduce operational and maintenance cost, and facilitate common understandings among stakeholders (i.e., policy maker, high-level management, IS developer/maintainer/ end-users). A logical data model , which contains requirements, rules, physical data representation as well as logical data object, clearly documents the baseline data requirements implemented by the legacy system and is crucial to achieve this strategic goal. Re-engineering products are captured in the dictionaries of a CASE tool (i.e., in the form of a business process decomposition hierarchy, as-is data model, normalized logical data model, and linkages among data objects) and are supplemented with traceability matrices in spreadsheets. The re-engineered data products are used as follows: (1) migration of the legacy databases to relational database management systems, (2) automatically generation of databases and applications for migration from mainframes to client-server, (3) enterprise data standardization, (4) integration of disparate information systems, (5) re-documentation, (6) data quality assessment and assurance, and (7) baseline specifications for future systems. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a requirement-based approach to data modeling and re-engineering

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alice h. muntz , christian t. ramiller
",y
"LEFT id: NA
RIGHT id: 2010

LEFT text: Abstract This article discusses the challenges for Database Management in the Internet of Things. We provide scenarios to illustrate the new world that will be produced by the Internet of Things, where physical objects are fully integrated into the information highway. We discuss the different types of data that will be part of the Internet of Things. These include identification, positional, environmental, historical, and descriptive data. We consider the challenges brought by the need to manage vast quantities of data across heterogeneous systems. In particular, we consider the areas of querying, indexing, process modeling, transaction handling, and integration of heterogeneous systems. We refer to the earlier work that might provide solutions for these challenges. Finally we discuss a road map for the Internet of Things and respective technical priorities.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: aurora : a data stream management system

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: d. abadi , d. carney , u. &#199; etintemel , m. cherniack , c. convey , c. erwin , e. galvez , m. hatoun , a. maskey , a. rasin , a. singer , m. stonebraker , n. tatbul , y. xing , r. yan , s. zdonik
",n
"LEFT id: NA
RIGHT id: 1358

LEFT text: There are high expectations in all sectors of society for immediate access to biological knowledge of all kinds. To fully exploit and manage the value of biological resources, society must have the intellectual tools to store, retrieve, collate, analyze, and synthesize organism-level and ecological scale information. However, it currently is difficult to discover, access, and use biodiversity data because of the long history of “bottom-up” evolution of scientific biodiversity information, the mismatch between the distribution of biodiversity itself and the distribution of the data about it, and, most importantly, the inherent complexity of biodiversity and ecological data. This stems from, among many factors, numerous data types, the nonexistence of a common underlying (binary) language, and the multiple perceptions of different researchers/data recorders across spatial or temporal distance or both.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: biodiversity informatics : the challenge of rapid development , large databases , and complex data ( keynote )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: meridith a. lane , james l. edwards , ebbe nielsen
",y
"LEFT id: NA
RIGHT id: 2022

LEFT text: XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lockx : a system for efficiently querying secure xml

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sungran cho , sihem amer-yahia , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 418

LEFT text: In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the onion technique : indexing for linear optimization queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yuan-chi chang , lawrence bergman , vittorio castelli , chung-sheng li , ming-ling lo , john r. smith
",n
"LEFT id: NA
RIGHT id: 557

LEFT text: Intrusion detection is an essential component of computer security mechanisms. It requires accurate and efficient analysis of a large amount of system and network audit data. It can thus be an application area of data mining. There are several characteristics of audit data: abundant raw data, rich system and network semantics, and ever ""streaming"". Accordingly, when developing data mining approaches, we need to focus on: feature extraction and construction, customization of (general) algorithms according to semantic information, and optimization of execution efficiency of the output models. In this paper, we describe a data mining framework for mining audit data for intrusion detection models. We discuss its advantages and limitations, and outline the open research problems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining system audit data : opportunities and challenges

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: wenke lee , wei fan
",y
"LEFT id: NA
RIGHT id: 1912

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: extraction of object-oriented structures from existing relational databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: shekar ramanathan , julia hodges
",n
"LEFT id: NA
RIGHT id: 1711

LEFT text: In this article we present an approach to integrity maintenance, consisting of automatically generating production rules for integrity enforcement. Constraints are expressed as particular formulas of Domain Relational Calculus; they are automatically translated into a set of repair actions, encoded as production rules of an active database system. Production rules may be redundant (they enforce the same constraint in different ways) and conflicting (because repairing one constraint may cause the violation of another constraint). Thus, it is necessary to develop techniques for analyzing the properties of the set of active rules and for ensuring that any computation of production rules after any incorrect transaction terminates and produces a consistent database state. Along these guidelines, we describe a specific architecture for constraint definition and enforcement. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: automatic generation of production rules for integrity maintenance

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: stefano ceri , piero fraternali , stefano paraboschi , letizia tanca
",n
"LEFT id: NA
RIGHT id: 2231

LEFT text: Semantic data modelling I is the established method for the requirements definition and the conceptual specification of application systems. In large projects and especially in enterprise data models the cost of creating a data model amount to a large proportion of the overall cost. On the other hand there is a general pressure to reduce the cost of data modelling for application systems to harness the skyrocketing costs of data processing in a colnpany. The standard textbook modelling process calls for the modelling of single entities to represent simple facts and combining these into a model in a bottom up fashion: 'An entity is a concept, person, thing

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: data integration in the large : the challenge of reuse

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: arnon rosenthal , leonard j. seligman
",n
"LEFT id: NA
RIGHT id: 202

LEFT text: Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate computation of multidimensional aggregates of sparse data using wavelets

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 974

LEFT text: These advantages do not come for free. The challenge of this architecture (as of any clustered or distributed architecture) is to provide data coherency for the independent users of the system. The way to do that is to use locking. Oracle uses multiple level locking: row locks on transaction levels, instance locks within instances, and global locks among the instances. The latter are specific to Oracle Parallel Server.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: architecture of oracle parallel server

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: roger bamford , d. butler , boris klots , n. macnaughton
",n
"LEFT id: NA
RIGHT id: 859

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: research directions in biodiversity informatics

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: john l. schnase
",n
"LEFT id: NA
RIGHT id: 1833

LEFT text: The key issue in performing spatial joins is finding the pairs of intersecting rectangles. For unindexed data sets, this is usually resolved by partitioning the data and then performing a plane sweep on the individual partitions. The resulting join can be viewed as a two-step process where the partition corresponds to a hash-based join while the plane-sweep corresponds to a sort-merge join. In this article, we look at extending the idea of the sort-merge join for one-dimensional data to multiple dimensions and introduce the Iterative Spatial Join. As with the sort-merge join, the Iterative Spatial Join is best suited to cases where the data is already sorted. However, as we show in the experiments, the Iterative Spatial Join performs well when internal memory is limited, compared to the partitioning methods. This suggests that the Iterative Spatial Join would be useful for very large data sets or in situations where internal memory is a shared resource and is therefore limited, such as with today's database engines which share internal memory amongst several queries. Furthermore, the performance of the Iterative Spatial Join is predictable and has no parameters which need to be tuned, unlike other algorithms. The Iterative Spatial Join is based on a plane sweep algorithm, which requires the entire data set to fit in internal memory. When internal memory overflows, the Iterative Spatial Join simply makes additional passes on the data, thereby exhibiting only a gradual performance degradation. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: size separation spatial join

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nick koudas , kenneth c. sevcik
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 2152

LEFT text: The Information Management Group at Dublin City University has research themes such as digital multimedia, interoperable systems and database engineering. In the area of digital multimedia, a collaboration with our School of Electronic Engineering has formed the Centre for Digital Video Processing, a university designated research centre whose aim is to research, develop and evaluate content-based operations on digital video information. To achieve this goal, the range of expertise in this centre covers the complete gamut from image analysis and feature extraction through to video search engine technology and interfaces to video browsing. The Interoperable Systems Group has research interests in federated databases and interoperability, object modelling and database engineering. This report describes the research activities of the major groupings within the Information Management community in Dublin City

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research in database engineering at the university of namur

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jean-luc hainaut
",n
"LEFT id: NA
RIGHT id: 952

LEFT text: We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing ontainment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1356

LEFT text: As the size of data warehouses increase to several hundreds of gigabytes or terabytes, the need for methods and tools that will automate the process of knowledge extraction, or guide the user to subsets of the dataset that are of particular interest, is becoming prominent. In this survey paper we explore the problem of identifying and extracting interesting knowledge from large collections of data residing in data warehouses, by using data mining techniques. Such techniques have the ability to identify patterns and build succinct models to describe the data. These models can also be used to achieve summarization and approximation. We review the associated work in the OLAP, data mining, and approximate query answering literature. We discuss the need for the traditional data mining techniques to adapt, and accommodate the specific characteristics of OLAP systems. We also examine the notion of interestingness of data, as a tool to guide the analysis process. We describe methods that have been proposed in the literature for determining what is interesting to the user and what is not, and how these approaches can be incorporated in the data mining algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: concurrency in the data warehouse

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: richard taylor
",n
"LEFT id: NA
RIGHT id: 1124

LEFT text: The database community has been researching problems in similarity query for time series databases for many years. The techniques developed in the area might shed light on the query by humming problem. In this demo, we treat both the melodies in the music databases and the user humming input as time series. Such an approach allows us to integrate many database indexing techniques into a query by humming system, improving the quality of such system over the traditional (contour) string databases approach. We design special searching techniques that are invariant to shifting, time scaling and local time warping. This makes the system robust and allows more flexible user humming input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: cache coherency in oracle parallel server

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: boris klots
",n
"LEFT id: NA
RIGHT id: 419

LEFT text: In this paper, we present continuous research on data analysis based on our previous work on the shrinking approach. Shrinking[19] is a novel data preprocessing technique which optimizes the inner structure of data inspired by the Newton's Universal Law of Gravitation[16]in the real world. It can be applied in many data mining fields. In this approach data are moved along the direction of the density gradient, thus making the inner structure of data more prominent. It is conducted on a sequence of grids with different cell sizes. In this paper, we applied the Fuzzy concept to improve the performance of the shrinking approach, targeting the better decision making for the movement for individual data points in each iteration. This approach can assist to improve the performance of existing data analysis approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on effective multi-dimensional indexing for strings

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: h. v. jagadish , nick koudas , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1130

LEFT text: The volume of medical imaging data produced per year is rapidly increasing, overtaxing the capabilities of Picture Archival and Communication (PACS) systems. Image compression methods can lessen the problem by encoding digital images into more space-efficient forms. Image compression is achieved by reducing redundancy in the imaging data. Existing methods reduce redundancy in individual images. However, these methods ignore an additional source of redundancy, which is based on the common information stored in more than one image in a set of similar images. We use the term ""set redundancy"" to describe this type of redundancy. Medical image databases contain large sets of similar images, therefore they also contain significant amounts of set redundancy.This paper presents two methods that extract set redundancy from medical imaging data: the Min-Max Differential (MMD), and the Min-Max Predictive (MMP) methods. These methods can improve compression of standard image compression techniques for sets of medical images. Our tests compressing CT brain scans have shown an average of as much as 129% improvement for Huffman encoding, 93% for Arithmetic Coding, and 37% for Lempel-Ziv compression when they are combined with Min-Max methods. Both MMD and MMP are based on reversible operations, hence they provide lossless compression.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast nearest neighbor search in medical image databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: flip korn , nikolaos sidiropoulos , christos faloutsos , eliot siegel , zenon protopapas
",n
"LEFT id: NA
RIGHT id: 284

LEFT text: Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ominisearch : a method for searching dynamic content on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: david buttler , ling liu , calton pu , henrique paques , wei han , wei tang
",n
"LEFT id: NA
RIGHT id: 2107

LEFT text: Queries on XML documents typically combine selections on element contents, and, via path expressions, the structural relationships between tagged elements. Structural joins are used to find all pairs of elements satisfying the primitive structural relationships specified in the query, namely, parent-child and ancestor-descendant relationships. Efficient support for structural joins is thus the key to efficient implementations of XML queries. Recently proposed node numbering schemes enable the capturing of the XML document structure using traditional indices (such as B+-trees or R-trees). This paper proposes efficient structural join algorithms in the presence of tag indices. We first concentrate on using B+- trees and show how to expedite a structural join by avoiding collections of elements that do not participate in the join. We then introduce an enhancement (based on sibling pointers) that further improves performance. Such sibling pointers are easily implemented and dynamically maintainable. We also present a structural join algorithm that utilizes R-trees. An extensive experimental comparison shows that the B+-tree structural joins are more robust. Furthermore, they provide drastic improvement gains over the current state of the art.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , rimon barr , michael carey , bruce lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 1097

LEFT text: The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is significantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimization of queries with user-defined predicates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: surajit chaudhuri , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 538

LEFT text: A unifying model for the study of database performance is proposed. Applications of the model are shown to relate and extend important work concerning batched searching, transposed files, index selection, dynamic hash-based files, generalized access path structures, differential files, network databases, and multifile query processing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: cost models for overlapping and multiversion structures

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias , jun zhang
",y
"LEFT id: NA
RIGHT id: 2119

LEFT text: Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing ” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: y. cui , j. widom
",n
"LEFT id: NA
RIGHT id: 1729

LEFT text: This metadatabase system provides several functions for performing the semantic associative search for images by using the metadata representing the features of images. These functions are realized by using our proposed mathematical model of meaning. The mathematical model of meaning is extended to compute specific meanings of keywords which are used for retrieving images unambiguously and dynamically. The main feature of this model is that the semantic associative search is performed in the orthogonal semantic space. This space is created for dynamically computing semantic equivalence or similarity between the metadata items of the images and keywords.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a metadatabase system for semantic image search by a mathematical model of meaning

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: yasushi kiyoki , takashi kitagawa , takanari hayama
",y
"LEFT id: NA
RIGHT id: 2019

LEFT text: XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1614

LEFT text: Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: an annotated bibliography on real-time database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: &#214; zg &#252; r ulusoy
",n
"LEFT id: NA
RIGHT id: 584

LEFT text: Various relation-based systems, concerned with the qualitative representation and processing of spatial knowledge, have been developed in numerous application domains. In this article, we identify the common concepts underlying qualitative spatial knowledge representation, we compare the representational properties of the different systems, and we outline the computational tasks involved in relation-based spatial information processing. We also describesymbolic spatial indexes, relation-based structures that combine several ideas in spatial knowledge representation. A symbolic spatial index is an array that preserves only a set of spatial relations among distinct objects in an image, called the modeling space; the index array discards information, such as shape and size of objects, and irrelevant spatial relations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: qualitative representation of spatial knowledge in two-dimensional space

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dimitris papadias , timos sellis
",y
"LEFT id: NA
RIGHT id: 1803

LEFT text: The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an alternative storage organization for rolap aggregate views based on cubetrees

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1864

LEFT text: In this paper we investigate the problem of incremental maintenance of materialized views in data warehouses. We consider views defined by relational algebraic operators and aggregate functions. We show that a materialized view can be maintained without accessing the view itself by materializing and maintaining additional relations. These relations are derived from the intermediate results of the view computation. We first give an algorithm for determining what additional relations need to be materialized in order to maintain a materialized view incrementally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the whips prototype for data warehouse creation and maintenance

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: wilburt j. labio , yue zhuge , janet l. wiener , himanshu gupta , h &#233; ctor garc &#237; a-molina , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1223

LEFT text: Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: wavecluster : a wavelet-based clustering approach for spatial data in very large databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: gholamhosein sheikholeslami , surojit chatterjee , aidong zhang
",n
"LEFT id: NA
RIGHT id: 1906

LEFT text: Multidimensional discrete data (MDD) i.e., arrays of arbitrary size, dimension, and base type appear in a variety of business, technical, and scientific application fields. RasDaMan is an effort to give comprehensive domain-independent MDD database support. Based on a formal algebraic array model, RasDaMan offers declarative array operators embedded in standard SQL; key DBMS components are an MDD query optimizer and a streamlined storage manager for efficient access to subsets of huge arrays. We present the RasDaMan approach to MDD management based on the medical and geographic application fields addressed in the project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating modelling systems for environmental management information systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david j. abel , kerry taylor , dean kun
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: Querying large numbers of data sources is gaining importance due to increasing numbers of independent data providers. One of the key challenges is executing queries on all relevant information sources in a scalable fashion and retrieving fresh results. The key to scalability is to send queries only to the relevant servers and avoid wasting resources on data sources which will not provide any results. Thus, a catalog service, which would determine the relevant data sources given a query, is an essential component in efficiently processing queries in a distributed environment. This paper proposes a catalog framework which is distributed across the data sources themselves and does not require any central infrastructure. As new data sources become available, they automatically become part of the catalog service infrastructure, which allows scalability to large numbers of nodes. Furthermore, we propose techniques for workload adaptability. Using simulation and real-world data we show that our approach is valid and can scale to thousands of data sources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 143

LEFT text: Clustering of large databases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understand the results, which is especially important if the data under consideration is high dimensional and has not been collected for the purpose of being analyzed. Visualization technology may help to solve this problem since it allows an effective support of different clustering paradigms and provides means for a visual inspection of the results.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: supersql : an extended sql for database publishing and presentation

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: motomichi toyama
",y
"LEFT id: NA
RIGHT id: 591

LEFT text: In this paper, we present two algorithms for deriving optimal and near-optimal vertical class partitioning schemes. The cost-driven algorithm provides the optimal vertical class partitioning schemes by enumerating, exhaustively, all the schemes and calculating the number of disk accesses required to execute a given set of applications. For this, a cost model for executing a set of methods in an OODB system is developed. Since exhaustive enumeration is costly and only works for classes with a small number of instance variables, a hill-climbing heuristic algorithm (HCHA) is developed, which takes the solution provided by the affinity-based algorithm and improves it, thereby further reducing the total number of disk accesses incurred.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index configuration in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: elisa bertino
",n
"LEFT id: NA
RIGHT id: 60

LEFT text: E-services are business functions made available via the Internet by service providers, and accessible by clients that could be human users or software applications. The main benefit of the e-services environment is that clients are able to dynamically discover the available e-service that best meets their needs, to examine its properties and capabilities, and to determine if and how to access it. However, in order to deliver e-services to clients, service providers are faced with several challenges. In particular, they need to describe e-services in a way that is accessible and understandable by the clients and to advertise them in web directories, so that they can be discovered by brokers as well as by end-users. In this tutorial we discuss the main requirements for models and languages for service description and discovery, and we present relevant approaches proposed by standardization consortia.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: automatic discovery of language models for text databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jamie callan , margaret connell , aiqun du
",n
"LEFT id: NA
RIGHT id: 2245

LEFT text: This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: challenges for global information systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , abraham silberschatz , divesh srivastava , maria zemankova
",n
"LEFT id: NA
RIGHT id: 337

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: schemasql : an extension to sql for multidatabase interoperability

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: laks v. s. lakshmanan , fereidoon sadri , subbu n. subramanian
",n
"LEFT id: NA
RIGHT id: 2007

LEFT text: We have introduced a Multi-Dimensional Clustering (MDC) physical layout scheme in DB2 version 8.0 for relational tables. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. Each clustering key is allocated one or more blocks of physical storage with the aim of storing the multiple records belonging to the cluster in almost contiguous fashion. Block oriented indexes are created to access these blocks. In this paper, we describe novel techniques for query processing operations that provide significant performance improvements for MDC tables. Current database systems employ a repertoire of access methods including table scans, index scans, index ANDing, and index ORing. We have extended these access methods for efficiently processing the block based MDC tables. One important concept at the core of processing MDC tables is the block oriented access technique. In addition, since MDC tables can include regular record oriented indexes, we employ novel techniques to combine block and record indexes. Block oriented processing is extended to nested loop joins and star joins as well. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: multi-dimensional clustering : a new data layout scheme in db2

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: sriram padmanabhan , bishwaranjan bhattacharjee , tim malkemus , leslie cranston , matthew huras
",n
"LEFT id: NA
RIGHT id: 542

LEFT text: The chair of ACRP’s Association Board of Trustees recounts how he left manufacturing and research and development a decade ago to take a position as a clinical research administrator. It was a move that place him into a role he knew little about, having not been engaged in clinical research beforehand. If this sounds like a familiar experience to others, the lessons shared in this column highlight the importance of the individual’s ongoing will to learn, and of organizational support for that learning.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: treasurer 's message

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 1426

LEFT text: A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: near neighbor search in large metric spaces

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: sergey brin
",n
"LEFT id: NA
RIGHT id: 453

LEFT text: Many problems encountered when building applications of database systems involve the manipulation of models. By ""model,"" we mean a complex structure that represents a design artifact, such as a relational schema, object-oriented interface, UML model, XML DTD, web-site schema, semantic network, complex document, or software configuration. Many uses of models involve managing changes in models and transformations of data from one model into another. These uses require an explicit representation of ""mappings"" between models. We propose to make database systems easier to use for these applications by making ""model"" and ""model mapping"" first-class objects with special operations that simplify their use. We call this capability model management.In addition to making the case for model management, our main contribution is a sketch of a proposed data model. The data model consists of formal, object-oriented structures for representing models and model mappings, and of high-level algebraic operations on those structures, such as matching, differencing, merging, selection, inversion and instantiation. We focus on structure and semantics, not implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a vision for management of complex models

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: phillip a. bernstein , alon y. halevy , rachel a. pottinger
",y
"LEFT id: NA
RIGHT id: 865

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a case-based approach to information integration

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: maurizio panti , luca spalazzi , alberto giretti
",n
"LEFT id: NA
RIGHT id: 1407

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: transaction timestamping in ( temporal ) databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian s. jensen , david b. lomet
",n
"LEFT id: NA
RIGHT id: 1471

LEFT text: One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",n
"LEFT id: NA
RIGHT id: 1697

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constructing the next 100 database management systems : like the handyman or like the engineer ?

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: andreas geppert , klaus r. dittrich
",y
"LEFT id: NA
RIGHT id: 1490

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems 101

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jim gray
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1758

LEFT text: While the number of database management systems (DBMSs) increases and the various DBMSs get more and more complex, no uniform method for DBMS construction exists. As a result, developers are forced to start more or less from scratch again for every desired system, resulting in a waste of time, effort, and cost. Hence, the database community is challenged with the development of an appropriate method, i.e. the time-saving application of engineering principles (e.g., reuse). Problems related to a construction method are described, as well as approaches towards solutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: lore : a database management system for semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jason mchugh , serge abiteboul , roy goldman , dallas quass , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1674

LEFT text: This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: genesys : a system for efficient spatial query processing

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: thomas brinkhoff , hans-peter kriegel , ralf schneider , bernhard seeger
",n
"LEFT id: NA
RIGHT id: 879

LEFT text: Publisher Summary eXtensible Markup Language (XML) is becoming the predominant data exchange format in a variety of application domains (supply-chain, scientific data processing, telecommunication infrastructure, etc.). Not only is an increasing amount of XML data now being processed, but XML is also increasingly being used in business-critical applications. Efficient and reliable storage is an important requirement for these applications. By relying on relational engines for this purpose, XML developers can benefit from a complete set of data management services (including concurrency control, crash recovery, and scalability) and from the highly optimized relational query processors. Strategies that automate the process of generating XML to relational mappings have been proposed in the literature. Due to the flexibility of the XML infrastructure, different XML applications exhibit widely different characteristics (for example, permissive vs. strict schemas, different access patterns).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficiently publishing relational data as xml documents

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene j. shekita , rimon barr , michael j. carey , bruce g. lindsay , hamid pirahesh , berthold reinwald
",n
"LEFT id: NA
RIGHT id: 333

LEFT text: Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information. Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies. Which strategy is appropriate depends very much on the known interdependencies among the events involved. Previous work on probabilistic databases has assumed a fixed and restrictivecombination strategy (e.g., assuming all events are pairwise independent). In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a genericprobabilistic relational algebra that neatly captures various strategiessatisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0. We validate our complexity results with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probabilistic object bases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: thomas eiter , james j. lu , thomas lukasiewicz , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 220

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation in spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: swarup acharya , viswanath poosala , sridhar ramaswamy
",n
"LEFT id: NA
RIGHT id: 2032

LEFT text: We consider the problem of mapping data in peer-to-peer data-sharing systems. Such systems often rely on the use of mapping tables listing pairs of corresponding values to search for data residing in different peers. In this paper, we address semantic and algorithmic issues related to the use of mapping tables. We begin by arguing why mapping tables are appropriate for data mapping in a peer-to-peer environment. We discuss alternative semantics for these tables and we present a language that allows the user to specify mapping tables under different semantics. Then, we show that by treating mapping tables as constraints (called mapping constraints) on the exchange of information between peers it is possible to reason about them. We motivate why reasoning capabilities are needed to manage mapping tables and show the importance of inferring new mapping tables from existing ones. We study the complexity of this problem and we propose an efficient algorithm for its solution. Finally, we present an implementation along with experimental results that show that mapping tables may be managed efficiently in practice.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mapping data in peer-to-peer systems : semantics and algorithmic issues

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: anastasios kementsietsidis , marcelo arenas , ren &#233; e j. miller
",y
"LEFT id: NA
RIGHT id: 331

LEFT text: Although the relational model for databases provides a great range of advantages over other data models, it lacks a comprehensive way to handle incomplete and uncertain data. Uncertainty in data values, however, is pervasive in all real-world environments and has received much attention in the literature. Several methods have been proposed for incorporating uncertain data into relational databases. However, the current approaches have many shortcomings and have not established an acceptable extension of the relational model. In this paper, we propose a consistent extension of the relational model. We present a revised relational structure and extend the relational algebra. The extended algebra is shown to be closed, a consistent extension of the conventional relational algebra, and reducible to the latter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: probabilistic temporal databases , i : algebra

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alex dekhtyar , robert ross , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 1436

LEFT text: We propose a product specification database which is suited to product evolution, modeling the product specification as an object. In this database, we propose a behavioral constraint to maintain consistency. Furthermore, this database can manage visual specification, such as operational specification, which is hard to handle in an ordinary database. We have been developing Visual CASE: an object-oriented software development system for home appliances. Visual CASE is a visual prototyping system based on the object model we propose. In this paper, we show that the product specification is easy to examine, using visual prototyping. We also discuss implementation issues of the database applied to the home appliance software development process. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a product specification database for visual prototyping

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: kazutoshi sumiya , kouichi yasutake , hirohiko tanaka , norio sanada , yoshihiko imai
",y
"LEFT id: NA
RIGHT id: 577

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficiently instantiating view-objects from remote relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: byung suk lee , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 316

LEFT text: Publisher Summary  This chapter presents the first XPath query evaluation algorithm that runs in polynomial time with respect to the size of both the data and of the query. XPath has been proposed by the W3C as a practical language for selecting nodes from XML document trees. XPath is important because of its potential application as an XML query language per se, it being at the core of several other XML-related technologies, such as XSLT, XPointer, and XQuery, and the great and well-deserved interest such technologies receive. Since XPath and related technologies will be tested in ever-growing deployment scenarios, its implementations need to scale well both with respect to the size of the XML data and the growing size and intricacy of the queries (usually referred to as combined complexity).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: generating efficient plans for queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: foto n. afrati , chen li , jeffrey d. ullman
",n
"LEFT id: NA
RIGHT id: 255

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data mining techniques

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jiawei han
",n
"LEFT id: NA
RIGHT id: 334

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences in influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 469

LEFT text: We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on computing correlated aggregates over continual data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: johannes gehrke , flip korn , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 841

LEFT text: Here is an old joke: what is black and white and red all over? A newspaper. Why though? As we assume that nothing could really be black and white and red all over, we infer that ‘red’ should be heard as ‘read.’ In the grand philosophical tradition of making even humour unfunny, I want to take issue with this assumption. My thesis is that it is possible to see two objects in black and white, while at the same time seeing one of them as redder than the other. More generally, I argue that it is possible to perceptually represent colour relations between two objects, without perceptually representing their colours. I call this primitive relational colour representation (PRCR). This goes against the orthodox view that we represent colour relations by virtue of representing colours. This orthodoxy has been challenged by several authors in the recent literature, and I here add my name to the chorus.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: datawarehousing has more colours than just black & ; white

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: thomas zurek , markus sinnwell
",y
"LEFT id: NA
RIGHT id: 2063

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamic sample selection for approximate query processing

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: brian babcock , surajit chaudhuri , gautam das
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 246

LEFT text: The optimization capabilities of RDBMSs make them attractive for executing data transformations. However, despite the fact that many useful data transformations can be expressed as relational queries, an important class of data transformations that produce several output tuples for a single input tuple cannot be expressed in that way. To overcome this limitation, we propose to extend Relational Algebra with a new operator named data mapper. In this paper, we formalize the data mapper operator and investigate some of its properties. We then propose a set of algebraic rewriting rules that enable the logical optimization of expressions with mappers and prove their correctness. Finally, we experimentally study the proposed optimizations and identify the key factors that influence the optimization gains.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: metu interoperable database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: asuman dogac , ugur halici , ebru kilic , gokhan ozhan , fatma ozcan , sena nural , cevdet dengi , sema mancuhan , budak arpinar , pinar koksal , cem evrendilek
",n
"LEFT id: NA
RIGHT id: 454

LEFT text: In a mobile computing system, caching data items at the mobile clients is important to reduce the data access delay in an unreliable and low bandwidth mobile network. However, efficient methods must be used to ensure the coherence between the cached items and the data items at the database server. By exploring the real time properties of the data items, we propose a cache invalidation scheme called: Invalidation by Absolute Validity Interval (IAVI). We define an absolute validate interval (AVI) for each data item based on its real time property, e.g. update interval. A mobile client can verify the validity of a cached item by comparing the last update time and its AVI. A cached item is invalidated if the current time is greater than the last update time by its AVI. With this self-invalidation mechanism, the IAVI scheme uses the invalidation report to inform the mobile clients about the change of AVI rather than the update event of the data item. As a result, the size of invalidation report can be reduced significantly. Performance studies show that the IAVI scheme can significantly reduce the mean response time and invalidation report size under various system parameters.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: cache invalidation scheme for mobile computing systems with real-time data

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: joe chun-hung yuen , edward chan , kam-yiu lam , h. w. leung
",y
"LEFT id: NA
RIGHT id: 1973

LEFT text: Query size estimation is crucial for many database system components. In particular, query optimizers need efficient and accurate query size estimation when deciding among alternative query plans. In this paper we propose a novel sampling technique based on the golden rule of sampling, introduced by von Neumann in 1947, for estimating range queries. The proposed technique randomly samples the frequency domain using the cumulative frequency distribution and yields good estimates without any a priori knowledge of the actual underlying distribution of spatial objects. We show experimentally that the proposed sampling technique gives smaller approximation error than the Min-Skew histogram based and wavelet based approaches for both synthetic and real datasets. Moreover, the proposed technique can be easily extended for higher dimensional datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: bifocal sampling for skew-resistant join size estimation

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sumit ganguly , phillip b. gibbons , yossi matias , avi silberschatz
",n
"LEFT id: NA
RIGHT id: 1345

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: caching technologies for web applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: c. mohan
",n
"LEFT id: NA
RIGHT id: 1276

LEFT text: QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: unql : a query language and algebra for semistructured data based on structural recursion

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: peter buneman , mary fernandez , dan suciu
",n
"LEFT id: NA
RIGHT id: 1048

LEFT text: The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations.    Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities.    Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies.    The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 2159

LEFT text: Multidimensional access methods have shown high potential for significant performance improvements in various application domains. However, only few approaches have made their way into commercial products. In commercial database management systems (DBMSs) the BTree is still the prevalent indexing technique. Integrating new indexing methods into existing database kernels is in general a very complex and costly task. Exceptions exist, as our experience of integrating the UB-Tree into TransBase, a commercial DBMS, shows. The UB-Tree is a very promising multidimensional index, which has shown its superiority over traditional access methods in different scenarios, especially in OLAP applications. In this paper we discuss the major issues of a UB-Tree integration. As we will show, the complexity and cost of this task is reduced significantly due to the fact that the UBTree relies on the classical B-Tree. Even though commercial DBMSs provide interfaces for index extensions, we favor the kernel integration because of the tight coupling with the query optimizer, which allows for optimal usage of the UBTree in execution plans. Measurements on a real-world data warehouse show that the kernel integration leads to an additional performance improvement compared to our prototype implementation and competing index methods.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: very large databases in a commercial application environment

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: karl-heinz hess
",y
"LEFT id: NA
RIGHT id: 639

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: tigukat : a uniform behavioral objectbase management system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu , randal peters , duane szafron , boman irani , anna lipka , adriana mu &#241; oz
",n
"LEFT id: NA
RIGHT id: 28

LEFT text: Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: exploratory mining via constrained frequent set queries

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: raymond ng , laks v. s. lakshmanan , jiawei han , teresa mah
",n
"LEFT id: NA
RIGHT id: 1191

LEFT text: S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: efficient similarity search for market basket data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexandros nanopoulos , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 958

LEFT text: Building such a database system requires fundamental changes in the architecture of the query processing engine; we present the system-level interfaces of PREDATOR that support E-ADTs, and describe the internal design details.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the case for enhanced abstract data types

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: praveen seshadri , miron livny , raghu ramakrishnan
",y
"LEFT id: NA
RIGHT id: 299

LEFT text: To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: independence is good : dependency-based histogram synopses for high-dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amol deshpande , minos garofalakis , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1962

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query caching and optimization in distributed mediator systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: s. adali , k. s. candan , y. papakonstantinou , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 742

LEFT text: Table of contentsI1 Proceedings of the 4th World Conference on Research IntegrityConcurrent Sessions:1. Countries' systems and policies to foster research integrityCS01.1 Second time around: Implementing and embedding a review of responsible conduct of research policy and practice in an Australian research-intensive universitySusan Patricia O'BrienCS01.2 Measures to promote research integrity in a university: the case of an Asian universityDanny Chan, Frederick Leung2. Examples of research integrity education programmes in different countriesCS02.1 Development of a state-run “cyber education program of research ethics” in KoreaEun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho LeeCS02.3 Responsible conduct of research teachers’ training courses in Germany: keeping on drilling through hard boards for more RCR teachersHelga Nolte, Michael Gommel, Gerlinde Sponholz3. The research environment and policies to encourage research integrity

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the acm fourth international workshop on data warehousing and olap ( dolap 2001 )

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: joachim hammer
",n
"LEFT id: NA
RIGHT id: 1511

LEFT text: Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: temporal conditions and integrity constraints in active database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: a. prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 2105

LEFT text: Heavily used in both academic and corporate R&D settings, ACM Transactions on Database Systems (TODS) is a key publication for computer scientists working in data abstraction, data modeling, and designing data management systems. Topics include storage and retrieval, transaction management, distributed and federated databases, semantics of data, intelligent databases, and operations and algorithms relating to these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: minicon : a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rachel pottinger , alon halevy
",n
"LEFT id: NA
RIGHT id: 232

LEFT text: This article presents a database programming language, Thémis, which supports subtyping and class hierarchies, and allows for the definition of integrity constraints in a global and declarative way. We first describe the salient features of the language: types, names, classes, integrity constraints (including methods), and transactions. The inclusion of methods into integrity constraints allows an increase of the declarative power of these constraints. Indeed, the information needed to define a constraint is not always stored in the database through attributes, but is sometimes computed or derived data. Then, we address the problem of efficiently checking constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: safe query languages for constraint databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: peter z. revesz
",n
"LEFT id: NA
RIGHT id: 1752

LEFT text: Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper proposes an incremental maintenance algorithm for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. our algorithm produces a set of queries that compute the updates to the view based upon an update of the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to apply our incremental maintenance algorithm to the view than to recompute the view from the database, even when there are thousands of updates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: management of semistructured data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: dan suciu
",n
"LEFT id: NA
RIGHT id: 1138

LEFT text: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be “perfectly” translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable “closeness” metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying heterogeneous information sources using source descriptions

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alon y. levy , anand rajaraman , joann j. ordille
",n
"LEFT id: NA
RIGHT id: 1072

LEFT text: We consider the view data lineageproblem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for materialized view design in data warehousing environment

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: jian yang , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 501

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on second international workshop on advanced issues of e-commerce and web-based information systems

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kun-lung wu , philip s. yu
",n
"LEFT id: NA
RIGHT id: 1313

LEFT text: The growing acceptance of XML as a standard for semi-structured documents on the Web opens up challenging opportunities for Web query languages. In this paper we introduce XML-GL, a graphical query language for XML documents. The use of a visual formalism for representing both the content of XML documents (and of their DTDs) and the syntax and semantics of queries enables an intuitive expression of queries, even when they are rather complex. XML-GL is inspired by G-log, a general purpose, logic-based language for querying structured and semi-structured data. The paper presents the basic capabilities of XML-GL through a sequence of examples of increasing complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a fast index for semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: brian cooper , neal sample , michael j. franklin , gisli r. hjaltason , moshe shadmon
",n
"LEFT id: NA
RIGHT id: 848

LEFT text: In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 482

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and management of data warehouses report on the dmdw '99 workshop

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: stella gatziu , manfred jeusfeld , martin staudt , yannis vassiliou
",n
"LEFT id: NA
RIGHT id: 848

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: integrating heterogenous overlapping databases through object-oriented transformations

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vanja josifovski , tore risch
",n
"LEFT id: NA
RIGHT id: 1892

LEFT text: To begin with, which RM do we mean? There are several lines of RM and each has had its own evolution. The original line was originated by E.F.Codd who developed during the Seventies what he later named RM/VI. In 1979 he proposed a new model, the RM/T that meant a huge change from the original RM approach. In the Eighties, sensing that plain people did not keep up with him

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a response to r. camps ' article domains , relations and religious wars

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: c. j. date
",n
"LEFT id: NA
RIGHT id: 169

LEFT text: Internet-based communication defines two main types of services as Pull and Push services, depending on the side that sends the request for transmission of information. In contrast to Pull services, whose request for transmission is initiated by the client, Push service denotes a type of transmission where the request for a given exchange of information is initiated by the publisher or central server. The newly proposed Alert Notification Service (ANS) represents an important implementation of these Push services, for the sites that neither implement it or that think it is not worth to do it economically. In this paper we present details on system functionalities, architecture model, design, and integration of essential modules into a fully working system as a service. The main contribution is in design of a highly scalable solution for an elastic cloud architecture, by separating the static and dynamic parts, correspondingly assigned to different virtual machines.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data in your face : push technology in perspective

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: michael franklin , stan zdonik
",y
"LEFT id: NA
RIGHT id: 1775

LEFT text: Microsoft’s strategic interest in the database field dates from 1993 and the efforts of David Vaskevitch, who is now the Microsoft Vice President in charge of the database and transaction processing product development groups. David’s vision was that the world would need millions of servers, and that this presented a wonderful opportunity to a company like Microsoft that sells software in high volume and at low prices. Database systems played an important role in Vaskevitch’s vision, and, indeed, in Microsoft’s current product plans. David began looking for premier database and transaction processing people in late 1993. The scope of Vaskevitch’s efforts included a desire for Microsoft to establish a database research group. Rick Rashid, Microsoft Research Vice President, collaborated with Vaskevitch in recruiting David Lomet from Digital’s Cambridge Research Lab to initiate the Microsoft Database Research Group. Lomet joined Microsoft Research in January of 1995. Hence, Microsoft’s Database Research Group is now a little over three and a half years old. One person does not a group make. Recruiting efforts continued. Surajit Chaudhuri, a researcher from HP Labs joined the Database Group in February of 1996. Paul Larson, a professor from the University of Waterloo joined in May of that year. Vivek Narasayya was initially an intern as a graduate student from the University of Washington in the summer of 1996, officially joining the group in April of 1997. Roger Barga, the newest member of the group and a new Oregon Graduate Institute Ph.D., joined in December, 1997.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: predator : a resource for database research

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: praveen seshadri
",n
"LEFT id: NA
RIGHT id: 815

LEFT text: PM3 is an orthogonally persistent extension of the Modula-3 systems programming language, supporting persistence by reachability from named persistent roots. We describe the design and implementation of the PM3 prototype, and show that its performance is competitive with its nonorthogonal counterparts by direct comparison with the SHORE/C++ language binding to the SHORE object store. Experimental results, using the traversal portions of the OO7 benchmark, reveal that the overheads of orthogonal persistence are not inherently more expensive than for nonorthogonal persistence, and justify our claim that orthogonal persistence deserves a level of acceptance similar to that now emerging for automatic memory management (i.e., “garbage collection”), even in performance-conscious settings. The consequence will be safer and more flexible persistent systems that do not compromise performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pm3 : an orthogonal persistent systems programming language - design , implementation , performance

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: antony l. hosking , jiawan chen
",y
"LEFT id: NA
RIGHT id: 546

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",y
"LEFT id: NA
RIGHT id: 263

LEFT text: Over the past years several works have proposed access con- trol models for XML data where only read-access rights over non-recursive DTDs are considered. A small number of works have studied the access rights for updates. In this paper, we present a general model for specifying access con- trol on XML data in the presence of the update operations of W3C XQuery Update Facility. Our approach for enforc- ing such update specification is based on the notion of query rewriting. A major issue is that query rewriting for recursive DTDs is still an open problem. We show that this limitation can be avoided using only the expressive power of the stan- dard XPath, and we propose a linear algorithm to rewrite each update operation defined over an arbitrary DTD (re- cursive or not) into a safe one in order to be evaluated only over the XML data which can be updated by the user. This paper represents the first effort for securely XML updating in the presence of arbitrary DTDs (recursive or not) and a rich fragment of XPath

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: updating xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: igor tatarinov , zachary g. ives , alon y. halevy , daniel s. weld
",y
"LEFT id: NA
RIGHT id: 1026

LEFT text: In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 2191

LEFT text: To benefit from mature database technology RDF stores are built on top of relational databases and SPARQL queries are mapped into SQL. Using a shared-nothing computer cluster is a way to achieve scalability by carrying out query processing on top of large RDF datasets in a distributed fashion. Aiming to this the current paper elaborates on the impact of relational schema design when queries are mapped into Apache Spark SQL. A single triple table, a set of tables resulting from partitioning by predicate, a single wide table covering all properties, and a set of tables based on the application model specification called domain-dependent-schema, are the considered designs. For each of the mentioned approaches, the rows of the corresponding tables are stored in the distributed file system HDFS using the columnar-store Parquet. Experiments using standard benchmarks demonstrate that the single wide property table approach, despite its simplicity, is superior to other approaches. Further experiments demonstrate that this single table approach continues to be attractive even when repartitioning by key (RDF subject) is applied before executing queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on processing xml in ldap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: pedro jos &#233; marr &#243; n , georg lausen
",y
"LEFT id: NA
RIGHT id: 807

LEFT text: e consider the execution of multi-join queries in a hierarchical parallel system, i.e., a shared-nothing system whose nodes are shared-memory multiprocessors. In this context, load balancing must be addressed at two levels, locally among the processors of each shared-memory node and globally among all nodes. In this paper, we propose a dynamic execution model that maximizes local load balancing within shared-memory nodes and minimizes the need for load sharing across nodes. This is obtained by allowing each processor to execute any operator that can be processed locally, thereby taking full advantage of inter- and intra-operator parallelism. We conducted a performance evaluation using an implementation on a 72-processor KSR1 computer.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing for parallel association rule mining on heterogenous pc cluster systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: masahisa tamura , masaru kitsuregawa
",n
"LEFT id: NA
RIGHT id: 1443

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of multiple-level association rules from large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu
",n
"LEFT id: NA
RIGHT id: 1418

LEFT text: In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: form-based proxy caching for database-backed web sites

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: qiong luo , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 1178

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: matthias jarke
",n
"LEFT id: NA
RIGHT id: 127

LEFT text: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: semantic integration of semistructured and structured data sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: s. bergamaschi , s. castano , m. vincini
",n
"LEFT id: NA
RIGHT id: 287

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: vqbd : exploring semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sudarshan s. chawathe , thomas baby , jihwang yoo
",n
"LEFT id: NA
RIGHT id: 952

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dataguides : enabling query formulation and optimization in semistructured databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: roy goldman , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1489

LEFT text: With the growing popularity of the internet and the World Wide Web (Web), there is a fast growing demand for access to database management systems (DBMS) from the Web. We describe here techniques that we invented to bridge the gap between HTML, the standard markup language of the Web, and SQL, the standard query language used to access relational DBMS. We propose a flexible general purpose variable substitution mechanism that provides cross-language variable substitution between HTML input and SQL query strings as well as between SQL result rows and HTML output thus enabling the application developer to use the full capabilities of HTML for creation of query forms and reports, and SQL for queries and updates. The cross-language variable substitution mechanism has been used in the design and implementation of a system called DB2 WWW Connection that enables quick and easy construction of applications that access relational DBMS data from the Web.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: carnot and infosleuth : database technology and the world wide web

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: d. woelk , b. bohrer , n. jacobs , k. ong , c. tomlinson , c. unnikrishnan
",n
"LEFT id: NA
RIGHT id: 480

LEFT text: On March 7 and 8, 1996, the First International Workshop on Real-Time Databases (RTDB'96) was held in Newport Beach, California. There were about 50 workshop participants from 12 countries. Twenty two papers were presented and actively discussed in the 2-day single-track 6-session technical program. There were also two panel sessions to assess the state of RTDB research. In this report, we provide highlights of the workshop.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering,

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",n
"LEFT id: NA
RIGHT id: 1961

LEFT text: This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: birch : an efficient data clustering method for very large databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tian zhang , raghu ramakrishnan , miron livny
",n
"LEFT id: NA
RIGHT id: 480

LEFT text: With the use of data warehousing and online analytical processing (OLAP) for decision support applications new security issues arise. The goal of this paper is to introduce an OLAP security design methodology, pointing out fields that require further research work. We present possible access control requirements categorized by their complexity. OLAP security mechanisms and their implementations in commercial systems are presented and checked for their suitability to address the requirements.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on ngits ' 99 : the fourth international workshop on next generation information technologies and systems

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: opher etzion
",n
"LEFT id: NA
RIGHT id: 1959

LEFT text: The Prospector Multimedia Object Manager prototype is a general-purpose content analysis multimedia server designed for massively parallel processor environments. Prospector defines and manipulates user defined functions which are invoked in parallel to analyze/manipulate the contents of multimedia objects. Several computationally intensive applications of this technology based on large persistent datasets include: fingerprint matching, signature verification, face recognition, and speech recognition/translation [OIS96].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fault-tolerant architectures for continuous media servers

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: banu &#214; zden , rajeev rastogi , prashant shenoy , avi silberschatz
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1853

LEFT text: ions in RBE and the WYSIWIG process of programming starting from an example. Rendering By Example Rendering By Example (RBE) was proposed in [KZ95] as a declarative language to express a rendering of data, where a rendering is defined as a presentation of data with subsequent browsing and interaction semantics. A rendering of data consists of a set of screen widgets populated with data. Such rendering applications allow the browsing and interaction with the data in ways specific to that application. These application-specific rendering applications facilitate the user to find and assimilate the data. Rendering of data is widely used in most applications that have a GUI interface and therefore is not a new concept. But the novelty here is the process of constructing such rendering applications. Traditional programming using a state of the art GUI builder would require the following steps. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: picture programming project

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nita goyal , charles hoch , ravi krishnamurthy , brian meckler , michael suchow , moshe zloof
",y
"LEFT id: NA
RIGHT id: 1998

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: accessing relational databases from the world wide web

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tam nguyen , v. srinivasan
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 476

LEFT text: We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can b...

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: constraints for semistructured data and xml

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter buneman , wenfei fan , j &#233; r &#244; me sim &#233; on , scott weinstein
",n
"LEFT id: NA
RIGHT id: 1505

LEFT text: Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient maintenance of materialized mediated views

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: james j. lu , guido moerkotte , joachim schue , v. s. subrahmanian
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 70

LEFT text: Materialized views (or Automatic Summary Tables—ASTs) are commonly used to improve the performance of aggregation queries by orders of magnitude. In contrast to regular tables, ASTs are synchronized by the database system. In this paper, we present techniques for maintaining cube ASTs. Our implementation is based on IBM DB2 UDB.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: answering complex sql queries using automatic summary tables

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: markos zaharioudakis , roberta cochrane , george lapis , hamid pirahesh , monica urata
",n
"LEFT id: NA
RIGHT id: 695

LEFT text: We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing. In this paper we describe an efficient, non-blocking mechanism for reordering, which can be used over arbitrary data streams from files, indexes, and continuous data feeds. We also investigate several policies for the reordering based on the performance goals of various typical applications. We present results from an implementation used in Online Aggregation in the Informix Dynamic Server with Universal Data Option, and in sorting and scrolling in a large-scale spreadsheet. Our experiments demonstrate that for a variety of data distributions and applications, reordering is responsive to dynamic preference changes, imposes minimal overheads in overall completion time, and provides dramatic improvements in the quality of the feedback over time. Surprisingly, preliminary experiments indicate that online reordering can also be useful in traditional batch query processing, because it can serve as a form of pipelined, approximate sorting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partial results for online query processing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 304

LEFT text: Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in compressed database systems

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: zhiyuan chen , johannes gehrke , flip korn
",n
"LEFT id: NA
RIGHT id: 1460

LEFT text: The World Wide Web (WWW) is a fast growing global information resource. It contains an enormous amount of information and provides access to a variety of services. Since there is no central control and very few standards of information organization or service offering, searching for information and services is a widely recognized problem. To some degree this problem is solved by “search services,” also known as “indexers,” such as Lycos, AltaVista, Yahoo, and others. These sites employ search engines known as “robots” or “knowbots” that scan the network periodically and form text-based indices. These services are limited in certain important aspects. First, the structural information, namely, the organization of the document into parts pointing to each other, is usually lost. Second, one is limited by the kind of textual analysis provided by the “search service.” Third, search services are incapable of navigating “through” forms. Finally, one cannot prescribe a complex database-like search. We view the WWW as a huge database. We have designed a high-level SQL-like language called W3QL to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. We have implemented a system called W3QS to execute W3QL queries. In W3QS, query results are declaratively specified and continuously maintained as views when desired. The current architecture of W3QS provides a server that enables users to pose queries as well as integrate their own data analysis tools. The system and its query language set a framework for the development of database-like tools over the WWW. A significant contribution of this article is in formalizing the WWW and query processing over it.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: w3qs : a query system for the world-wide web

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: david konopnicki , oded shmueli
",n
"LEFT id: NA
RIGHT id: 676

LEFT text: In this paper, we present a comparison of nonparametric estimation methods for computing approximations of the selectivities of queries, in particular range queries. In contrast to previous studies, the focus of our comparison is on metric attributes with large domains which occur for example in spatial and temporal databases. We also assume that only small sample sets of the required relations are available for estimating the selectivity. In addition to the popular histogram estimators, our comparison includes so-called kernel estimation methods. Although these methods have been proven to be among the most accurate estimators known in statistics, they have not been considered for selectivity estimation of database queries, so far. We first show how to generate kernel estimators that deliver accurate approximate selectivities of queries. Thereafter, we reveal that two parameters, the number of samples and the so-called smoothing parameter, are important for the accuracy of both kernel estimators and histogram estimators. For histogram estimators, the smoothing parameter determines the number of bins (histogram classes). We first present the optimal smoothing parameter as a function of the number of samples and show how to compute approximations of the optimal parameter. Moreover, we propose a new selectivity estimator that can be viewed as an hybrid of histogram and kernel estimators. Experimental results show the performance of different estimators in practice. We found in our experiments that kernel estimators are most efficient for continuously distributed data sets, whereas for our real data sets the hybrid technique is most promising.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: selectivity estimation for spatio-temporal queries to moving objects

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong-jin choi , chin-wan chung
",n
"LEFT id: NA
RIGHT id: 396

LEFT text: The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sqlem : fast clustering in sql using the em algorithm

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: carlos ordonez , paul cereghini
",n
"LEFT id: NA
RIGHT id: 372

LEFT text: The discussions during the panel stayed largely but not entirely focused on the question of active database research issues from the application perspective. There were nine panelists. Each panelist was asked to prepare brief answers to a set of questions. The sets of answers were discussed by all participants, and finally a number of more general issues were discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advances in real-time database systems research

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: azer bestavros
",n
"LEFT id: NA
RIGHT id: 2019

LEFT text: There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a system for watermarking relational databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: rakesh agrawal , peter j. haas , jerry kiernan
",n
"LEFT id: NA
RIGHT id: 1907

LEFT text: Higher education institutions (HEIs) are increasingly addressing their environmental impact and to do this need to improve access to environmental information in order to improve decision making and sustainability efforts. Educational institutions have a different focus to other more industrial organisations and therefore frameworks for these organisations are not necessarily suited to educational environments. Whilst several environmental management information systems (EMIS) have been proposed there is a lack of understanding of the components which should be included in such a system due to the existing broad definitions. In addition there are not many best practice guidelines to support the design and development of these systems. This paper proposes a framework for EMISs in higher education. The framework is derived from several literature studies and includes guidelines which can assist with the design of an EMIS. These guidelines are classified according to the components of EMIS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving access to environmental data using context information

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: anthony tomasic , eric simon
",n
"LEFT id: NA
RIGHT id: 1556

LEFT text: A novel execution model for rule application in active databases is developed and applied to the problem of updating derived data in a database represented using a semantic, object-based database model. The execution model is based on the use of “limited ambiguity rules” (LARs), which permit disjunction in rule actions. The execution model essentially performs a breadth-first exploration of alternative extensions of a user-requested update. Given an object-based database schema, both integrity constraints and specifications of derived classes and attributes are compiled into a family of limited ambiguity rules. A theoretical analysis shows that the approach is sound: the execution model returns all valid “completions” of a user-requested update, or terminates with an appropriate error notification. The complexity of the approach in connection with derived data update is considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an execution model for limited ambiguity rules and its application to derived data update

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: i.-min a. chen , richard hull , dennis mcleod
",y
"LEFT id: NA
RIGHT id: 924

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1733

LEFT text: In this article metadata for mulimedia documents are classified in conformity with their nature, and the different kinds of metadata are brought into relation with the different purposes intended. We describe how metadata may be organized in accordance with the ISO standards SGML, which facilitates the handling of structured documents, and DFR, which supports the storage of collections of documents. Finally, we outline the impact of our observations on future developments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: metadata for mixed-media access

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: francine chen , marti hearst , julian kupiec , jan pedersen , lynn wilcox
",n
"LEFT id: NA
RIGHT id: 1802

LEFT text: The contribution of this project is threefold: (1) efficient generation for large itemsets by hash method (2) effective reduction on itemsets scan required by the division approach and (3) the option of reducing the number of database scans required Our proposed hash and division-based techniques, HD algorithm, is very efficient for the generation of candidate large itemsets where the number of candidate large itemsets generated by HD is, smaller than that by many methods such as the Apriori algorithm, DHP algorithm and DIC algorithm According to our simulation results, the proposed approach is more efficient than any existing algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: incremental distance join algorithms for spatial databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: g &#237; sli r. hjaltason , hanan samet
",n
"LEFT id: NA
RIGHT id: 285

LEFT text: The MOMIS project (Mediator envirOnment for Multiple Information Sources) developed in the past years allows the integration of data from structured and semi-structured data sources. SI-Designer (Source Integrator Designer) is a designer support tool implemented within the MOMIS project for semi-automatic integration of heterogeneous sources schemata. It is a java application where all modules involved are available as CORBA Object and interact using established IDL interfaces. The goal of this demonstration is to present a new tool: SI-Web (Source Integrator on Web), it offers the same features of SI-Designer but it has got the great advantage of being usable on Internet through a web browser.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: securing xml documents : the author-x project demonstration

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: elisa bertino , silvana castano , elena ferrari
",n
"LEFT id: NA
RIGHT id: 700

LEFT text: Existing and past generations of Prolog compilers have left deduction to run-time and this may account for the poor run-time performance of existing Prolog systems. Our work tries to minimize run-time deduction by shifting the deductive process to compile-time. In addition, we offer an alternative inferencing procedure based on translating logic to mixed integer programming. This makes available for research and implementation in deductive databases, all the theorems, algorithms, and software packages developed by the operations research community over the past 50 years. The method keeps the same query language as for disjunctive deductive databases, only the inferencing procedure changes. The language is purely declarative, independent of the order of rules in the program, and independent of the order in which literals occur in clause bodies. The technique avoids Prolog's problem of infinite looping. It saves run-time by doing primary inferencing at compile-time. Furthermore, it is incremental in nature. The first half of this article translates disjunctive clauses, integrity constraints, and database facts into Boolean equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute —least models of definite deductive databases, and —minimal models and the Generalized Closed World Assumption of disjunctive databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: implementing database operations using simd instructions

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jingren zhou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 928

LEFT text: This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the microsoft repository

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: philip a. bernstein , brian harry , paul sanders , david shutt , jason zander
",n
"LEFT id: NA
RIGHT id: 1215

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an experimental object-based sharing system for networked databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: doug fang , shahram ghandeharizadeh , dennis mcleod
",n
"LEFT id: NA
RIGHT id: 1820

LEFT text: The increasing ability to interconnect computers through internet-working, wireless networks, high-bandwidth satellite, and cable networks has spawned a new class of information-centered applications based on data dissemination. These applications employ broadcast to deliver data to very large client populations. We have proposed the Broadcast Disks paradigm [Zdon94, Acha95b] for organizing the contents of a data broadcast program and for managing client resources in response to such a program. Our previous work on Broadcast Disks focused exclusively on the “push-based” approach, where data is sent out on the broadcast channel according to a periodic schedule, in anticipation of client requests. In this paper, we study how to augment the push-only model with a “pull-based” approach of using a backchannel to allow clients to send explicit requests for data to the server. We analyze the scalability and performance of a broadcast-based system that integrates push and pull and study the impact of this integration on both the steady state and warm-up performance of clients. Our results show that a client backchannel can provide significant performance improvement in the broadcast environment, but that unconstrained use of the backchannel can result in scalability problems due to server saturation. We propose and investigate a set of three techniques that can delay the onset of saturation and thus, enhance the performance and scalability of the system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: balancing push and pull for data broadcast

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: swarup acharya , michael franklin , stanley zdonik
",y
"LEFT id: NA
RIGHT id: 971

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: heterogeneous database query optimization in db2 universal datajoiner

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shivakumar venkataraman , tian zhang
",n
"LEFT id: NA
RIGHT id: 1548

LEFT text: Abstract. Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex “data cleansing” procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide  the design of data warehouses that enable efficient lineage tracing.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data extraction and transformation for the data warehouse

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: case squire
",n
"LEFT id: NA
RIGHT id: 895

LEFT text: As the size of data warehouses increase to several hundreds of gigabytes or terabytes, the need for methods and tools that will automate the process of knowledge extraction, or guide the user to subsets of the dataset that are of particular interest, is becoming prominent. In this survey paper we explore the problem of identifying and extracting interesting knowledge from large collections of data residing in data warehouses, by using data mining techniques. Such techniques have the ability to identify patterns and build succinct models to describe the data. These models can also be used to achieve summarization and approximation. We review the associated work in the OLAP, data mining, and approximate query answering literature. We discuss the need for the traditional data mining techniques to adapt, and accommodate the specific characteristics of OLAP systems. We also examine the notion of interestingness of data, as a tool to guide the analysis process. We describe methods that have been proposed in the literature for determining what is interesting to the user and what is not, and how these approaches can be incorporated in the data mining algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: controlling data warehouses with knowledge networks

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: elvira schaefer , jan-dirk becker , andreas boehmer , matthias jarke
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: Query processing is one of the most, critical issues in Object-Oriented DBMSs. Extensible opt,imizers with efficient, search strategies require a cost model to select the most efficient execution plans. In this paper we propose and partially validate a generic cost-model for Object-Oriented DBMSs. The storage model and its access methods support clust,ered and nested collections, links, and path indexes. Queries may involve complex predicates with qualified path expressions. We propose a, method for estimating the number of block a,ccesses to clustered collections and a paramet,erized execution model for evaluating predicat,es. We estimate the costs of path expression traversals in different cases of physical clustering of the supporting collections. Thr model is validated through experiments with the 02 DBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present SABRE, a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 1898

LEFT text: Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1982

LEFT text: Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an open abstract-object storage system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stephen blott , lukas relly , hans-j &#246; rg schek
",n
"LEFT id: NA
RIGHT id: 197

LEFT text: Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: an adaptive query execution system for data integration

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: zachary g. ives , daniela florescu , marc friedman , alon levy , daniel s. weld
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: This is my first issue as associate editor of software reviews for The American Statistician. In this column, I will introduce myself, comment on the types of software reviews that can be published in this section of The American Statistician, and encourage others in the profession to consider taking on the task of reviewing statistical software packages.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1100

LEFT text: The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dynamic load balancing in hierarchical parallel database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: luc bouganim , daniela florescu , patrick valduriez
",n
"LEFT id: NA
RIGHT id: 10

LEFT text: In this paper, we demonstrate that a rich set of marketplace-specific services such as automated discovery of the needed services, comparison shopping, and negotiation can be offered to market participants by introducing a marketplace as an eCo business. For this purpose, a previously developed marketplace, namely MOPPET, is made eCo-compliant. We demonstrate that introducing MOPPET as an eCo business increases the functionality of the eCo market in the sense that several market specific services become available to the market participants.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design and implementation of rmp : a virtual electronic market place

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: susanne boll , wolfgang klas , bernard battaglin
",n
"LEFT id: NA
RIGHT id: 1597

LEFT text: The ODMG proposal has helped to focus the work on object-oriented databases (OODBs) onto a common object model and query language. Nevertheless there are several shortcomings of the current proposal stemming from the adaption of concepts of object-oriented programming and a lack of formalization. In this paper we present a formalization of the ODMG model and the OQL query language that is used in the CROQUE project as a basis for query optimization. An essential part is a complete, formally sound type system that allows us to reason about the types of intermediate query results and gives rise to fully orthogonal queries, including useful extensions of projections and set operations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: towards an effective calculus for object query languages

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: leonidas fegaras , david maier
",n
"LEFT id: NA
RIGHT id: 1223

LEFT text: BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle ""noise"" (data points that are not part of the underlying pattern) effectively.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: wavecluster : a wavelet-based clustering approach for spatial data in very large databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: gholamhosein sheikholeslami , surojit chatterjee , aidong zhang
",n
"LEFT id: NA
RIGHT id: 1384

LEFT text: Spatio-temporal data warehouses store enormous amount of data. They are usually exploited by spatiotemporal OLAP systems to extract relevant information. For extracting interesting information, the current user launches spatio-temporal OLAP (ST-OLAP) queries to navigate within a geographic data cube (Geo-cube). Very often choosing which part of the Geo-cube to navigate further, and thus designing the forthcoming ST-OLAP query, is a difficult task. So, to help the current user refine his queries after launching in the geo-cube his current query, we need a ST-OLAP queries suggestion by exploiting a Geo-cube. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: temporal queries in olap

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: alberto o. mendelzon , alejandro a. vaisman
",y
"LEFT id: NA
RIGHT id: 1898

LEFT text: Workflow management has gained increasing attention recently as an important technology to improve information system development in dynamic and distributed organizations. To develop a workflow application, selected business processes of an organization are modelled, optimized and specified as workflow schemas, using workflow languages [2]. Workflow schemas are used by workflow management systems to control the execution of workflow instances, i.e., representations of real-world business processes [3]. The first generation of workflow management systems (WFMS) were developed mainly to model and control the execution of business processes with fairly static structures, to be executed in homogeneous environments. Recently, the need for enhanced flexibility of workflow modeling and execution and the integration of applications in heterogeneous environments emerged in the workflow context [I]. The WASA project aims at supporting flexible and distributed workflows in heterogeneous environments [4]. This paper briefly overviews the conceptual design and implementation of the object-oriented workflow management system WASAZ, and sketches the proposed demo. References to work that relates to ours or that we started from are given in the cited WASA papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the mariposa distributed database management system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: jeff sidell
",n
"LEFT id: NA
RIGHT id: 1924

LEFT text: Global E-Commerce and E-Governance programs have brought into sharp focus for the need of database systems to store and manipulate data efficiently in a suite of multiple languages. While existing database systems provide some means of storing and querying multilingual data, they suffer from redundancy proportional to the number of language support. In this paper, we propose a system for multilingual data management in distributed environment that stores data in information theoretic way in encoded form with minimum redundancy. Query operation can be performed from the encoded data only and the result is obtained by decompressing it using the corresponding language dictionaries for text data or without dictionary for other data. The system has been evaluated by both syntactic data and real data obtained from a real life schema. We have compared the performance of our system with existing systems. Our system outperformed the existing systems in terms of both space and time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: on the semantics of now in databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: james clifford , curtis dyreson , tom &#225; s isakowitz , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1805

LEFT text: Arrays are a common and important class of data. At present, database systems do not provide adequate array support: arrays can neither be easily defined nor conveniently manipulated. Further, array manipulations are not optimized. This paper describes a language called the Array Manipulation Language (AML), for expressing array manipulations, and a collection of optimization techniques for AML expressions.In the AML framework for array manipulation, arbitrary externally-defined functions can be applied to arrays in a structured manner. AML can be adapted to different application domains by choosing appropriate external function definitions. This paper concentrates on arrays occurring in databases of digital images such as satellite or medical images.AML queries can be treated declaratively and subjected to rewrite optimizations. Rewriting minimizes the number of applications of potentially costly external functions required to compute a query result. AML queries can also be optimized for space. Query results are generated a piece at a time by pipelined execution plans, and the amount of memory required by a plan depends on the order in which pieces are generated. An optimizer can consider generating the pieces of the query result in a variety of orders, and can efficiently choose orders that require less space.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: similarity query processing using disk arrays

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: apostolos n. papadopoulos , yannis manolopoulos
",n
"LEFT id: NA
RIGHT id: 527

LEFT text: Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: kenneth a. ross , theodore johnson , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 1075

LEFT text: Publisher Summary  Traditional databases allow for the storage and retrieval of large amounts of data, but do not make any concessions for uncertainty in the data. In many domains, it is difficult, if not impossible, to state all information with 100% certainty. Scientific research, for example, is subject to a great deal of uncertainty and error that cannot be modeled by traditional database systems. Error-prone experimental machinery, polluted samples, and simple human error are a few of the many possible sources of this uncertainty. With the recent importance of the web, and the many textual (and HTML encoded) sources of information that it makes available, information extraction has become a hot area. The idea is to use natural language analysis tools to create structured representations of free-form text documents. This information extraction is an error-prone endeavor: even the best systems can only hope to be right part of the time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: using probabilistic information in data integration

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: daniela florescu , daphne koller , alon y. levy
",n
"LEFT id: NA
RIGHT id: 1445

LEFT text: In this paper we present a prototype system for the management of earth science data which is novel in that it takes a DBMS centric view of the the task. Our prototype -called ""BigSur"" -is shown in the context of its use by two geographically distributed scientific groups with demanding data storage and processing requirements. BigSur currently stores 1 Terabyte of data, about one thousandth of the volume EOSDIS must store. We claim that the design principles embodied in BigSur provide sufficient flexibility to achieve the difficult scientific and technical objectives of Mission to Planet Earth.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: bigsur : a system for the management of earth science data

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: paul brown , michael stonebraker
",y
"LEFT id: NA
RIGHT id: 1847

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: languages for multi-database interoperability

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: fr &#233; d &#233; ric gingras , laks v. s. lakshmanan , iyer n. subramanian , despina papoulis , nematollaah shiri
",n
"LEFT id: NA
RIGHT id: 1208

LEFT text: We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the gmap : a versatile tool for physical data independence

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: odysseas g. tsatalos , marvin h. solomon , yannis e. ioannidis
",n
"LEFT id: NA
RIGHT id: 450

LEFT text: 1 Aims and scope The Web is changing every aspect of our lives, but no area is undergoing as rapid and significant changes as the way businesses operate. Today, large and small companies are using the Web to communicate with their partners, to connect with their back-end systems, and to perform electronic commerce transactions. The next chapter of the Internet story is the evolution of today's e-business and e-commerce systems into ""e-services"", such as order procurement, on-line trading, customer relationship management, product promotion, or real-time car navigation and traffic information services. In order to make e-services available to customers, service providers need to address several issues, such as: • e-service description: which are the attributes of an e-service that should be made visible to customers or applications, and how they should be described. • e-service advertisement: how service providers can publish service description so that they can be discovered and accessed by customers and applications. • e-service discovery and selection: how customers and applications can discover and select the e-service (or the combination of e-services) that best fulfill their requirements. • e-service composition: how basic e-services (possibly offered by different companies) can be combined to form value-added, reliable services. Which architectures, models, and languages can achieve zero latency service integration and cross-organizational business process automation. • e-service delivery: how e-services are delivered to businesses and to customers • e-service monitoring and analysis: how service executions can be monitored and how service execution data can be analyzed in order to improve the service quality or efficiency • e-service contracts: how to agree on and perform legal contracts between service providers and clients electronically. • e-service ratings: how to validate service claims and evaluate the quality of the different service providers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: report on the vldb workshop on technologies for e-services ( tes )

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: fabio casati , umesh dayal , ming-chien shan
",y
"LEFT id: NA
RIGHT id: 586

LEFT text: In this paper we propose a new operator for advanced exploration of large multidimensional databases. The proposed operator can automatically generalize from a specific problem case in detailed data and return the broadest context in which the problem occurs. Such a functionality would be useful to an analyst who after observing a problem case, say a drop in sales for a product in a store, would like to find the exact scope of the problem. With existing tools he would have to manually search around the problem tuple trying to draw a pattern. This process is both tedious and imprecise. Our proposed operator can automate these manual steps and return in a single step a compact and easy-to-interpret summary of all possible maximal generalizations along various roll-up paths around the case. We present a fle xible cost-based framework that can generalize various kinds of behaviour (not simply drops) while requiring little additional customization from the user. We design an algorithm that can work efficiently on large multidimensional hierarchical data cubes so as to be usable in an interactive setting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: management of multidimensional discrete data

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: peter baumann
",n
"LEFT id: NA
RIGHT id: 644

LEFT text: A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient evaluation of queries in a mediator for websources

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vladimir zadorozhny , louiqa raschid , maria esther vidal , tolga urhan , laura bright
",n
"LEFT id: NA
RIGHT id: 1164

LEFT text: A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: parametric query optimization

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: yannis e. ioannidis , raymond t. ng , kyuseok shim , timos k. sellis
",n
"LEFT id: NA
RIGHT id: 715

LEFT text: This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xcache : a semantic caching system for xml queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: li chen , elke a. rundensteiner , song wang
",n
"LEFT id: NA
RIGHT id: 1630

LEFT text: In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: unisql/x unified relational and object-oriented database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 171

LEFT text: The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: database systems management and oracle8

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: c. gregory doherty
",n
"LEFT id: NA
RIGHT id: 650

LEFT text: Content placement algorithm: An ACDN must decide which applications to deploy where and when. Content placement is solved trivially in traditional CDNs by cache replacement algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: influential papers

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: ken ross
",n
"LEFT id: NA
RIGHT id: 1216

LEFT text: In this special issue on metadata management, we present a new work on creating, gathering, managing, and understanding metadata. The work in this issue highlights the reality that the lack of metadata and effective techniques for managing them is currently one of the biggest challenges to meaningful use and sharing of the wealth (or should we say glut) of data available today.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo atzeni , alberto o. mendelzon
",n
"LEFT id: NA
RIGHT id: 378

LEFT text: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: improving timeliness in real-time secure database systems

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sang h. son , rasikan david , bhavani thuraisingham
",n
"LEFT id: NA
RIGHT id: 589

LEFT text: Here we survey the compactness and geometric stability conjectures formulated by the participants at the 2018 IAS Emerging Topics Workshop on Scalar Curvature and Convergence. We have tried to survey all the progress towards these conjectures as well as related examples, although it is impossible to cover everything. We focus primarily on sequences of compact Riemannian manifolds with nonnegative scalar curvature and their limit spaces.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: an introduction to spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ralf hartmut g &#252; ting
",n
"LEFT id: NA
RIGHT id: 1456

LEFT text: Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a data transformation system for biological data sources

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: peter buneman , susan b. davidson , kyle hart , g. christian overton , limsoon wong
",y
"LEFT id: NA
RIGHT id: 284

LEFT text: The design of webbases, database systems for supporting Web-based applications, is currently an active area of research. In this paper, we propose a 3-year architecture for designing and implementing webbases for querying dynamic Web content(i.e., data that can only be extracted by filling out multiple forms). The lowest layer, virtual physical layer, provides navigation independence by shielding the user from the complexities associated with retrieving data from raw Web sources. Next, the traditional logical layer supports site independence. The top layer is analogous to the external schema layer in traditional databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: ominisearch : a method for searching dynamic content on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: david buttler , ling liu , calton pu , henrique paques , wei han , wei tang
",n
"LEFT id: NA
RIGHT id: 908

LEFT text: This short paper is a gentle introduction to the theory, focusing on the results most relevant for database theory. Interested readers are referred to Downey and Fellow’s monograph [6] to learn more about parameterized complexity theory. The paper is organised as follows: In Section 2 we describe two simple fixed-parameter tractable algorithms in an informal way. Section 3 presents the formal framework of parameterized complexity theory. Section 4 is a brief survey of the parameterized complexity of database query evaluation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: parameterized complexity for the database theorist

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: martin grohe
",y
"LEFT id: NA
RIGHT id: 1346

LEFT text: Despite the success of the Oracle8i Extensibility Framework to index data from diverse domains (including text, images, spatial objects, chemical compounds, molecular structures, and genomic sequences), developing an indexing scheme is perceived as a difficult task, to be embarked upon only by experts, that too, for building support for complex domains. The goal of this demonstration is to show that: 1) the task of building and integrating an indexing scheme with the Oracle8i Extensibility Framework is quite simple and 2) the applicability of the framework is not limited to complex domains. We chose to develop an indexing scheme for XML document collections, since XML is becoming widely popular. Using the Oracle8i Extensibility Framework we will demonstrate 1) the ability to define domain operators with user-defined cost and selectivity functions, 2) the ability to define domain-specific indexing schemes, 3) the ability to specify user-defined index cost and statistics collection functions, 4) the ability to optimize queries involving domain operators via userdefined optimizer functions and 5) the ability to execute queries via domain indexes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: developing an indexing scheme for xml document collection using the oracle8i extensibility framework

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: seema sundara , ying hu , timothy chorma , nipun agarwal , jagannathan srinivasan
",y
"LEFT id: NA
RIGHT id: 313

LEFT text: We propose a file structure to index high-dimensionality data, which are typically points in some feature space. The idea is to use only a few of the features, using additional features only when the additional discriminatory power is absolutely necessary. We present in detail the design of our tree structure and the associated algorithms that handle such “varying length” feature vectors. Finally, we report simulation results, comparing the proposed structure with theR*-tree, which is one of the most successful methods for low-dimensionality spaces.The results illustrate the superiority of our method, which saves up to 80% in disk accesses.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: epsilon grid order : an algorithm for the similarity join on massive high-dimensional data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: christian b &#246; hm , bernhard braunm &#252; ller , florian krebs , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 414

LEFT text: Building a data-intensive web site is a complex task. Ad hoc rapid prototyping approaches easily lead to unsatisfactory results, e.g. poor maintainability and extensibility. To address this problem, a number of model-based approaches have been proposed, which attempt to simplify the design and development of data-intensive web sites. However, these approaches typically lack expressive meta-models and, as a result, suffer from a number of limitations, e.g. the lack of appropriate support for the creation of complex user interfaces, for the specification of layouts and presentation styles, and for customization. In this paper we describe a new software tool OntoWeaver, which uses ontologies to drive the design and development of data-intensive web sites. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: homer : a model-based case tool for data-intensive web sites

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: paolo merialdo , paolo atzeni , marco magnante , giansalvatore mecca , marco pecorone
",n
"LEFT id: NA
RIGHT id: 1209

LEFT text: The authors propose a declarative Pictorial Query Language (called PQL) that is able to express queries on an Object-Oriented geographic database drawing the features which form the query. These features refer to the classic ones of a geographic environment (geo-null, geo-points, geo-polyline, and geo-region) and define the alphabet of the above mentioned language. This language, extended with respect to a previous one, considers twelve positional operators and a set of their specifications. Moreover, the possibility to use the mentioned language to query multidimensional databases is discussed. Finally, the characteristic of the mentioned language by a query example is shown.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: algebraic query optimisation for database programming languages

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 669

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Elements of Parallel Computing, by Eric Aubanel. A textbook on the underpinnings of this important and fascinating area of computer science. Review by Michele Amoretti. 3. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years. Review by William Gasarch.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book review column

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1812

LEFT text: We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-e clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions” . However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). We also present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40% to 60% of all the rules were pruned on two real-life datasets. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: scalable parallel data mining for association rules

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: eui-hong han , george karypis , vipin kumar
",n
"LEFT id: NA
RIGHT id: 479

LEFT text: Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: infosphere project : system support for information flow applications

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: calton pu , karsten schwan , jonathan walpole
",n
"LEFT id: NA
RIGHT id: 1205

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: index nesting - an efficient approach to indexing in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: beng chin ooi , jiawei han , hongjun lu , kian lee tan
",n
"LEFT id: NA
RIGHT id: 2276

LEFT text: A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: cost-driven vertical class partitioning for methods in object oriented databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: chi-wai fung , kamalakar karlapalem , qing li
",n
"LEFT id: NA
RIGHT id: 2155

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: large databases for remote sensing and gis

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: a. r. dasgupta
",n
"LEFT id: NA
RIGHT id: 567

LEFT text: Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: advanced xml data processing : guest editor 's introduction

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1512

LEFT text: New types of data processing applications are no longer satisfied with the capabilities offered by the relational data model. One example of this phenomenon is the growing use of the Internet as a source of data. The data on the Internet is inherently non-relational. As a result, demand developed for database management systems natively built on advanced data models. The semantic binary data model (Rishe, 1992), satisfies the criteria for the models required for today’s applications by providing the ability to build rich schemas with arbitrarily flexible relationships between objects. In this paper, we discuss a new design for a semantic database management system which is based on the semantic binary data model. Our challenge was to design and implement a database engine which, while being native to the model, is reasonably efficient on a wide variety of industrial applications, and which surpasses relational systems in performance and flexibility on those applications that require non-relational modelling. Special attention is given to multi-platform support by the semantic database engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semantic assumptions and query evaluation in temporal databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: claudio bettini , x. sean wang , elisa bertino , sushil jajodia
",n
"LEFT id: NA
RIGHT id: 1829

LEFT text: Disk-based database systems benefit from concurrency among transactions - usually with marginal overhead. For main-memory database systems, however, locking overhead can have a serious impact on performance. This paper proposes SP, a serial protocol for the execution of transactions in main-memory systems, and evaluates its performance against that of strict two-phase locking. The novelty of SP lies in the use of timestamps and mutexes to allow one transaction to begin before its predecessors' commit records have been written to disk, while also ensuring that no committed transactions read uncommitted data. We demonstrate seven-fold and two-fold increases in maximum throughput for read-and update-intensive workloads, respectively. At fixed loads, we demonstrate ten-fold and two-fold improvements in response time for the same transaction mixes.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: secure transaction processing in firm real-time database systems

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: binto george , jayant haritsa
",n
"LEFT id: NA
RIGHT id: 722

LEFT text: This paper describes how database systems can use and exploit a cost-effective active storage hierarchy. By active storage hierarchy we mean a database system that uses all storage media (i.e. optical, tape, and disk) to store and retrieve data and not just disk. We describe and emphasize the active part, whereby all storage types are used to store raw data that is converted to strategic business information. We describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against the historic queries. We also describe a Data Warehouse information collection, flow and central data store Hub-and-Spoke architecture, used to feed data into Data Marts. We also describe a commercial product; StorHouse/Relational Manager (RM). RM is a commercial relational database system that executes SQL queries directly against data stored on the storage hierarchy (i.e. tape, optical, disk). We conclude with a brief overview of a real world AT&T Call Detail Warehouse (CDW) case study. 1.0 Introduction Commercial Database Management Systems (DBMS) have evolved and been developed to diverse and ubiquitous range of applications. DBMS have been based on hierarchical, network, relational, objectoriented and the new emerging object/relational database model. With few exceptions these database systems and applications primarily use disk media as their storage. Hierarchical Storage Management (HSM) is used by some of these applications to exploit some of the benefits of cost-effective optical storage systems. We propose and analyze that database systems use and exploit a complete active storage hierarchy (i.e. tape, optical, and disk). The key proposal is that active data be also stored, queried and analyzed on tape farm libraries and optical jukeboxes. Figure 1 shows the cost, performance, size and reliability considerations. In this paper, we use the term active storage hierarchy when (say SQL) queries execute against data stored on diverse media.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: web caching for database applications with oracle web cache

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: jesse anton , lawrence jacobs , xiang liu , jordan parker , zheng zeng , tie zhong
",n
"LEFT id: NA
RIGHT id: 601

LEFT text: In this paper, we present a comparison of nonparametric estimation methods for computing approximations of the selectivities of queries, in particular range queries. In contrast to previous studies, the focus of our comparison is on metric attributes with large domains which occur for example in spatial and temporal databases. We also assume that only small sample sets of the required relations are available for estimating the selectivity. In addition to the popular histogram estimators, our comparison includes so-called kernel estimation methods. Although these methods have been proven to be among the most accurate estimators known in statistics, they have not been considered for selectivity estimation of database queries, so far. We first show how to generate kernel estimators that deliver accurate approximate selectivities of queries. Thereafter, we reveal that two parameters, the number of samples and the so-called smoothing parameter, are important for the accuracy of both kernel estimators and histogram estimators. For histogram estimators, the smoothing parameter determines the number of bins (histogram classes). We first present the optimal smoothing parameter as a function of the number of samples and show how to compute approximations of the optimal parameter. Moreover, we propose a new selectivity estimator that can be viewed as an hybrid of histogram and kernel estimators. Experimental results show the performance of different estimators in practice. We found in our experiments that kernel estimators are most efficient for continuously distributed data sets, whereas for our real data sets the hybrid technique is most promising.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: dynamic maintenance of data distribution for selectivity estimation

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: kyu young whang , sang wook kim , gio wiederhold
",n
"LEFT id: NA
RIGHT id: 552

LEFT text: As our dependency on hlformation systems grows, the threat of having those disrupted by cyber attacks becomes a very pressing reality. We have witnesses nmltiple occurrences of attacks in the recent past that have seriously disrupted businesses and organizations. And, unfortunately, this trend is only increasing. For some time now, some research groups have been doing research oil data mining techniques that can potentially help in meeting the challenges posed by the attacks. This special issue is an attempt to bring some of these people together and disseminate some of the results among the SIGMOD audience, and perhaps spark the interest of the community for this emerging field. In this issue we have six papers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on data mining for intrusion detection and threat analysis

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: daniel barbar &#225;
",n
"LEFT id: NA
RIGHT id: 280

LEFT text: SQL Server 7.0 offers three different styles of replication that we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means that data changes can be performed at any replica, and that the changes performed at multiple replicas are later merged together. Because Merge Replication allows updates to disconnected replicas, it is particularly well suited to applications that require a lot of autonomy. A special process called the Merge Agent propagates changes between replicas, filters data as appropriate, and detects and handles conflicts according to user-specified rules.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view and index selection tool for microsoft sql server 2000

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1807

LEFT text: During the past few years our research efforts have been inspired by two different needs. On one hand, the number of non-expert users accessing databases is growing apace. On the other, information systems will no longer be characterized by a single centralized architecture, but rather by several heterogeneous component systems. In order to address such needs we have designed a new query system with both user-oriented and multidatabase features. The system's main components are an adaptive visual interface, providing the user with different and interchangeable interaction modalities, and a “translation layer”, which creates and offers to the user the illusion of a single homogeneous schema out of several heterogeneous components. Both components are founded on a common ground, i.e. a formally defined and semantically rich data model, the Graph Model, and a minimal set of Graphical Primitives, in terms of which general query operations may be visually expressed. The Graph Model has a visual syntax, so that graphical operations can be applied on its components without unnecessary mappings, and an object-based semantics. The aim of this paper is twofold. We first present an overall view of the system architecture and then give a comprehensive description of the lower part of the system itself. In particular, we show how schemata expressed in different data models can be translated in terms of Graph Model, possibly by exploiting reverse engineering techniques. Moreover, we show how mappings can be established between well-known query languages and the Graphical Primitives. Finally, we describe in detail how queries expressed by using the Graphical Primitives can be translated in terms of relational expressions so to be processed by actual DBMSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: integration of heterogeneous databases without common domains using queries based on textual similarity

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: william w. cohen
",n
"LEFT id: NA
RIGHT id: 1159

LEFT text: he λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation. Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: exact : an extensible approach to active object-oriented databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: oscar d &#237; az , arturo jaime
",n
"LEFT id: NA
RIGHT id: 1002

LEFT text: Discovery of association rules .is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a Random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: algorithms for mining association rules for binary segmentations of huge categorical databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yasuhiko morimoto , takeshi fukuda , hirofumi matsuzawa , takeshi tokuyama , kunikazu yoda
",n
"LEFT id: NA
RIGHT id: 2240

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: relational data sharing in peer-based data management systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: beng chin ooi , yanfeng shu , kian-lee tan
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: Garbage collection is important in object-oriented databases to free the programmer from explicitly deallocating memory. In this paper, we present a garbage collection algorithm, called Transactional Cyclic Reference Counting (TCRC), for object-oriented databases. The algorithm is based on a variant of a reference-counting algorithm proposed for functional programming languages The algorithm keeps track of auxiliary reference count information to detect and collect cyclic garbage. The algorithm works correctly in the presence of concurrently running transactions, and system failures. It does not obtain any long-term locks, thereby minimizing interference with transaction processing. It uses recovery subsystem logs to detect pointer updates; thus, existing code need not be rewritten.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 1675

LEFT text: Active object oriented database management systems (AODBMS) are finding increasing application in different application domains and especially for cooperative and long duration activity management. In this paper, we propose a concurrency control mechanism for open nested transactions in an AODBMS. It exploits the semantics of the transactions to achieve controlled cooperation and concurrency among the transactions. Atomic AODBMS transactions are treated as base transactions. A complex transaction type is formed from a collection of base and complex transactions, a set of detached mode ECA rules and a state transition model. The cooperation semantics of a complex transaction type with other complex transaction types is specified by associating with each state of a complex transaction, a set of cooperating complex transaction types.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: open object database management systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley
",y
"LEFT id: NA
RIGHT id: 551

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 704

LEFT text: Multi-dimensional data is being generated at an ever increasing rate in practically all modern applications. The development of techniques and tools to extract useful information out of such data is one of critical challenges to be tackled in the 21st century. Visualization is one popular technique for achieving effective data exploration by exploiting the visual perception abilities of domain experts. Visualization involves the graphical presentation of data and information for the purposes of communicating results, verifying hypotheses, and qualitative exploration. In this demonstration, we present our solution to particular challenges we have been tackling in this area in the context of our XMDVtool project, a multi-year effort funded by NSF. These include multivariate data visualization to facilitate outlier and pattern discovery via a variety of displays, visual interaction tools, scalability of these visualization techniques to large data sets, interaction with commercial database technology and, more recently, extensions to handle data of very high-dimensionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: hd-eye : visual clustering of high dimensional data

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alexander hinneburg , daniel a. keim , markus wawryniuk
",n
"LEFT id: NA
RIGHT id: 632

LEFT text: Traditional approaches to addressing historical queries assume asingle line of time evolution; that is, a system (database, relation) evolves over time through a sequence of transactions. Each transaction always applies to the unique, current state of the system, resulting in a new current state. There are, however, complex applications where the system's state evolves intomultiple lines of evolution. In general, this creates a tree (hierarchy) of evolution lines, where each tree node represents the time evolution of a particular subsystem. Multiple lines create novel historical queries, such asvertical orhorizontal historical queries. The key characteristic of these problems is that portions of the history are shared; answering historical queries should not necessitate duplication of shared histories as this could increase the storage requirements dramatically. Both the vertical and horizontal historical queries have two parts: a “search” part, where the time of interest is located together with the appropriate subsystem, and a reconstruction part, where the subsystem's state is reconstructed for that time. This article focuses on the search part; several reconstruction methods, designed for single evolution lines can be applied once the appropriate time of interest is located. For both the vertical and the horizontal historical queries, we present algorithms that work without duplicating shared histories. Combinations of the vertical and horizontal queries are possible, and enable searching in both dimensions of the tree of evolutions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: historical queries along multiple lines of time evolution

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gad m. landau , jeanette p. schmidt , vassilis j. tsotras
",y
"LEFT id: NA
RIGHT id: 1983

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 320

LEFT text: The paper presents a systematic review of the relative efficacy of traditional listing and the USPS address list as sampling frames for national probability samples of households. NORC and ISR collaborated to compare these two national area-probability sampling frames for household surveys. We conducted this comparison in an ongoing survey operation which combines the current wave of the HRS with the first wave of NSHAP. Since 2000, survey samplers have been exploring the potential of the USPS address lists to serve as a sampling frame for probability samples from the general population. We report the relative coverage properties of the two frames, as well as predictors of the coverage and performance of the USPS frame. The research provides insight into the coverage and cost/benefit trade-offs that researchers can expect from traditionally listed frames and USPS address databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the indian institute of technology , bombay

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan
",n
"LEFT id: NA
RIGHT id: 503

LEFT text: We examine how to apply the hash-join paradigm to spatial joins, and define a new framework for spatial hash-joins. Our spatial partition functions have two components: a set of bucket extents and an assignment function, which may map a data item into multiple buckets. Furthermore, the partition functions for the two input datasets may be different.We have designed and tested a spatial hash-join method based on this framework. The partition function for the inner dataset is initialized by sampling the dataset, and evolves as data are inserted. The partition function for the outer dataset is immutable, but may replicate a data item from the outer dataset into multiple buckets. The method mirrors relational hash-joins in other aspects. Our method needs no pre-computed indices. It is therefore applicable to a wide range of spatial joins.Our experiments show that our method outperforms current spatial join algorithms based on tree matching by a wide margin.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: spatial operators

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: eliseo clementini , paolino di felice
",n
"LEFT id: NA
RIGHT id: 1995

LEFT text: Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans. We also map the ma

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: algorithms for deferred view maintenance

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: latha s. colby , timothy griffin , leonid libkin , inderpal singh mumick , howard trickey
",n
"LEFT id: NA
RIGHT id: 1609

LEFT text: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with an algorithm for their creation and an overview of incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: space optimization in deductive databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: divesh srivastava , s. sudarshan , raghu ramakrishnan , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 588

LEFT text: Persistent Application Systems (PASs) are of increasing social and economic importance. They have the potential to be long-lived, concurrently accessed, and consist of large bodies of data and programs. Typical examples of PASs are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient-care support systems in hospitals. Orthogonally persistent object systems are intended to provide improved support for the design, construction, maintenance, and operation of PASs. Persistence abstraction allows the creation and manipulation of data in a manner that is independent of its lifetime, thereby integrating the database view of information with the programming language view. This yields a number of advantages in terms of orthogonal design and programmer productivity which are beneficial for PASs. Design principles have been proposed for persistent systems. By following these principles, languages that provide persistence as a basic abstraction have been developed. In this paper, the motivation for orthogonal persistence is reviewed along with the above mentioned design principles. The concepts for integrating programming languages and databases through the persistence abstraction, and their benefits, are given. The technology to support persistence, the achievements, and future directions of persistence research are then discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on spatial database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: h. j. schek
",n
"LEFT id: NA
RIGHT id: 1318

LEFT text: A prototyped data mining system, DBLearn, has been developed, which efficiently and effectively extracts different kinds of knowledge rules from relational databases. It has the following features: high level learning interfaces, tightly integrated with commercial relational database systems, automatic refinement of concept hierarchies, efficient discovery algorithms and good performance. Substantial extensions of its knowledge discovery power towards knowledge mining in object-oriented, deductive and spatial databases are under research and development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: vxmlr : a visual xml-relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: aoying zhou , hongjun lu , shihui zheng , yuqi liang , long zhang , wenyun ji , zengping tian
",n
"LEFT id: NA
RIGHT id: 2025

LEFT text: Beginning in 1989 an ad-hoc collection of senior DBMS researchers has gathered periodically to perform a "" group grope "" ; i.e. an assessment of the state of the art in DBMS research as well as a prediction concerning what problems and problem areas deserve additional focus. The fifth ad-hoc meeting was held May 4-6, 2003 in Lowell, Ma. A report on the meeting is in preparation, and this panel discussion will summarize the upcoming document and discuss its conclusions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the lowell report

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: jim gray , hans schek , michael stonebraker , jeff ullman
",y
"LEFT id: NA
RIGHT id: 547

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 1787

LEFT text: Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query unnesting in object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: leonidas fegaras
",n
"LEFT id: NA
RIGHT id: 2259

LEFT text: As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: join index hierarchies for supporting efficient navigations in object-oriented databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: zhaohui xie , jiawei han
",n
"LEFT id: NA
RIGHT id: 1785

LEFT text: We consider the problem of substring searching in large databases. Typical applications of this problem are genetic data, web data, and event sequences. Since the size of such databases grows exponentially, it becomes impractical to use inmemory algorithms for these problems. In this paper, we propose to map the substrings of the data into an integer space with the help of wavelet coefficients. Later, we index these coefficients using MBRs (Minimum Bounding Rectangles). We define a distance function which is a lower bound to the actual edit distance between strings. We experiment with both nearest neighbor queries and range queries. The results show that our technique prunes significant amount of the database (typically 50-95%), thus reducing both the disk I/O cost and the CPU cost significantly .

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 1872

LEFT text: Several alternatives to manage large XML document collections exist, ranging from file systems over relational or other database systems to specifically tailored XML base management systems. In this paper we give a tour of Natix, a database management system designed from scratch for storing and processing XML data. Contrary to the common belief that management of XML data is just another application for traditional databases like relational systems, we illustrate how almost every component in a database system is affected in terms of adequacy and performance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the active database management system manifesto : a rulebase of adbms features

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: corporate act-net consortium
",n
"LEFT id: NA
RIGHT id: 1665

LEFT text: We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits “horizontal” aggregation and even aggregation over more general “blocks” of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a language based multidatabase system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eva k &#252; hn , thomas tschernko , konrad schwarz
",n
"LEFT id: NA
RIGHT id: 1387

LEFT text: Analysts and decision-makers use what-if analysis to assess the e®ects of hypotheti- cal scenarios. What-if analysis is currently supported by spreadsheets and ad-hoc O L AP tools. Unfortunately, the former lack seam- less integration with the data and the lat- ter lack °exibility and performance appropri- ate for O L AP applications. To tackle these problems we developed the Sesamesystem, which models an hypothetical scenario as a list of hypothetical modications on the ware- house views and fact data. We provide formal scenario syntax and semantics, which extend view update semantics for accomodating the special requirements of O L AP. We focus on query algebra operators suitable for perform- ing spreadsheet-style computations. Then we present Sesame's optimizer and its corner- stone substitution and rewriting mechanisms. Substitution enables lazy evaluation of the hy- pothetical updates. The substitution module delivers orders-of-magnitude optimizations in cooperation withtherewriterthatusesknowl- edge of arithmetic, relational, ¯nancial and other operators. Finally we discuss the chal- lenges that the size of the scenario specica- tionsandthe arbitrarynatureof theoperators pose to the rewriter.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: hypothetical queries in an olap environment

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: andrey balmin , thanos papadimitriou , yannis papakonstantinou
",y
"LEFT id: NA
RIGHT id: 530

LEFT text: Amongst the wide range of parking solutions that can contribute to reduce parking problems or regulate parking activities, e Parking looks at developing and applying an innovative e-business application for parking space optimization. The purpose of this paper is to present the innovative e-business platform that has been deve loped, from a technical point of view, by the University of Zurich. The ideas are coming from a transcross European consortium within the framework of the IST Information Society Technologies of the 5th framework program. E-Parking provides a database-centered Web application solution based on our proposed conceptual model CIA (Channel, Integration, Application) for Web applications. The WAP, WEB and Bluetooth communication channels enable drivers to obtain early information on available parking space, make a reservation, access the reserved place and pay for the service booked. In reaching this goal, the innovative solutions seek to benefit all social segments, to optimize existing parking resources, and to contribute to achieving a more sustainable urban transport, reducing congestion and pollution.t

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database principles

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: leonid libkin
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",y
"LEFT id: NA
RIGHT id: 306

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a robust , optimization-based approach for approximate answering of aggregate queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: surajit chaudhuri , gautam das , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 1137

LEFT text: This paper covers what we at NCR have learned about the TPC-D benchmark as we executed and published our first set of volume points for the Teradata Database. Areas where customers should read the Full Disclosure Report carefully are pointed out as well as the weaknesses in the benchmark relative to real customer applications. The key execution and optimization elements of the Teradata Database and the 5100 WorldMark platform that contribute to our published results are discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: tpc-d : the challenges , issues and results

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: ramesh bhashyam
",y
"LEFT id: NA
RIGHT id: 815

LEFT text: Abstract Many modern programming languages support basic generics, sufficient to implement type-safe polymorphic containers. Some languages have moved beyond this basic support, and in doing so have enabled a broader, more powerful form of generic programming. This paper reports on a comprehensive comparison of facilities for generic programming in eight programming languages: C++, Standard ML, Objective Caml, Haskell, Eiffel, Java, C# (with its proposed generics extension), and Cecil. By implementing a substantial example in each of these languages, we illustrate how the basic roles of generic programming can be represented in each language. We also identify eight language properties that support this broader view of generic programming: support for multi-type concepts, multiple constraints on type parameters, convenient associated type access, constraints on associated types, retroactive modeling, type aliases, separate compilation of algorithms and data structures, and implicit argument type deduction for generic algorithms. We find that these features are necessary to avoid awkward designs, poor maintainability, and painfully verbose code. As languages increasingly support generics, it is important that language designers understand the features necessary to enable the effective use of generics and that their absence can cause difficulties for programmers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: pm3 : an orthogonal persistent systems programming language - design , implementation , performance

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: antony l. hosking , jiawan chen
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",y
"LEFT id: NA
RIGHT id: 1507

LEFT text: Data warehouses store materialized views over base data from external sources. Clients typically perform complex read-only queries on the views. The views are refreshed periodically by maintenance transactions, which propagate large batch updates from the base tables. In current warehousing systems, maintenance transactions usually are isolated from client read activity, limiting availability and/or size of the warehouse. We describe an algorithm called 2VNL that allows warehouse maintenance transactions to run concurrently with readers. By logically maintaining two versions of the database, no locking is required and serializability is guaranteed. We present our algorithm, explain its relationship to other multi-version concurrency control algorithms, and describe how it can be implemented on top of a conventional relational DBMS using a query rewrite approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: view maintenance in a warehousing environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: yue zhuge , h &#233; ctor garc &#237; a-molina , joachim hammer , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1374

LEFT text: This chapter reveals that the loader and compressor convert XML documents in a compressed, yet queryable format. The compressed repository stores the compressed documents and provides: access methods to this compressed data, and a set of compression-specific utilities that enable, e.g., the comparison of two compressed values. The query processor optimizes and evaluates XQuery queries over the compressed documents. Its complete set of physical operators allows for efficient evaluation over the compressed repository. The chapter motivates the choice of the storage structures for compressed XML, and of the compression algorithms employed. It describes the XQueC query processor, its set of physical operators, and outlines its optimization algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: optimizing queries on compressed bitmaps

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sihem amer-yahia , theodore johnson
",n
"LEFT id: NA
RIGHT id: 1649

LEFT text: This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: formal query languages for secure relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: marianne winslett , kenneth smith , xiaolei qian
",n
"LEFT id: NA
RIGHT id: 1340

LEFT text: The experimental results show that distributed commit processing can have considerably more influence than distributed data processing on the throughput performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best transaction throughput performance for a variety of workloads and system configurations. In fact, OPT's peak throughput is often close to the upper bound on achievable performance. Even more interestingly, a three-phase (i.e., non-blocking) version of OPT provides better peak throughput performance than all of the standard two-phase (i.e., blocking protocols evaluated in our study

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: smooth - a distributed multimedia database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: harald kosch , l &#225; szl &#243; b &#246; sz &#246; rm &#233; nyi , alexander bachlechner , christian hanin , christian hofbauer , margit lang , carmen riedler , roland tusch
",n
"LEFT id: NA
RIGHT id: 1785

LEFT text: Many applications require the management of spatial data. Clustering large spatial databases is an important problem which tries to find the densely populated regions in the feature space to be used in data mining, knowledge discovery, or efficient information retrieval. A good clustering approach should be efficient and detect clusters of arbitrary shape. It must be insensitive to the outliers (noise) and the order of input data. We pro-pose WaveCluster, a novel clustering approach based on wavelet transforms, which satisfies all the above requirements. Using multi-resolution property of wavelet transforms, we can effectively identify arbitrary shape clus-ters at different degrees of accuracy. We also demonstrate that WaveCluster is highly effi-cient in terms of time complexity. Experi-mental results on very large data sets are pre-sented which show the efficiency and effective-ness of the proposed approach compared to the other recent clustering methods

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cure : an efficient clustering algorithm for large databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: sudipto guha , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 2269

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementing lazy database updates for an object database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: fabrizio ferrandina , thorsten meyer , roberto zicari
",n
"LEFT id: NA
RIGHT id: 1327

LEFT text: Workflow management systems (WfMSs) are software platforms that allow the definition, execution, monitoring, and management of business processes. WfMSs log every event that occurs during process execution. Therefore, workflow logs include a significant amount of information that can be used to analyze process executions, understand the causes of high - and low-quality process executions, and rate the performance of interna l resources and business partners. In this paper we present a packaged data warehousing solution, coupled with HP Process Manager, for collecting and analyzing workflow execution data. We first present the main challenges involved in this effort, and then detail the proposed approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: warehousing workflow data : challenges and opportunities

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: angela bonifati , fabio casati , umeshwar dayal , ming-chien shan
",y
"LEFT id: NA
RIGHT id: 1814

LEFT text: Since multimedia retrieval is based on similarity calculations of semantics and media-based search, exact matches are not expected. We view querying multimedia database as a combination of IR, image matching, and traditional database query processing and it should be conducted in a way of perpetual query reformulation for honing target results. In this paper we present a hybrid multimedia database system, which employs a hierarchical database statistics structure for both query optimization and reformulation analysis without adding additional query processing cost.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: devise : integrated querying and visual exploration of large datasets

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: m. livny , r. ramakrishnan , k. beyer , g. chen , d. donjerkovic , s. lawande , j. myllymaki , k. wenger
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1557

LEFT text: Although “now<” is expressed in SQL and CURRENT_TIMESTAMP within queries, this value cannot be stored in the database. How ever, this notion of an ever-increasing current-time value has been reflected in some temporal data models by inclusion of database-resident variables, such as “now<” “until-changed,< ” “**,” “@,” and “-”. Time variables are very desirable, but their used also leads to a new type of database, consisting of tuples with variables, termed a variable database.<

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: a structured approach for the definition of the semantics of active databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: piero fraternali , letizia tanca
",n
"LEFT id: NA
RIGHT id: 540

LEFT text: We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the cougar approach to in-network query processing in sensor networks

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yong yao , johannes gehrke
",n
"LEFT id: NA
RIGHT id: 343

LEFT text: Unfortunately, there is very little money to support the health promotion initiatives in this plan. Health promotion receives very little of the $17 billion NIH research budget and few health promotion procedures are covered by the $400+ billion spent annually for Medicare and Medicaid. The Office of Health Promotion and Disease Prevention has a budget so small that very few health promotion professionals ever encounter this office directly during their careers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization. >

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",n
"LEFT id: NA
RIGHT id: 1622

LEFT text: Research and development of multidatabase systems was triggered by the need to integrate data from heterogeneous and physically distributed information sources Issues on multidatabase architectures and semantic heterogeneity have been explored. This paper presents the multidatabase systems done and currently being done at De La Salle University. These research projects focus on multidatabase architectures, and on identifying methods in resolving various forms of schema conflicts.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: parallel database systems in the 1990 's

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: michael j. carey
",n
"LEFT id: NA
RIGHT id: 2143

LEFT text: Reusable ontologies are becoming increasingly important for tasks such as information integration, knowledge-level interoperation and knowledge-base development. We have developed a set of tools and services to support the process of achieving consensus on commonly shared ontologies by geographically distributed groups. These tools make use of the World Wide Web to enable wide access and provide users with the ability to publish, browse, create and edit ontologies stored on anontology server. Users can quickly assemble a new ontology from a library of modules. We discuss how our system was constructed, how it exploits existing protocols and browsing tools, and our experience supporting hundreds of users. We describe applications using our tools to achieve consensus on ontologies and to integrate information.The Ontolingua Server may be accessed through the URLhttp://ontolingua.stanford.edu

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: mapinfo spatialware : a spatial information server for rdbms

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chebel mina
",n
"LEFT id: NA
RIGHT id: 867

LEFT text: Publish/Subscribe is the paradigm in which users express long-term interests (“subscriptions”) and some agent “publishes” events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: publish/subscribe on the web at extreme speed

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: jo &#227; o pereira , fran &#231; oise fabret , fran &#231; ois llirbat , radu preotiuc-pietro , kenneth a. ross , dennis shasha
",y
"LEFT id: NA
RIGHT id: 7

LEFT text: Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the jungle database search engine

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: michael b &#246; hlen , linas bukauskas , curtis dyreson
",y
"LEFT id: NA
RIGHT id: 860

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: toward learning based web query processing

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: yanlei diao , hongjun lu , songting chen , zengping tian
",n
"LEFT id: NA
RIGHT id: 280

LEFT text: This dissertation describes techniques for speeding up Online Analytical Processing or OLAP queries. OLAP systems allow users to quickly obtain the answers to complex business queries. Quickly answering these queries which aggregate large amounts of data, calls for various specialized techniques. One technique used by OLAP systems to speed up multidimensional data analysis is to precompute aggregates on some subsets of dimensions and their corresponding hierarchies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: materialized view and index selection tool for microsoft sql server 2000

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sanjay agrawal , surajit chaudhuri , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 228

LEFT text: The Microsoft Repository is an object-oriented repository that ships as a component of Visual Basic (Version 5.0). It includes a set of ActiveX interfaces that a developer can use to define information models, and a repository engine that is the underlying storage mechanism for these information models. The repository engine sits on top of a SQL database system. The repository is designed to meet the persistent storage needs of software tools. Its two main technical goals are: . compatibility with Microsoft’s existing ActiveX object architecture consisting of the Component Object Model (COM) and Automation and that a developer can use to define information models, and a repository engine that is the underlying storage mechanism for these information ‘models. (Znformurion model is repository terminology for database schema [3].) The repository engine sits on top of either Microsoft SQL Server or Microsoft Jet (the database system in Microsoft Access) and supports both navigational access via the object-oriented interfaces and direct SQL access to the underlying store. In addition, the Repository includes a set of information models that cover the data sharing needs of software tools. . extensibility by customers and independent software vendors who need to tailor the repository by adding functionality to objects stored by the repository engine and extending information models provided by Microsoft and others. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the microsoft database research group

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: david lomet , roger barga , surajit chaudhuri , paul larson , vivek narasayya
",n
"LEFT id: NA
RIGHT id: 583

LEFT text: When building a database, it is mandatory to design a friendly interface, which allo ws the nal user to easily access the data of interest. V ery often,such an interface exploits the pow er of visualization and direct manipulation mechanisms. How ever, it is not suÆcient to associate \any"" visual represen tation to a database, but the visual representation should be carefully chosen to e ectively con vey all and only the database information content. We are curren tly w orkingon a general theory (see ) for establishing the adequacy of a visual representation, once speci ed the database characteristics, and we are developing a system, called D ARE: Drawing Adequate REpresentations, which implements such a theory.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on prototypes of deductive database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: k. ramamohanarao
",n
"LEFT id: NA
RIGHT id: 1307

LEFT text: The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: observations on the odmg-93 proposal for an object-oriented database language

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: won kim
",n
"LEFT id: NA
RIGHT id: 347

LEFT text: E-commerce is not a static field, but is constantly evolving to discover new and more effective ways of supporting businesses. Data management is an integral part of this effort, This special issue aims to report on some of the recent developments and identify some research directions in this area. Initially, e-commeree involved the use of ED] and intranets. Today we see the dominance of XML. Almost all recent elecn'onic commerce standards are based on X1VD... As a consequence, the amount of XML data being stored is large, and it is increasing. This naturally leads to the question of how to store and query the XML documents. The paper by Tian, DeWitt, Chen and Zhang describes the design and performance evaluation of alternative XM]., storage strategies. The results of this performance study provide valuable hints on how to store the XM1., files depending on the application. Personalization in e-commerce is about building customer loyalty by understanding and thus addressing the needs of each individual. E-commerce systems need customers' profiles to provide better services, 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: asuman dogac
",n
"LEFT id: NA
RIGHT id: 1535

LEFT text: Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the naos system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: c. collet , t. coupaye
",n
"LEFT id: NA
RIGHT id: 1463

LEFT text: When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: accessing a relational database through an object-oriented database interface

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: jack a. orenstein , d. n. kamber
",y
"LEFT id: NA
RIGHT id: 1400

LEFT text: However, in the context of in-memory searches, we find that the traditional notion of considering good trajectory splits by minimizing the volume of MBBs so as to reduce index overlap is not well-suited to improve the performance of in-memory distance threshold searches. Another finding is that computing good trajectory splits to achieve a trade-off between the time to search the index-tree and the time to process the candidate set of trajectory segments may not be beneficial when considering large datasets. The GPU is an attractive technology for distance threshold searches because of the inherent data parallelism involved in calculating moving distances between pairs of polylines; however, challenges arise from the SIMD programming model and limited GPU memory. We study the processing of distance threshold searches using GPU implementations that avoid the use of index-trees.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: novel approaches in query processing for moving object trajectories

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dieter pfoser , christian s. jensen , yannis theodoridis
",y
"LEFT id: NA
RIGHT id: 730

LEFT text: A materialized view or Materialized Query Table (MQT) is an auxiliary table with precomputed data that can be used to significantly improve the performance of a database query. A Materialized Query Table Advisor (MQTA) is often used to recommend and create MQTs. The state-of-the-art MQTA works in a standalone database server where MQTs are placed on the same server as that in which the base tables are located. The MQTA does not apply to a federated or scaleout scenario in which MQTs need to be placed on other servers close to applications (i.e. a frontend database server) for offloading the workload on the backend database server. In this paper, we propose a Data Placement Advisor (DPA) and load balancing strategies for multi-tiered database systems. Built on top of the MQTA, DPA recommends MQTs and advises placement strategies for minimizing the response time for a query workload. To demonstrate the benefit of the data placement advising, we implemented a prototype of DPA that works with the MQTA in the IBM^(R) DB2^(R) Universal Database(TM) (DB2 UDB) and the IBM WebSphere^(R) Information Integrator (WebSphere II). The evaluation results showed substantial improvements of workload response times when MQTs are intelligently recommended and placed on a frontend database server subject to space and load characteristics for TPC-H and OLAP type workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: garlic : a new flavor of federated query processing for db2

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vanja josifovski , peter schwarz , laura haas , eileen lin
",y
"LEFT id: NA
RIGHT id: 1919

LEFT text: Remote sensing provides the basic data to undertake inventory of land, as well as the temporal information required to monitor sustainable land management practices. In this paper, the current use of remote sensing for sustainable land management is reviewed, and the potential of future (new) satellite systems to contribute to sustainable development is explored. Other elements for successful sustainable development (ie, good policy and participatory approaches) are then compared and contrasted with information requirements. Sustainable land management refers to the activities of humans and implies that activity will continue in perpetuity. It is a term which attempts to balance the often conflicting ideals of economic growth and maintaining environmental quality and viability. Economic activities may range from intensive agriculture to the management of natural areas. It is argued that in order to effectively “manage” resources, three elements must be present. These are information about natural resources, clear policies on how the resource may be managed (eg, Acts of Government, policy papers, administrative procedures), and participation of everyone (including local people) with an interest or “stake” in the land. In this paper, we concentrate on methods to generate information about the resources, with an emphasis on how recent innovations in remote sensing fit with sustainable land management methods. In particular, we assess how resources may be inventoried by remote sensing, and techniques and data which may ascertain whether the activity is indeed sustainable. A concluding section discusses how the information (generated from remote sensing) is linked to policy and local participation. Thus, three specific questions are addressed. First, what cover is present? This question requires that remote sensing provides information on land cover as well as terrain attributes such as slope, aspect and terrain position. The second question addresses whether the use (management) of the cover is sustainable. This question requires temporal data collection to monitor whether the environment is degrading or otherwise changing. The third question is: How can remote sensing and GIS contribute to the policy tools of generating policy, providing information and ensuring participation by all stakeholders?

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: open gis and on-line environmental libraries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: kenn gardels
",y
"LEFT id: NA
RIGHT id: 777

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: evaluating functional joins along nested reference sets in object-relational and object-oriented databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: reinhard braumandl , jens clau &#223; en , alfons kemper
",n
"LEFT id: NA
RIGHT id: 256

LEFT text: This article investigates the minimization problem for a wide fragment of XPath (namely X P[✶]), where the use of the most common operators (child, descendant, wildcard and branching) is allowed with some syntactic restrictions. The examined fragment consists of expressions which have not been specifically studied in the relational setting before: neither are they mere conjunctive queries (as the combination of “//” and “*” enables an implicit form of disjunction to be expressed) nor do they coincide with disjunctive ones (as the latter are more expressive). Three main contributions are provided. The “global minimality” property is shown to hold: the minimization of a given XPath expression can be accomplished by removing pieces of the expression, without having to re-formulate it (as for “general” disjunctive queries). Then, the complexity of the minimization problem is characterized, showing that it is the same as the containment problem. Finally, specific forms of XPath expressions are identified, which can be minimized in polynomial time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: minimization of tree pattern queries

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sihem amer-yahia , sungran cho , laks v. s. lakshmanan , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1521

LEFT text: Component-based approaches are becoming more and more popular to support Internet-based application development. Different component modeling approaches, however, can be adopted, obtaining different abstraction levels (either conceptual or operational). In this paper we present a component-based architecture for the design of e-applications, and discuss the concept of wrapper components as building blocks for the development of e-services, where these services are based on legacy systems. We discuss their characteristics and their applicability in Internet-based application development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: semint : a system prototype for semantic integration in heterogeneous databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: wen-syan li , chris clifton
",n
"LEFT id: NA
RIGHT id: 320

LEFT text: The Indian Institute of Technology, Bombay is one of the leading universities in India. Located in Powai, a suburb of the vibrant city of Bombay (which is soon to revert to its original name, Mumbai), it is a scenic campus extending over 500 acres on the shores of Lake Powai. The institute has a faculty strength of about 400, and has about 2500 students. The Department of Computer Science has a faculty strength of 25, and around 150 undergraduate and 70 postgraduate students. The Database Group in the Department of Computer Science and Engineering is the largest database group in India. The group currently has four faculty members, D. B. Phatak, N. L. Sarda, S. Seshadri and S. Sudarshan. The group also currently has three research scholars, ten Masters students, ten undergraduate students and nine project engineers.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at the indian institute of technology , bombay

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: d. b. phatak , n. l. sarda , s. seshadri , s. sudarshan
",y
"LEFT id: NA
RIGHT id: 2101

LEFT text: The paper shows how modern architectures can be used to speed up ranking algorithms. In the paper “On optimality-ratio and coverage in ranking of joined search results”, the authors study a novel ranking problem. Instead of ranking individual items, they consider ranking of combinations of items, e.g., a combination of a hotel and two restaurants. They study the semantics and query processing algorithms in this context. The paper shows the kind of new ranking problems that emerge in these new-age applications. The paper titled “Distributed top-k query processing by exploiting skyline summaries” studies top-k processing in distributed environments. With increasing volumes of data, the data is typically distributed over multiple servers. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: amr el abbadi , gunter schlageter , kyu-young whang
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: This paper presents an approach to the development of a practical deductive objectoriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally defined data model has been used as the starting point for the development of the two languages. This has enabled a seamless integration of the two languages, which is the central theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are presented, one of which has been chosen for implementation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 2111

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: user-cognizant multidimensional analysis

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 646

LEFT text: We present the design and implementation of the XSQ system for querying streaming XML data using XPath 1.0. Using a clean design based on a hierarchical arrangement of pushdown transducers augmented with buffers, XSQ supports features such as multiple predicates, closures, and aggregation. XSQ not only provides high throughput, but is also memory efficient: It buffers only data that must be buffered by any streaming XPath processor. We also present an empirical study of the performance characteristics of XPath features, as embodied by XSQ and several other systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: processing complex aggregate queries over data streams

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alin dobra , minos garofalakis , johannes gehrke , rajeev rastogi
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases, that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 2246

LEFT text: In this paper, we consider the filter step of the spatial join problem, for the case where neither of the inputs are indexed. We present a new algorithm, Scalable Sweeping-Based Spatial Join (SSSJ), that achieves both efficiency on real-life data and robustness against highly skewed and worst-case data sets. The algorithm combines a method with theoretically optimal bounds on I/O transfers based on the recently proposed distribution-sweeping technique with a highly optimized implementation of internal-memory plane-sweeping. We present experimental results based on an efficient implementation of the SSSJ algorithm, and compare it to the state-ofthe-art Partition-Based Spatial-Merge (PBSM) algorithm of Patel and DeWitt.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: iterative spatial join

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: edwin h. jacox , hanan samet
",n
"LEFT id: NA
RIGHT id: 1665

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: a language based multidatabase system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: eva k &#252; hn , thomas tschernko , konrad schwarz
",n
"LEFT id: NA
RIGHT id: 1679

LEFT text: A prototyped data mining system, DBLearn, has been developed, which efficiently and effectively extracts different kinds of knowledge rules from relational databases. It has the following features: high level learning interfaces, tightly integrated with commercial relational database systems, automatic refinement of concept hierarchies, efficient discovery algorithms and good performance. Substantial extensions of its knowledge discovery power towards knowledge mining in object-oriented, deductive and spatial databases are under research and development.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dblearn : a system prototype for knowledge discovery in relational databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jiawei han , yongjian fu , yue huang , yandong cai , nick cercone
",y
"LEFT id: NA
RIGHT id: 342

LEFT text: As teachers, if we believe content knowledge matters the most for successful instruction, we may not understand that getting to know students and discovering their strengths as learners are equally important. Teachers are instructional islands with a lot of content to share but perhaps unconnected to the learners that make up the classroom. If as teachers, we describe students by saying, “she is a math wizard,” “she is a science ace,” or “he is a sponge for historical facts,” we can communicate a lot about students with minimal language. These metaphors help us make comparisons that evoke multiple layers of meaning, and yet thinking metaphorically is also an aspect of everyday life. Cognitive scientists Lakoff and Johnson (2008) argued that our conceptualizations of the world around us are metaphorical and provided examples of metaphors such as “time is money” and suggested that the way we construe argument is conceived in metaphors of war when we “attack a position,” for example, to support a philosophical claim. From an educational philosophy perspective, Greene contended learning is a landscape (1973) and teachers are philosophers working to help learners resist the forces that limit and oppress them (1988) to attain freedom to think for themselves. These theorists recognized the epistemological power of metaphor and challenged us to see its educational potential. Comparisons through metaphoric thinking afford different perspectives and open imaginative possibilities, challenging us to see familiar relationships in new ways. Metaphors can push us to think about teacher education differently as well and move beyond familiar views of clinical experiences, teacher interns, teacher preparation programs, and who we are as educators to see these concepts more complexly. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1637

LEFT text: In this paper, we study how to find such optimal schedules. In particular, we consider two optimization criteria: (i) one based on maximizing the number of piggybacked clips, and (ii) the other based on maximizing the impact on buffer space. We show that the optimal schedule under the first criterion is equivalent to a maximum matching in a suitably defined bipartite graph, and that under the second criterion, the optimal schedule is equivalent to a maximum matching in a suitably defined weighted bipartite graph.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimizing queries on files

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mariano p. consens , tova milo
",n
"LEFT id: NA
RIGHT id: 368

LEFT text: The tutorial surveys state-of-the-art methods for storing and retrieving multimedia data from large databases. Records (= documents) may consist of formatted fields, text, images, voice, animation etc. .4 sample query that we would like to support is ‘in a collection of 2-d color images, find images that are similar to a sunset photograph’. Indexing for images and other media is a new, active area of research; the tutorial will present recent approaches and prototype systems, for 2-d and 3-d medical image databases, 2-d color image databases, and l-d time series databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: standard for multimedia databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: john r. smith
",n
"LEFT id: NA
RIGHT id: 583

LEFT text: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: special issue on prototypes of deductive database systems

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: k. ramamohanarao
",n
"LEFT id: NA
RIGHT id: 56

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data management issues in electronic commerce

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: m. tamer &#214; zsu
",n
"LEFT id: NA
RIGHT id: 1165

LEFT text: A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: on applying hash filters to improving the execution of multi-join queries

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: ming-syan chen , hui-i hsiao , philip s. yu
",n
"LEFT id: NA
RIGHT id: 2051

LEFT text: We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the design of an acquisitional query processor for sensor networks

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: samuel madden , michael j. franklin , joseph m. hellerstein , wei hong
",y
"LEFT id: NA
RIGHT id: 1866

LEFT text: Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: strudel : a web site management system

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: mary fernandez , daniela florescu , jaewoo kang , alon levy , dan suciu
",n
"LEFT id: NA
RIGHT id: 576

LEFT text: This paper presents an algorithm, called ARIES/CSA (Algorithm for Recovery and Isolation Exploiting Semantics for Client-Server Architectures), for performing recovery correctly in client-server (CS) architectures. In CS, the server manages the disk version of the database. The clients, after obtaining database pages from the server, cache them in their buffer pools. Clients perform their updates on the cached pages and produce log records. The log records are buffered locally in virtual storage and later sent to the single log at the server. ARIES/CSA supports a write-ahead logging (WAL), fine-granularity (e.g., record) locking, partial rollbacks and flexible buffer management policies like steal and no-force. It does not require   that the clocks on the clients and the server be synchronized. Checkpointing by the server and the clients allows for flexible and easier recovery.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 774

LEFT text: The key issue in performing spatial joins is finding the pairs of intersecting rectangles. For unindexed data sets, this is usually resolved by partitioning the data and then performing a plane sweep on the individual partitions. The resulting join can be viewed as a two-step process where the partition corresponds to a hash-based join while the plane-sweep corresponds to a sort-merge join. In this article, we look at extending the idea of the sort-merge join for one-dimensional data to multiple dimensions and introduce the Iterative Spatial Join. As with the sort-merge join, the Iterative Spatial Join is best suited to cases where the data is already sorted. However, as we show in the experiments, the Iterative Spatial Join performs well when internal memory is limited, compared to the partitioning methods. This suggests that the Iterative Spatial Join would be useful for very large data sets or in situations where internal memory is a shared resource and is therefore limited, such as with today's database engines which share internal memory amongst several queries. Furthermore, the performance of the Iterative Spatial Join is predictable and has no parameters which need to be tuned, unlike other algorithms. The Iterative Spatial Join is based on a plane sweep algorithm, which requires the entire data set to fit in internal memory. When internal memory overflows, the Iterative Spatial Join simply makes additional passes on the data, thereby exhibiting only a gradual performance degradation. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: scalable sweeping-based spatial join

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: lars arge , octavian procopiuc , sridhar ramaswamy , torsten suel , jeffrey scott vitter
",n
"LEFT id: NA
RIGHT id: 1778

LEFT text: Specifically, we use state-of-the-art concepts from morphology, n;,mely the ‘pattern spectrum’ of a shape, to map each shape to a point in n-dimensional space. FollowingThis text is a guide to the foundations of method engineering, a developing field concerned with the definition of techniques for designing software systems. The approach is based on metamodeling, the construction of a model about a collection of other models. The book applies the metamodeling approach in five case studies, each describing a solution to a problem in a specific domain. Suitable for classroom use, the book is also useful as a reference for practitioners. The book first presents the theoretical basis of metamodeling for method engineering, discussing information modeling, the potential of metamodeling for software systems development, and the introduction of the metamodeling tool ConceptBase. , we organize the n-d points in an R-tree. We show that the L, (= max) norm in the n-d space lower-bounds the actual distance. This guarantees no false dismissals for range queries. In addition, we present a nearest neighbor algorithm that also guarantees no false dismissals.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: optimal multi-step k-nearest neighbor search

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: thomas seidl , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 1740

LEFT text: Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: sigmod challenges paper : database issues in telecommunications network management

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: ilsoo ahn
",n
"LEFT id: NA
RIGHT id: 1202

LEFT text: We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called  cache completeness . A separate issue,  cache currency , deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: a predicate-based caching scheme for client-server database architectures

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: arthur m. keller , julie basu
",y
"LEFT id: NA
RIGHT id: 1347

LEFT text: Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: update propagation strategies for improving the quality of data on the web

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: alexandros labrinidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1771

LEFT text: In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: mining fuzzy association rules in databases

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: chan man kuok , ada fu , man hon wong
",n
"LEFT id: NA
RIGHT id: 365

LEFT text: Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: online query processing : a tutorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: peter j. haas , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 594

LEFT text: One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: describing semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luca cardelli
",n
"LEFT id: NA
RIGHT id: 2239

LEFT text: Together with techniques developed for relational databases, this basis in logic means that deductive databases are capable of handling large amounts of information as well as performing reasoning based on that information. There are many application areas for deductive database technology. One area is that of decision support systems. In particular, the exploitation of an organization's resources requires fi~tbniy sufficient information about the current and future status of the resources themselves, but also a way of reasoning effectively about plans for the future. The present generation of decision support systems are severely deficient when it comes to reasoning about future plans. Deductive database technology is an appropriate solution to this problem. Another fruitful application area is that of expert systems. There are many computing applications in which there are large amounts of information, from which the important facts may be distilled by a simple yet tedious analysis. For example, medical analysis and monitoring can generate a large amount of data, and an error can have disastrous consequences.We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an effective deductive object-oriented database through language integration

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: maria l. barja , norman w. paton , alvaro a. a. fernandes , m. howard williams , andrew dinn
",n
"LEFT id: NA
RIGHT id: 1047

LEFT text: In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: answering queries with aggregation using views

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: divesh srivastava , shaul dar , h. v. jagadish , alon y. levy
",n
"LEFT id: NA
RIGHT id: 2126

LEFT text: This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: research issues for data communication in mobile ad-hoc network database systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: leslie d. fife , le gruenwald
",n
"LEFT id: NA
RIGHT id: 866

LEFT text: We have developed new sort algorithms which eliminate almost all the compares, provide functional parallelism which can be exploited by multiple execution units, significantly reduce the number of passes through keys, and improve data locality. These new algorithms outperform traditional sort algorithms by a large factor.For the Datamation disk to disk sort benchmark (one million 100-byte records), at SIGMOD'94, Chris Nyberg et al presented several new performance records using DEC alpha processor based systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rachel pottinger , alon y. levy
",n
"LEFT id: NA
RIGHT id: 619

LEFT text: In this column, we review these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answers to a question many of us have probably asked; and what we can do about it. Review by Shoshana Marcus. 2. Theorems of the 21st Century: Volume I, by Bogdan Grechuk. The background, context, and statement of numerous important (accessible) mathematical theorems from the past 20 years.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: book reviews

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: karl aberer
",n
"LEFT id: NA
RIGHT id: 1046

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying multiple features of groups in relational databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: damianos chatziantoniou , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 230

LEFT text: The World-Wide Web (WWW) is an ever growing, distributed, non-administered, global information resource. It resides on the worldwide computer network and allows access to heterogeneous information: text, image, video, sound and graphic data. Currently, this wealth of information is difficult to mine. One can either manually, slowly and tediously navigate through the WWW or utilize indexes and libraries which are built by automatic search engines (called knowbots or robots). We have designed and are now implementing a high level SQL-like language to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. Query results are intuitively presented and continuously maintained when desired. The language itself integrates new utilities and existing Unix tools (e.g. grep, awk).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database techniques for the world-wide web : a survey

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: daniela florescu , alon levy , alberto mendelzon
",n
"LEFT id: NA
RIGHT id: 226

LEFT text: Unfortunately, this will be my last influential papers column. I've been editor for about five years now (how time flies!) and have enjoyed it immensely. I've always found it rewarding to step back and look at why we do the research we do, and this column makes a big contribution to the process of self-examination. Further, I feel that there's a strong need for ways to publicly and explicitly highlight ""quality"" in papers. Criticism is easy, and is the more common experience given the amount of reviewing (and being reviewed) we typically engage in. I look forward to seeing this column in future issues.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: reminiscences on influential papers

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 1025

LEFT text: The result size of a query that involves multiple attributes from the same relation depends on these attributes’joinr data distribution, i.e., the frequencies of all combinations of attribute values. To simplify the estimation of that size, most commercial systems make the artribute value independenceassumption and maintain statistics (typically histograms) on individual attributes only. In reality, this assumption is almost always wrong and the resulting estimations tend to be highly inaccurate. In this paper, we propose two main alternatives to effectively approximate (multi-dimensional) joint data distributions. (a) Using a multi-dimensional histogram, (b) Using the Singular Value Decomposition (SVD) technique from linear algebra. An extensive set of experiments demonstrates the advantages and disadvantages of the two approaches and the benefits of both compared to the independence assumption.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: selectivity estimation without the attribute value independence assumption

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: viswanath poosala , yannis e. ioannidis
",y
"LEFT id: NA
RIGHT id: 196

LEFT text: XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent flexibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly different query execution strategies for different databases. We define appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported. This is a short version of the paper Query Optimization for Semistructured Data which is available at: http://www-db.stanford.edu/~mchughj/publications/qo.ps

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization in the presence of limited access patterns

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniela florescu , alon levy , ioana manolescu , dan suciu
",n
"LEFT id: NA
RIGHT id: 457

LEFT text: As XML is emerging as the data format of the internet era, there is an substantial increase of the amount of data in XML format. To better describe such XML data structures and constraints, several XML schema languages have been proposed. In this paper, we present a comparative analysis of six noteworthy XML schema languages. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: comparative analysis of six xml schema languages

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: dongwon lee , wesley w. chu
",y
"LEFT id: NA
RIGHT id: 1015

LEFT text: DTL's DataSpot is an advanced, programming-free tool that lets Web designers and database developers automatically publish their databases for Web browser access. DataSpot enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation, in a fashion similar to using search engines such as Alta Vista to search text files on the Internet.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: dtl 's dataspot : database exploration using plain language

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: shaul dar , gadi entin , shai geva , eran palmon
",n
"LEFT id: NA
RIGHT id: 207

LEFT text: The rapid growth of the Internet and support for interoperability protocols has increased the number of Web accessible sources, WebSources. Current wrapper mediator architectures need to be extended with a wrapper cost model (WCM) for WebSources that can estimate the response time (delays) to access sources as well as other relevant statistics. In this paper, we present a Web prediction tool (WebPT), a tool that is based on learning using query feedback from WebSources. The WebPT uses dimensions time of day, day, and quantity of data, to learn response times from a particular WebSource, and to predict the expected response time (delay) for some query.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: query optimization for selections using bitmaps

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: ming-chuan wu
",n
"LEFT id: NA
RIGHT id: 695

LEFT text: Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: partial results for online query processing

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: vijayshankar raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 120

LEFT text: The amount of services and deployed software agents in the most famous o spring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an e ective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester, and middle agents. Service providers provide some type of service, such as nding information, or performing some particular domain speci c problem solving. Requester agents need provider agents to perform some service for them.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: dynamic service matchmaking among agents in open information environments

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: katia sycara , matthias klusch , seth widoff , jianguo lu
",y
"LEFT id: NA
RIGHT id: 2078

LEFT text: Continuous Query (CQ) systems typically exploit commonality among query expressions to achieve improved efficiency through shared processing. Recently proposed CQ systems have introduced window specifications in order to support unbounded data streams. There has been, however, little investigation of sharing for windowed query operators. In this paper, we address the shared execution of windowed joins, a core operator for CQ systems. We show that the strategy used in systems to date has a previously unreported performance flaw that can negatively impact queries with relatively small windows. We then propose two new execution strategies for shared joins. We evaluate the alternatives using both analytical models and implementation in a DBMS. The results show that one strategy, called MQT, provides the best performance over a range of workload settings.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",n
"LEFT id: NA
RIGHT id: 312

LEFT text: In this paper, we propose an efficient direct and indirect file transfer protocol (C2CFTP) that transfers files between clients in a client-server system. Existing file transfer methods use an indirect transfer method through a server to transfer files between sending and receiving clients or a direct transfer method that connects a direct data channel between clients. However, in the case of indirect transmission, unnecessary file input/output (I/O) is required by the server, and in the case of direct transmission, a problem arises in that the file transmission delay time is increased due to channel management cost. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: proxy-server architectures for olap

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: panos kalnis , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 1326

LEFT text: To overcome the verbosity problem, the research on compressors for XML data has been conducted. However, some XML compressors do not support querying compressed data, while other XML compressors which support querying compressed data blindly encode tags and data values using predefined encoding methods. Thus, the query performance on compressed XML data is degraded.In this paper, we propose XPRESS, an XML compressor which supports direct and efficient evaluations of queries on compressed XML data. XPRESS adopts a novel encoding method, called reverse arithmetic encoding, which is intended for encoding label paths of XML data, and applies diverse encoding methods depending on the types of data values.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query engines for web-accessible xml data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: leonidas fegaras , ramez elmasri
",n
"LEFT id: NA
RIGHT id: 2048

LEFT text: Our design enables self starting distributed queries that jump directly to the lowest common ancestor of the query result, dramatically reducing query response times. We present a novel query-evaluate gather technique (using XSLT) for detecting (1) which data in a local database fragment is part of the query result, and (2) how to gather the missing parts. We define partitioning and cache invariants that ensure that even partial matches on cached data are exploited and that correct answers are returned, despite our dynamic query-driven caching. Experimental results demonstrate that our techniques dramatically increase query throughputs and decrease query response times in wide area sensor databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: cache-and-query for wide area sensor databases

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: amol deshpande , suman nath , phillip b. gibbons , srinivasan seshan
",y
"LEFT id: NA
RIGHT id: 1927

LEFT text: The longitudinal stability of personality is low in childhood but increases substantially into adulthood. Theoretical explanations for this trend differ in the emphasis placed on intrinsic maturation and socializing influences. To what extent does the increasing stability of personality result from the continuity and crystallization of genetically influenced individual differences, and to what extent does the increasing stability of life experiences explain increases in personality trait stability? Behavioral genetic studies, which decompose longitudinal stability into sources associated with genetic and environmental variation, can help to address this question. We aggregated effect sizes from 24 longitudinal behavioral genetic studies containing information on a total of 21,057 sibling pairs from 6 types that varied in terms of genetic relatedness and ranged in age from infancy to old age. A combination of linear and nonlinear meta-analytic regression models were used to evaluate age trends in levels of heritability and environmentality, stabilities of genetic and environmental effects, and the contributions of genetic and environmental effects to overall phenotypic stability. Both the genetic and environmental influences on personality increase in stability with age. The contribution of genetic effects to phenotypic stability is moderate in magnitude and relatively constant with age, in part because of small-to-moderate decreases in the heritability of personality over child development that offset increases in genetic stability. In contrast, the contribution of environmental effects to phenotypic stability increases from near zero in early childhood to moderate in adulthood. The life-span trend of increasing phenotypic stability, therefore, predominantly results from environmental mechanisms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: query previews for networked information systems : a case study with nasa environmental data

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: khoa doan , catherine plaisant , ben shneiderman , tom bruns
",n
"LEFT id: NA
RIGHT id: 218

LEFT text: There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: an efficient method for checking object-oriented database schema correctness

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: a. formica , h. d. groger , m. missikoff
",n
"LEFT id: NA
RIGHT id: 323

LEFT text: Increasingly, we are becoming a data-driven society with massive information requirements and evermore numerous on-line data sources. The research activities of the Database Group at ETH are centred on the investigation of architectures and techniques for exploring and managing the data COSMOS with its proliferation and diversity of data, and with its inherent heterogeneity. Our key aim is to provide a spectrum of data connectivity whereby data sources and application systems may cooperate at various levels of interoperability and integration. Multilevel interoperability allows application systems to cooperate with application systems, database systems to coopera.te with dat,a.base systems, and storage services to cooperate with storage services.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: database research at arizona state university

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan d. urban , suzanne w. dietrich , forouzan golshani
",n
"LEFT id: NA
RIGHT id: 1320

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: querying xml views of relational data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , jerry kiernan , eugene j. shekita , catalina fan , john funderburk
",y
"LEFT id: NA
RIGHT id: 1220

LEFT text: This paper takes the next logical step: It considers the use of timestamping for capturing transaction and valid time in the context of transactions. The paper initially identifies and analyzes several problems with straightforward timestamping, then proceeds to propose a variety of techniques aimed at solving these problems. Timestamping the results of a transaction with the commit time of the transaction is a promising approach. The paper studies how this timestamping may be done using a spectrum of techniques. While many database facts are valid until now, the current time, this value is absent from the existing temporal types. Techniques that address this problem using different substitute values are presented. Using a stratum architecture, the performance of the different proposed techniques are studied. Although querying and modifying time-varying data is accompanied by a number of subtle problems, we present a comprehensive approach that provides application programmers with simple, consistent, and efficient support for modifying bitemporal databases in the context of user transactions.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: effective timestamping in databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kristian torp , christian s. jensen , richard thomas snodgrass
",y
"LEFT id: NA
RIGHT id: 1595

LEFT text: A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent as well as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent “off the air”, i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient optimistic concurrency control using loosely synchronized clocks

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: atul adya , robert gruber , barbara liskov , umesh maheshwari
",n
"LEFT id: NA
RIGHT id: 1166

LEFT text: The end of the Cold War bas brought significant changes for GM Hughes Electronics, one of the world’s leading satellite and defense electronics companies. Their response to the loss of defense revenue was to marry their satellite communications expertise with the rapidly expanding entertainment industry to produce DIRECTV”, the first all-digital direct broadcast satellite (DBS) service in the United States. For years, customers in rural areas have used large, unsightly satellite dishes to receive television programming. The cost, size and complexity of these systems has limited their appeal. Hughes has now launched two geosynchronous satellites that use higher powered transmitters to send streams of compressed digital data to 18 inch antennas that can be mounted inauspiciously. A specialized video processor decompresses the signal and displays it on the consumer’s television with better-thanbroadcast quality audio and video. The programming offered inchrdes a number of cable-like television broadcast services, music and scores of offerings of pay-per-view movies, sports and special events.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the impact of object technology on commercial transaction processing

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: edward e. cobb
",n
"LEFT id: NA
RIGHT id: 1452

LEFT text: Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention, because the physical plans---unaware of each other---compete for access to the underlying I/O and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly when multiple complex queries run at the same time.    We describe an augmentation of traditional query engines that improves join throughput in large-scale concurrent data warehouses. In contrast to the conventional query-at-a-time model, our approach employs a single physical plan that can share I/O, computation, and tuple storage across all in-flight join queries. We use an ""always-on"" pipeline of non-blocking operators, coupled with a controller that continuously examines the current query mix and performs run-time optimizations. Our design allows the query engine to scale gracefully to large data sets, provide predictable execution times, and reduce contention. In our empirical evaluation, we found that our prototype outperforms conventional commercial systems by an order of magnitude for tens to hundreds of concurrent queries.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the oracle warehouse

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: gary hallmark
",n
"LEFT id: NA
RIGHT id: 1239

LEFT text: This second Long Range Planning special issue on PLS-SEMin strategic management research and practice seeks to further progress towards this goal. The journal received 41 articles for its special issue on PLS-SEM, twelve of which completed a thorough review process successfully. Based on the number of high quality manuscripts, a decision was made to split the special issue. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: guest editorial

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: alon y. halevy
",n
"LEFT id: NA
RIGHT id: 791

LEFT text: Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or ""vertical"" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the ""horizontal"" dimension of UnQL.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: implementation of two semantic query optimization techniques in db2 universal database

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: qi cheng , jarek gryz , fred koo , t. y. cliff leung , linqi liu , xiaoyan qian , k. bernhard schiefer
",n
"LEFT id: NA
RIGHT id: 1209

LEFT text: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: algebraic query optimisation for database programming languages

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: alexandra poulovassilis , carol small
",n
"LEFT id: NA
RIGHT id: 2078

LEFT text: Exact answers NOT always required : DSS applications usually exploratory: early feedback to help identify “interesting” regions Aggregate queries: precision to “last decimal” not needed e.g., “What are the total sales of product X in NJ?” Base data can be remote or unavailable: approximate processing using locally-cached data synopses is the only option

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",n
"LEFT id: NA
RIGHT id: 2243

LEFT text: We describe the integration of a structured-text retrieval system (TextMachine) into an object-oriented database system (OpOur approach is a light-weight one, using the external function capability of the database system to encapsulate the text retrieval system as an external information source. Yet, we are able to provide a tight integration in the query language and processing; the user can access the text retrieval system using a standard database query language. The effcient and effective retrieval of structured text performed by the text retrieval system is seamlessly combined with the rich modeling and general-purpose querying capabilities of the database system, resulting in an integrated system with querying power beyond those of the underlying systems. The integrated system also provides uniform access to textual data in the text retrieval system and structured data in the database system, thereby achieving information fusion. We discuss the design and implementation of our prototype system, and address issues such as the proper framework for external integration, the modeling of complex categorization and structure hierarchies of documents (under automatic document schema impand techniques to reduce the performance overhead of accessing an external source.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: odefs : a file system interface to an object-oriented database

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: narain h. gehani , h. v. jagadish , william d. roome
",n
"LEFT id: NA
RIGHT id: 680

LEFT text: Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient algorithms for minimizing tree pattern queries

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: prakash ramanan
",n
"LEFT id: NA
RIGHT id: 2084

LEFT text: XML has emerged as the standard data exchange format for Internet-based business applications. This has created the need to publish existing business data, stored in relational databases, as XML. A general way to publish relational data as XML is to provide XML views over relational data, and allow business partners to query these views using an XML query language. In this paper, we address the problem of evaluating XML queries over XML views of relational data. This paper makes two main contributions. The first is a general framework for processing arbitrarily complex queries specified using the XQuery query language. The second is a technique for efficiently evaluating XML queries by pushing most of the query computation down to the relational engine.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: rights protection for relational data

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: radu sion , mikhail atallah , sunil prabhakar
",n
"LEFT id: NA
RIGHT id: 1645

LEFT text: Implementing crash recovery in an Object-Oriented Database System (OODBMS) raises several challenging issues for performance that are not present in traditional DBMSs. These performance concerns result both from significant architectural differences between OODBMSs and traditional database systems and differences in OODBMS's target applications. This paper compares the performance of several alternative approaches to implementing crash recovery in an OODBMS based on a client-server architecture. The four basic recovery techniques examined in the paper are termed page differencing, sub-page differencing, whole-page logging, and redo-at-server. All of the recovery techniques were implemented in the context of QuickStore, a memory-mapped store built using the EXODUS Storage Manager, and their performance is compared using the OO7 database benchmark. The results of the performance study show that the techniques based on differencing generally provide superior performance to whole-page logging.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: quickstore : a high performance mapped object store

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: seth j. white , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 1845

LEFT text: The InfoSleuth Project at MCC [7, 9, 8, 1] is developing and deploying technologies for finding information in corp~ rate networks and in external networka, such as networks baaed on the emerging National Information Infrastructure. InfoSleuth is baaed on MCC’S previously developed Carnot technology [2, 6, 10], which was successfully used to integrate heterogeneous information resources. The Carnot project developed semantic modeling techniques that enable description of the information resources and pioneered the use of agents to provide interoperation among autonomous systems. The InfoSleuth Project investigates the use of Carnot technologies in a dynamically changing environment, such as the Internet, where there is no formal control of the registration of new information sources and the identities of the resources to be used may be unknown at the time the application is developed. InfoSleuth deploys semantic agents [9, 5, 3] that carry out coordinated searches and cooperate with each other to merge the retrieved data into understandable information. The project is developing technologies to support mediated interoperation of data and services over information networks in a dynamically changing environment, including:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the infosleuth project

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: r. j. bayardo , jr. , w. bohrer , r. brice , a. cichocki , j. fowler , a. halal , v. kashyap , t. ksiezyk , g. martin , m. nodine , m. rashid , m. rusinkiewicz , r. shea , c. unnikrishnan , a. unruh , d. woelk
",y
"LEFT id: NA
RIGHT id: 443

LEFT text: In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: congressional samples for approximate answering of group-by queries

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: swarup acharya , phillip b. gibbons , viswanath poosala
",n
"LEFT id: NA
RIGHT id: 1801

LEFT text: We describe a system that supports arbitrarily complex SQL queries with ”uncertain” predicates. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is query evaluation. We describe an optimization algorithm that can compute eciently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any ecient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: simultaneous optimization and evaluation of multiple dimensional queries

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yihong zhao , prasad m. deshpande , jeffrey f. naughton , amit shukla
",n
"LEFT id: NA
RIGHT id: 1044

LEFT text: To support efficient similarity searches in an NDDS, we propose a new dynamic indexing technique, called the ND-tree. The key idea is to extend the relevant geometric concepts as well as some indexing strategies used in CDSs to NDDSs. Efficient algorithms for ND-tree construction are presented. Our experimental results on synthetic and genomic sequence data demonstrate that the performance of the ND-tree is significantly better than that of the linear scan and M-tree in high dimensional NDDSs.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: the x-tree : an index structure for high-dimensional data

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: stefan berchtold , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 779

LEFT text: Publisher Summary Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: atomicity versus anonymity : distributed transactions for electronic commerce

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: j. d. tygar
",n
"LEFT id: NA
RIGHT id: 198

LEFT text: In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mind your vocabulary : query mapping across heterogeneous information sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chen-chuan k. chang , h &#233; ctor garc &#237; a-molina
",y
"LEFT id: NA
RIGHT id: 301

LEFT text: As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, τXQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a τXQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach: selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: adaptable query optimization and evaluation in temporal middleware

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: giedrius slivinskas , christian s. jensen , richard thomas snodgrass
",n
"LEFT id: NA
RIGHT id: 459

LEFT text: In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: the implementation and performance of compressed databases

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: till westmann , donald kossmann , sven helmer , guido moerkotte
",y
"LEFT id: NA
RIGHT id: 576

LEFT text: The gains of parallel query execution can be limited because of high start-up time, interference between execution entities, and poor load balancing. In this paper, we present a solution which reduces these limitations in DBS3, a shared-memory parallel database system. This solution combines static data partitioning and dynamic processor allocation to adapt to the execution context. It makes DBS3 almost insensitive to data skew and allows decoupling the degree of parallelism from the degree of data partitioning. To address the problem of load balancing in the presence of data skew, we analyze three important factors that influence the behavior of our parallel execution model: skew factor, degree of parallelism and degree of partitioning. We report on experiments varying these three parameters with the DBS3 prototype on a 72-node KSR1 multiprocessor. The results demonstrate high performance gains, even with highly skewed data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: the aditi deductive database system

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jayen vaghani , kotagiri ramamohanarao , david b. kemp , zoltan somogyi , peter j. stuckey , tim s. leask , james harland
",n
"LEFT id: NA
RIGHT id: 1332

LEFT text: The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views fit into a prespecified storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. In addition, view selection is a core problem for intelligent data placement over a wide-area network for data integration applications and data management for ubiquitous computing. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equality-selection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show a somewhat surprising result, namely, that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a formal perspective on the view selection problem

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: rada chirkova , alon y. halevy , dan suciu
",n
"LEFT id: NA
RIGHT id: 753

LEFT text: With the popularity of XML, it is increasingly common to nd data in the XML format. This highlights an important question: given an XML document S and a DTD D, how to extract data from S and construct another XML document T such that T conforms to the xed D? Let us refer to this as DTD-conforming XML to XML transformation. The need for this is evident in, e.g., data exchange: enterprises exchange their XML documents with respect to a certain prede ned DTD. Although a number of XML query languages (e.g., XQuery, XSLT) are currently being used to transform XML data, they cannot guarantee DTD conformance. Type inference and (static) checking for XML transformations are too expensive to be used in practice; worse, they provide no guidance for how to specify a DTD-conforming XML to XML transformation. In response to the need we have developed TREX (TRansformation Engine for XML), a middleware system for DTDconforming XML to XML transformations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a pictorial query language for querying geographic databases using positional and olap operators

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: elaheh pourabbas , maurizio rafanelli
",n
"LEFT id: NA
RIGHT id: 1155

LEFT text: In this paper we present a solution to the problem of concurrent operations in R-trees, a dynamic access structure capable of storing multidimensional and spatial data. We describe the R-link tree, a variant of the R-tree that adds sibling pointers to nodes, a technique first deployed in B-link tree, a variant of the R-tree that adds sibling pointers to nodes, a technique first deployed in B-link trees, to compensate for concurrent structure modifications. The main obstacle to the use of sibling pointers is the lack of linear ordering among the keys in an R-tree; we overcome this by assigning sequence numbers to nodes that let us reconstruct the ""lineage"" of a node at any point in time.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: concurrency and recovery for index trees

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: david lomet , betty salzberg
",n
"LEFT id: NA
RIGHT id: 2108

LEFT text: In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: monotonic complements for independent data warehouses

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: d. laurent , j. lechtenb &#246; rger , n. spyratos , g. vossen
",n
"LEFT id: NA
RIGHT id: 1472

LEFT text: Most of new databases are no more built from scratch, but re-use existing data from several autonomous data stores. To facilitate application development, the data to be re-used should preferably be redefined as a virtual database, providing for the logical unification of the underlying data sets. This unification process is called database integration. This chapter provides a global picture of the issues raised and the approaches that have been proposed to tackle the problem.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: managing intra-operator parallelism in parallel database systems

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: manish mehta , david j. dewitt
",n
"LEFT id: NA
RIGHT id: 2207

LEFT text: This paper presents a set of aggregation algorithms on very large compressed data warehouses for multidimensional OLAP. These algorithms operate directly on compressed datasets without the need to first decompress them. They are applicable to data warehouses that are compressed using variety of data compression methods. The algorithms have different performance behavior as a function of dataset parameters, sizes of outputs and main memory availability. The analysis and experimental results show that the algorithms have better performance than the traditional aggregation algorithms.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: fast algorithms for mining association rules in large databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: rakesh agrawal , ramakrishnan srikant
",n
"LEFT id: NA
RIGHT id: 1408

LEFT text: Structural matching and discovery in documents such as SGML and HTML is important for data warehousing [6], version management [7, 11], hypertext authoring, digital libraries [4] and Internet databases. As an example, a user of the World Wide Web may be interested in knowing changes in an HTML document [2, 5, 10]. Such changes can be detected by comparing the old and new version of the document (referred to as structural matching of documents). As another example, in hypertext authoring, a user may wish to find the common portions in the history list of a document or in a database of documents (referred to as structural discovery of documents). In SIGMOD 95 demo sessions, we exhibited a software package, called TreeDiff [13], for comparing two latex documents and showing their differences. Given two documents, the tool represents the documents as ordered labeled trees and finds an optimal sequence of edit operations to transform one document (tree) to the other. An edit operation could be an insert, delete, or change of a node in the trees. The tool is so named because documents are represented and compared using approximate tree matching techniques [9, 12, 14].

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: efficient index structures for string databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: tamer kahveci , ambuj k. singh
",n
"LEFT id: NA
RIGHT id: 1026

LEFT text: Some significant progress related to multidimensional data analysis has been achieved in the past few years, including the design of fast algorithms for computing datacubes, selecting some precomputed group-bys to materialize, and designing efficient storage structures for multidimensional data. However, little work has been carried out on multidimensional query optimization issues. Particularly the response time (or evaluation cost) for answering several related dimensional queries simultaneously is crucial to the OLAP applications. Recently, Zhao et al. first exploited this problem by presenting three heuristic algorithms. In this paper we first consider in detail two cases of the problem in which all the queries are either hash-based star joins or index-based star joins only. In the case of the hash-based star join, we devise a polynomial approximation algorithm which delivers a plan whose evaluation cost is $ O(n^{\epsilon }$) times the optimal, where n is the number of queries and $ \epsilon $ is a fixed constant with $0<\epsilon \leq 1$. We also present an exponential algorithm which delivers a plan with the optimal evaluation cost. In the case of the index-based star join, we present a heuristic algorithm which delivers a plan whose evaluation cost is n times the optimal, and an exponential algorithm which delivers a plan with the optimal evaluation cost. We then consider a general case in which both hash-based star-join and index-based star-join queries are included. For this case, we give a possible improvement on the work of Zhao et al., based on an analysis of their solutions. We also develop another heuristic and an exact algorithm for the problem. We finally conduct a performance study by implementing our algorithms. The experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal, which confirms our theoretical upper bounds. Actually these experiments produce much better results than our theoretical estimates. To the best of our knowledge, this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated. The previous approaches including that of [ZDNS98] may generate a feasible plan for the problem in these two cases, but they do not provide any performance guarantee, i.e., the plans generated by their algorithms can be arbitrarily far from the optimal one.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a foundation for multi-dimensional databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: marc gyssens , laks v. s. lakshmanan
",n
"LEFT id: NA
RIGHT id: 857

LEFT text: Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a database platform for bioinformatics

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: sandeepan banerjee
",n
"LEFT id: NA
RIGHT id: 360

LEFT text: XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: xml data management ( panel session ) : go native or spruce up relational systems ?

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: per - &#197; ke larson , dana florescu , goetz graefe , guido moerkotte , hamid pirahesh , harald sch &#246; ning
",y
"LEFT id: NA
RIGHT id: 2187

LEFT text: The proliferation of mobile and pervasive computing devices has brought energy constraints into the limelight, together with performance considerations. Energy-conscious design is important at all levels of the system architecture, and the software has a key role to play in conserving the battery energy on these devices. With the increasing popularity of spatial database applications, and their anticipated deployment on mobile devices (such as road atlases and GPS based applications), it is critical to examine the energy implications of spatial data storage and access methods for memory resident datasets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: analyzing energy behavior of spatial access methods for memory-resident data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ning an , anand sivasubramaniam , narayanan vijaykrishnan , mahmut t. kandemir , mary jane irwin , sudhanva gurumurthi
",y
"LEFT id: NA
RIGHT id: 1471

LEFT text: In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an efficient algorithm for mining association rules in large databases

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: ashoka savasere , edward omiecinski , shamkant b. navathe
",y
"LEFT id: NA
RIGHT id: 469

LEFT text: Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - ""given the current information, it is possible that X will become true at time t"" - instead of no information at all.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: on computing correlated aggregates over continual data streams

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: johannes gehrke , flip korn , divesh srivastava
",n
"LEFT id: NA
RIGHT id: 1983

LEFT text: Abstract. With respect to the specific requirements of advanced OODB applications, index data structures for type hierarchies in OODBMS have to provide efficient support for multiattribute queries and have to allow index optimization for a particular query profile. We describe the multikey type index and an efficient implementation of this indexing scheme. It meets both requirements: in addition to its multiattribute query capabilities it is designed as a mediator between two standard design alternatives, key-grouping and type-grouping. A prerequisite for the multikey type index is a linearization algorithm which maps type hierarchies to linearly ordered attribute domains in such a way that each subhierarchy is represented by an interval of this domain. The algorithm extends previous results with respect to multiple inheritance. The subsequent evaluation of our proposal focuses on storage space overhead as well as on the number of disk I/O operations needed for query execution. The analytical results for the multikey type index are compared to previously published figures for well-known single-key search structures. The comparison clearly shows the superiority of the multikey type index for a large class of query profiles.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: static detection of security flaws in object-oriented databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: keishi tajima
",n
"LEFT id: NA
RIGHT id: 1502

LEFT text: In this paper, we examine presentations in three different domains (heavyweight, middleweight, and lightweight) and provide buffer management and admission control algorithms for the three domains. We propose two improvements (flattening and dynamic-adjustments) on the schedules created for the heavyweight presentations. Results from a simulation environment are presented.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fault tolerant design of multimedia servers

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: steven berson , leana golubchik , richard r. muntz
",n
"LEFT id: NA
RIGHT id: 685

LEFT text: An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: time-parameterized queries in spatio-temporal databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: yufei tao , dimitris papadias
",n
"LEFT id: NA
RIGHT id: 53

LEFT text: In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: dynamat : a dynamic view management system for data warehouses

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: yannis kotidis , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 866

LEFT text: Heavily used in both academic and corporate R&D settings, ACM Transactions on Database Systems (TODS) is a key publication for computer scientists working in data abstraction, data modeling, and designing data management systems. Topics include storage and retrieval, transaction management, distributed and federated databases, semantics of data, intelligent databases, and operations and algorithms relating to these areas.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: a scalable algorithm for answering queries using views

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: rachel pottinger , alon y. levy
",n
"LEFT id: NA
RIGHT id: 430

LEFT text: This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient and extensible algorithms for multi query optimization

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: prasan roy , s. seshadri , s. sudarshan , siddhesh bhobe
",n
"LEFT id: NA
RIGHT id: 175

LEFT text: Microsoft Universal Data Access defines a platform for developing multi-tier enterprise applications that require efficient access to diverse relational or non-relational data sources across intranets or the Internet. Universal Data Access consists of a collection of software components that interact with each other using system-level interfaces defined by OLE DB and providing an application-level data access model called ActiveX Data Objects (ADO). This talk provides an overview of the platform.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: microsoft universal data access platform

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: jos &#233; a. blakeley , michael j. pizzo
",y
"LEFT id: NA
RIGHT id: 427

LEFT text: Data warehouses are used to collect and analyze data from remote sources. The data collected often originate from transactional information and can become very large. This paper presents a framework for incrementally removing warehouse data (without a need to fully recompute offering two choices. One is to expunge data, in which case the result is as if the data had never existed. The second is to expire data, in which case views defined over the data are not necessarily affected. the framework, a user or administrator can specify what data to expire or expunge, what auxiliary data is to be kept for facilitating incremental view maintenance, what type of updates are expected from external sources, and how the system should compensate when data is expired or other parameters changed. We present algorithms for the various expiration and compensation actions, and we show how our framework can be implemented on top of a conventional RDBMS.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: microsoft terraserver : a spatial data warehouse

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: tom barclay , jim gray , don slutz
",n
"LEFT id: NA
RIGHT id: 1824

LEFT text: We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: supporting multiple view maintenance policies

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: latha s. colby , akira kawaguchi , daniel f. lieuwen , inderpal singh mumick , kenneth a. ross
",n
"LEFT id: NA
RIGHT id: 594

LEFT text: Detecting changes by comparing data snapshots is an important requirement for difference queries, active databases, and version and configuration management. In this paper we focus on detecting meaningful changes in hierarchically structured data, such as nested-object data. This problem is much more challenging than the corresponding one for relational or flat-file data. In order to describe changes better, we base our work not just on the traditional “atomic” insert, delete, update operations, but also on operations that move an entire sub-tree of nodes, and that copy an entire sub-tree. These operations allows us to describe changes in a semantically more meaningful way. Since this change detection problem is NP-hard, in this paper we present a heuristic change detection algorithm that yields close to “minimal” descriptions of the changes, and that has fewer restrictions than previous algorithms. Our algorithm is based on transforming the change detection problem to a problem of computing a minimum-cost edge cover of a bipartite graph. We study the quality of the solution produced by our algorithm, as well as the running time, both analytically and experimentally.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: describing semistructured data

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: luca cardelli
",n
"LEFT id: NA
RIGHT id: 2201

LEFT text: Query optimization which is done by making a graph of the query and moving predicates around in the graph so that they will be applied early in the optimized query generated from the graph. Predicates are first propagated up from child nodes of the graph to parent nodes and then down into different child nodes. After the predicates have been moved, redundant predicates are detected and removed. Predicates are moved through aggregation operations and new predicates are deduced from aggregation operations and from functional dependencies. The optimization is not dependent on join order and works where nodes of the graph cannot be merged.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: query optimization by predicate move-around

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: alon y. levy , inderpal singh mumick , yehoshua sagiv
",y
"LEFT id: NA
RIGHT id: 49

LEFT text: An information retrieval (IR) engine can rank documents based on textual proximity of keywords within each document. In this paper we apply this notion to search across an entire database for objects that are ""near"" other relevant objects. Proximity search enables simple ""focusing"" queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is defined based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very effective for quickly finding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly find the distance between objects at search time. Experiments show that our algorithms are effcient and scale well.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: efficient geometry-based similarity search of 3d spatial databases

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: daniel a. keim
",n
"LEFT id: NA
RIGHT id: 811

LEFT text: Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQLbased database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on top of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: relational databases for querying xml documents : limitations and opportunities

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , kristin tufte , chun zhang , gang he , david j. dewitt , jeffrey f. naughton
",n
"LEFT id: NA
RIGHT id: 855

LEFT text: Queries navigate semistructured data via path expressions, and can be accelerated using an index. Our solution encodes paths as strings, and inserts those strings into a special index that is highly optimized for long and complex keys. We describe the Index Fabric, an indexing structure that provides the efficiency and flexibility we need. We discuss how ""raw paths"" are used to optimize ad hoc queries over semistructured data, and how ""refined paths"" optimize specific access paths. Although we can use knowledge about the queries and structure of the data to create refined paths, no such knowledge is needed for raw paths.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: capturing and querying multiple aspects of semistructured data

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: curtis e. dyreson , michael h. b &#246; hlen , christian s. jensen
",n
"LEFT id: NA
RIGHT id: 1730

LEFT text: Sequoia 2000 schema development is based on emerging geospatial standards to accelerate development and facilitate data exchange. This paper focuses on the metadata schema for digital satellite images. We examine how satellite metadata are defined, used, and maintained. We discuss the geospatial standards we are using, and describe a SQL prototype that is based on the Spatial Archive and Interchange Format (SAIF) standard and implemented in the Illustra object-relational database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: sequoia 2000 metadata schema for satellite images

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: jean t. anderson , michael stonebraker
",y
"LEFT id: NA
RIGHT id: 1961

LEFT text: A new access method, called M-tree, is proposed to organize and search large data sets from a generic “metric space”, i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O’s and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: birch : an efficient data clustering method for very large databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: tian zhang , raghu ramakrishnan , miron livny
",n
"LEFT id: NA
RIGHT id: 2242

LEFT text: Databases have employed a schema-based approach to store and retrieve structured data for decades. For peer-to-peer (P2P) networks, similar approaches are just beginning to emerge. While quite a few database techniques can be re-used in this new context, a P2P data management infrastructure poses additional challenges which have to be solved before schema-based P2P networks become as common as schema-based databases. We will describe some of these challenges and discuss approaches to solve them. Our discussion will be based on the design decisions we have employed in our Edutella infrastructure, a schema-based P2P network based on RDF and RDF schemas, and will also point out additional work addressing the issues discussed.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: design issues and challenges for rdf - and schema-based peer-to-peer systems

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: wolfgang nejdl , wolf siberski , michael sintek
",y
"LEFT id: NA
RIGHT id: 1203

LEFT text: Abstract. The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS.  In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required.  We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: mariposa : a wide-area distributed database system

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: michael stonebraker , paul m. aoki , witold litwin , avi pfeffer , adam sah , jeff sidell , carl staelin , andrew yu
",n
"LEFT id: NA
RIGHT id: 1738

LEFT text: In a mobile computing system, caching data items at the mobile clients is important to reduce the data access delay in an unreliable and low bandwidth mobile network. However, efficient methods must be used to ensure the coherence between the cached items and the data items at the database server. By exploring the real time properties of the data items, we propose a cache invalidation scheme called: Invalidation by Absolute Validity Interval (IAVI). We define an absolute validate interval (AVI) for each data item based on its real time property, e.g. update interval. A mobile client can verify the validity of a cached item by comparing the last update time and its AVI. A cached item is invalidated if the current time is greater than the last update time by its AVI. With this self-invalidation mechanism, the IAVI scheme uses the invalidation report to inform the mobile clients about the change of AVI rather than the update event of the data item. As a result, the size of invalidation report can be reduced significantly. Performance studies show that the IAVI scheme can significantly reduce the mean response time and invalidation report size under various system parameters.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: data replication for mobile computers

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: yixiu huang , prasad sistla , ouri wolfson
",n
"LEFT id: NA
RIGHT id: 77

LEFT text: This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mocha : a self-extensible database middleware system for distributed data sources

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: manuel rodr &#237; guez-mart &#237; nez , nick roussopoulos
",n
"LEFT id: NA
RIGHT id: 1409

LEFT text: Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: lineage tracing for general data warehouse transformations

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: yingwei cui , jennifer widom
",n
"LEFT id: NA
RIGHT id: 1481

LEFT text: As large numbers of text databases have become available on the Internet, it is getting harder to locate the right sources for given queries. In this paper we present gGlOSS, a generalized Glossary-Of-Servers Server, that keeps statistics on the available databases to estimate which databases are the potentially most useful for a given query. gGlOSS extends our previous work, which focused on databases using the boolean model of document retrieval, to cover databases using the more sophisticated vector-space retrieval model. We evaluate our new techniques using real-user queries and 53 databases. Finally, we further generalize our approach by showing how to build a hierarchy of gGlOSS brokers. The top level of the hierarchy is so small it could be widely replicated, even at end-user workstations.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: generalizing gloss to vector-space databases and broker hierarchies

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: luis gravano , hector garcia-molina
",y
"LEFT id: NA
RIGHT id: 1048

LEFT text: Text data in the Internet can be partitioned into many databases naturally. Efficient retrieval of desired data can be achieved if we can accurately predict the usefulness of each database, because with such information, we only need to retrieve potentially useful documents from useful databases. In this paper, we propose two new methods for estimating the usefulness of text databases. For a given query, the usefulness of a text database in this paper is defined to be the number of documents in the database that are sufficiently similar to the query. Such a usefulness measure enables naive-users to make informed decision about which databases to search. We also consider the collection fusion problem. Because local databases may employ similarity functions that are different from that used by the global database, the threshold used by a local database to determine whether a document is potentially useful may be different from that used by the global database. We provide techniques that determine the best threshold for a given local database.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: database management systems and the internet

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: susan malaika
",n
"LEFT id: NA
RIGHT id: 342

LEFT text: A number of researchers have become interested in the design of global-scale networked systems and applications. Our thesis here is that the database community's principles and technologies have an important role to play in the design of these systems. The point of departure is at the roots of database research: we generalize Codd's notion of data independence to physical environments beyond storage systems. We note analogies between the development of database indexes and the new generation of structured peer-to-peer networks. We illustrate the emergence of data independence in networks by surveying a number of recent network facilities and applications, seen through a database lens. We present a sampling of database query processing techniques that can contribute in this arena, and discuss methods for adoption of these technologies.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: editorial

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: richard snodgrass
",n
"LEFT id: NA
RIGHT id: 2287

LEFT text: In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: an approach for building secure database federations

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dirk jonscher , klaus r. dittrich
",n
"LEFT id: NA
RIGHT id: 1929

LEFT text: A data warehouse materializes views derived from data that may not reside at the warehouse. Maintaining these views effciently in response to base updates is diffcult, since it may involve querying external sources where the base data reside. This paper considers the problem of view self-maintenance, where the views are maintained without using all the base data. Without full use of the base data, however, maintaining a view unambiguously is not always possible. Thus, the two critical questions that must be addressed are to determine, in a given situation, whether a view is maintainable, and how to maintain it. W e provide algorithms that answer these questions for a general class of views, and for an important subclass, generate SQL queries that test whether a view is self-maintainable and update the view if it is. We improve significantly on previous work by solving the view self-maintenance problem in the presence of multiple views, with optional access to a subset of the base data, and under arbitrary mixes of insertions and deletions. We provide better insight into the problem by showing that view self-maintainability can be reduced to the problem of deciding query containment. 

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: maintenance of data cubes and summary tables in a warehouse

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: inderpal singh mumick , dallan quass , barinderpal singh mumick
",n
"LEFT id: NA
RIGHT id: 2078

LEFT text: XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: approximate join processing over data streams

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: abhinandan das , johannes gehrke , mirek riedewald
",n
"LEFT id: NA
RIGHT id: 384

LEFT text: METU Object-Oriented DBMS1 includes the implementation of a database kernel, an object-oriented SQL-like language and a graphical user interface. Kernel functions are divided between a SQL Interpreter and a C++ compiler. Thus the interpretation of functions are avoided increasing the efficiency of the system. The compiled by C++ functions are used by the system through the Function Manager. The system is realized on Exodus Storage Manager (ESM), thus exploiting some of the kernel functions readily provided by ESM. The additional functions provided by the MOOD kernel are the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: lambda-db : an odmg-based object-oriented dbms

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: leonidas fegaras , chandrasekhar srinivasan , arvind rajendran , david maier
",n
"LEFT id: NA
RIGHT id: 1478

LEFT text: In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: estimating the selectivity of spatial queries using the ` correlation ' fractal dimension

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: alberto belussi , christos faloutsos
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: We present techniques for computing small space representations of massive data streams. These are inspired by traditional wavelet-based approximations that consist of specific linear projections of the underlying data. We present general “sketch” based methods for capturing various linear projections of the data and use them to provide pointwise and rangesum estimation of data streams. These methods use small amounts of space and per-item time while streaming through the data, and provide accurate representation as our experiments with real data streams show.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 373

LEFT text: This qualifying dissertation is intended to review the state-of-the-art of Real-Time Database Systems under a uniprocessor and centralized environments. Due to the heterogeneity of the issues, the large amounts of information , and space limitation, we limit our presentation to the most important issues to the overall design, construction , and advancement of Real-Time Database Systems. Such topics are believed to include Transaction these issues, the most emphasis is placed on Concurrency Control and Conflict Resolution protocols due to their severe role on the overall systems performance. Other important issues that were not included in our presentation include Fault Tolerance and Failure Recovery, Predictability, and most important of all, Minimizing Transaction Support; i.e., Relaxing Atomicity and Serializability. Various solutions to many of the included topics are listed in chronological order along with their advantages, disadvantages, and limitations. While we took the liberty to debate some solutions, we list the debates of other researchers as well. The presentation concludes with the identification of five research areas, all of which are believed to be very important to the advancement of Real-Time Database Systems.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: integrating temporal , real-time , an active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: krithi ramamritham , raju sivasankaran , john a. stankovic , don t. towsley , ming xiong
",n
"LEFT id: NA
RIGHT id: 1612

LEFT text: We believe that the support of video on mobile systems will indeed make possible many new interesting applications. However, providing mobile video is a non-trivial task, and much work needs to be done before practical systems are widely available. In this short note we address the issue of mobile multimedia from a practitioner's perspective. We note what software and hardware are currently available in the market in support of mobile multimedia, and point out some of their deficiencies. We also discuss some of the communication and data management research issues that need to be tackled in order to address said deficiencies. Exploring these research issues is the focus of our project.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: managing video data in a mobile environment

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: rafael alonso , yuh-lin chang , liviu iftode , v. s. mani
",y
"LEFT id: NA
RIGHT id: 1041

LEFT text: To solve this problem, we first present an aggregation computation model, called the Disjoint-Inclusive Partition (DIP) computation model, that is the formal basis of our approach. Based on this model, we then present the one-pass aggregation algorithm. This algorithm computes aggregations using the one-pass buffer size, which is the minimum buffer size required for guaranteeing one disk access per page. We prove that our aggregation algorithm is optimal with respect to the one-pass buffer size under our aggregation computation model. Using the DIP computation model allows us to correctly predict the order of accessing data pages in advance. Thus, our algorithm achieves the optimal one-pass buffer size by using a buffer replacement policy, such as Belady's B0 or Toss-Immediate policies, that exploits the page access order computed in advance.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: on the computation of multidimensional aggregates

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: sameet agarwal , rakesh agrawal , prasad deshpande , ashish gupta , jeffrey f. naughton , raghu ramakrishnan , sunita sarawagi
",n
"LEFT id: NA
RIGHT id: 1798

LEFT text: A radar transmitter apparatus comprising a radar transmitter equipped with a modulator arranged in an oil-filled housing, the modulator being held in spaced relationship with respect to the inner walls of the housing in order to form an intermediate space for the convection flow of the oil. The housing is substantially trough or vat-shaped and covered by a trough or vat-shaped cover member. In the internal chamber or space between the cover member and the modulator, which internal space is wetted by the oil, there is arranged, on the one hand, a magnetron attached at the cover member and, on the other hand, a thyratron which is mounted directly below an opening at the cover member. This opening is closable by means of oil sealed throughpassage means.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: nodose-a tool for semi-automatically extracting structured and semistructured data from text documents

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: brad adelberg
",y
"LEFT id: NA
RIGHT id: 1555

LEFT text: This paper provides an overview of the current database research activities within the Intelligent Information Systems Group in the Department of Computer Science and Engineering at Arizona State University. The focus of our research is on the integration of data and knowledge management issues, with specific emphasis on multimedia systems, object-oriented databases, active databases, deductive databases, and heterogeneous, distributed database environments.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: florida international university high performance database research center

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: naphtali rishe , wei sun , david barton , yi deng , cyril orji , michael alexopoulos , leonardo loureiro , carlos ordonez , mario sanchez , artyom shaposhnikov
",n
"LEFT id: NA
RIGHT id: 198

LEFT text: In this paper we present a mechanism for approximately translating Boolean query constraints across heterogeneous information sources. Achieving the best translation is challenging because sources support different constraints for formulating queries, and often these constraints cannot be precisely translated. For instance, a query [score > 8] might be “perfectly” translated as [rating > 0.8] at some site, but can only be approximated as [grade = A] at another. Unlike other work, our general framework adopts a customizable “closeness” metric for the translation that combines both precision and recall. Our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts. As the basis, we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units. Our algorithm then translates complex queries by rewriting them in terms of the semantic units.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: mind your vocabulary : query mapping across heterogeneous information sources

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: chen-chuan k. chang , h &#233; ctor garc &#237; a-molina
",n
"LEFT id: NA
RIGHT id: 889

LEFT text: The processing of spatial joins can be greatly improved by the use of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of nonintersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. This article presents a polygon approximation for spatial join processing which we call four-colors raster signature (4CRS). The performance of a filter using this approximation was evaluated with real world data sets. The results showed that our approach, when compared to other approaches presented in the related literature, reduced the inconclusive answers by a factor of more than two. As a result, the need for retrieving the representation of polygons and carrying out exact geometry tests is reduced by a factor of more than two, as well. A Raster Approximation for the Processing of Spatial Joins

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: approximate query processing using wavelets

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: kaushik chakrabarti , minos n. garofalakis , rajeev rastogi , kyuseok shim
",n
"LEFT id: NA
RIGHT id: 847

LEFT text: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SSL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: online dynamic reordering for interactive data processing

LEFT year: NA
RIGHT year: 1999

LEFT authors: NA
RIGHT authors: vijayshankar raman , bhaskaran raman , joseph m. hellerstein
",n
"LEFT id: NA
RIGHT id: 1375

LEFT text: This paper provides an introduction to the major research directions in biodiversity informatics. The biodiversity enterprise is a vast and complex information domain. I describe the need to build infrastructure for this domain, major research thrusts needed to improve its work practices, and areas of research that could contribute to the advancement of the field. I emphasize that the science of biodiversity is fundamentally an information science, worthy of special attention from the computer and information science communities because of its distinctive attributes of scale and socio-technical complexity.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: work and information practices in the sciences of biodiversity

LEFT year: NA
RIGHT year: 2000

LEFT authors: NA
RIGHT authors: geoffrey c. bowker
",n
"LEFT id: NA
RIGHT id: 1939

LEFT text: The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: the ecrc multi database system

LEFT year: NA
RIGHT year: 1995

LEFT authors: NA
RIGHT authors: willem jonker , heribert sch &#252; tz
",n
"LEFT id: NA
RIGHT id: 608

LEFT text: Technology and education have wandered many separate but rarely intersecting paths throughout the 20th Century. In the 21st Century, the convergence of cost effective computing and networking products, methodologies, and services is finally enabling more researchers and practitioners than ever before to explore innovative ways to use computer technologies to manage and enhance the teaching and learning experience. Recognizing the importance of these trends, this Special Section solicited submissions belonging to one or all of the three mainstream learning domains, i.e., contents, methodologies, and technologies, addressing the above convergence in matters related, for example, to openness (e.g., source, access, and educational resources), online and hybrid or blended individualized and group instruction, collaborative methodologies, adaptive learning, Big Data and cloud computing applications in education, mobile learning, educational technology standards and social issues (e.g., privacy and security).

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: special section on semantic web and data management

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: robert meersman , amit sheth
",n
"LEFT id: NA
RIGHT id: 1213

LEFT text: To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: priority assignment in real-time active databases

LEFT year: NA
RIGHT year: 1996

LEFT authors: NA
RIGHT authors: rajendran m. sivasankaran , john a. stankovic , don towsley , bhaskar purimetla , krithi ramamritham
",n
"LEFT id: NA
RIGHT id: 2070

LEFT text: In this article, we describe techniques for time management for new faculty members, covering a wide range of topics ranging from advice on scheduling meetings, email, to writing grant proposals and teaching.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: time management for new faculty

LEFT year: NA
RIGHT year: 2003

LEFT authors: NA
RIGHT authors: anastassia ailamaki , johannes gehrke
",y
"LEFT id: NA
RIGHT id: 602

LEFT text: In this article, we explore new correctness criteria and scheduling methods that capture temporal transaction dependencies and belong to, the broad area between these two extreme approaches. We introduce the concepts ofsuccession dependency andchronological dependency and define correctness criteria under which temporal dependencies between transactions are preserved even if the dependent transactions execute concurrently. We also propose achronological scheduler that can guarantee that transaction executions satisfy their chronological constraints.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: the vldb journal -- the international journal on very large data bases

LEFT title: NA
RIGHT title: chronological scheduling of transactions with temporal dependencies

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: dimitrios georgakopoulos , marek rusinkiewicz , witold litwin
",y
"LEFT id: NA
RIGHT id: 1031

LEFT text: Motivation  The field of data warehousing has emerged over the last decades. A data warehouse is developed at a moment in time to support business intelligence for an indefinite period of time. During the life of the data warehouse the world around it evolves, including the systems that are a source for the data warehouse. In order for a data warehouse to remain functioning and guarantee the quality of its data, it needs to be adjusted to the evolving world around it. The concept of Delta Impact Analysis is used by the company BI4U for the activities of analysing the impact of specific changes. This concept is important because it provides insight into how a data warehouse can be adjusted to the evolving world around it. The motivation to perform this research was the fact that a clear definition on the concept of DIA and what it comprehends was lacking.  Goals  The main goals of the research were to examine the topic of DIA in practice, to gather insights from literature and other research, to design and develop a practical model for DIA, to test the DIA model, and to provide recommendations to better support changes in data warehouse source systems. These goals resulted in the following main problem statement: How can a Delta Impact Analysis model be designed that supports the process of analyzing the impact of changes in a data warehouse source system situation?  Approach  The research approach is based on the design science framework by Hevner in combination with action science theory to validate the resulting DIA model from the research. The design science perspective resulted in an approach that is both rigorous, by performing a literature study, and relevant, by applying the research to practice. The research started by investigating the concept of DIA in practice at BI4U, in order to provide more insight into what it comprehends and what was relevant for the focus of the research. Next a thorough literature study was performed. Finally an artifact was proposed, a model for the process of DIA, which was validated in practice with a field study.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: multiple-view self-maintenance in data warehousing environments

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: nam huyn
",n
"LEFT id: NA
RIGHT id: 569

LEFT text: XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an “outer union plan” to generate the content of an XML document.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: a general technique for querying xml documents using a relational database system

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: jayavel shanmugasundaram , eugene shekita , jerry kiernan , rajasekar krishnamurthy , efstratios viglas , jeffrey naughton , igor tatarinov
",n
"LEFT id: NA
RIGHT id: 474

LEFT text: The exponential growth of resources on the web, and the wide deployment of devices for multimodal access to the Internet, lead to new problems in information management. In this context, and as part of the European project Vision, we have built an interactive telematic handbook of the culture and the territory of Sardinia. A team of cultural experts browsed the web to get a large collection of Internet resources.The system built for the management of this data uses emerging Internet technologies such as the XML language suite and its applications.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: eiha ?!? : deploying web and wap services using xml technology

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: chiara biancheri , jean-christophe pazzaglia , gavino paddeu
",y
"LEFT id: NA
RIGHT id: 184

LEFT text: The database query optimizer requires the estimation of the query selectivity to find the most efficient access plan. For queries referencing multiple attributes from the same relation, we need a multi-dimensional selectivity estimation technique when the attributes are dependent each other because the selectivity is determined by the joint data distribution of the attributes. Additionally, for multimedia databases, there are intrinsic requirements for the multi-dimensional selectivity estimation because feature vectors are stored in multi-dimensional indexing trees. In the 1-dimensional case, a histogram is practically the most preferable. In the multi-dimensional case, however, a histogram is not adequate because of high storage overhead and high error rates.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: wavelet-based histograms for selectivity estimation

LEFT year: NA
RIGHT year: 1998

LEFT authors: NA
RIGHT authors: yossi matias , jeffrey scott vitter , min wang
",n
"LEFT id: NA
RIGHT id: 1324

LEFT text: Mining for a.ssociation rules between items in a large database of sales transactions has been described as an important database mining problem. In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to previous algorithms, our algorithm not only reduces the I/O overhead significantly but also has lower CPU overhead for most cases. We have performed extensive experiments and compared the performance of our algorithm with one of the best existing algorithms. It was found that for large databases, the CPU overhead was reduced by as much as a factor of four and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: discovery of influence sets in frequently updated databases

LEFT year: NA
RIGHT year: 2001

LEFT authors: NA
RIGHT authors: ioana stanoi , mirek riedewald , divyakant agrawal , amr el abbadi
",y
"LEFT id: NA
RIGHT id: 2277

LEFT text: In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: very large data bases

LEFT title: NA
RIGHT title: some issues in design of distributed deductive databases

LEFT year: NA
RIGHT year: 1994

LEFT authors: NA
RIGHT authors: mukesh k. mohania , nandlal l. sarda
",n
"LEFT id: NA
RIGHT id: 1921

LEFT text: Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications- for example, in Medicine and CAD. In this paper, we present a new geometrybased solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: international conference on management of data

LEFT title: NA
RIGHT title: fast parallel similarity search in multimedia databases

LEFT year: NA
RIGHT year: 1997

LEFT authors: NA
RIGHT authors: stefan berchtold , christian b &#246; hm , bernhard braunm &#252; ller , daniel a. keim , hans-peter kriegel
",n
"LEFT id: NA
RIGHT id: 539

LEFT text: Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm transactions on database systems ( tods )

LEFT title: NA
RIGHT title: fast incremental maintenance of approximate histograms

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: phillip b. gibbons , yossi matias , viswanath poosala
",y
"LEFT id: NA
RIGHT id: 518

LEFT text: In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.

RIGHT text: NA

LEFT venue: NA
RIGHT venue: acm sigmod record

LEFT title: NA
RIGHT title: introduction to constraint databases

LEFT year: NA
RIGHT year: 2002

LEFT authors: NA
RIGHT authors: bart kuijpers
",n
